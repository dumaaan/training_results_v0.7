+ echo 'Beginning trial 2 of 5'
Beginning trial 2 of 5
+ srun --ntasks=64 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592880966721, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592880966754, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592880966754, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592880966755, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592880966755, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "64xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=64 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n053
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n055
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n095
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n008
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n016
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n057
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n032
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n026
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n022
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n097
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n004
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n051
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n028
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n005
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n013
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n025
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n009
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n029
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n096
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n020
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n047
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n024
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n030
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n010
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n006
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n012
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n015
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n058
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n014
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n011
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n019
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n021
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n003
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n018
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n034
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n059
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n056
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n033
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n017
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n023
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n052
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n031
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n054
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n035
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n094
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n007
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n060
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n093
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n002
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n027
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n001
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=64 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592880973634, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973678, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973682, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973692, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973694, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973704, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973712, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973717, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973720, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973720, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973721, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973722, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973722, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973724, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973732, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973738, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973753, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973754, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973755, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973764, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973764, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973766, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973766, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973766, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973768, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973771, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973771, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973772, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973776, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973781, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973787, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973794, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973795, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973796, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973797, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973806, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973805, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973807, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973806, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973818, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973822, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973825, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973825, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973825, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973829, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973835, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973840, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973861, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973864, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973866, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973866, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973873, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973880, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973895, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973904, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973905, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973915, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973928, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973937, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973951, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973961, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973976, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880973989, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592881063301, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=1024 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14043861/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 0 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 12 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 10 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 4 ']'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
running benchmark
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 7 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 11 ']'
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 11 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' -n 11 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 3 ']'
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 11 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 3 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 1 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ '[' -n 7 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 15 ']'
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 1 ']'
running benchmark
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
running benchmark
+ REMAIN_STEPS=1605
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
running benchmark
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
running benchmark
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
running benchmark
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
running benchmark
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 11 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 1 ']'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 3 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
running benchmark
+ '[' -n 9 ']'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
running benchmark
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 10 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 9 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
running benchmark
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 10 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 0 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ '[' -n 9 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 6 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ '[' -n 12 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
running benchmark
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ '[' -n 13 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ MATH=fp16
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 11 ']'
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ MATH=fp16
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ '[' -n 3 ']'
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' -n 6 ']'
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 1 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 7 ']'
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 1 ']'
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 6 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' -n 5 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ declare -a CMD
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ DATASET_DIR=/data
running benchmark
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 8 ']'
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
running benchmark
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 14 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 14 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
running benchmark
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 15 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' -n 12 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 8 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ MATH=fp16
+ '[' -n 9 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 6 ']'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 10 ']'
+ echo 'running benchmark'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' -n 2 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 12 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 12 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ LR=5.0e-3
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ declare -a CMD
+ declare -a CMD
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ '[' -n 6 ']'
+ '[' -n 3 ']'
+ '[' -n 3 ']'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ TARGET=24.0
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ NUMEPOCHS=14
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 5 ']'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 15 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 12 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
running benchmark
+ '[' -n 15 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
running benchmark
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 11 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
running benchmark
+ '[' -n 3 ']'
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ '[' -n 11 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 12 ']'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 9 ']'
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 10 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 7 ']'
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' -n 3 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 5 ']'
+ NUMEPOCHS=14
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 1 ']'
+ DATASET_DIR=/data
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ '[' -n 15 ']'
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
running benchmark
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 15 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ '[' -n 12 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ declare -a CMD
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 11 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 13 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ declare -a CMD
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ '[' -n 0 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 15 ']'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
running benchmark
+ '[' -n 13 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 6 ']'
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 4 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 10 ']'
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 9 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 15 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 10 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' -n 0 ']'
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 10 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
running benchmark
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
running benchmark
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 1 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 2 ']'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ NUMEPOCHS=14
running benchmark
+ DATASET_DIR=/data
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ NUMEPOCHS=14
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 10 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
running benchmark
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 12 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
running benchmark
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ '[' -n 3 ']'
+ '[' -n 9 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
running benchmark
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ '[' -n 1 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' -n 4 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ '[' -n 0 ']'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 8 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 10 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ LR=5.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 2 ']'
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 12 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ echo 'running benchmark'
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 11 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 10 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 7 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' -n 15 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 3 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 13 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
running benchmark
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -n 11 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 14 ']'
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 11 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 12 ']'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' -n 13 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 11 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 9 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ LR=5.0e-3
+ declare -a CMD
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' -n 0 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ '[' -n 8 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 11 ']'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 12 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 14 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ declare -a CMD
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 2 ']'
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
running benchmark
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
running benchmark
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ '[' -n 11 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' -n 12 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' -n 15 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 12 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ '[' -n 12 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ '[' -n 7 ']'
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 14 ']'
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 1 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 11 ']'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
running benchmark
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 13 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:57:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ declare -a CMD
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 10 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 13 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 11 ']'
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 1 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 2 ']'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 2 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DIST_OPTS=
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 6 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 12 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 3 ']'
+ declare -a CMD
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ '[' -n 5 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 14 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 6 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 2 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 12 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 6 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 8 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881066704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881066810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
num_sockets = 2 num_nodes=2 cores_per_socket=24
:::MLLOG {"namespace": "", "time_ms": 1592881067324, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067329, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
num_sockets = 2 num_nodes=2 cores_per_socket=24
:::MLLOG {"namespace": "", "time_ms": 1592881067332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881067333, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067352, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881067358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881067358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881067358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067360, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881067362, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881067362, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
num_sockets = 2 num_nodes=2 cores_per_socket=24
:::MLLOG {"namespace": "", "time_ms": 1592881067368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067997, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067998, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881067999, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881068059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068122, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068147, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068183, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068203, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068230, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068236, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068255, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068258, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068262, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068264, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068264, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068277, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068280, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068286, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068285, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068288, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068289, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068291, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068296, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068300, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068336, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068350, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068359, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068361, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068376, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068383, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068390, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068391, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068416, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068428, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068448, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068469, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068534, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068545, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068548, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068558, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068558, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068558, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068558, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068562, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068564, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068564, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068569, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068572, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068571, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068589, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068600, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068603, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068608, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068612, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068613, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068614, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068619, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068619, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068621, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068622, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068624, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068629, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068711, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068746, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068747, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068799, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068832, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068843, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068878, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068878, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068878, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068884, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068884, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068883, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068884, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068883, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068884, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068889, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068889, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068909, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068925, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068925, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068934, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068934, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068937, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068937, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068937, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068939, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068939, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068940, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068940, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068940, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068940, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068944, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068945, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068952, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068951, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068951, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068951, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068952, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068952, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068952, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068954, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068954, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068955, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068953, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068960, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068960, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068962, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068963, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068967, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068967, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068968, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068968, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068971, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068971, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068976, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068976, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068977, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068979, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068978, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068980, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068982, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068982, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068986, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068987, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068990, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068993, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068996, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068996, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881068995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069015, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069019, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069024, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069037, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881069196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=32134259, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=201, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=14, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.005, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=1605, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=4, test_loader_workers=0, train_batch_size=16, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3978165875
:::MLLOG {"namespace": "", "time_ms": 1592881092118, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3978165875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2695252087
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.005}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.005
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592881095650, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592881095651, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.005, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592881095651, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592881095651, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592881095651, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592881098860, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592881100026, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592881100026, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592881100271, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16384, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592881100272, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3948544, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592881100273, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 1605, 'decay_interval': 201, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1605
0: Scheduler decay interval: 201
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592881100273, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592881100274, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592881100274, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 201, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592881100274, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592881100274, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592881100274, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 1605, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592881100274, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592881100275, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881100275, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 1825199416
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/241]	Time 0.415 (0.415)	Data 2.73e-01 (2.73e-01)	Tok/s 783 (783)	Loss/tok 10.5087 (10.5087)	LR 5.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [0][10/241]	Time 0.032 (0.093)	Data 6.39e-05 (2.49e-02)	Tok/s 45799 (27925)	Loss/tok 9.7855 (10.3270)	LR 5.610e-05
0: TRAIN [0][20/241]	Time 0.025 (0.064)	Data 6.94e-05 (1.31e-02)	Tok/s 26010 (30795)	Loss/tok 8.9466 (9.8554)	LR 7.063e-05
0: TRAIN [0][30/241]	Time 0.032 (0.052)	Data 6.37e-05 (8.89e-03)	Tok/s 45634 (31954)	Loss/tok 8.8161 (9.5553)	LR 8.891e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [0][40/241]	Time 0.022 (0.046)	Data 7.96e-05 (6.73e-03)	Tok/s 28229 (33051)	Loss/tok 8.6102 (9.3086)	LR 1.094e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][50/241]	Time 0.032 (0.042)	Data 6.27e-05 (5.43e-03)	Tok/s 45855 (32611)	Loss/tok 8.4753 (9.1827)	LR 1.315e-04
0: TRAIN [0][60/241]	Time 0.025 (0.039)	Data 6.34e-05 (4.55e-03)	Tok/s 27402 (32070)	Loss/tok 8.1849 (9.0583)	LR 1.656e-04
0: TRAIN [0][70/241]	Time 0.028 (0.037)	Data 6.29e-05 (3.92e-03)	Tok/s 38909 (31517)	Loss/tok 8.3723 (8.9409)	LR 2.084e-04
0: TRAIN [0][80/241]	Time 0.025 (0.036)	Data 6.41e-05 (3.44e-03)	Tok/s 27464 (31615)	Loss/tok 7.7539 (8.8230)	LR 2.624e-04
0: TRAIN [0][90/241]	Time 0.028 (0.035)	Data 6.63e-05 (3.07e-03)	Tok/s 35615 (31628)	Loss/tok 7.9913 (8.7162)	LR 3.303e-04
0: TRAIN [0][100/241]	Time 0.028 (0.034)	Data 6.39e-05 (2.77e-03)	Tok/s 38008 (31160)	Loss/tok 8.0052 (8.6337)	LR 4.159e-04
0: TRAIN [0][110/241]	Time 0.025 (0.034)	Data 6.34e-05 (2.53e-03)	Tok/s 24701 (31167)	Loss/tok 7.6084 (8.5578)	LR 5.236e-04
0: TRAIN [0][120/241]	Time 0.028 (0.033)	Data 6.44e-05 (2.32e-03)	Tok/s 38389 (31476)	Loss/tok 7.8949 (8.4875)	LR 6.591e-04
0: TRAIN [0][130/241]	Time 0.025 (0.032)	Data 6.41e-05 (2.15e-03)	Tok/s 25210 (31228)	Loss/tok 7.0591 (8.4227)	LR 8.298e-04
0: TRAIN [0][140/241]	Time 0.025 (0.032)	Data 6.51e-05 (2.00e-03)	Tok/s 27739 (31124)	Loss/tok 7.0965 (8.3507)	LR 1.045e-03
0: TRAIN [0][150/241]	Time 0.036 (0.032)	Data 6.27e-05 (1.88e-03)	Tok/s 53329 (31718)	Loss/tok 7.6231 (8.2621)	LR 1.315e-03
0: TRAIN [0][160/241]	Time 0.025 (0.031)	Data 8.70e-05 (1.76e-03)	Tok/s 23980 (31476)	Loss/tok 6.6677 (8.2005)	LR 1.656e-03
0: TRAIN [0][170/241]	Time 0.025 (0.031)	Data 6.53e-05 (1.66e-03)	Tok/s 24312 (31195)	Loss/tok 6.5680 (8.1351)	LR 2.084e-03
0: Upscaling, new scale: 4.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][180/241]	Time 0.022 (0.031)	Data 6.32e-05 (1.58e-03)	Tok/s 15492 (31493)	Loss/tok 5.5742 (8.0442)	LR 2.564e-03
0: TRAIN [0][190/241]	Time 0.028 (0.031)	Data 1.02e-04 (1.50e-03)	Tok/s 36681 (31671)	Loss/tok 6.5675 (7.9648)	LR 3.228e-03
0: TRAIN [0][200/241]	Time 0.032 (0.031)	Data 6.34e-05 (1.43e-03)	Tok/s 43673 (31770)	Loss/tok 6.5922 (7.8905)	LR 4.064e-03
0: TRAIN [0][210/241]	Time 0.032 (0.030)	Data 6.32e-05 (1.36e-03)	Tok/s 44482 (31875)	Loss/tok 6.4556 (7.8093)	LR 5.000e-03
0: TRAIN [0][220/241]	Time 0.028 (0.030)	Data 6.20e-05 (1.30e-03)	Tok/s 38401 (31873)	Loss/tok 6.0515 (7.7291)	LR 5.000e-03
0: TRAIN [0][230/241]	Time 0.028 (0.030)	Data 6.20e-05 (1.25e-03)	Tok/s 36701 (32150)	Loss/tok 5.9886 (7.6321)	LR 5.000e-03
0: TRAIN [0][240/241]	Time 0.025 (0.030)	Data 4.12e-05 (1.20e-03)	Tok/s 24901 (32253)	Loss/tok 5.2615 (7.5384)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881107566, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881107566, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.339 (0.339)	Decoder iters 149.0 (149.0)	Tok/s 1752 (1752)
0: Running moses detokenizer
0: BLEU(score=0.9666502260267853, counts=[11728, 1232, 216, 53], totals=[70555, 67552, 64553, 61575], precisions=[16.622493090496775, 1.82378019895784, 0.3346087710873236, 0.08607389362565976], bp=1.0, sys_len=70555, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881108077, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.0097, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881108077, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 7.5408	Test BLEU: 0.97
0: Performance: Epoch: 0	Training: 32989514 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592881108078, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881108078, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881108078, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 1537432663
0: TRAIN [1][0/241]	Time 0.362 (0.362)	Data 2.71e-01 (2.71e-01)	Tok/s 3957 (3957)	Loss/tok 5.9745 (5.9745)	LR 5.000e-03
0: TRAIN [1][10/241]	Time 0.025 (0.057)	Data 6.46e-05 (2.47e-02)	Tok/s 26645 (29214)	Loss/tok 5.2041 (5.5949)	LR 5.000e-03
0: TRAIN [1][20/241]	Time 0.022 (0.042)	Data 5.96e-05 (1.30e-02)	Tok/s 16019 (28321)	Loss/tok 4.4430 (5.4509)	LR 5.000e-03
0: TRAIN [1][30/241]	Time 0.032 (0.038)	Data 6.37e-05 (8.82e-03)	Tok/s 47126 (32379)	Loss/tok 5.1382 (5.3606)	LR 5.000e-03
0: TRAIN [1][40/241]	Time 0.036 (0.036)	Data 6.18e-05 (6.69e-03)	Tok/s 52473 (33706)	Loss/tok 5.2378 (5.2640)	LR 5.000e-03
0: TRAIN [1][50/241]	Time 0.025 (0.034)	Data 6.65e-05 (5.39e-03)	Tok/s 24469 (33012)	Loss/tok 4.7494 (5.1680)	LR 5.000e-03
0: TRAIN [1][60/241]	Time 0.028 (0.032)	Data 6.25e-05 (4.52e-03)	Tok/s 37999 (31916)	Loss/tok 4.7851 (5.0789)	LR 5.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [1][70/241]	Time 0.028 (0.032)	Data 6.75e-05 (3.89e-03)	Tok/s 37379 (31776)	Loss/tok 4.5449 (5.0006)	LR 5.000e-03
0: TRAIN [1][80/241]	Time 0.025 (0.031)	Data 6.68e-05 (3.42e-03)	Tok/s 24995 (32753)	Loss/tok 4.3478 (4.9190)	LR 5.000e-03
0: TRAIN [1][90/241]	Time 0.028 (0.031)	Data 6.22e-05 (3.05e-03)	Tok/s 35857 (32729)	Loss/tok 4.3082 (4.8606)	LR 5.000e-03
0: TRAIN [1][100/241]	Time 0.025 (0.030)	Data 6.77e-05 (2.75e-03)	Tok/s 25979 (32428)	Loss/tok 3.8096 (4.7986)	LR 5.000e-03
0: TRAIN [1][110/241]	Time 0.028 (0.030)	Data 6.99e-05 (2.51e-03)	Tok/s 37977 (32650)	Loss/tok 3.9462 (4.7490)	LR 5.000e-03
0: TRAIN [1][120/241]	Time 0.025 (0.030)	Data 7.77e-05 (2.31e-03)	Tok/s 24008 (32951)	Loss/tok 4.0131 (4.7090)	LR 5.000e-03
0: TRAIN [1][130/241]	Time 0.028 (0.030)	Data 6.87e-05 (2.14e-03)	Tok/s 34012 (32991)	Loss/tok 4.5181 (4.6668)	LR 5.000e-03
0: TRAIN [1][140/241]	Time 0.025 (0.030)	Data 6.84e-05 (1.99e-03)	Tok/s 25702 (33084)	Loss/tok 3.5002 (4.6177)	LR 5.000e-03
0: TRAIN [1][150/241]	Time 0.025 (0.030)	Data 6.75e-05 (1.87e-03)	Tok/s 25287 (33295)	Loss/tok 3.9020 (4.5711)	LR 5.000e-03
0: TRAIN [1][160/241]	Time 0.037 (0.029)	Data 6.99e-05 (1.75e-03)	Tok/s 50274 (33295)	Loss/tok 4.2727 (4.5387)	LR 5.000e-03
0: TRAIN [1][170/241]	Time 0.025 (0.029)	Data 6.82e-05 (1.66e-03)	Tok/s 25776 (32879)	Loss/tok 3.6090 (4.5017)	LR 5.000e-03
0: TRAIN [1][180/241]	Time 0.023 (0.029)	Data 7.01e-05 (1.57e-03)	Tok/s 13397 (32471)	Loss/tok 2.9851 (4.4676)	LR 5.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [1][190/241]	Time 0.032 (0.029)	Data 7.27e-05 (1.49e-03)	Tok/s 46410 (32418)	Loss/tok 4.5321 (4.4421)	LR 5.000e-03
0: TRAIN [1][200/241]	Time 0.025 (0.029)	Data 7.25e-05 (1.42e-03)	Tok/s 23934 (32503)	Loss/tok 3.8741 (4.4128)	LR 5.000e-03
0: TRAIN [1][210/241]	Time 0.025 (0.029)	Data 7.80e-05 (1.36e-03)	Tok/s 25569 (32645)	Loss/tok 3.8791 (4.3875)	LR 5.000e-03
0: TRAIN [1][220/241]	Time 0.032 (0.029)	Data 7.01e-05 (1.30e-03)	Tok/s 42976 (32494)	Loss/tok 4.1750 (4.3662)	LR 5.000e-03
0: TRAIN [1][230/241]	Time 0.025 (0.029)	Data 6.94e-05 (1.24e-03)	Tok/s 25081 (32312)	Loss/tok 3.4345 (4.3429)	LR 5.000e-03
0: TRAIN [1][240/241]	Time 0.025 (0.028)	Data 4.32e-05 (1.20e-03)	Tok/s 27606 (32335)	Loss/tok 3.3944 (4.3226)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881114981, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881114982, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.279 (0.279)	Decoder iters 118.0 (118.0)	Tok/s 2369 (2369)
0: Running moses detokenizer
0: BLEU(score=15.783379458618843, counts=[29176, 12605, 6461, 3400], totals=[55126, 52123, 49120, 46127], precisions=[52.9260240177049, 24.18318208852138, 13.153501628664495, 7.370954104971058], bp=0.8409361968353747, sys_len=55126, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881115477, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1578, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881115477, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 4.3138	Test BLEU: 15.78
0: Performance: Epoch: 1	Training: 33074936 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592881115478, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881115478, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881115478, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3009897703
0: TRAIN [2][0/241]	Time 0.359 (0.359)	Data 2.85e-01 (2.85e-01)	Tok/s 2906 (2906)	Loss/tok 3.5812 (3.5812)	LR 5.000e-03
0: TRAIN [2][10/241]	Time 0.025 (0.056)	Data 6.96e-05 (2.60e-02)	Tok/s 26618 (26336)	Loss/tok 3.4925 (3.5219)	LR 5.000e-03
0: TRAIN [2][20/241]	Time 0.022 (0.041)	Data 7.49e-05 (1.36e-02)	Tok/s 14539 (25876)	Loss/tok 2.7907 (3.5286)	LR 5.000e-03
0: TRAIN [2][30/241]	Time 0.031 (0.036)	Data 7.51e-05 (9.26e-03)	Tok/s 46087 (26984)	Loss/tok 4.0612 (3.5293)	LR 5.000e-03
0: TRAIN [2][40/241]	Time 0.028 (0.034)	Data 8.15e-05 (7.02e-03)	Tok/s 39828 (28675)	Loss/tok 3.6234 (3.5504)	LR 5.000e-03
0: TRAIN [2][50/241]	Time 0.028 (0.033)	Data 7.06e-05 (5.66e-03)	Tok/s 39578 (29952)	Loss/tok 3.5624 (3.5610)	LR 5.000e-03
0: TRAIN [2][60/241]	Time 0.028 (0.032)	Data 6.89e-05 (4.74e-03)	Tok/s 37877 (29807)	Loss/tok 3.9312 (3.5622)	LR 5.000e-03
0: TRAIN [2][70/241]	Time 0.025 (0.031)	Data 7.15e-05 (4.08e-03)	Tok/s 27400 (30199)	Loss/tok 3.5098 (3.5767)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [2][80/241]	Time 0.025 (0.031)	Data 7.87e-05 (3.59e-03)	Tok/s 40594 (30870)	Loss/tok 3.3962 (3.6039)	LR 5.000e-03
0: TRAIN [2][90/241]	Time 0.025 (0.030)	Data 7.46e-05 (3.20e-03)	Tok/s 27172 (30878)	Loss/tok 3.3640 (3.6037)	LR 5.000e-03
0: TRAIN [2][100/241]	Time 0.028 (0.030)	Data 7.22e-05 (2.89e-03)	Tok/s 37963 (31155)	Loss/tok 3.4709 (3.6103)	LR 5.000e-03
0: TRAIN [2][110/241]	Time 0.025 (0.030)	Data 8.01e-05 (2.64e-03)	Tok/s 24095 (31398)	Loss/tok 3.4574 (3.6119)	LR 5.000e-03
0: TRAIN [2][120/241]	Time 0.028 (0.029)	Data 7.20e-05 (2.43e-03)	Tok/s 36768 (31546)	Loss/tok 3.5479 (3.6112)	LR 5.000e-03
0: TRAIN [2][130/241]	Time 0.032 (0.029)	Data 7.13e-05 (2.25e-03)	Tok/s 46464 (31358)	Loss/tok 4.0663 (3.6135)	LR 5.000e-03
0: TRAIN [2][140/241]	Time 0.025 (0.029)	Data 7.10e-05 (2.09e-03)	Tok/s 26872 (31324)	Loss/tok 3.1538 (3.6074)	LR 5.000e-03
0: TRAIN [2][150/241]	Time 0.036 (0.029)	Data 7.10e-05 (1.96e-03)	Tok/s 51491 (31617)	Loss/tok 4.2316 (3.6126)	LR 5.000e-03
0: TRAIN [2][160/241]	Time 0.025 (0.029)	Data 7.15e-05 (1.84e-03)	Tok/s 26082 (31851)	Loss/tok 3.1919 (3.6060)	LR 5.000e-03
0: TRAIN [2][170/241]	Time 0.025 (0.029)	Data 6.99e-05 (1.74e-03)	Tok/s 26413 (32023)	Loss/tok 3.5247 (3.6142)	LR 5.000e-03
0: TRAIN [2][180/241]	Time 0.022 (0.029)	Data 6.63e-05 (1.65e-03)	Tok/s 14176 (31934)	Loss/tok 2.2923 (3.6088)	LR 5.000e-03
0: TRAIN [2][190/241]	Time 0.032 (0.029)	Data 6.56e-05 (1.56e-03)	Tok/s 47826 (31927)	Loss/tok 4.2830 (3.6189)	LR 5.000e-03
0: TRAIN [2][200/241]	Time 0.025 (0.028)	Data 7.65e-05 (1.49e-03)	Tok/s 25628 (32047)	Loss/tok 3.6367 (3.6174)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [2][210/241]	Time 0.025 (0.028)	Data 6.63e-05 (1.42e-03)	Tok/s 26302 (32132)	Loss/tok 3.3803 (3.6163)	LR 5.000e-03
0: TRAIN [2][220/241]	Time 0.032 (0.028)	Data 6.65e-05 (1.36e-03)	Tok/s 46402 (32263)	Loss/tok 3.6591 (3.6175)	LR 5.000e-03
0: TRAIN [2][230/241]	Time 0.025 (0.028)	Data 6.60e-05 (1.30e-03)	Tok/s 25773 (32399)	Loss/tok 2.9619 (3.6078)	LR 5.000e-03
0: TRAIN [2][240/241]	Time 0.028 (0.028)	Data 4.36e-05 (1.25e-03)	Tok/s 37550 (32395)	Loss/tok 3.3982 (3.6000)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881122343, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881122343, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.334 (0.334)	Decoder iters 149.0 (149.0)	Tok/s 2088 (2088)
0: Running moses detokenizer
0: BLEU(score=20.35474619885472, counts=[34605, 15963, 8541, 4739], totals=[64622, 61619, 58616, 55619], precisions=[53.54987465569001, 25.905970561028255, 14.57110686501979, 8.520469623689745], bp=0.9991647203072117, sys_len=64622, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881122792, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.20350000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881122792, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.5815	Test BLEU: 20.35
0: Performance: Epoch: 2	Training: 33289808 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592881122793, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881122793, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881122793, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 104453673
0: TRAIN [3][0/241]	Time 0.370 (0.370)	Data 2.92e-01 (2.92e-01)	Tok/s 1818 (1818)	Loss/tok 2.7146 (2.7146)	LR 5.000e-03
0: TRAIN [3][10/241]	Time 0.028 (0.057)	Data 6.70e-05 (2.66e-02)	Tok/s 36673 (27654)	Loss/tok 3.3336 (3.2902)	LR 5.000e-03
0: TRAIN [3][20/241]	Time 0.025 (0.044)	Data 6.63e-05 (1.40e-02)	Tok/s 25481 (31842)	Loss/tok 2.9651 (3.4510)	LR 5.000e-03
0: TRAIN [3][30/241]	Time 0.028 (0.038)	Data 6.60e-05 (9.50e-03)	Tok/s 37950 (31939)	Loss/tok 3.3049 (3.4509)	LR 5.000e-03
0: TRAIN [3][40/241]	Time 0.028 (0.036)	Data 6.77e-05 (7.20e-03)	Tok/s 35636 (31980)	Loss/tok 3.5636 (3.4289)	LR 5.000e-03
0: TRAIN [3][50/241]	Time 0.025 (0.034)	Data 6.65e-05 (5.80e-03)	Tok/s 26131 (32204)	Loss/tok 3.6321 (3.4042)	LR 5.000e-03
0: TRAIN [3][60/241]	Time 0.032 (0.033)	Data 7.03e-05 (4.86e-03)	Tok/s 45047 (32734)	Loss/tok 3.6994 (3.4413)	LR 5.000e-03
0: TRAIN [3][70/241]	Time 0.025 (0.032)	Data 6.99e-05 (4.18e-03)	Tok/s 27043 (32508)	Loss/tok 3.3831 (3.4310)	LR 5.000e-03
0: TRAIN [3][80/241]	Time 0.025 (0.031)	Data 6.84e-05 (3.68e-03)	Tok/s 27983 (32238)	Loss/tok 2.9047 (3.4117)	LR 5.000e-03
0: TRAIN [3][90/241]	Time 0.028 (0.031)	Data 6.91e-05 (3.28e-03)	Tok/s 37495 (32143)	Loss/tok 3.2636 (3.4145)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [3][100/241]	Time 0.025 (0.030)	Data 6.89e-05 (2.96e-03)	Tok/s 24525 (31919)	Loss/tok 3.5292 (3.4155)	LR 5.000e-03
0: TRAIN [3][110/241]	Time 0.025 (0.030)	Data 6.70e-05 (2.70e-03)	Tok/s 28497 (31766)	Loss/tok 3.0165 (3.4011)	LR 5.000e-03
0: TRAIN [3][120/241]	Time 0.025 (0.030)	Data 6.79e-05 (2.48e-03)	Tok/s 26810 (31969)	Loss/tok 3.0756 (3.4069)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [3][130/241]	Time 0.032 (0.030)	Data 7.30e-05 (2.30e-03)	Tok/s 58302 (32230)	Loss/tok 3.7600 (3.4187)	LR 5.000e-03
0: TRAIN [3][140/241]	Time 0.028 (0.029)	Data 6.89e-05 (2.14e-03)	Tok/s 37703 (32339)	Loss/tok 3.9178 (3.4311)	LR 5.000e-03
0: TRAIN [3][150/241]	Time 0.028 (0.029)	Data 6.41e-05 (2.00e-03)	Tok/s 38391 (32173)	Loss/tok 2.9151 (3.4198)	LR 5.000e-03
0: TRAIN [3][160/241]	Time 0.036 (0.029)	Data 6.82e-05 (1.88e-03)	Tok/s 52063 (32325)	Loss/tok 4.0434 (3.4272)	LR 5.000e-03
0: TRAIN [3][170/241]	Time 0.028 (0.029)	Data 7.03e-05 (1.78e-03)	Tok/s 37860 (32352)	Loss/tok 3.3283 (3.4236)	LR 5.000e-03
0: TRAIN [3][180/241]	Time 0.025 (0.029)	Data 8.77e-05 (1.68e-03)	Tok/s 25737 (32298)	Loss/tok 3.3971 (3.4284)	LR 5.000e-03
0: TRAIN [3][190/241]	Time 0.025 (0.029)	Data 6.99e-05 (1.60e-03)	Tok/s 24869 (32252)	Loss/tok 3.8220 (3.4289)	LR 5.000e-03
0: TRAIN [3][200/241]	Time 0.022 (0.029)	Data 6.70e-05 (1.52e-03)	Tok/s 14185 (32101)	Loss/tok 2.2368 (3.4343)	LR 5.000e-03
0: TRAIN [3][210/241]	Time 0.025 (0.029)	Data 6.82e-05 (1.45e-03)	Tok/s 25606 (32162)	Loss/tok 3.0477 (3.4342)	LR 5.000e-03
0: TRAIN [3][220/241]	Time 0.025 (0.028)	Data 6.60e-05 (1.39e-03)	Tok/s 25952 (32194)	Loss/tok 3.0882 (3.4284)	LR 5.000e-03
0: TRAIN [3][230/241]	Time 0.032 (0.028)	Data 6.60e-05 (1.33e-03)	Tok/s 45419 (32355)	Loss/tok 3.5936 (3.4367)	LR 5.000e-03
0: TRAIN [3][240/241]	Time 0.032 (0.028)	Data 4.60e-05 (1.28e-03)	Tok/s 45636 (32444)	Loss/tok 3.6573 (3.4325)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881129675, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881129676, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.376 (0.376)	Decoder iters 149.0 (149.0)	Tok/s 1986 (1986)
0: Running moses detokenizer
0: BLEU(score=20.689986462224905, counts=[36115, 17005, 9303, 5273], totals=[68268, 65265, 62262, 59263], precisions=[52.901798792992324, 26.055312954876275, 14.941697985930423, 8.897625837369016], bp=1.0, sys_len=68268, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881130160, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2069, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881130161, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.3982	Test BLEU: 20.69
0: Performance: Epoch: 3	Training: 33323822 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592881130161, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881130161, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881130161, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 3470235269
0: TRAIN [4][0/241]	Time 0.355 (0.355)	Data 2.87e-01 (2.87e-01)	Tok/s 1745 (1745)	Loss/tok 2.6642 (2.6642)	LR 5.000e-03
0: TRAIN [4][10/241]	Time 0.028 (0.056)	Data 6.70e-05 (2.61e-02)	Tok/s 36913 (27848)	Loss/tok 3.4525 (3.1663)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [4][20/241]	Time 0.028 (0.043)	Data 6.60e-05 (1.37e-02)	Tok/s 40181 (32110)	Loss/tok 3.4661 (3.2318)	LR 5.000e-03
0: TRAIN [4][30/241]	Time 0.032 (0.037)	Data 6.82e-05 (9.32e-03)	Tok/s 45092 (31274)	Loss/tok 3.4980 (3.1879)	LR 5.000e-03
0: TRAIN [4][40/241]	Time 0.025 (0.035)	Data 6.65e-05 (7.07e-03)	Tok/s 25152 (31416)	Loss/tok 3.2894 (3.1711)	LR 5.000e-03
0: TRAIN [4][50/241]	Time 0.022 (0.033)	Data 6.82e-05 (5.69e-03)	Tok/s 14715 (31041)	Loss/tok 2.2951 (3.1606)	LR 5.000e-03
0: TRAIN [4][60/241]	Time 0.028 (0.032)	Data 6.63e-05 (4.77e-03)	Tok/s 39583 (31611)	Loss/tok 3.1093 (3.2144)	LR 5.000e-03
0: TRAIN [4][70/241]	Time 0.032 (0.032)	Data 6.60e-05 (4.11e-03)	Tok/s 46454 (32210)	Loss/tok 3.2622 (3.2446)	LR 5.000e-03
0: TRAIN [4][80/241]	Time 0.025 (0.031)	Data 6.39e-05 (3.61e-03)	Tok/s 25078 (32652)	Loss/tok 2.8020 (3.2490)	LR 5.000e-03
0: TRAIN [4][90/241]	Time 0.028 (0.031)	Data 6.77e-05 (3.22e-03)	Tok/s 38061 (32305)	Loss/tok 3.2521 (3.2573)	LR 5.000e-03
0: TRAIN [4][100/241]	Time 0.028 (0.030)	Data 6.56e-05 (2.91e-03)	Tok/s 36534 (32830)	Loss/tok 3.3833 (3.2812)	LR 5.000e-03
0: TRAIN [4][110/241]	Time 0.032 (0.030)	Data 6.70e-05 (2.65e-03)	Tok/s 46237 (32758)	Loss/tok 3.5275 (3.2731)	LR 5.000e-03
0: TRAIN [4][120/241]	Time 0.025 (0.030)	Data 6.60e-05 (2.44e-03)	Tok/s 27176 (32673)	Loss/tok 2.7365 (3.2699)	LR 5.000e-03
0: TRAIN [4][130/241]	Time 0.028 (0.029)	Data 6.53e-05 (2.26e-03)	Tok/s 37499 (32574)	Loss/tok 3.4353 (3.2728)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [4][140/241]	Time 0.032 (0.029)	Data 6.41e-05 (2.10e-03)	Tok/s 42932 (32941)	Loss/tok 3.7084 (3.2980)	LR 5.000e-03
0: TRAIN [4][150/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.97e-03)	Tok/s 25283 (32706)	Loss/tok 2.8402 (3.2863)	LR 5.000e-03
0: TRAIN [4][160/241]	Time 0.025 (0.029)	Data 6.82e-05 (1.85e-03)	Tok/s 25973 (32609)	Loss/tok 2.9648 (3.2798)	LR 5.000e-03
0: TRAIN [4][170/241]	Time 0.032 (0.029)	Data 8.85e-05 (1.75e-03)	Tok/s 45554 (32565)	Loss/tok 3.4161 (3.2769)	LR 5.000e-03
0: TRAIN [4][180/241]	Time 0.025 (0.029)	Data 6.51e-05 (1.66e-03)	Tok/s 27650 (32760)	Loss/tok 2.7962 (3.2849)	LR 5.000e-03
0: TRAIN [4][190/241]	Time 0.025 (0.029)	Data 7.92e-05 (1.57e-03)	Tok/s 25438 (32814)	Loss/tok 3.3285 (3.2958)	LR 5.000e-03
0: TRAIN [4][200/241]	Time 0.025 (0.029)	Data 6.87e-05 (1.50e-03)	Tok/s 25011 (32984)	Loss/tok 2.7935 (3.3020)	LR 5.000e-03
0: TRAIN [4][210/241]	Time 0.028 (0.029)	Data 6.72e-05 (1.43e-03)	Tok/s 35706 (32828)	Loss/tok 3.1868 (3.2981)	LR 5.000e-03
0: TRAIN [4][220/241]	Time 0.028 (0.029)	Data 6.44e-05 (1.37e-03)	Tok/s 39560 (32883)	Loss/tok 3.4090 (3.2973)	LR 5.000e-03
0: TRAIN [4][230/241]	Time 0.025 (0.028)	Data 6.44e-05 (1.31e-03)	Tok/s 26974 (32778)	Loss/tok 2.7610 (3.2966)	LR 5.000e-03
0: TRAIN [4][240/241]	Time 0.028 (0.028)	Data 4.55e-05 (1.26e-03)	Tok/s 36329 (32589)	Loss/tok 3.5861 (3.2929)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881137027, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881137027, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.221 (0.221)	Decoder iters 95.0 (95.0)	Tok/s 3198 (3198)
0: Running moses detokenizer
0: BLEU(score=22.57293202759924, counts=[35954, 17382, 9667, 5612], totals=[64711, 61708, 58705, 55707], precisions=[55.5608783668928, 28.168146755688078, 16.46708116855464, 10.07413790008437], bp=1.0, sys_len=64711, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881137468, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2257, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881137468, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.2950	Test BLEU: 22.57
0: Performance: Epoch: 4	Training: 33332750 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592881137469, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881137469, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881137469, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 3544197128
0: TRAIN [5][0/241]	Time 0.370 (0.370)	Data 3.15e-01 (3.15e-01)	Tok/s 1890 (1890)	Loss/tok 2.9594 (2.9594)	LR 5.000e-03
0: TRAIN [5][10/241]	Time 0.022 (0.057)	Data 6.89e-05 (2.87e-02)	Tok/s 14475 (26736)	Loss/tok 2.4359 (3.0390)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [5][20/241]	Time 0.022 (0.043)	Data 6.75e-05 (1.51e-02)	Tok/s 14621 (29112)	Loss/tok 2.5207 (3.1484)	LR 5.000e-03
0: TRAIN [5][30/241]	Time 0.025 (0.038)	Data 6.51e-05 (1.02e-02)	Tok/s 27806 (31820)	Loss/tok 2.8568 (3.2150)	LR 5.000e-03
0: TRAIN [5][40/241]	Time 0.028 (0.035)	Data 6.82e-05 (7.76e-03)	Tok/s 37389 (32171)	Loss/tok 3.9030 (3.2114)	LR 5.000e-03
0: TRAIN [5][50/241]	Time 0.036 (0.034)	Data 6.77e-05 (6.25e-03)	Tok/s 51817 (32659)	Loss/tok 3.9238 (3.2525)	LR 5.000e-03
0: TRAIN [5][60/241]	Time 0.028 (0.033)	Data 6.84e-05 (5.23e-03)	Tok/s 38214 (32488)	Loss/tok 3.5382 (3.2510)	LR 5.000e-03
0: TRAIN [5][70/241]	Time 0.022 (0.032)	Data 6.51e-05 (4.51e-03)	Tok/s 16980 (32537)	Loss/tok 2.8832 (3.2487)	LR 5.000e-03
0: TRAIN [5][80/241]	Time 0.022 (0.031)	Data 6.56e-05 (3.96e-03)	Tok/s 16366 (32523)	Loss/tok 2.5121 (3.2485)	LR 5.000e-03
0: TRAIN [5][90/241]	Time 0.028 (0.031)	Data 6.63e-05 (3.53e-03)	Tok/s 37316 (33335)	Loss/tok 3.0687 (3.2640)	LR 5.000e-03
0: TRAIN [5][100/241]	Time 0.028 (0.031)	Data 6.89e-05 (3.19e-03)	Tok/s 36806 (33066)	Loss/tok 2.9204 (3.2505)	LR 5.000e-03
0: TRAIN [5][110/241]	Time 0.028 (0.030)	Data 6.87e-05 (2.91e-03)	Tok/s 37991 (33424)	Loss/tok 3.3502 (3.2578)	LR 5.000e-03
0: TRAIN [5][120/241]	Time 0.032 (0.030)	Data 6.99e-05 (2.67e-03)	Tok/s 43889 (33295)	Loss/tok 3.2982 (3.2559)	LR 5.000e-03
0: TRAIN [5][130/241]	Time 0.036 (0.030)	Data 6.46e-05 (2.47e-03)	Tok/s 53300 (33299)	Loss/tok 3.4006 (3.2592)	LR 5.000e-03
0: TRAIN [5][140/241]	Time 0.025 (0.030)	Data 6.70e-05 (2.30e-03)	Tok/s 27742 (33260)	Loss/tok 3.3014 (3.2492)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [5][150/241]	Time 0.025 (0.029)	Data 6.82e-05 (2.16e-03)	Tok/s 26291 (33067)	Loss/tok 3.0146 (3.2473)	LR 5.000e-03
0: TRAIN [5][160/241]	Time 0.025 (0.029)	Data 6.51e-05 (2.03e-03)	Tok/s 24841 (32930)	Loss/tok 3.4210 (3.2459)	LR 5.000e-03
0: TRAIN [5][170/241]	Time 0.022 (0.029)	Data 6.32e-05 (1.91e-03)	Tok/s 16152 (32553)	Loss/tok 3.0540 (3.2367)	LR 5.000e-03
0: TRAIN [5][180/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.81e-03)	Tok/s 26127 (32574)	Loss/tok 3.1687 (3.2374)	LR 5.000e-03
0: TRAIN [5][190/241]	Time 0.025 (0.029)	Data 6.87e-05 (1.72e-03)	Tok/s 27584 (32390)	Loss/tok 2.5308 (3.2299)	LR 5.000e-03
0: TRAIN [5][200/241]	Time 0.032 (0.029)	Data 6.53e-05 (1.64e-03)	Tok/s 45863 (32414)	Loss/tok 3.3651 (3.2234)	LR 5.000e-03
0: TRAIN [5][210/241]	Time 0.036 (0.029)	Data 6.60e-05 (1.56e-03)	Tok/s 50635 (32562)	Loss/tok 3.3543 (3.2258)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][220/241]	Time 0.022 (0.028)	Data 9.85e-05 (1.50e-03)	Tok/s 29947 (32499)	Loss/tok 2.8194 (3.2197)	LR 5.000e-03
0: TRAIN [5][230/241]	Time 0.025 (0.028)	Data 7.70e-05 (1.43e-03)	Tok/s 27611 (32548)	Loss/tok 2.9575 (3.2339)	LR 5.000e-03
0: TRAIN [5][240/241]	Time 0.022 (0.028)	Data 4.70e-05 (1.38e-03)	Tok/s 13624 (32556)	Loss/tok 2.0403 (3.2348)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881144359, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881144359, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.334 (0.334)	Decoder iters 149.0 (149.0)	Tok/s 2518 (2518)
0: Running moses detokenizer
0: BLEU(score=22.0848378426119, counts=[35812, 17276, 9583, 5512], totals=[65476, 62473, 59470, 56472], precisions=[54.69485002138188, 27.65354633201543, 16.114007062384395, 9.760589318600369], bp=1.0, sys_len=65476, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881144803, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2208, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881144803, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.2407	Test BLEU: 22.08
0: Performance: Epoch: 5	Training: 33278020 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1592881144803, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881144803, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 7, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881144804, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 7}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 475197172
0: TRAIN [6][0/241]	Time 0.362 (0.362)	Data 3.30e-01 (3.30e-01)	Tok/s 2720 (2720)	Loss/tok 3.2717 (3.2717)	LR 5.000e-03
0: TRAIN [6][10/241]	Time 0.025 (0.058)	Data 6.82e-05 (3.01e-02)	Tok/s 24228 (29734)	Loss/tok 3.1240 (3.1543)	LR 5.000e-03
0: TRAIN [6][20/241]	Time 0.032 (0.043)	Data 6.68e-05 (1.58e-02)	Tok/s 45368 (31685)	Loss/tok 3.3799 (3.2180)	LR 5.000e-03
0: TRAIN [6][30/241]	Time 0.032 (0.038)	Data 6.79e-05 (1.07e-02)	Tok/s 44087 (32259)	Loss/tok 3.4183 (3.2233)	LR 5.000e-03
0: TRAIN [6][40/241]	Time 0.025 (0.035)	Data 7.06e-05 (8.12e-03)	Tok/s 26683 (31261)	Loss/tok 2.8903 (3.2298)	LR 5.000e-03
0: TRAIN [6][50/241]	Time 0.025 (0.033)	Data 7.06e-05 (6.54e-03)	Tok/s 27282 (31268)	Loss/tok 3.2837 (3.2255)	LR 5.000e-03
0: TRAIN [6][60/241]	Time 0.028 (0.032)	Data 6.46e-05 (5.48e-03)	Tok/s 38486 (31688)	Loss/tok 3.1225 (3.1971)	LR 5.000e-03
0: TRAIN [6][70/241]	Time 0.025 (0.031)	Data 6.34e-05 (4.72e-03)	Tok/s 25837 (31539)	Loss/tok 3.2404 (3.1919)	LR 5.000e-03
0: TRAIN [6][80/241]	Time 0.032 (0.031)	Data 6.68e-05 (4.15e-03)	Tok/s 47093 (31945)	Loss/tok 3.4830 (3.1853)	LR 5.000e-03
0: TRAIN [6][90/241]	Time 0.028 (0.031)	Data 7.03e-05 (3.70e-03)	Tok/s 37369 (32807)	Loss/tok 3.2443 (3.1840)	LR 5.000e-03
0: TRAIN [6][100/241]	Time 0.022 (0.030)	Data 6.63e-05 (3.34e-03)	Tok/s 15606 (32625)	Loss/tok 2.3692 (3.1863)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [6][110/241]	Time 0.025 (0.030)	Data 7.01e-05 (3.05e-03)	Tok/s 25990 (32615)	Loss/tok 2.9079 (3.1720)	LR 5.000e-03
0: TRAIN [6][120/241]	Time 0.028 (0.030)	Data 6.82e-05 (2.80e-03)	Tok/s 36593 (33022)	Loss/tok 3.1424 (3.1829)	LR 5.000e-03
0: TRAIN [6][130/241]	Time 0.025 (0.030)	Data 6.72e-05 (2.59e-03)	Tok/s 26884 (32736)	Loss/tok 3.0538 (3.1801)	LR 5.000e-03
0: TRAIN [6][140/241]	Time 0.025 (0.029)	Data 6.68e-05 (2.41e-03)	Tok/s 25348 (32720)	Loss/tok 3.4567 (3.1821)	LR 5.000e-03
0: TRAIN [6][150/241]	Time 0.022 (0.029)	Data 6.65e-05 (2.26e-03)	Tok/s 16358 (33146)	Loss/tok 2.6078 (3.1997)	LR 5.000e-03
0: TRAIN [6][160/241]	Time 0.032 (0.029)	Data 6.60e-05 (2.12e-03)	Tok/s 43826 (32814)	Loss/tok 2.9627 (3.1929)	LR 5.000e-03
0: TRAIN [6][170/241]	Time 0.025 (0.029)	Data 6.56e-05 (2.00e-03)	Tok/s 25833 (32751)	Loss/tok 2.5331 (3.1889)	LR 5.000e-03
0: TRAIN [6][180/241]	Time 0.022 (0.029)	Data 6.48e-05 (1.89e-03)	Tok/s 14374 (32311)	Loss/tok 2.7278 (3.1826)	LR 2.500e-03
0: TRAIN [6][190/241]	Time 0.025 (0.029)	Data 7.20e-05 (1.80e-03)	Tok/s 25074 (32455)	Loss/tok 2.7916 (3.1739)	LR 2.500e-03
0: TRAIN [6][200/241]	Time 0.036 (0.029)	Data 6.56e-05 (1.71e-03)	Tok/s 51105 (32464)	Loss/tok 3.3257 (3.1751)	LR 2.500e-03
0: TRAIN [6][210/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.64e-03)	Tok/s 27973 (32552)	Loss/tok 3.0107 (3.1748)	LR 2.500e-03
0: TRAIN [6][220/241]	Time 0.025 (0.028)	Data 6.68e-05 (1.56e-03)	Tok/s 24996 (32389)	Loss/tok 3.0426 (3.1694)	LR 2.500e-03
0: TRAIN [6][230/241]	Time 0.025 (0.028)	Data 6.89e-05 (1.50e-03)	Tok/s 28171 (32388)	Loss/tok 3.0522 (3.1655)	LR 2.500e-03
0: Upscaling, new scale: 64.0
0: TRAIN [6][240/241]	Time 0.028 (0.028)	Data 4.43e-05 (1.44e-03)	Tok/s 38495 (32522)	Loss/tok 3.0788 (3.1644)	LR 2.500e-03
:::MLLOG {"namespace": "", "time_ms": 1592881151681, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881151682, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 7}}
0: Running evaluation on test set
0: TEST [6][0/1]	Time 0.333 (0.333)	Decoder iters 145.0 (145.0)	Tok/s 2244 (2244)
0: Running moses detokenizer
0: BLEU(score=23.636736241247462, counts=[36658, 18179, 10311, 6120], totals=[65180, 62177, 59175, 56177], precisions=[56.24117827554465, 29.23749939688309, 17.424588086185043, 10.894138170425618], bp=1.0, sys_len=65180, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881152120, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2364, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881152121, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 7}}
0: Summary: Epoch: 6	Training Loss: 3.1836	Test BLEU: 23.64
0: Performance: Epoch: 6	Training: 33234742 Tok/s
0: Finished epoch 6
:::MLLOG {"namespace": "", "time_ms": 1592881152121, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881152121, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 8, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881152121, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 8}}
0: Starting epoch 7
0: Sampler for epoch 7 uses seed 39631203
0: TRAIN [7][0/241]	Time 0.364 (0.364)	Data 2.89e-01 (2.89e-01)	Tok/s 1793 (1793)	Loss/tok 3.3999 (3.3999)	LR 2.500e-03
0: TRAIN [7][10/241]	Time 0.025 (0.057)	Data 7.10e-05 (2.63e-02)	Tok/s 26555 (28102)	Loss/tok 2.9515 (3.0016)	LR 2.500e-03
0: TRAIN [7][20/241]	Time 0.022 (0.043)	Data 7.49e-05 (1.38e-02)	Tok/s 16445 (32109)	Loss/tok 2.5086 (3.0234)	LR 2.500e-03
0: TRAIN [7][30/241]	Time 0.025 (0.038)	Data 7.34e-05 (9.39e-03)	Tok/s 26142 (31069)	Loss/tok 2.8291 (3.0345)	LR 2.500e-03
0: TRAIN [7][40/241]	Time 0.025 (0.035)	Data 7.44e-05 (7.11e-03)	Tok/s 27522 (29796)	Loss/tok 3.0176 (2.9916)	LR 2.500e-03
0: TRAIN [7][50/241]	Time 0.025 (0.033)	Data 7.15e-05 (5.73e-03)	Tok/s 26718 (29584)	Loss/tok 2.9678 (3.0049)	LR 2.500e-03
0: TRAIN [7][60/241]	Time 0.028 (0.032)	Data 7.18e-05 (4.81e-03)	Tok/s 38324 (29791)	Loss/tok 3.1509 (2.9979)	LR 2.500e-03
0: TRAIN [7][70/241]	Time 0.032 (0.031)	Data 7.56e-05 (4.14e-03)	Tok/s 45506 (30777)	Loss/tok 3.5636 (3.0287)	LR 2.500e-03
0: TRAIN [7][80/241]	Time 0.022 (0.031)	Data 7.18e-05 (3.64e-03)	Tok/s 15101 (31498)	Loss/tok 2.2837 (3.0692)	LR 2.500e-03
0: TRAIN [7][90/241]	Time 0.022 (0.030)	Data 8.01e-05 (3.25e-03)	Tok/s 15094 (31206)	Loss/tok 2.8887 (3.0629)	LR 2.500e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [7][100/241]	Time 0.032 (0.030)	Data 7.22e-05 (2.93e-03)	Tok/s 44731 (31556)	Loss/tok 3.2514 (3.0809)	LR 2.500e-03
0: TRAIN [7][110/241]	Time 0.025 (0.030)	Data 7.32e-05 (2.67e-03)	Tok/s 25625 (31737)	Loss/tok 2.7593 (3.0676)	LR 2.500e-03
0: TRAIN [7][120/241]	Time 0.028 (0.030)	Data 1.15e-04 (2.46e-03)	Tok/s 40082 (31860)	Loss/tok 3.0825 (3.0693)	LR 2.500e-03
0: TRAIN [7][130/241]	Time 0.028 (0.029)	Data 6.56e-05 (2.28e-03)	Tok/s 39951 (32012)	Loss/tok 2.9231 (3.0761)	LR 2.500e-03
0: TRAIN [7][140/241]	Time 0.025 (0.029)	Data 6.41e-05 (2.12e-03)	Tok/s 27324 (32008)	Loss/tok 2.8151 (3.0748)	LR 1.250e-03
0: TRAIN [7][150/241]	Time 0.022 (0.029)	Data 6.41e-05 (1.98e-03)	Tok/s 14149 (31770)	Loss/tok 2.2803 (3.0618)	LR 1.250e-03
0: TRAIN [7][160/241]	Time 0.028 (0.029)	Data 6.29e-05 (1.86e-03)	Tok/s 39814 (31938)	Loss/tok 3.0718 (3.0609)	LR 1.250e-03
0: TRAIN [7][170/241]	Time 0.025 (0.029)	Data 6.60e-05 (1.76e-03)	Tok/s 26754 (31852)	Loss/tok 3.1050 (3.0540)	LR 1.250e-03
0: TRAIN [7][180/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.67e-03)	Tok/s 36776 (31963)	Loss/tok 3.0217 (3.0540)	LR 1.250e-03
0: TRAIN [7][190/241]	Time 0.032 (0.029)	Data 6.32e-05 (1.58e-03)	Tok/s 46185 (32168)	Loss/tok 3.2249 (3.0579)	LR 1.250e-03
0: TRAIN [7][200/241]	Time 0.028 (0.029)	Data 6.63e-05 (1.51e-03)	Tok/s 36896 (32247)	Loss/tok 2.7962 (3.0562)	LR 1.250e-03
0: TRAIN [7][210/241]	Time 0.025 (0.028)	Data 6.63e-05 (1.44e-03)	Tok/s 25889 (32270)	Loss/tok 2.8597 (3.0595)	LR 1.250e-03
0: Upscaling, new scale: 64.0
0: TRAIN [7][220/241]	Time 0.036 (0.029)	Data 6.51e-05 (1.38e-03)	Tok/s 49107 (32509)	Loss/tok 3.1275 (3.0704)	LR 1.250e-03
0: TRAIN [7][230/241]	Time 0.025 (0.028)	Data 6.39e-05 (1.32e-03)	Tok/s 25753 (32446)	Loss/tok 2.7191 (3.0647)	LR 1.250e-03
0: TRAIN [7][240/241]	Time 0.032 (0.028)	Data 4.43e-05 (1.27e-03)	Tok/s 44934 (32469)	Loss/tok 3.5412 (3.0692)	LR 1.250e-03
:::MLLOG {"namespace": "", "time_ms": 1592881159005, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881159006, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 8}}
0: Running evaluation on test set
0: TEST [7][0/1]	Time 0.303 (0.303)	Decoder iters 118.0 (118.0)	Tok/s 2404 (2404)
0: Running moses detokenizer
0: BLEU(score=24.04623597387037, counts=[36978, 18393, 10472, 6242], totals=[64982, 61979, 58977, 55981], precisions=[56.90498907389739, 29.676180641830296, 17.75607440188548, 11.150211678962505], bp=1.0, sys_len=64982, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881159414, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24050000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881159414, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 8}}
0: Summary: Epoch: 7	Training Loss: 3.0860	Test BLEU: 24.05
0: Performance: Epoch: 7	Training: 33280026 Tok/s
0: Finished epoch 7
:::MLLOG {"namespace": "", "time_ms": 1592881159415, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 8}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592881159415, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,101,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:26 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:57:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/48897/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
slurmstepd: error: _is_a_lwp: open() /proc/92418/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
slurmstepd: error: _is_a_lwp: open() /proc/27854/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/68951/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/68954/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
ENDING TIMING RUN AT 2020-06-22 07:59:28 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,102,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/17633/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/41064/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/95021/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/22226/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:57:45 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
ENDING TIMING RUN AT 2020-06-22 07:59:29 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:57:46 PM
