+ echo 'Beginning trial 5 of 5'
Beginning trial 5 of 5
+ srun --ntasks=60 --container-name=translation python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.TRANSFORMER)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592673613675, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "transformer", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 84}}
:::MLLOG {"namespace": "", "time_ms": 1592673613713, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592673613713, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 93}}
:::MLLOG {"namespace": "", "time_ms": 1592673613713, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 97}}
:::MLLOG {"namespace": "", "time_ms": 1592673613713, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "60xNVIDIA DGX A100", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 101}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=60 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0027
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0137
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0015
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0131
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0129
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0133
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0017
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0077
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0056
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0135
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0117
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0028
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0021
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0057
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0016
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0132
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0018
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0128
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0122
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0045
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0134
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0126
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0118
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0019
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0034
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0125
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0029
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0010
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0119
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0032
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0059
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0127
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0022
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0121
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0002
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0035
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0140
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0047
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0030
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0033
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0130
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0053
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0120
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0136
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0060
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0116
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0023
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0012
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0001
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=60 --container-name=translation python -c '
from mlperf_logging.mllog import constants
from mlperf_log_utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592673619699, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619731, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619756, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619761, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619761, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619766, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619766, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619772, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619772, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619779, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619779, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619781, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619782, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619785, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619793, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619795, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619799, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619805, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619806, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619804, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619808, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619809, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619809, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619810, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619810, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619810, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619812, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619817, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619820, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619820, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619829, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619829, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619834, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619835, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619838, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619841, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619843, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619846, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619846, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619847, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619848, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619851, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619852, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619853, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619857, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619859, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619858, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619861, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619863, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619865, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619869, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619875, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619876, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619884, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619884, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619893, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619896, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619903, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619943, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673619951, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ export SEED=21280
+ SEED=21280
+ srun --mpi=none --ntasks=480 --ntasks-per-node=8 --container-name=translation --container-mounts=/lustre/fsr/datasets/xformer_v0p6/utf8:/data,/lustre/fsw/mlperf-ci/13964286/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date +%s
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 5 ']'
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 4 ']'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 5 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 2 ']'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+++ date +%s
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
++ START=1592673622
++ START=1592673622
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
+ SEED=21280
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
+ MODE=TRAIN
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 2 ']'
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
++ '[' -n 3 ']'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' -n 1 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ NUMEPOCHS=30
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+++ date +%s
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ export DGXSYSTEM
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 1 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 6 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ '[' -n 7 ']'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
+++ date +%s
++ export DGXSYSTEM
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
+ MODE=TRAIN
+ DATASET_DIR=/data
++ export SLURM_NTASKS_PER_NODE
+ NUMEPOCHS=30
++ export SLURM_NNODES
+ MODE=TRAIN
++ declare -a CMD
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date +%s
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
+ SEED=21280
++ '[' 480 -gt 60 ']'
+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 4 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ SEED=21280
+ MODE=TRAIN
+ MAX_TOKENS=1536
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
+ case "$MODE" in
+++ date +%s
+ MODE=TRAIN
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ NUMEPOCHS=30
+ SEED=21280
+ case "$MODE" in
+ MAX_TOKENS=1536
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
++ START=1592673622
+ MAX_TOKENS=1536
+++ date +%s
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date +%s
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ START=1592673622
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
++ export DGXSYSTEM
+ MODE=TRAIN
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
+ MAX_TOKENS=1536
+ case "$MODE" in
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ '[' -n 4 ']'
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 3 ']'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
+ SEED=21280
+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 0 ']'
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+++ date +%s
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
+ case "$MODE" in
+ NUMEPOCHS=30
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ case "$MODE" in
++ '[' -n 6 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
+++ date +%s
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ '[' -n 5 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 0 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NNODES
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ '[' -n 6 ']'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 2 ']'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 0 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592673622
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 6 ']'
+ SEED=21280
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
++ START=1592673622
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 2 ']'
+ SEED=21280
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ START=1592673622
+ MODE=TRAIN
+ NUMEPOCHS=30
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 1 ']'
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 2 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 7 ']'
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ export SLURM_NNODES
++ '[' -n 5 ']'
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MODE=TRAIN
+ DATASET_DIR=/data
+ NUMEPOCHS=30
+ MODE=TRAIN
+ case "$MODE" in
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+++ date +%s
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export DGXSYSTEM
+ SEED=21280
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ case "$MODE" in
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ START=1592673622
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
++ START=1592673622
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 5 ']'
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
++ '[' -n 0 ']'
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
+ SEED=21280
+ SEED=21280
+ MAX_TOKENS=1536
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ case "$MODE" in
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date +%s
+ SEED=21280
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MAX_TOKENS=1536
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ '[' -n 1 ']'
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 6 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ case "$MODE" in
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ DATASET_DIR=/data
+ NUMEPOCHS=30
+ MODE=TRAIN
+ case "$MODE" in
+ NUMEPOCHS=30
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+++ date +%s
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 4 ']'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
++ [[ 480 -ne 1 ]]
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
+ SEED=21280
+ DATASET_DIR=/data
+ MODE=TRAIN
++ export DGXSYSTEM
+ NUMEPOCHS=30
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ case "$MODE" in
++ export SLURM_NNODES
+ MODE=TRAIN
+ source run_training.sh
++ declare -a CMD
++ START=1592673622
+++ date +%s
+ NUMEPOCHS=30
+ SEED=21280
++ '[' -n 6 ']'
+ case "$MODE" in
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ source run_training.sh
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+ case "$MODE" in
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date +%s
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
++ [[ 480 -ne 1 ]]
++ START=1592673622
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+ case "$MODE" in
++ export DGXSYSTEM
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ '[' -n 7 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date +%s
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
++ export DGXSYSTEM
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
++ export SLURM_NNODES
+ NUMEPOCHS=30
++ declare -a CMD
++ '[' -n 4 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ '[' -n 3 ']'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ export SLURM_NNODES
+++ date +%s
+ MAX_TOKENS=1536
++ [[ 480 -ne 1 ]]
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
++ export DGXSYSTEM
+ NUMEPOCHS=30
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ case "$MODE" in
++ export SLURM_NNODES
+ DATASET_DIR=/data
+ MODE=TRAIN
+ source run_training.sh
++ '[' -n 0 ']'
+ SEED=21280
++ declare -a CMD
+ DATASET_DIR=/data
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 3 ']'
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
+ MODE=TRAIN
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
+++ date +%s
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ '[' -n 3 ']'
++ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 6 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 7 ']'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 7 ']'
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ '[' -n 4 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 7 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
+ MAX_TOKENS=1536
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
++ export DGXSYSTEM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
++ export SLURM_NNODES
+ case "$MODE" in
++ declare -a CMD
+ source run_training.sh
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
++ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 1 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ '[' -n 6 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 5 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ MAX_TOKENS=1536
+ SEED=21280
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ NUMEPOCHS=30
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ case "$MODE" in
+ MAX_TOKENS=1536
+ source run_training.sh
+ DATASET_DIR=/data
+++ date '+%Y-%m-%d %r'
+ MODE=TRAIN
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
++ declare -a CMD
+ MAX_TOKENS=1536
++ '[' -n 6 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ case "$MODE" in
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
+ MODE=TRAIN
++ export SLURM_NNODES
++ declare -a CMD
+ NUMEPOCHS=30
+ SEED=21280
+ case "$MODE" in
+ source run_training.sh
+ MAX_TOKENS=1536
++ '[' -n 4 ']'
+++ date +%s
+ DATASET_DIR=/data
++ START=1592673622
+ MODE=TRAIN
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
+ SEED=21280
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
+ MAX_TOKENS=1536
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
++ '[' -n 0 ']'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
+ source run_training.sh
++ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 6 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ MODE=TRAIN
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
+ MODE=TRAIN
+ NUMEPOCHS=30
++ declare -a CMD
+ SEED=21280
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ '[' -n 4 ']'
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 1 ']'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ '[' -n 3 ']'
++ export SLURM_NTASKS_PER_NODE
++ '[' 480 -gt 60 ']'
++ export SLURM_NNODES
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 3 ']'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ declare -a CMD
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ MAX_TOKENS=1536
++ export SLURM_NNODES
++ START=1592673622
+ DATASET_DIR=/data
++ declare -a CMD
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ MAX_TOKENS=1536
+ SEED=21280
+ MAX_TOKENS=1536
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START=1592673622
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ export DGXSYSTEM
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
++ export DGXSYSTEM
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
++ '[' -n 7 ']'
++ [[ 480 -ne 1 ]]
++ export DGXSYSTEM
+ DATASET_DIR=/data
+ MODE=TRAIN
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ source run_training.sh
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START=1592673622
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
+ SEED=21280
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ '[' -n 0 ']'
+ case "$MODE" in
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
+ source run_training.sh
+ SEED=21280
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ export SLURM_NNODES
+ source run_training.sh
++ declare -a CMD
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 1 ']'
++ export DGXSYSTEM
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 5 ']'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
++ '[' 480 -gt 60 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 6 ']'
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START=1592673622
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ SEED=21280
++ START_FMT='2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
+ SEED=21280
++ '[' -n 1 ']'
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ [[ 480 -ne 1 ]]
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
+++ date +%s
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ SEED=21280
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date +%s
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ source run_training.sh
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
+ MAX_TOKENS=1536
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
++ declare -a CMD
+++ date +%s
+ DATASET_DIR=/data
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
+ MODE=TRAIN
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+++ date +%s
++ '[' -n 6 ']'
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ [[ 480 -ne 1 ]]
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
+ MAX_TOKENS=1536
+ NUMEPOCHS=30
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date +%s
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
+ SEED=21280
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
++ export SLURM_NNODES
+ DATASET_DIR=/data
+ MAX_TOKENS=1536
+ MODE=TRAIN
+ NUMEPOCHS=30
++ declare -a CMD
+ case "$MODE" in
+ SEED=21280
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 4 ']'
+ SEED=21280
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
++ '[' -n 0 ']'
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 2 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
+ SEED=21280
++ START=1592673622
++ export DGXSYSTEM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ SEED=21280
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ export SLURM_NNODES
+++ date +%s
+ MODE=TRAIN
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
++ '[' -n 3 ']'
+++ date +%s
+ source run_training.sh
++ '[' 480 -gt 60 ']'
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+++ date +%s
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ SEED=21280
+ MAX_TOKENS=1536
++ export SLURM_NNODES
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
++ export SLURM_NNODES
+ NUMEPOCHS=30
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
+ case "$MODE" in
++ '[' 480 -gt 60 ']'
+ source run_training.sh
++ declare -a CMD
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ '[' -n 1 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
++ START=1592673622
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ MAX_TOKENS=1536
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ '[' -n 6 ']'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ NUMEPOCHS=30
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ MAX_TOKENS=1536
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+ case "$MODE" in
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
+ DATASET_DIR=/data
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ MODE=TRAIN
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
+ DATASET_DIR=/data
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ MODE=TRAIN
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ case "$MODE" in
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
+ MAX_TOKENS=1536
++ [[ 480 -ne 1 ]]
++ START=1592673622
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
+ case "$MODE" in
++ export SLURM_NNODES
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
++ declare -a CMD
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 5 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 4 ']'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ SEED=21280
+ MODE=TRAIN
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ source run_training.sh
+++ date +%s
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ '[' -n 5 ']'
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
+ SEED=21280
++ '[' -n 6 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MAX_TOKENS=1536
++ '[' -n 6 ']'
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MODE=TRAIN
+ SEED=21280
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
+ MAX_TOKENS=1536
++ START=1592673622
+ case "$MODE" in
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ export SLURM_NTASKS_PER_NODE
+ MODE=TRAIN
++ export SLURM_NNODES
++ export DGXSYSTEM
+ NUMEPOCHS=30
+++ date +%s
++ declare -a CMD
+ SEED=21280
+ SEED=21280
+ case "$MODE" in
++ '[' -n 0 ']'
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
+ MAX_TOKENS=1536
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
++ export DGXSYSTEM
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
+ case "$MODE" in
+ MODE=TRAIN
+ source run_training.sh
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ '[' -n 0 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ export DGXSYSTEM
++ START=1592673622
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 2 ']'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ SEED=21280
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ export DGXSYSTEM
+ MAX_TOKENS=1536
+ source run_training.sh
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+ MODE=TRAIN
++ START=1592673622
++ export DGXSYSTEM
++ '[' -n 0 ']'
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 2 ']'
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 7 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ export SLURM_NNODES
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
++ export SLURM_NNODES
+++ date +%s
++ '[' -n 6 ']'
++ declare -a CMD
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ '[' -n 5 ']'
++ START=1592673622
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MODE=TRAIN
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 5 ']'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ export SLURM_NNODES
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ SEED=21280
++ '[' -n 5 ']'
+ MAX_TOKENS=1536
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ source run_training.sh
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ source run_training.sh
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
++ '[' -n 2 ']'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ case "$MODE" in
+ DATASET_DIR=/data
+ MODE=TRAIN
++ export SLURM_NNODES
+++ date +%s
+ source run_training.sh
+ NUMEPOCHS=30
++ export DGXSYSTEM
+ SEED=21280
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
++ declare -a CMD
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
++ export SLURM_NNODES
++ declare -a CMD
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 2 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
+++ date +%s
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ '[' -n 6 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
++ START=1592673622
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
+ MODE=TRAIN
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+++ date +%s
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 4 ']'
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
++ export SLURM_NNODES
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ '[' -n 4 ']'
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
++ '[' 480 -gt 60 ']'
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 2 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
+++ date +%s
++ '[' -n 2 ']'
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592673622
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ '[' -n 7 ']'
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ '[' -n 5 ']'
++ declare -a CMD
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
+ DATASET_DIR=/data
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ MODE=TRAIN
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ declare -a CMD
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ declare -a CMD
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 3 ']'
+ DATASET_DIR=/data
+ MODE=TRAIN
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
+ case "$MODE" in
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ SEED=21280
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ case "$MODE" in
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ NUMEPOCHS=30
+++ date +%s
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+ source run_training.sh
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 3 ']'
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 5 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+ MODE=TRAIN
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export DGXSYSTEM
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ '[' -n 6 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
+++ date +%s
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ '[' -n 7 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ '[' -n 4 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
+ SEED=21280
+ MAX_TOKENS=1536
++ declare -a CMD
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date +%s
++ START=1592673622
++ '[' -n 5 ']'
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ declare -a CMD
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 3 ']'
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+ SEED=21280
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
++ '[' -n 1 ']'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
++ START=1592673622
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ export DGXSYSTEM
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ '[' -n 2 ']'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
++ '[' -n 7 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ SEED=21280
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ MAX_TOKENS=1536
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
+++ date +%s
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ SEED=21280
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
+ source run_training.sh
+ MAX_TOKENS=1536
++ '[' -n 6 ']'
++ '[' -n 4 ']'
+ SEED=21280
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
++ [[ 480 -ne 1 ]]
++ declare -a CMD
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
++ START=1592673622
++ '[' -n 2 ']'
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
+++ date +%s
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
+++ date +%s
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ export SLURM_NNODES
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
++ '[' -n 0 ']'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 0 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
++ '[' -n 3 ']'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
+ NUMEPOCHS=30
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 7 ']'
++ export SLURM_NNODES
+++ date +%s
++ '[' 480 -gt 60 ']'
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
++ export DGXSYSTEM
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
++ declare -a CMD
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 5 ']'
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date +%s
++ declare -a CMD
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ '[' -n 2 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
++ START=1592673622
++ '[' 480 -gt 60 ']'
+ source run_training.sh
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ DATASET_DIR=/data
++ declare -a CMD
+ SEED=21280
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
+ NUMEPOCHS=30
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
+ MODE=TRAIN
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
+++ date +%s
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date +%s
++ declare -a CMD
++ '[' -n 0 ']'
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 7 ']'
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
++ export DGXSYSTEM
++ export SLURM_NNODES
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ NUMEPOCHS=30
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
+ case "$MODE" in
+ source run_training.sh
++ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 3 ']'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
+ MAX_TOKENS=1536
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
+++ date +%s
+ MAX_TOKENS=1536
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export DGXSYSTEM
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
+ MODE=TRAIN
++ declare -a CMD
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
++ '[' -n 6 ']'
++ export SLURM_NTASKS_PER_NODE
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ export SLURM_NNODES
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ '[' -n 3 ']'
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
+ SEED=21280
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
+ MAX_TOKENS=1536
++ [[ 480 -ne 1 ]]
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
+ NUMEPOCHS=30
++ export SLURM_NTASKS_PER_NODE
+ case "$MODE" in
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
++ declare -a CMD
++ '[' -n 0 ']'
+ MODE=TRAIN
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ '[' -n 1 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 0 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
++ export DGXSYSTEM
++ declare -a CMD
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
++ export SLURM_NNODES
+ MODE=TRAIN
++ declare -a CMD
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
++ '[' -n 1 ']'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ '[' -n 3 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
+ case "$MODE" in
+ source run_training.sh
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
+ SEED=21280
++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ START=1592673622
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
++ declare -a CMD
++ START=1592673622
+ source run_training.sh
++ '[' -n 3 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
+++ date +%s
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ [[ 480 -ne 1 ]]
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ START=1592673622
+ SEED=21280
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
+ MAX_TOKENS=1536
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
+ SEED=21280
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+ MAX_TOKENS=1536
++ export DGXSYSTEM
++ '[' -n 6 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
+ MODE=TRAIN
++ declare -a CMD
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
++ '[' -n 6 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
+ source run_training.sh
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
+ SEED=21280
++ declare -a CMD
+ MAX_TOKENS=1536
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ [[ 480 -ne 1 ]]
+++ date +%s
++ '[' -n 4 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ '[' -n 3 ']'
++ '[' -n 7 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 7 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
+ SEED=21280
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
+ MAX_TOKENS=1536
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
+ DATASET_DIR=/data
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ export SLURM_NNODES
++ export DGXSYSTEM
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ case "$MODE" in
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
++ '[' -n 0 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
++ '[' 480 -gt 60 ']'
+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
++ export DGXSYSTEM
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ source run_training.sh
++ declare -a CMD
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 0 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
+ SEED=21280
+ MAX_TOKENS=1536
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ export DGXSYSTEM
+ case "$MODE" in
+ source run_training.sh
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ '[' -n 1 ']'
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
+ SEED=21280
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
+ DATASET_DIR=/data
+ MODE=TRAIN
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
+ case "$MODE" in
++ [[ 480 -ne 1 ]]
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ '[' -n 5 ']'
+ source run_training.sh
++ '[' -n 6 ']'
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ '[' -n 5 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export DGXSYSTEM
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 1 ']'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 2 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
++ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 2 ']'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 4 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 1 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ '[' -n 4 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ '[' -n 0 ']'
+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ '[' -n 2 ']'
+ case "$MODE" in
++ '[' 480 -gt 60 ']'
+ source run_training.sh
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
++ export SLURM_NNODES
++ declare -a CMD
+ MAX_TOKENS=1536
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ DATASET_DIR=/data
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
+ SEED=21280
++ '[' 480 -gt 60 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ [[ 480 -ne 1 ]]
+++ date +%s
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
+ SEED=21280
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 5 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
+++ date '+%Y-%m-%d %r'
++ '[' -n 6 ']'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ [[ 480 -ne 1 ]]
++ '[' -n 2 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ '[' -n 1 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 3 ']'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 4 ']'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ START=1592673622
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 4 ']'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ '[' -n 7 ']'
+ MODE=TRAIN
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+++ date +%s
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
+ NUMEPOCHS=30
+ case "$MODE" in
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+++ date +%s
+ SEED=21280
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+++ date +%s
+ DATASET_DIR=/data
+++ date +%s
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
++ [[ 480 -ne 1 ]]
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ source run_training.sh
+++ date +%s
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ SEED=21280
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+ MAX_TOKENS=1536
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ DATASET_DIR=/data
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ MODE=TRAIN
++ export SLURM_NNODES
+ NUMEPOCHS=30
++ declare -a CMD
+ case "$MODE" in
+ source run_training.sh
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ '[' -n 5 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 7 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ SEED=21280
+ DATASET_DIR=/data
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
+ DATASET_DIR=/data
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
+ source run_training.sh
++ '[' -n 0 ']'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
++ '[' -n 6 ']'
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ '[' -n 1 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
+ MAX_TOKENS=1536
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
+ DATASET_DIR=/data
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
++ '[' -n 5 ']'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ SEED=21280
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MAX_TOKENS=1536
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ export SLURM_NNODES
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' -n 4 ']'
++ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MODE=TRAIN
++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
+ source run_training.sh
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MODE=TRAIN
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ NUMEPOCHS=30
+ case "$MODE" in
+ SEED=21280
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+++ date +%s
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ case "$MODE" in
+ SEED=21280
+ source run_training.sh
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ MAX_TOKENS=1536
+ MODE=TRAIN
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ case "$MODE" in
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ SEED=21280
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ SEED=21280
+ DATASET_DIR=/data
+ MAX_TOKENS=1536
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+++ date +%s
+++ date +%s
+ MODE=TRAIN
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date +%s
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
+++ date +%s
+ SEED=21280
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
+ case "$MODE" in
++ START=1592673622
+ source run_training.sh
++ START=1592673622
+++ date +%s
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ export SLURM_NTASKS_PER_NODE
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 5 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date +%s
+ source run_training.sh
++ START=1592673622
++ START=1592673622
++ START=1592673622
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+ SEED=21280
+++ date '+%Y-%m-%d %r'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 7 ']'
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
++ export DGXSYSTEM
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
+ source run_training.sh
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
Run vars: id 367023 gpus 8 mparams
++ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ '[' -n 6 ']'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ [[ 480 -ne 1 ]]
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
+ SEED=21280
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MAX_TOKENS=1536
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+++ date '+%Y-%m-%d %r'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MAX_TOKENS=1536
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
++ '[' -n 3 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ declare -a CMD
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 6 ']'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' -n 1 ']'
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 7 ']'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 1 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ declare -a CMD
+ MODE=TRAIN
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
++ START_FMT='2020-06-20 10:20:22 AM'
+ case "$MODE" in
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 2 ']'
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 6 ']'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export DGXSYSTEM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
++ export SLURM_NNODES
++ START=1592673622
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 4 ']'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 5 ']'
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 4 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ '[' -n 2 ']'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ export DGXSYSTEM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ '[' -n 1 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' -n 2 ']'
++ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ [[ 480 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+ SEED=21280
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ SEED=21280
+ MAX_TOKENS=1536
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ [[ 480 -ne 1 ]]
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ '[' -n 4 ']'
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
++ '[' 480 -gt 60 ']'
+ DATASET_DIR=/data
+ MODE=TRAIN
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ source run_training.sh
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+++ date +%s
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ '[' -n 2 ']'
+ NUMEPOCHS=30
++ '[' 480 -gt 60 ']'
+ case "$MODE" in
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ source run_training.sh
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
+ SEED=21280
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+++ date +%s
++ START=1592673622
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ export DGXSYSTEM
+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
+ source run_training.sh
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ SEED=21280
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
+ MAX_TOKENS=1536
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+++ date '+%Y-%m-%d %r'
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START=1592673622
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ SEED=21280
+ MAX_TOKENS=1536
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
+ MODE=TRAIN
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ source run_training.sh
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ case "$MODE" in
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
+ MAX_TOKENS=1536
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ declare -a CMD
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ '[' -n 7 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NNODES
+++ date +%s
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
++ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
+ SEED=21280
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
++ export DGXSYSTEM
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
+ case "$MODE" in
+++ date +%s
+++ date +%s
+ source run_training.sh
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
+++ date +%s
+ source run_training.sh
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+ SEED=21280
++ START_FMT='2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
++ [[ 480 -ne 1 ]]
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ declare -a CMD
++ '[' -n 4 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
++ [[ 480 -ne 1 ]]
+ MAX_TOKENS=1536
+ SEED=21280
++ [[ 480 -ne 1 ]]
+ DATASET_DIR=/data
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
+ MAX_TOKENS=1536
++ export DGXSYSTEM
+ NUMEPOCHS=30
++ export SLURM_NTASKS_PER_NODE
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ MODE=TRAIN
++ export SLURM_NNODES
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ NUMEPOCHS=30
++ declare -a CMD
+ source run_training.sh
+ case "$MODE" in
++ '[' -n 1 ']'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 6 ']'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ SEED=21280
++ export DGXSYSTEM
++ export SLURM_NNODES
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ export SLURM_NTASKS_PER_NODE
+ NUMEPOCHS=30
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 4 ']'
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 7 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
+ source run_training.sh
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ declare -a CMD
++ export DGXSYSTEM
++ START=1592673622
++ export SLURM_NTASKS_PER_NODE
+ SEED=21280
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
+ MAX_TOKENS=1536
++ START=1592673622
++ declare -a CMD
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ MODE=TRAIN
+ NUMEPOCHS=30
++ declare -a CMD
++ '[' -n 6 ']'
++ START=1592673622
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+ case "$MODE" in
++ export SLURM_NTASKS_PER_NODE
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
+ source run_training.sh
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ export SLURM_NTASKS_PER_NODE
+ MODE=TRAIN
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NNODES
+ case "$MODE" in
++ declare -a CMD
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
Run vars: id 367023 gpus 8 mparams
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 4 ']'
+++ date +%s
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date '+%Y-%m-%d %r'
+++ date +%s
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
+ source run_training.sh
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ source run_training.sh
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592673622
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
++ START=1592673622
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 367023 gpus 8 mparams
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ START_FMT='2020-06-20 10:20:22 AM'
++ START=1592673622
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ [[ 480 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NNODES
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
Run vars: id 367023 gpus 8 mparams
++ '[' -n 6 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
++ START_FMT='2020-06-20 10:20:22 AM'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ case "$MODE" in
++ [[ 480 -ne 1 ]]
+ source run_training.sh
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date +%s
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START=1592673622
++ declare -a CMD
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ NUMEPOCHS=30
++ '[' -n 5 ']'
+ case "$MODE" in
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ source run_training.sh
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date +%s
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ declare -a CMD
++ '[' -n 3 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export DGXSYSTEM
++ '[' -n 5 ']'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ START=1592673622
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 6 ']'
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 0 ']'
++ START=1592673622
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START=1592673622
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 0 ']'
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592673622
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 6 ']'
++ [[ 480 -ne 1 ]]
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+++ date +%s
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date +%s
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
++ START_FMT='2020-06-20 10:20:22 AM'
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+ case "$MODE" in
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=21280
+ MAX_TOKENS=1536
+ SEED=21280
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=1536
Run vars: id 367023 gpus 8 mparams
+ MAX_TOKENS=1536
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ DATASET_DIR=/data
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
+ MODE=TRAIN
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ SEED=21280
+ MAX_TOKENS=1536
+ case "$MODE" in
+ DATASET_DIR=/data
+ case "$MODE" in
+ MODE=TRAIN
+ SEED=21280
+ NUMEPOCHS=30
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+++ date +%s
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date +%s
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ START=1592673622
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ '[' -n 1 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' 480 -gt 60 ']'
+++ date '+%Y-%m-%d %r'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NNODES
++ export DGXSYSTEM
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 7 ']'
++ declare -a CMD
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START_FMT='2020-06-20 10:20:22 AM'
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 4 ']'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaSTARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ export SLURM_NNODES
++ declare -a CMD
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592673622
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+++ date +%s
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
+ MODE=TRAIN
+ NUMEPOCHS=30
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ SEED=21280
+ MAX_TOKENS=1536
++ declare -a CMD
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 367023 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ SEED=21280
++ '[' -n 0 ']'
+ MAX_TOKENS=1536
++ '[' 480 -gt 60 ']'
+ DATASET_DIR=/data
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla+++ date +%s
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 367023 gpus 8 mparams
+ NUMEPOCHS=30
+ SEED=21280
+ case "$MODE" in
+ MAX_TOKENS=1536
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 367023 gpus 8 mparams
+ source run_training.sh
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START=1592673622
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592673622
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' -n 7 ']'
++ export SLURM_NNODES
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-fla++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flaRun vars: id 367023 gpus 8 mparams
t-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-20 10:20:22 AM'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
num_sockets = 2 num_nodes=8 cores_per_socket=64
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
++ START=1592673622
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ [[ 480 -ne 1 ]]
++ START_FMT='2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NNODES
++ declare -a CMD
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ [[ 480 -ne 1 ]]
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 367023 gpus 8 mparams
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ SEED=21280
+ NUMEPOCHS=30
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592673622
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
+ DATASET_DIR=/data
+ MODE=TRAIN
++ START_FMT='2020-06-20 10:20:22 AM'
+ NUMEPOCHS=30
+ case "$MODE" in
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
+ source run_training.sh
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592673622
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
Run vars: id 367023 gpus 8 mparams
+ SEED=21280
+ MAX_TOKENS=1536
++ START=1592673622
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START=1592673622
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ START_FMT='2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ [[ 480 -ne 1 ]]
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ declare -a CMD
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
+++ date '+%Y-%m-%d %r'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 7 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592673622
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ [[ 480 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-20 10:20:22 AM'
STARTING TIMING RUN AT 2020-06-20 10:20:22 AM
++ echo 'STARTING TIMING RUN AT 2020-06-20 10:20:22 AM'
++ [[ 480 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 480 -gt 60 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --d+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions num_sockets = 2 num_nodes=8 cores_per_socket=64
--dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dnum_sockets = 2 num_nodes=8 cores_per_socket=64
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -num_sockets = 2 num_nodes=8 cores_per_socket=64
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --d+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
--dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --ddwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dnum_sockets = 2 num_nodes=8 cores_per_socket=64
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -num_sockets = 2 num_nodes=8 cores_per_socket=64
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -num_sockets = 2 num_nodes=8 cores_per_socket=64
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --d+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --ddwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dnum_sockets = 2 num_nodes=8 cores_per_socket=64
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dnum_sockets = 2 num_nodes=8 cores_per_socket=64
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --d+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --ddwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -num_sockets = 2 num_nodes=8 cores_per_socket=64
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions num_sockets = 2 num_nodes=8 cores_per_socket=64
--dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions num_sockets = 2 num_nodes=8 cores_per_socket=64
--dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions ---dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions -num_sockets = 2 num_nodes=8 cores_per_socket=64
-dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --d+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --d+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --wu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --num_sockets = 2 num_nodes=8 cores_per_socket=64
dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions num_sockets = 2 num_nodes=8 cores_per_socket=64
--dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 21280 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1536 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 348
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 187
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 195
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 349
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 196
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 107
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 347
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 186
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 108
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 194
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 105
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 198
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 479
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 350
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 344
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 191
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 189
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 192
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 345
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 199
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 197
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 193
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 473
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 351
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 152
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 477
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 346
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 106
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 190
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 109
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 185
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 111
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 184
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 188
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 104
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 220
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 475
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 144
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 359
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 117
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 157
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 41
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 378
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 354
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 450
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 472
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 218
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 476
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 336
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 151
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 478
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 474
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 155
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 118
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 226
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 412
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 280
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 236
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 216
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 431
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 284
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 47
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 415
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 46
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 379
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 143
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 124
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 452
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 134
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 98
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 333
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 112
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 150
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 356
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 283
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 230
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 147
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 4
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 430
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 142
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 234
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 160
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 51
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 120
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 233
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 58
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 44
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 110
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 448
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 429
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 13
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 130
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 352
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 271
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 358
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 265
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 255
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 453
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 154
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 228
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 221
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 355
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 343
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 380
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 96
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 55
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 40
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 311
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 141
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 5
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 409
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 158
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 222
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 217
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 223
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 99
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 340
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 64
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 31
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 353
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 219
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 146
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 357
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 320
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 116
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 335
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 113
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 381
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 237
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 156
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 362
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 342
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 456
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 148
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 451
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 145
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 281
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 424
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 149
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 470
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 62
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 303
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 224
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 133
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 168
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 253
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 449
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 455
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 123
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 339
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 454
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 42
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 103
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 444
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 121
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 278
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 159
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 115
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 286
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 377
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 114
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 119
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 330
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 225
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 410
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 153
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 408
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 9
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 74
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 337
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 395
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 43
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 285
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 282
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 45
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 235
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 32
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 382
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 465
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 307
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 136
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 383
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 172
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 341
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 70
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 411
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 10
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 60
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 367
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 338
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 413
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 428
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 435
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 427
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 298
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 82
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 125
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 414
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 446
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 275
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 287
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 22
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 310
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 386
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 165
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 131
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 376
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 254
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 231
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 426
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 425
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 97
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 232
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 122
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 177
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 126
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 138
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 266
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 326
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 166
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 331
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 53
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 139
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 361
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 100
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 227
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 38
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 127
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 2
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 242
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 229
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 238
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 48
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 457
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 102
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 332
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 137
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 374
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 27
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 392
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 170
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 73
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 416
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 239
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 140
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 202
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 132
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 462
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 135
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 264
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 23
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 101
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 249
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 325
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 164
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 36
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 440
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 391
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 167
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 6
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 471
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 207
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 365
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 128
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 15
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 94
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 50
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 269
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 85
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 394
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 3
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 129
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 329
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 52
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 16
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 65
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 334
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 306
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 270
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 7
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 261
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 49
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 8
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 54
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 24
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 59
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 212
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 368
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 56
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 161
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 61
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 176
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 162
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 277
| distributed init (rank 0): env://
| distributed init done!
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 328
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 1
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 460
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 267
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 57
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 14
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 417
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 11
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 296
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 29
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 244
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 324
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 274
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 63
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 251
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 445
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 252
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 322
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 308
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 248
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 438
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 86
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 468
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 91
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 214
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 309
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 30
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 175
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 363
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 433
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 268
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 466
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 327
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 12
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 203
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 300
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 250
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 360
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 163
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 169
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 179
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 304
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 421
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 259
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 366
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 321
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 458
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 28
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 26
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626283, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 323
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 276
:::MLLOG {"namespace": "", "time_ms": 1592673626286, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 464
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 171
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 68
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 87
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 174
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 72
:::MLLOG {"namespace": "", "time_ms": 1592673626288, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626288, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 467
:::MLLOG {"namespace": "", "time_ms": 1592673626292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed init (rank 0): env://
:::MLLOG {"namespace": "", "time_ms": 1592673626293, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 67
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 240
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 299
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 447
:::MLLOG {"namespace": "", "time_ms": 1592673626294, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 301
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 463
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 305
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 469
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 297
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 17
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 396
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 69
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 364
:::MLLOG {"namespace": "", "time_ms": 1592673626297, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 34
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 443
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626296, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 173
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 441
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 302
:::MLLOG {"namespace": "", "time_ms": 1592673626300, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 35
:::MLLOG {"namespace": "", "time_ms": 1592673626299, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626301, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626301, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 370
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 213
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 77
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 272
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 273
:::MLLOG {"namespace": "", "time_ms": 1592673626302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 206
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 442
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 419
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 459
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626305, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 279
:::MLLOG {"namespace": "", "time_ms": 1592673626306, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 371
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 398
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 385
:::MLLOG {"namespace": "", "time_ms": 1592673626311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 393
:::MLLOG {"namespace": "", "time_ms": 1592673626311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 39
:::MLLOG {"namespace": "", "time_ms": 1592673626316, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 316
:::MLLOG {"namespace": "", "time_ms": 1592673626316, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626318, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626317, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 387
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 37
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 434
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 88
:::MLLOG {"namespace": "", "time_ms": 1592673626319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626320, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626320, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626323, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626321, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 76
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 182
:::MLLOG {"namespace": "", "time_ms": 1592673626325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 25
:::MLLOG {"namespace": "", "time_ms": 1592673626325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626323, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626324, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 373
:::MLLOG {"namespace": "", "time_ms": 1592673626326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 78
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 19
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 92
:::MLLOG {"namespace": "", "time_ms": 1592673626329, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 399
| distributed init (rank 0): env://
| distributed init done!
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 75
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 71
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 81
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 205
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 397
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 66
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 461
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 21
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 79
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 369
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 20
:::MLLOG {"namespace": "", "time_ms": 1592673626335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed init done!
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 80
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 33
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 437
:::MLLOG {"namespace": "", "time_ms": 1592673626337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 18
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 439
:::MLLOG {"namespace": "", "time_ms": 1592673626339, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 180
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626338, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626341, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 389
:::MLLOG {"namespace": "", "time_ms": 1592673626340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 432
:::MLLOG {"namespace": "", "time_ms": 1592673626341, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626343, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 83
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626341, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626341, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 263
:::MLLOG {"namespace": "", "time_ms": 1592673626342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 390
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626348, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 384
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 436
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626353, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626353, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 372
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 210
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 246
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 181
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 418
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 200
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 420
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 178
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 241
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 183
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 201
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 388
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626364, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626364, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626363, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 247
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 256
:::MLLOG {"namespace": "", "time_ms": 1592673626365, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 84
:::MLLOG {"namespace": "", "time_ms": 1592673626367, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626367, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626366, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 211
:::MLLOG {"namespace": "", "time_ms": 1592673626371, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 422
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 245
:::MLLOG {"namespace": "", "time_ms": 1592673626370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626371, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626372, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 243
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 215
:::MLLOG {"namespace": "", "time_ms": 1592673626376, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 95
:::MLLOG {"namespace": "", "time_ms": 1592673626379, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626381, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626381, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 209
:::MLLOG {"namespace": "", "time_ms": 1592673626383, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626383, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 318
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 204
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626388, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626388, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 93
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 260
:::MLLOG {"namespace": "", "time_ms": 1592673626390, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 423
:::MLLOG {"namespace": "", "time_ms": 1592673626394, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 89
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626393, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 208
:::MLLOG {"namespace": "", "time_ms": 1592673626397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626398, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 375
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 90
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626401, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 258
:::MLLOG {"namespace": "", "time_ms": 1592673626405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 312
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626413, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626417, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 257
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626418, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626420, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626423, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626423, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626423, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626426, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626425, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 262
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626427, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626431, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626431, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626435, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 290
:::MLLOG {"namespace": "", "time_ms": 1592673626433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626436, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626437, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626441, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 295
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 315
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626448, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626447, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626452, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626452, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626456, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626462, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626465, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626465, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626466, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626468, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626471, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626472, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626473, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626474, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626473, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626477, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626478, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626478, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626479, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626479, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 314
:::MLLOG {"namespace": "", "time_ms": 1592673626482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626480, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626480, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626484, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626487, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626489, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 319
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 313
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626493, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626497, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626499, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 317
:::MLLOG {"namespace": "", "time_ms": 1592673626503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626504, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 407
:::MLLOG {"namespace": "", "time_ms": 1592673626509, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626512, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626515, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626516, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626518, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 294
:::MLLOG {"namespace": "", "time_ms": 1592673626520, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 291
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626522, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626524, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626527, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626532, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626532, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626533, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626535, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626535, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626535, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626539, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626535, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626540, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626544, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626551, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626551, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626550, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626554, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626554, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626556, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 292
:::MLLOG {"namespace": "", "time_ms": 1592673626561, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626565, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626565, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 405
:::MLLOG {"namespace": "", "time_ms": 1592673626568, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626567, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626573, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 403
:::MLLOG {"namespace": "", "time_ms": 1592673626577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626576, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 289
:::MLLOG {"namespace": "", "time_ms": 1592673626581, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626585, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626583, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 288
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626598, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 293
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626602, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 404
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 402
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 406
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 400
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0001, MASTER_PORT: 55727, WORLD_SIZE: 480, RANK: 401
:::MLLOG {"namespace": "", "time_ms": 1592673626712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626738, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626962, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673626977, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626983, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673626985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627004, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627007, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627024, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627130, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627131, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627131, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627147, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627157, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627227, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627227, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627230, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592673627240, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| initialized host luna-0001 as rank 0 and device id 0
:::MLLOG {"namespace": "", "time_ms": 1592673627250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627258, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627259, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627262, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627263, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627263, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627263, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627265, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627267, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627268, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627277, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627278, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627280, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627286, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627289, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627291, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627291, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627291, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627293, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627297, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627298, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627298, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627299, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627306, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627305, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627306, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627308, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627308, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627307, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627312, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627316, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627313, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627316, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627316, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627321, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627323, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627321, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627324, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627330, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627333, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627336, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627338, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627339, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627336, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627339, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627345, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627348, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627350, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627350, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627353, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627359, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627359, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627363, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627363, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627363, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627371, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627375, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627372, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627373, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627377, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627377, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627378, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
Namespace(adam_betas='(0.86,0.92)', adam_eps=1e-09, adaptive_softmax_cutoff=None, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, batch_multiple_strategy='dynamic', batching_scheme='v0p5_better', beam=4, bucket_growth_factor=1.035, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data', dataloader_num_workers=2, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_weight_update=2, distributed_world_size=480, dropout=0.1, dwu_compute_L2_grad_norm=True, dwu_do_not_flatten_model=False, dwu_e5m2_allgather=False, dwu_flat_mt=True, dwu_full_pipeline=False, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=4, dwu_num_chunks=1, dwu_num_rs_pg=2, dwu_overlap_reductions=True, enable_dataloader_pin_memory=True, enable_global_stats=False, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fast_xentropy=True, fp16=True, gen_subset='test', ignore_case=True, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', lenpen=0.6, local_rank=0, log_format=None, log_interval=1000, log_translations=False, lr=[0.001732], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=30, max_len_a=1.0, max_len_b=50, max_sentences=None, max_sentences_valid=None, max_source_positions=76, max_target_positions=76, max_tokens=1536, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, multihead_attn_impl='fast_with_lyrnrm_and_dropoutadd', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_progress_bar=Tr:::MLLOG {"namespace": "", "time_ms": 1592673627384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627387, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627387, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627388, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627388, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627389, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627393, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627394, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627396, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627400, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627402, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627403, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627403, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627413, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627418, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627421, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
ue, no_save=True, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', parallel_backward_allred_cuda_nstreams=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=None, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, seed=21280, sentence_avg=False, seq_len_multiple=2, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_bleu=25.0, target_lang='de', task='translation', time_step=False, train_subset='train', uniform_n_seq_in_dataset=None, uniform_n_seq_per_batch=None, uniform_seq_len_per_batch=None, unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=400, weight_decay=0.0)
:::MLLOG {"namespace": "", "time_ms": 1592673627423, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 737280, "metadata": {"file": "/workspace/translation/train.py", "lineno": 133}}
:::MLLOG {"namespace": "", "time_ms": 1592673627423, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/translation/train.py", "lineno": 134}}
:::MLLOG {"namespace": "", "time_ms": 1592673627423, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.001732, "metadata": {"file": "/workspace/translation/train.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1592673627423, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 400, "metadata": {"file": "/workspace/translation/train.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1592673627424, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 76, "metadata": {"file": "/workspace/translation/train.py", "lineno": 139, "method": "discard"}}
:::MLLOG {"namespace": "", "time_ms": 1592673627424, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.86, "metadata": {"file": "/workspace/translation/train.py", "lineno": 140}}
:::MLLOG {"namespace": "", "time_ms": 1592673627424, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.92, "metadata": {"file": "/workspace/translation/train.py", "lineno": 141}}
:::MLLOG {"namespace": "", "time_ms": 1592673627424, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-09, "metadata": {"file": "/workspace/translation/train.py", "lineno": 142}}
:::MLLOG {"namespace": "", "time_ms": 1592673627424, "event_type": "POINT_IN_TIME", "key": "seed", "value": 21280, "metadata": {"file": "/workspace/translation/train.py", "lineno": 143}}
:::MLLOG {"namespace": "", "time_ms": 1592673627421, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627426, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627436, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627438, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592673627439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
Using master seed from command line: 21280
Worker 0 is using worker seed: 1449776243
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 210808832
| training on 480 GPUs
| max tokens per GPU = 1536 and max sentences per GPU = None
DistributedFusedAdam {'distributed_weight_update': 2, 'dwu_group_size': 0, 'dwu_num_blocks': 4, 'dwu_num_chunks': 1, 'dwu_num_rs_pg': 2, 'dwu_num_ar_pg': 2, 'dwu_num_ag_pg': 0, 'overlap_reductions': True, 'full_pipeline': False, 'compute_L2_grad_norm': True, 'flat_mt': True, 'e5m2_allgather': False, 'do_not_flatten_model': False}
self._net_total_param_size=210808832, self._total_param_size=210812928, dwu_min_page_size=8192, self._block_size=52703232, self._chunk_size=52703232, self._shard_size=6587904
[0, 15, 55, 104]
model_param_fragment.size()=torch.Size([6587904]), new_param_packed_fragment.size()=torch.Size([6587904]), master_param_fragment.size()=torch.Size([6587904])
model_param_fragment.size()=torch.Size([2800640]), new_param_packed_fragment.size()=torch.Size([2800640]), master_param_fragment.size()=torch.Size([2800640])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3783168]), new_param_packed_fragment.size()=torch.Size([3783168]), master_param_fragment.size()=torch.Size([3783168])
model_param_fragment.size()=torch.Size([465920]), new_param_packed_fragment.size()=torch.Size([465920]), master_param_fragment.size()=torch.Size([465920])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1918464]), new_param_packed_fragment.size()=torch.Size([1918464]), master_param_fragment.size()=torch.Size([1918464])
model_param_fragment.size()=torch.Size([2328576]), new_param_packed_fragment.size()=torch.Size([2328576]), master_param_fragment.size()=torch.Size([2328576])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3145728]), new_param_packed_fragment.size()=torch.Size([3145728]), master_param_fragment.size()=torch.Size([3145728])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([59904]), new_param_packed_fragment.size()=torch.Size([59904]), master_param_fragment.size()=torch.Size([59904])
:::MLLOG {"namespace": "", "time_ms": 1592673652667, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 212}}
:::MLLOG {"namespace": "", "time_ms": 1592673652667, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 214}}
filename: /data/train.en-de.en
raw_text: False
| /data train 4590101 examples
filename: /data/train1.en-de.en
raw_text: False
filename: /data/train1.de-en.en
raw_text: False
srcline: tensor([  855,     3,    45,    96,   156,    10,  2688,   177,  5596,   163,     5,  9336, 14909, 12630,   527,   297, 15690,    70,     3,    68,    17,   927,    45,   482,   151,   283,  3551,  2091,     7,     5,   546,    24, 26623,  1617,  5440,    86,    15,  1524,  3522,   434,     3,   264,   199,   182,    86,    15,  4489,  8360,    69,   114,     5,   253,    41,    69,  3823,   203,     8,     5,  9336, 14909, 12630,   527,     4,     2])
| Sentences are being padded to multiples of: 2
filename: /data/test.en-de.en
raw_text: False
| /data test 3003 examples
srcline: tensor([ 7549,  4344,    64, 32364,  1259,    20, 13504,  8959,  3868,     2])
| Sentences are being padded to multiples of: 2
filename: /data/test1.en-de.en
raw_text: False
filename: /data/test1.de-en.en
raw_text: False
:::MLLOG {"namespace": "", "time_ms": 1592673654207, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 4590101, "metadata": {"file": "/workspace/translation/train.py", "lineno": 224}}
:::MLLOG {"namespace": "", "time_ms": 1592673654207, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "/workspace/translation/train.py", "lineno": 227}}
self.dataset.src_sizes 4590101
self.dataset.tgt_sizes 4590101
generated 97802 batches in 1.465927s
got epoch iterator 1.4665563106536865
:::MLLOG {"namespace": "", "time_ms": 1592673655674, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673655674, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 1}}
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 001 | loss 10064.866 | nll_loss 0.000 | ppl 1.00 | wps 2.01136e+07 | ups 7.6 | wpb 652083 | bsz 44 | num_updates 201 | lr 0.00087033 | gnorm 48375.222 | clip 100% | oom 0 | loss_scale 16.000 | wall 27
epoch time  6.9531309604644775
:::MLLOG {"namespace": "", "time_ms": 1592673662628, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673662628, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 1}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 376 batches in 0.000902s
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
| Translated 8 sentences (96 tokens) in 0.2s (32.90 sentences/s, 394.75 tokens/s)
| Generate test with beam=4: bleu_score=0.8686
| Eval completed in: 0.69s
:::MLLOG {"namespace": "", "time_ms": 1592673663313, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673663315, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.00868628267198801, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 1}}
validation and scoring  0.6876330375671387
:::MLLOG {"namespace": "", "time_ms": 1592673663315, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673663317, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673663317, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 2}}
| epoch 002 | loss 6504.102 | nll_loss 0.000 | ppl 1.00 | wps 2.27123e+07 | ups 30.8 | wpb 652160 | bsz 43 | num_updates 405 | lr 0.00172128 | gnorm 30513.298 | clip 100% | oom 0 | loss_scale 16.000 | wall 33
epoch time  5.929284572601318
:::MLLOG {"namespace": "", "time_ms": 1592673669246, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592673669247, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 2}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 376 batches in 0.000915s
| Translated 8 sentences (39 tokens) in 0.1s (148.69 sentences/s, 724.85 tokens/s)
| Generate test with beam=4: bleu_score=16.0730
| Eval completed in: 0.71s
:::MLLOG {"namespace": "", "time_ms": 1592673669958, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592673669960, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1607295125722885, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 2}}
validation and scoring  0.7133049964904785
:::MLLOG {"namespace": "", "time_ms": 1592673669960, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592673669960, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673669960, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 3}}
| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 003 | loss 4929.124 | nll_loss 0.000 | ppl 1.00 | wps 2.32186e+07 | ups 31.3 | wpb 652135 | bsz 47 | num_updates 608 | lr 0.00140484 | gnorm 21885.777 | clip 100% | oom 0 | loss_scale 8.000 | wall 40
epoch time  5.778886079788208
:::MLLOG {"namespace": "", "time_ms": 1592673675739, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592673675739, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 3}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 376 batches in 0.000943s
| Translated 8 sentences (46 tokens) in 0.0s (162.45 sentences/s, 934.06 tokens/s)
| Generate test with beam=4: bleu_score=22.0141
| Eval completed in: 0.73s
:::MLLOG {"namespace": "", "time_ms": 1592673676468, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592673676470, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22014079988002777, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 3}}
validation and scoring  0.7311105728149414
:::MLLOG {"namespace": "", "time_ms": 1592673676470, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592673676470, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673676470, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 4}}
| epoch 004 | loss 4399.581 | nll_loss 0.000 | ppl 1.00 | wps 2.33248e+07 | ups 31.3 | wpb 652160 | bsz 41 | num_updates 812 | lr 0.00121563 | gnorm 16948.813 | clip 100% | oom 0 | loss_scale 8.000 | wall 46
epoch time  5.782496929168701
:::MLLOG {"namespace": "", "time_ms": 1592673682253, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673682253, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 4}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 376 batches in 0.000918s
| Translated 8 sentences (45 tokens) in 0.0s (197.24 sentences/s, 1109.48 tokens/s)
| Generate test with beam=4: bleu_score=24.0736
| Eval completed in: 0.73s
:::MLLOG {"namespace": "", "time_ms": 1592673682983, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673682985, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24073565006256104, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 4}}
validation and scoring  0.7319636344909668
:::MLLOG {"namespace": "", "time_ms": 1592673682985, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592673682985, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592673682985, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 5}}
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 005 | loss 4242.566 | nll_loss 0.000 | ppl 1.00 | wps 2.30588e+07 | ups 31.0 | wpb 652146 | bsz 46 | num_updates 1015 | lr 0.00108729 | gnorm 13821.474 | clip 100% | oom 0 | loss_scale 4.000 | wall 53
epoch time  5.821954250335693
:::MLLOG {"namespace": "", "time_ms": 1592673688807, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592673688808, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 5}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 376 batches in 0.000867s
| Translated 8 sentences (45 tokens) in 0.0s (160.80 sentences/s, 904.52 tokens/s)
| Generate test with beam=4: bleu_score=25.2902
| Eval completed in: 0.73s
:::MLLOG {"namespace": "", "time_ms": 1592673689542, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592673689543, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.25290244817733765, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 5}}
validation and scoring  0.736318826675415
:::MLLOG {"namespace": "", "time_ms": 1592673689544, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592673689544, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 300, "status": "success"}}
| done training in 36.9 seconds
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673695
++ END=1592673695
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:35 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:35 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:35 AM'
++ RESULT=73
++ RESULT_NAME=transformer
RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:35 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:35 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:35 AM'
++ RESULT=73
++ RESULT_NAME=transformer
RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673695
+++ date '+%Y-%m-%d %r'
++ END=1592673695
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:35 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:35 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:35 AM
++ RESULT=73
++ RESULT_NAME=transformer
RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:35 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:35 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:35 AM
++ RESULT=73
++ RESULT_NAME=transformer
RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,73,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ [[ 0 != 0 ]]
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673696
++ END=1592673696
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ END=1592673696
++ END=1592673696
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+++ date '+%Y-%m-%d %r'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673696
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+ set +x
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT=74
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ RESULT=74
++ RESULT_NAME=transformer
+ set +x
+ set +x
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ [[ 0 != 0 ]]
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ [[ 0 != 0 ]]
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ [[ 0 != 0 ]]
++ END=1592673696
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ RESULT=74
++ RESULT_NAME=transformer
+ set +x
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+++ date '+%Y-%m-%d %r'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END=1592673696
++ [[ 0 != 0 ]]
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ [[ 0 != 0 ]]
++ RESULT=74
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END=1592673696
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ [[ 0 != 0 ]]
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673696
++ END=1592673696
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673696
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END=1592673696
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END_FMT='2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ RESULT=74
++ RESULT_NAME=transformer
+ set +x
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ [[ 0 != 0 ]]
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date +%s
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
slurmstepd: error: _is_a_lwp: open() /proc/69600/status failed: No such file or directory
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
slurmstepd: error: _is_a_lwp: open() /proc/69605/status failed: No such file or directory
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ [[ 0 != 0 ]]
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ [[ 0 != 0 ]]
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673696
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673696
++ END=1592673696
+++ date +%s
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END=1592673696
++ [[ 0 != 0 ]]
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+++ date '+%Y-%m-%d %r'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END=1592673696
++ RESULT=74
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ END=1592673696
+ set +x
++ END=1592673696
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ [[ 0 != 0 ]]
+ set +x
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END_FMT='2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ RESULT_NAME=transformer
+ set +x
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ [[ 0 != 0 ]]
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ [[ 0 != 0 ]]
++ END=1592673696
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END=1592673696
+++ date +%s
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ END=1592673696
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ END=1592673696
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ END=1592673696
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ [[ 0 != 0 ]]
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END=1592673696
+++ date +%s
++ END=1592673696
++ END=1592673696
+++ date +%s
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END=1592673696
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
++ RESULT_NAME=transformer
+ set +x
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ END=1592673696
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ END=1592673696
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ [[ 0 != 0 ]]
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ [[ 0 != 0 ]]
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date +%s
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ [[ 0 != 0 ]]
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+++ date +%s
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT_NAME=transformer
++ RESULT=74
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ END_FMT='2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ [[ 0 != 0 ]]
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
+++ date +%s
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT=74
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
+ set +x
++ END=1592673696
+++ date '+%Y-%m-%d %r'
++ END=1592673696
++ END=1592673696
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673696
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:36 AM'
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:36 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:36 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:36 AM'
++ RESULT=74
++ RESULT_NAME=transformer
RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,74,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date +%s
++ END=1592673697
+++ date +%s
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END=1592673697
++ [[ 0 != 0 ]]
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END=1592673697
++ [[ 0 != 0 ]]
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ [[ 0 != 0 ]]
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ [[ 0 != 0 ]]
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ RESULT=75
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ [[ 0 != 0 ]]
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT=75
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ [[ 0 != 0 ]]
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
+++ date +%s
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+++ date +%s
+ set +x
++ RESULT_NAME=transformer
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
+ set +x
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END=1592673697
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END=1592673697
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+++ date '+%Y-%m-%d %r'
++ RESULT=75
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+++ date '+%Y-%m-%d %r'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673697
++ [[ 0 != 0 ]]
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END=1592673697
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END=1592673697
++ END=1592673697
+++ date +%s
++ END=1592673697
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END=1592673697
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
+ set +x
++ [[ 0 != 0 ]]
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ [[ 0 != 0 ]]
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ [[ 0 != 0 ]]
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
slurmstepd: error: _is_a_lwp: open() /proc/172774/status failed: No such file or directory
++ END=1592673697
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ END=1592673697
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ END=1592673697
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END=1592673697
+++ date +%s
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ END=1592673697
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ RESULT=75
++ RESULT_NAME=transformer
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date '+%Y-%m-%d %r'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date '+%Y-%m-%d %r'
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END=1592673697
+ set +x
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
slurmstepd: error: _is_a_lwp: open() /proc/235723/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END=1592673697
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+++ date '+%Y-%m-%d %r'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ [[ 0 != 0 ]]
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ [[ 0 != 0 ]]
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+++ date +%s
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT=75
++ RESULT_NAME=transformer
+++ date '+%Y-%m-%d %r'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END=1592673697
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ END=1592673697
++ RESULT=75
+++ date '+%Y-%m-%d %r'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ RESULT=75
+ set +x
++ RESULT_NAME=transformer
++ [[ 0 != 0 ]]
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END_FMT='2020-06-20 10:21:37 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ [[ 0 != 0 ]]
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+++ date +%s
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date +%s
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END=1592673697
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ END=1592673697
++ END=1592673697
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT_NAME=transformer
++ RESULT=75
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
+ set +x
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
+++ date +%s
+++ date +%s
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+++ date +%s
++ RESULT=75
+++ date +%s
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ END=1592673697
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END=1592673697
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END=1592673697
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
+ set +x
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
+++ date +%s
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
+ set +x
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ [[ 0 != 0 ]]
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
+++ date +%s
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ END_FMT='2020-06-20 10:21:37 AM'
++ RESULT=75
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ END=1592673697
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592673697
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ [[ 0 != 0 ]]
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
++ [[ 0 != 0 ]]
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ [[ 0 != 0 ]]
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
++ END=1592673697
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673697
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:37 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:37 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:37 AM
++ RESULT=75
++ RESULT_NAME=transformer
RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,75,root,2020-06-20 10:20:22 AM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592673698
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-20 10:21:38 AM'
++ echo 'ENDING TIMING RUN AT 2020-06-20 10:21:38 AM'
ENDING TIMING RUN AT 2020-06-20 10:21:38 AM
++ RESULT=76
++ RESULT_NAME=transformer
RESULT,transformer,21280,76,root,2020-06-20 10:20:22 AM
++ echo 'RESULT,transformer,21280,76,root,2020-06-20 10:20:22 AM'
+ set +x
