+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590510399, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592590510429, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592590510429, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592590510429, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592590510429, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n003
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590517625, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/13929695/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=8
+ LR=2.875e-3
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -n 13 ']'
+ '[' 16 -gt 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 16 -gt 1 ']'
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ declare -a CMD
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ '[' -n 0 ']'
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ '[' -n 14 ']'
+ '[' 16 -gt 1 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 12 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 15 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-19 11:15:21 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 4 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592590522994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523410, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523420, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523424, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523440, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523443, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523445, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523445, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523448, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523452, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523467, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590523479, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 844123880
:::MLLOG {"namespace": "", "time_ms": 1592590541761, "event_type": "POINT_IN_TIME", "key": "seed", "value": 844123880, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1088486106
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([3044864]), new_param_packed_fragment.size()=torch.Size([3044864]), master_param_fragment.size()=torch.Size([3044864])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2756160]), new_param_packed_fragment.size()=torch.Size([2756160]), master_param_fragment.size()=torch.Size([2756160])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([2052608]), new_param_packed_fragment.size()=torch.Size([2052608]), master_param_fragment.size()=torch.Size([2052608])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([534976]), new_param_packed_fragment.size()=torch.Size([534976]), master_param_fragment.size()=torch.Size([534976])
model_param_fragment.size()=torch.Size([3948032]), new_param_packed_fragment.size()=torch.Size([3948032]), master_param_fragment.size()=torch.Size([3948032])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4982336]), new_param_packed_fragment.size()=torch.Size([4982336]), master_param_fragment.size()=torch.Size([4982336])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([496128]), new_param_packed_fragment.size()=torch.Size([496128]), master_param_fragment.size()=torch.Size([496128])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([2005568]), new_param_packed_fragment.size()=torch.Size([2005568]), master_param_fragment.size()=torch.Size([2005568])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3698176]), new_param_packed_fragment.size()=torch.Size([3698176]), master_param_fragment.size()=torch.Size([3698176])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2188736]), new_param_packed_fragment.size()=torch.Size([2188736]), master_param_fragment.size()=torch.Size([2188736])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1891840]), new_param_packed_fragment.size()=torch.Size([1891840]), master_param_fragment.size()=torch.Size([1891840])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([246272]), new_param_packed_fragment.size()=torch.Size([246272]), master_param_fragment.size()=torch.Size([246272])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1399296]), new_param_packed_fragment.size()=torch.Size([1399296]), master_param_fragment.size()=torch.Size([1399296])
model_param_fragment.size()=torch.Size([3621888]), new_param_packed_fragment.size()=torch.Size([3621888]), master_param_fragment.size()=torch.Size([3621888])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2141696]), new_param_packed_fragment.size()=torch.Size([2141696]), master_param_fragment.size()=torch.Size([2141696])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2302464]), new_param_packed_fragment.size()=torch.Size([2302464]), master_param_fragment.size()=torch.Size([2302464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2795008]), new_param_packed_fragment.size()=torch.Size([2795008]), master_param_fragment.size()=torch.Size([2795008])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3083712]), new_param_packed_fragment.size()=torch.Size([3083712]), master_param_fragment.size()=torch.Size([3083712])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592590563795, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592590563795, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592590563795, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592590563795, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592590563795, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592590568313, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592590568314, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592590568314, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592590568558, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592590568559, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592590568559, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592590568560, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592590568561, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590568561, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3921060182
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.456 (0.456)	Data 3.26e-01 (3.26e-01)	Tok/s 27753 (27753)	Loss/tok 10.7066 (10.7066)	LR 2.942e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][10/1291]	Time 0.137 (0.123)	Data 1.11e-04 (2.98e-02)	Tok/s 130972 (103803)	Loss/tok 9.8021 (10.2000)	LR 3.619e-05
0: TRAIN [0][20/1291]	Time 0.103 (0.106)	Data 1.17e-04 (1.56e-02)	Tok/s 124976 (108954)	Loss/tok 9.2943 (9.8388)	LR 4.557e-05
0: TRAIN [0][30/1291]	Time 0.072 (0.108)	Data 1.11e-04 (1.06e-02)	Tok/s 109825 (112201)	Loss/tok 8.8865 (9.5665)	LR 5.736e-05
0: TRAIN [0][40/1291]	Time 0.103 (0.105)	Data 1.11e-04 (8.07e-03)	Tok/s 123222 (113031)	Loss/tok 8.6024 (9.3751)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.073 (0.102)	Data 1.13e-04 (6.51e-03)	Tok/s 105298 (112969)	Loss/tok 8.3419 (9.2346)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.103 (0.104)	Data 1.18e-04 (5.46e-03)	Tok/s 122793 (114549)	Loss/tok 8.2629 (9.0559)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.137 (0.104)	Data 1.08e-04 (4.71e-03)	Tok/s 129512 (114891)	Loss/tok 8.2013 (8.9281)	LR 1.408e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][80/1291]	Time 0.073 (0.102)	Data 1.22e-04 (4.14e-03)	Tok/s 105630 (114338)	Loss/tok 7.8612 (8.8301)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.138 (0.103)	Data 1.11e-04 (3.70e-03)	Tok/s 127400 (115121)	Loss/tok 8.0191 (8.7311)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.074 (0.102)	Data 1.11e-04 (3.34e-03)	Tok/s 104479 (115110)	Loss/tok 7.8182 (8.6604)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.073 (0.101)	Data 1.13e-04 (3.05e-03)	Tok/s 107715 (114916)	Loss/tok 7.7339 (8.5965)	LR 3.457e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][120/1291]	Time 0.073 (0.100)	Data 1.09e-04 (2.81e-03)	Tok/s 105540 (114560)	Loss/tok 7.6901 (8.5410)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.073 (0.101)	Data 1.13e-04 (2.60e-03)	Tok/s 106997 (114925)	Loss/tok 7.5821 (8.4819)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.137 (0.099)	Data 1.08e-04 (2.43e-03)	Tok/s 128245 (114323)	Loss/tok 7.7957 (8.4346)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.043 (0.098)	Data 1.11e-04 (2.27e-03)	Tok/s 92382 (113988)	Loss/tok 6.5861 (8.3914)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.103 (0.098)	Data 1.11e-04 (2.14e-03)	Tok/s 120603 (114036)	Loss/tok 7.5752 (8.3420)	LR 1.068e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][170/1291]	Time 0.137 (0.098)	Data 1.13e-04 (2.02e-03)	Tok/s 129435 (114217)	Loss/tok 7.7261 (8.2890)	LR 1.314e-03
0: TRAIN [0][180/1291]	Time 0.072 (0.098)	Data 1.11e-04 (1.92e-03)	Tok/s 106810 (114422)	Loss/tok 6.9560 (8.2331)	LR 1.654e-03
0: TRAIN [0][190/1291]	Time 0.103 (0.099)	Data 1.06e-04 (1.82e-03)	Tok/s 122566 (114596)	Loss/tok 7.2142 (8.1677)	LR 2.083e-03
0: TRAIN [0][200/1291]	Time 0.136 (0.098)	Data 1.08e-04 (1.74e-03)	Tok/s 128570 (114635)	Loss/tok 6.9868 (8.1089)	LR 2.622e-03
0: TRAIN [0][210/1291]	Time 0.072 (0.098)	Data 1.10e-04 (1.66e-03)	Tok/s 108819 (114480)	Loss/tok 6.2981 (8.0505)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.136 (0.098)	Data 1.10e-04 (1.59e-03)	Tok/s 128901 (114588)	Loss/tok 6.6213 (7.9813)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.072 (0.096)	Data 1.08e-04 (1.52e-03)	Tok/s 107818 (114328)	Loss/tok 6.2992 (7.9281)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.072 (0.096)	Data 1.10e-04 (1.47e-03)	Tok/s 108474 (114255)	Loss/tok 6.0030 (7.8626)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.072 (0.096)	Data 1.11e-04 (1.41e-03)	Tok/s 107078 (114249)	Loss/tok 5.9320 (7.7933)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.042 (0.096)	Data 1.12e-04 (1.36e-03)	Tok/s 92947 (114127)	Loss/tok 5.1034 (7.7325)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.042 (0.095)	Data 1.08e-04 (1.32e-03)	Tok/s 95549 (114012)	Loss/tok 4.8359 (7.6705)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.103 (0.095)	Data 1.13e-04 (1.27e-03)	Tok/s 120134 (114114)	Loss/tok 5.7070 (7.6010)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.103 (0.095)	Data 1.11e-04 (1.23e-03)	Tok/s 120071 (114133)	Loss/tok 5.7076 (7.5372)	LR 2.875e-03
0: Upscaling, new scale: 64.0
0: TRAIN [0][300/1291]	Time 0.103 (0.095)	Data 1.16e-04 (1.20e-03)	Tok/s 122813 (114077)	Loss/tok 5.4961 (7.4756)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.072 (0.094)	Data 1.12e-04 (1.16e-03)	Tok/s 107238 (113852)	Loss/tok 5.2106 (7.4229)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.136 (0.094)	Data 1.12e-04 (1.13e-03)	Tok/s 126547 (113967)	Loss/tok 5.5794 (7.3539)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.103 (0.094)	Data 1.14e-04 (1.10e-03)	Tok/s 121168 (114044)	Loss/tok 5.1456 (7.2857)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.072 (0.094)	Data 1.18e-04 (1.07e-03)	Tok/s 106715 (114100)	Loss/tok 4.8003 (7.2159)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.137 (0.094)	Data 1.13e-04 (1.04e-03)	Tok/s 127256 (114095)	Loss/tok 5.0457 (7.1505)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.042 (0.094)	Data 1.12e-04 (1.02e-03)	Tok/s 92052 (113952)	Loss/tok 3.8891 (7.0949)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.072 (0.094)	Data 1.10e-04 (9.92e-04)	Tok/s 108997 (113919)	Loss/tok 4.6734 (7.0343)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.076 (0.094)	Data 1.28e-04 (9.69e-04)	Tok/s 101056 (113840)	Loss/tok 4.3472 (6.9786)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.072 (0.094)	Data 1.11e-04 (9.47e-04)	Tok/s 106824 (113795)	Loss/tok 4.3977 (6.9233)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.137 (0.094)	Data 1.13e-04 (9.27e-04)	Tok/s 127250 (113897)	Loss/tok 4.8005 (6.8578)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.072 (0.094)	Data 1.09e-04 (9.07e-04)	Tok/s 104945 (113824)	Loss/tok 4.1096 (6.8069)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][420/1291]	Time 0.072 (0.094)	Data 1.18e-04 (8.88e-04)	Tok/s 109503 (113837)	Loss/tok 4.1427 (6.7522)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.073 (0.093)	Data 1.15e-04 (8.70e-04)	Tok/s 106720 (113721)	Loss/tok 4.1313 (6.7059)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.103 (0.093)	Data 1.08e-04 (8.53e-04)	Tok/s 124816 (113712)	Loss/tok 4.5803 (6.6566)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.072 (0.093)	Data 1.10e-04 (8.36e-04)	Tok/s 108047 (113697)	Loss/tok 4.0489 (6.6052)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.042 (0.093)	Data 1.12e-04 (8.21e-04)	Tok/s 95643 (113575)	Loss/tok 3.5592 (6.5629)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.137 (0.093)	Data 1.12e-04 (8.06e-04)	Tok/s 126534 (113628)	Loss/tok 4.5561 (6.5124)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.103 (0.093)	Data 1.09e-04 (7.91e-04)	Tok/s 119831 (113647)	Loss/tok 4.4390 (6.4664)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.072 (0.092)	Data 1.12e-04 (7.78e-04)	Tok/s 107941 (113609)	Loss/tok 4.0230 (6.4248)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.072 (0.093)	Data 1.08e-04 (7.64e-04)	Tok/s 106939 (113649)	Loss/tok 3.9173 (6.3783)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.072 (0.093)	Data 1.04e-04 (7.52e-04)	Tok/s 112380 (113676)	Loss/tok 3.8249 (6.3343)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.073 (0.093)	Data 1.09e-04 (7.39e-04)	Tok/s 107944 (113777)	Loss/tok 4.0243 (6.2885)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.103 (0.093)	Data 1.09e-04 (7.28e-04)	Tok/s 122834 (113958)	Loss/tok 4.0983 (6.2366)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.072 (0.093)	Data 1.07e-04 (7.16e-04)	Tok/s 108009 (113876)	Loss/tok 3.8190 (6.2013)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][550/1291]	Time 0.104 (0.093)	Data 1.12e-04 (7.05e-04)	Tok/s 121022 (113933)	Loss/tok 4.1685 (6.1597)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.175 (0.093)	Data 1.13e-04 (6.95e-04)	Tok/s 126818 (113991)	Loss/tok 4.3886 (6.1185)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.137 (0.093)	Data 1.12e-04 (6.85e-04)	Tok/s 128615 (113933)	Loss/tok 4.1874 (6.0857)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.073 (0.093)	Data 1.08e-04 (6.75e-04)	Tok/s 105889 (113950)	Loss/tok 3.7041 (6.0496)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.103 (0.093)	Data 1.06e-04 (6.65e-04)	Tok/s 122000 (113964)	Loss/tok 4.1162 (6.0148)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.073 (0.093)	Data 1.07e-04 (6.56e-04)	Tok/s 105082 (113884)	Loss/tok 3.6953 (5.9852)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.072 (0.093)	Data 1.16e-04 (6.47e-04)	Tok/s 107141 (113890)	Loss/tok 3.6760 (5.9516)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.072 (0.093)	Data 1.09e-04 (6.39e-04)	Tok/s 108561 (113852)	Loss/tok 3.6726 (5.9222)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.042 (0.093)	Data 1.11e-04 (6.30e-04)	Tok/s 95375 (113870)	Loss/tok 3.1858 (5.8889)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.138 (0.093)	Data 1.15e-04 (6.22e-04)	Tok/s 124083 (113933)	Loss/tok 4.2367 (5.8554)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.103 (0.093)	Data 1.12e-04 (6.14e-04)	Tok/s 124110 (113938)	Loss/tok 3.7406 (5.8241)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.104 (0.093)	Data 1.09e-04 (6.07e-04)	Tok/s 124583 (113910)	Loss/tok 3.9720 (5.7969)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.137 (0.093)	Data 1.06e-04 (5.99e-04)	Tok/s 127684 (113899)	Loss/tok 4.0684 (5.7683)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][680/1291]	Time 0.073 (0.093)	Data 1.13e-04 (5.92e-04)	Tok/s 108715 (113919)	Loss/tok 3.6826 (5.7398)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.138 (0.093)	Data 1.14e-04 (5.85e-04)	Tok/s 125918 (113927)	Loss/tok 4.1137 (5.7111)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.072 (0.093)	Data 1.17e-04 (5.79e-04)	Tok/s 107371 (113946)	Loss/tok 3.6028 (5.6831)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.042 (0.093)	Data 1.13e-04 (5.72e-04)	Tok/s 93285 (113966)	Loss/tok 2.9223 (5.6550)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.072 (0.093)	Data 1.17e-04 (5.66e-04)	Tok/s 109115 (113970)	Loss/tok 3.4977 (5.6299)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.104 (0.093)	Data 1.11e-04 (5.60e-04)	Tok/s 123484 (113984)	Loss/tok 3.8845 (5.6067)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.072 (0.093)	Data 1.11e-04 (5.54e-04)	Tok/s 105570 (113925)	Loss/tok 3.5069 (5.5857)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.104 (0.093)	Data 1.11e-04 (5.48e-04)	Tok/s 122394 (113864)	Loss/tok 3.7926 (5.5658)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.138 (0.093)	Data 1.10e-04 (5.42e-04)	Tok/s 126861 (113928)	Loss/tok 4.0320 (5.5393)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.104 (0.093)	Data 1.12e-04 (5.36e-04)	Tok/s 122747 (113891)	Loss/tok 3.6966 (5.5189)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.104 (0.093)	Data 1.11e-04 (5.31e-04)	Tok/s 121479 (113850)	Loss/tok 3.7862 (5.4990)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.104 (0.093)	Data 1.15e-04 (5.26e-04)	Tok/s 121952 (113908)	Loss/tok 3.7821 (5.4753)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.137 (0.093)	Data 1.10e-04 (5.20e-04)	Tok/s 126256 (113943)	Loss/tok 4.0514 (5.4528)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][810/1291]	Time 0.042 (0.093)	Data 1.13e-04 (5.15e-04)	Tok/s 92607 (113939)	Loss/tok 2.8711 (5.4316)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][820/1291]	Time 0.073 (0.093)	Data 1.11e-04 (5.11e-04)	Tok/s 107833 (113927)	Loss/tok 3.5366 (5.4114)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.104 (0.093)	Data 1.17e-04 (5.06e-04)	Tok/s 121196 (113975)	Loss/tok 3.8024 (5.3890)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.104 (0.093)	Data 1.10e-04 (5.01e-04)	Tok/s 121370 (113962)	Loss/tok 3.7983 (5.3700)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.073 (0.093)	Data 1.11e-04 (4.96e-04)	Tok/s 104198 (113926)	Loss/tok 3.4077 (5.3522)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.072 (0.093)	Data 1.11e-04 (4.92e-04)	Tok/s 105661 (113939)	Loss/tok 3.3766 (5.3336)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.103 (0.093)	Data 1.14e-04 (4.88e-04)	Tok/s 121788 (113971)	Loss/tok 3.7419 (5.3143)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.073 (0.093)	Data 1.11e-04 (4.83e-04)	Tok/s 106501 (113976)	Loss/tok 3.6074 (5.2963)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.073 (0.093)	Data 1.13e-04 (4.79e-04)	Tok/s 107661 (113918)	Loss/tok 3.4187 (5.2812)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.174 (0.093)	Data 1.10e-04 (4.75e-04)	Tok/s 129111 (113927)	Loss/tok 3.9796 (5.2627)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.175 (0.093)	Data 1.12e-04 (4.71e-04)	Tok/s 128235 (113941)	Loss/tok 4.0431 (5.2448)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.175 (0.093)	Data 1.13e-04 (4.67e-04)	Tok/s 129652 (113986)	Loss/tok 4.1168 (5.2266)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.073 (0.093)	Data 1.12e-04 (4.64e-04)	Tok/s 108542 (114004)	Loss/tok 3.4218 (5.2092)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][940/1291]	Time 0.104 (0.094)	Data 1.17e-04 (4.60e-04)	Tok/s 120317 (114085)	Loss/tok 3.7259 (5.1897)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.073 (0.094)	Data 1.10e-04 (4.56e-04)	Tok/s 108923 (114064)	Loss/tok 3.3819 (5.1747)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.073 (0.094)	Data 1.23e-04 (4.53e-04)	Tok/s 106755 (114034)	Loss/tok 3.4187 (5.1596)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.073 (0.093)	Data 1.15e-04 (4.49e-04)	Tok/s 104824 (114021)	Loss/tok 3.5141 (5.1447)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.042 (0.093)	Data 1.14e-04 (4.46e-04)	Tok/s 91804 (113995)	Loss/tok 2.8146 (5.1300)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.104 (0.094)	Data 1.08e-04 (4.42e-04)	Tok/s 121757 (114012)	Loss/tok 3.5814 (5.1149)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.042 (0.093)	Data 1.10e-04 (4.39e-04)	Tok/s 94758 (113974)	Loss/tok 2.9712 (5.1021)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.073 (0.093)	Data 1.16e-04 (4.36e-04)	Tok/s 106146 (113944)	Loss/tok 3.5275 (5.0889)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.073 (0.093)	Data 1.15e-04 (4.33e-04)	Tok/s 106317 (113937)	Loss/tok 3.3972 (5.0752)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.073 (0.093)	Data 1.12e-04 (4.30e-04)	Tok/s 106963 (113903)	Loss/tok 3.4164 (5.0626)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.138 (0.093)	Data 1.10e-04 (4.27e-04)	Tok/s 127171 (113918)	Loss/tok 3.9051 (5.0480)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1050/1291]	Time 0.072 (0.093)	Data 1.09e-04 (4.24e-04)	Tok/s 103418 (113932)	Loss/tok 3.3547 (5.0341)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.073 (0.093)	Data 1.18e-04 (4.21e-04)	Tok/s 105831 (113935)	Loss/tok 3.4317 (5.0213)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.073 (0.093)	Data 1.12e-04 (4.18e-04)	Tok/s 108588 (113930)	Loss/tok 3.3781 (5.0087)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.104 (0.093)	Data 1.13e-04 (4.15e-04)	Tok/s 122358 (113978)	Loss/tok 3.5472 (4.9938)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.073 (0.093)	Data 1.15e-04 (4.12e-04)	Tok/s 107596 (113940)	Loss/tok 3.3512 (4.9827)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.104 (0.093)	Data 1.16e-04 (4.10e-04)	Tok/s 119999 (113969)	Loss/tok 3.5361 (4.9694)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.073 (0.093)	Data 1.15e-04 (4.07e-04)	Tok/s 107998 (113930)	Loss/tok 3.3544 (4.9588)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.104 (0.093)	Data 1.12e-04 (4.04e-04)	Tok/s 120774 (113915)	Loss/tok 3.5472 (4.9469)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.042 (0.093)	Data 1.16e-04 (4.02e-04)	Tok/s 94592 (113900)	Loss/tok 2.9577 (4.9355)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.073 (0.093)	Data 1.14e-04 (3.99e-04)	Tok/s 106472 (113882)	Loss/tok 3.3125 (4.9244)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.104 (0.093)	Data 1.12e-04 (3.97e-04)	Tok/s 121139 (113870)	Loss/tok 3.6417 (4.9137)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.072 (0.093)	Data 1.12e-04 (3.94e-04)	Tok/s 108641 (113878)	Loss/tok 3.3210 (4.9019)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.104 (0.093)	Data 1.12e-04 (3.92e-04)	Tok/s 122707 (113889)	Loss/tok 3.5603 (4.8904)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1180/1291]	Time 0.176 (0.093)	Data 1.13e-04 (3.89e-04)	Tok/s 128606 (113902)	Loss/tok 3.9305 (4.8789)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.073 (0.093)	Data 1.11e-04 (3.87e-04)	Tok/s 105928 (113866)	Loss/tok 3.3758 (4.8693)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.073 (0.093)	Data 1.12e-04 (3.85e-04)	Tok/s 108298 (113853)	Loss/tok 3.2763 (4.8589)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.073 (0.093)	Data 1.18e-04 (3.83e-04)	Tok/s 107204 (113847)	Loss/tok 3.5126 (4.8486)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.073 (0.093)	Data 1.13e-04 (3.80e-04)	Tok/s 105901 (113870)	Loss/tok 3.3104 (4.8378)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.072 (0.093)	Data 1.63e-04 (3.78e-04)	Tok/s 107291 (113879)	Loss/tok 3.3267 (4.8271)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.073 (0.093)	Data 1.15e-04 (3.76e-04)	Tok/s 106276 (113838)	Loss/tok 3.3014 (4.8183)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.073 (0.093)	Data 1.11e-04 (3.74e-04)	Tok/s 105288 (113856)	Loss/tok 3.2587 (4.8076)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.073 (0.093)	Data 1.10e-04 (3.72e-04)	Tok/s 105469 (113890)	Loss/tok 3.4137 (4.7967)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.104 (0.093)	Data 1.60e-04 (3.70e-04)	Tok/s 121224 (113905)	Loss/tok 3.5344 (4.7863)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.104 (0.093)	Data 1.27e-04 (3.68e-04)	Tok/s 120490 (113898)	Loss/tok 3.6720 (4.7768)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.175 (0.094)	Data 5.36e-05 (3.68e-04)	Tok/s 126239 (113926)	Loss/tok 3.8965 (4.7662)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590689852, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590689853, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.487 (0.487)	Decoder iters 149.0 (149.0)	Tok/s 18610 (18610)
0: Running moses detokenizer
0: BLEU(score=19.554524452562266, counts=[34005, 15454, 8128, 4452], totals=[64296, 61293, 58291, 55293], precisions=[52.88820455393804, 25.213319628668852, 13.943833524900928, 8.051652107861754], bp=0.9941072651871035, sys_len=64296, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590691065, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1955, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590691065, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7707	Test BLEU: 19.55
0: Performance: Epoch: 0	Training: 1821872 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592590691065, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590691065, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590691065, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1343855048
0: TRAIN [1][0/1291]	Time 0.393 (0.393)	Data 2.87e-01 (2.87e-01)	Tok/s 19370 (19370)	Loss/tok 3.1714 (3.1714)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][10/1291]	Time 0.073 (0.134)	Data 1.18e-04 (2.62e-02)	Tok/s 108061 (109521)	Loss/tok 3.2385 (3.5511)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.074 (0.116)	Data 1.15e-04 (1.38e-02)	Tok/s 105318 (111954)	Loss/tok 3.3002 (3.5185)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.073 (0.108)	Data 1.19e-04 (9.36e-03)	Tok/s 108474 (112206)	Loss/tok 3.1831 (3.5102)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.104 (0.110)	Data 1.15e-04 (7.11e-03)	Tok/s 122013 (114554)	Loss/tok 3.5281 (3.5284)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.104 (0.109)	Data 1.18e-04 (5.74e-03)	Tok/s 123540 (115246)	Loss/tok 3.5497 (3.5317)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.073 (0.107)	Data 1.17e-04 (4.82e-03)	Tok/s 109337 (114935)	Loss/tok 3.3960 (3.5336)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.104 (0.107)	Data 1.24e-04 (4.15e-03)	Tok/s 122231 (115272)	Loss/tok 3.4971 (3.5458)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.073 (0.106)	Data 1.16e-04 (3.66e-03)	Tok/s 106576 (115134)	Loss/tok 3.3432 (3.5427)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.104 (0.106)	Data 1.31e-04 (3.27e-03)	Tok/s 120602 (115381)	Loss/tok 3.5092 (3.5405)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.104 (0.103)	Data 1.18e-04 (2.95e-03)	Tok/s 121211 (114751)	Loss/tok 3.4504 (3.5316)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.073 (0.102)	Data 1.06e-04 (2.70e-03)	Tok/s 105188 (114200)	Loss/tok 3.3110 (3.5246)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.073 (0.100)	Data 1.11e-04 (2.49e-03)	Tok/s 103710 (113992)	Loss/tok 3.2850 (3.5200)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][130/1291]	Time 0.073 (0.099)	Data 1.21e-04 (2.31e-03)	Tok/s 105080 (113778)	Loss/tok 3.1645 (3.5189)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.072 (0.099)	Data 1.22e-04 (2.15e-03)	Tok/s 106886 (113984)	Loss/tok 3.2662 (3.5117)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.073 (0.099)	Data 1.29e-04 (2.02e-03)	Tok/s 108355 (114268)	Loss/tok 3.2311 (3.5130)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.043 (0.099)	Data 1.30e-04 (1.91e-03)	Tok/s 93214 (114193)	Loss/tok 2.8451 (3.5084)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.138 (0.098)	Data 1.87e-04 (1.80e-03)	Tok/s 125623 (113857)	Loss/tok 3.6848 (3.5078)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.105 (0.097)	Data 1.11e-04 (1.71e-03)	Tok/s 118034 (113540)	Loss/tok 3.5009 (3.5011)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.104 (0.098)	Data 1.16e-04 (1.63e-03)	Tok/s 120626 (113719)	Loss/tok 3.3777 (3.5003)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.074 (0.098)	Data 1.85e-04 (1.55e-03)	Tok/s 101101 (113676)	Loss/tok 3.3464 (3.5022)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.074 (0.097)	Data 1.15e-04 (1.49e-03)	Tok/s 103158 (113454)	Loss/tok 3.3851 (3.5004)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.105 (0.097)	Data 1.76e-04 (1.43e-03)	Tok/s 120493 (113444)	Loss/tok 3.3928 (3.4981)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.043 (0.096)	Data 1.18e-04 (1.37e-03)	Tok/s 91274 (113339)	Loss/tok 2.8641 (3.4952)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.043 (0.096)	Data 1.17e-04 (1.32e-03)	Tok/s 91707 (113088)	Loss/tok 2.8037 (3.4912)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.105 (0.096)	Data 1.20e-04 (1.27e-03)	Tok/s 119785 (112959)	Loss/tok 3.4508 (3.4896)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][260/1291]	Time 0.176 (0.096)	Data 1.70e-04 (1.23e-03)	Tok/s 128118 (112954)	Loss/tok 3.7323 (3.4900)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.043 (0.096)	Data 1.28e-04 (1.19e-03)	Tok/s 90945 (113102)	Loss/tok 2.8053 (3.4940)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.074 (0.097)	Data 1.17e-04 (1.15e-03)	Tok/s 105307 (113049)	Loss/tok 3.2909 (3.4959)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.105 (0.096)	Data 1.70e-04 (1.11e-03)	Tok/s 120607 (112904)	Loss/tok 3.4526 (3.4929)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.105 (0.096)	Data 1.34e-04 (1.08e-03)	Tok/s 122952 (112912)	Loss/tok 3.5061 (3.4899)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.043 (0.096)	Data 1.91e-04 (1.05e-03)	Tok/s 90226 (112903)	Loss/tok 2.8224 (3.4868)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.104 (0.096)	Data 1.19e-04 (1.02e-03)	Tok/s 118681 (112893)	Loss/tok 3.7509 (3.4896)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.104 (0.096)	Data 1.80e-04 (9.97e-04)	Tok/s 122049 (112949)	Loss/tok 3.4646 (3.4906)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.074 (0.096)	Data 1.15e-04 (9.71e-04)	Tok/s 106669 (112834)	Loss/tok 3.0956 (3.4865)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.074 (0.096)	Data 1.16e-04 (9.47e-04)	Tok/s 107587 (113007)	Loss/tok 3.1858 (3.4873)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.105 (0.096)	Data 1.98e-04 (9.25e-04)	Tok/s 119866 (112990)	Loss/tok 3.5069 (3.4852)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][370/1291]	Time 0.138 (0.097)	Data 1.76e-04 (9.04e-04)	Tok/s 125944 (113160)	Loss/tok 3.6346 (3.4897)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.105 (0.097)	Data 1.76e-04 (8.84e-04)	Tok/s 118150 (113177)	Loss/tok 3.5180 (3.4881)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.105 (0.097)	Data 1.15e-04 (8.65e-04)	Tok/s 120911 (113225)	Loss/tok 3.6693 (3.4877)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.074 (0.097)	Data 1.26e-04 (8.46e-04)	Tok/s 104342 (113202)	Loss/tok 3.2174 (3.4870)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.105 (0.097)	Data 1.16e-04 (8.29e-04)	Tok/s 120226 (113306)	Loss/tok 3.3857 (3.4878)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.074 (0.097)	Data 1.16e-04 (8.12e-04)	Tok/s 104645 (113385)	Loss/tok 3.1307 (3.4868)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.043 (0.096)	Data 1.09e-04 (7.97e-04)	Tok/s 94384 (113220)	Loss/tok 2.7518 (3.4827)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.074 (0.097)	Data 1.71e-04 (7.82e-04)	Tok/s 105338 (113236)	Loss/tok 3.2168 (3.4834)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.074 (0.096)	Data 1.67e-04 (7.67e-04)	Tok/s 101966 (113202)	Loss/tok 3.2282 (3.4806)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.074 (0.096)	Data 1.10e-04 (7.53e-04)	Tok/s 105711 (113078)	Loss/tok 3.2001 (3.4783)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.104 (0.096)	Data 1.11e-04 (7.41e-04)	Tok/s 120886 (113102)	Loss/tok 3.2545 (3.4776)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.073 (0.096)	Data 1.81e-04 (7.28e-04)	Tok/s 104727 (113087)	Loss/tok 3.2223 (3.4766)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][490/1291]	Time 0.137 (0.096)	Data 1.22e-04 (7.16e-04)	Tok/s 129810 (113213)	Loss/tok 3.5534 (3.4760)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.073 (0.096)	Data 1.81e-04 (7.05e-04)	Tok/s 106644 (113224)	Loss/tok 3.2704 (3.4760)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.042 (0.096)	Data 1.91e-04 (6.94e-04)	Tok/s 92457 (113150)	Loss/tok 2.8076 (3.4742)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.073 (0.096)	Data 1.27e-04 (6.83e-04)	Tok/s 107953 (113141)	Loss/tok 3.2524 (3.4737)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.104 (0.095)	Data 1.15e-04 (6.72e-04)	Tok/s 121665 (113022)	Loss/tok 3.5490 (3.4708)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.138 (0.096)	Data 1.45e-04 (6.62e-04)	Tok/s 125711 (113085)	Loss/tok 3.7571 (3.4733)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.073 (0.096)	Data 1.14e-04 (6.53e-04)	Tok/s 108241 (113083)	Loss/tok 3.1395 (3.4722)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.042 (0.095)	Data 1.16e-04 (6.44e-04)	Tok/s 93715 (112951)	Loss/tok 2.8413 (3.4703)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.042 (0.095)	Data 1.11e-04 (6.35e-04)	Tok/s 94071 (112968)	Loss/tok 2.7291 (3.4699)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.104 (0.095)	Data 1.14e-04 (6.26e-04)	Tok/s 123066 (113003)	Loss/tok 3.4057 (3.4686)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.104 (0.095)	Data 1.26e-04 (6.18e-04)	Tok/s 121431 (113146)	Loss/tok 3.3777 (3.4700)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.073 (0.095)	Data 1.14e-04 (6.10e-04)	Tok/s 105857 (113134)	Loss/tok 3.2974 (3.4688)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.138 (0.096)	Data 1.30e-04 (6.02e-04)	Tok/s 127775 (113229)	Loss/tok 3.6569 (3.4694)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][620/1291]	Time 0.138 (0.095)	Data 1.70e-04 (5.95e-04)	Tok/s 126387 (113203)	Loss/tok 3.5687 (3.4679)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.138 (0.096)	Data 1.76e-04 (5.88e-04)	Tok/s 125168 (113267)	Loss/tok 3.5876 (3.4691)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.104 (0.095)	Data 1.31e-04 (5.81e-04)	Tok/s 122594 (113228)	Loss/tok 3.5516 (3.4673)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.073 (0.095)	Data 1.16e-04 (5.74e-04)	Tok/s 102308 (113179)	Loss/tok 3.1250 (3.4649)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.104 (0.095)	Data 1.16e-04 (5.67e-04)	Tok/s 125244 (113239)	Loss/tok 3.2949 (3.4638)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][670/1291]	Time 0.042 (0.095)	Data 1.16e-04 (5.61e-04)	Tok/s 97118 (113287)	Loss/tok 2.8006 (3.4642)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.073 (0.095)	Data 1.18e-04 (5.54e-04)	Tok/s 105231 (113273)	Loss/tok 3.1207 (3.4636)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.042 (0.095)	Data 1.14e-04 (5.48e-04)	Tok/s 94161 (113206)	Loss/tok 2.6915 (3.4623)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.104 (0.095)	Data 1.20e-04 (5.42e-04)	Tok/s 120212 (113155)	Loss/tok 3.3120 (3.4614)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.073 (0.095)	Data 1.16e-04 (5.37e-04)	Tok/s 108595 (113172)	Loss/tok 3.3860 (3.4607)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.138 (0.095)	Data 1.40e-04 (5.31e-04)	Tok/s 128751 (113187)	Loss/tok 3.5469 (3.4599)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.139 (0.095)	Data 1.19e-04 (5.26e-04)	Tok/s 127128 (113185)	Loss/tok 3.5974 (3.4587)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.138 (0.096)	Data 1.74e-04 (5.21e-04)	Tok/s 129156 (113313)	Loss/tok 3.6133 (3.4613)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.073 (0.095)	Data 1.18e-04 (5.15e-04)	Tok/s 105944 (113296)	Loss/tok 3.1838 (3.4605)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.104 (0.095)	Data 1.73e-04 (5.10e-04)	Tok/s 122934 (113292)	Loss/tok 3.3987 (3.4605)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.073 (0.095)	Data 1.17e-04 (5.05e-04)	Tok/s 107036 (113226)	Loss/tok 3.3006 (3.4591)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.138 (0.095)	Data 1.08e-04 (5.01e-04)	Tok/s 126430 (113225)	Loss/tok 3.6946 (3.4583)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.073 (0.095)	Data 1.80e-04 (4.96e-04)	Tok/s 106447 (113216)	Loss/tok 3.2834 (3.4567)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][800/1291]	Time 0.104 (0.095)	Data 1.14e-04 (4.92e-04)	Tok/s 121898 (113185)	Loss/tok 3.4492 (3.4558)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.138 (0.095)	Data 1.19e-04 (4.87e-04)	Tok/s 126293 (113146)	Loss/tok 3.6004 (3.4543)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.073 (0.095)	Data 1.16e-04 (4.83e-04)	Tok/s 106018 (113147)	Loss/tok 3.2356 (3.4531)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.176 (0.095)	Data 1.12e-04 (4.79e-04)	Tok/s 128462 (113148)	Loss/tok 3.5997 (3.4520)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.104 (0.095)	Data 1.91e-04 (4.75e-04)	Tok/s 123314 (113181)	Loss/tok 3.4547 (3.4517)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.138 (0.095)	Data 1.15e-04 (4.70e-04)	Tok/s 127700 (113243)	Loss/tok 3.5376 (3.4511)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.104 (0.095)	Data 1.26e-04 (4.67e-04)	Tok/s 121135 (113272)	Loss/tok 3.3986 (3.4503)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.104 (0.095)	Data 1.75e-04 (4.63e-04)	Tok/s 123037 (113316)	Loss/tok 3.2499 (3.4498)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.073 (0.095)	Data 1.13e-04 (4.59e-04)	Tok/s 107068 (113250)	Loss/tok 3.1963 (3.4480)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.104 (0.095)	Data 1.13e-04 (4.56e-04)	Tok/s 123198 (113246)	Loss/tok 3.4243 (3.4468)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.176 (0.095)	Data 1.23e-04 (4.52e-04)	Tok/s 125340 (113249)	Loss/tok 3.8028 (3.4463)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.104 (0.095)	Data 1.79e-04 (4.49e-04)	Tok/s 119218 (113231)	Loss/tok 3.4528 (3.4450)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.074 (0.095)	Data 1.82e-04 (4.45e-04)	Tok/s 107750 (113192)	Loss/tok 3.1462 (3.4436)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][930/1291]	Time 0.074 (0.095)	Data 1.16e-04 (4.42e-04)	Tok/s 102336 (113156)	Loss/tok 3.2514 (3.4432)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.105 (0.094)	Data 1.14e-04 (4.39e-04)	Tok/s 119713 (113117)	Loss/tok 3.5088 (3.4431)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.074 (0.094)	Data 1.96e-04 (4.36e-04)	Tok/s 101245 (113092)	Loss/tok 3.2282 (3.4412)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][960/1291]	Time 0.043 (0.094)	Data 1.64e-04 (4.32e-04)	Tok/s 90634 (113050)	Loss/tok 2.6532 (3.4400)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.105 (0.094)	Data 1.73e-04 (4.29e-04)	Tok/s 117526 (113089)	Loss/tok 3.4022 (3.4402)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.178 (0.094)	Data 1.71e-04 (4.26e-04)	Tok/s 124996 (113049)	Loss/tok 3.7579 (3.4392)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.074 (0.094)	Data 1.17e-04 (4.23e-04)	Tok/s 103694 (113041)	Loss/tok 3.2694 (3.4389)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.104 (0.094)	Data 1.19e-04 (4.21e-04)	Tok/s 122589 (113030)	Loss/tok 3.3178 (3.4380)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.073 (0.094)	Data 1.17e-04 (4.18e-04)	Tok/s 106727 (113008)	Loss/tok 3.2245 (3.4372)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.104 (0.094)	Data 1.85e-04 (4.15e-04)	Tok/s 118989 (112974)	Loss/tok 3.4300 (3.4358)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.073 (0.094)	Data 1.16e-04 (4.12e-04)	Tok/s 105359 (112962)	Loss/tok 3.1987 (3.4349)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.073 (0.094)	Data 1.79e-04 (4.10e-04)	Tok/s 106820 (112995)	Loss/tok 3.1690 (3.4342)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.07e-04)	Tok/s 108045 (112980)	Loss/tok 3.0654 (3.4337)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.104 (0.094)	Data 1.29e-04 (4.05e-04)	Tok/s 122522 (113007)	Loss/tok 3.3584 (3.4329)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1070/1291]	Time 0.176 (0.094)	Data 1.21e-04 (4.02e-04)	Tok/s 126152 (113008)	Loss/tok 3.7412 (3.4331)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.104 (0.094)	Data 1.21e-04 (4.00e-04)	Tok/s 122753 (113029)	Loss/tok 3.3623 (3.4333)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.104 (0.094)	Data 1.14e-04 (3.97e-04)	Tok/s 121709 (113088)	Loss/tok 3.3266 (3.4336)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.138 (0.094)	Data 1.19e-04 (3.95e-04)	Tok/s 126446 (113101)	Loss/tok 3.3671 (3.4325)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.92e-04)	Tok/s 104742 (113092)	Loss/tok 3.1200 (3.4311)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.104 (0.094)	Data 1.19e-04 (3.90e-04)	Tok/s 122019 (113130)	Loss/tok 3.4352 (3.4319)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.073 (0.094)	Data 1.83e-04 (3.88e-04)	Tok/s 106778 (113143)	Loss/tok 3.1524 (3.4308)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.104 (0.094)	Data 1.17e-04 (3.86e-04)	Tok/s 123005 (113101)	Loss/tok 3.3645 (3.4289)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.138 (0.094)	Data 1.37e-04 (3.84e-04)	Tok/s 127413 (113106)	Loss/tok 3.5257 (3.4285)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.104 (0.094)	Data 1.11e-04 (3.82e-04)	Tok/s 122240 (113082)	Loss/tok 3.4499 (3.4276)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.80e-04)	Tok/s 106452 (113088)	Loss/tok 3.1311 (3.4276)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.073 (0.094)	Data 1.16e-04 (3.78e-04)	Tok/s 106319 (113084)	Loss/tok 3.0824 (3.4266)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.104 (0.094)	Data 1.26e-04 (3.75e-04)	Tok/s 122181 (113095)	Loss/tok 3.3700 (3.4262)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1200/1291]	Time 0.104 (0.094)	Data 1.10e-04 (3.74e-04)	Tok/s 121154 (113092)	Loss/tok 3.3258 (3.4251)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.138 (0.094)	Data 1.29e-04 (3.72e-04)	Tok/s 126235 (113132)	Loss/tok 3.4701 (3.4253)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.70e-04)	Tok/s 109660 (113183)	Loss/tok 3.0017 (3.4251)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.104 (0.094)	Data 1.70e-04 (3.68e-04)	Tok/s 121014 (113191)	Loss/tok 3.3393 (3.4242)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.104 (0.094)	Data 1.79e-04 (3.66e-04)	Tok/s 120133 (113228)	Loss/tok 3.4944 (3.4244)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.104 (0.094)	Data 1.14e-04 (3.64e-04)	Tok/s 119320 (113226)	Loss/tok 3.3448 (3.4235)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.104 (0.094)	Data 1.85e-04 (3.62e-04)	Tok/s 120610 (113253)	Loss/tok 3.4811 (3.4234)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.073 (0.094)	Data 1.47e-04 (3.61e-04)	Tok/s 103180 (113230)	Loss/tok 3.2362 (3.4221)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.073 (0.094)	Data 1.16e-04 (3.59e-04)	Tok/s 106486 (113174)	Loss/tok 3.2313 (3.4209)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.073 (0.094)	Data 5.51e-05 (3.59e-04)	Tok/s 106225 (113200)	Loss/tok 3.1132 (3.4207)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590813323, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590813324, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.466 (0.466)	Decoder iters 149.0 (149.0)	Tok/s 18912 (18912)
0: Running moses detokenizer
0: BLEU(score=22.108844885176378, counts=[35732, 17156, 9426, 5380], totals=[64401, 61398, 58395, 55396], precisions=[55.483610502942504, 27.94227825010587, 16.141792961726175, 9.711892555419164], bp=0.9957389839703397, sys_len=64401, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590814484, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2211, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590814484, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4249	Test BLEU: 22.11
0: Performance: Epoch: 1	Training: 1810993 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592590814484, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590814485, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590814485, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3463570464
0: TRAIN [2][0/1291]	Time 0.422 (0.422)	Data 2.88e-01 (2.88e-01)	Tok/s 30070 (30070)	Loss/tok 3.2527 (3.2527)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.073 (0.122)	Data 1.10e-04 (2.63e-02)	Tok/s 107108 (105981)	Loss/tok 3.0234 (3.2575)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.073 (0.111)	Data 1.15e-04 (1.38e-02)	Tok/s 106896 (111204)	Loss/tok 3.0430 (3.2468)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.075 (0.107)	Data 1.13e-04 (9.40e-03)	Tok/s 107436 (112211)	Loss/tok 2.9447 (3.2597)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][40/1291]	Time 0.104 (0.103)	Data 1.89e-04 (7.14e-03)	Tok/s 121759 (111954)	Loss/tok 3.2293 (3.2552)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.104 (0.101)	Data 1.16e-04 (5.76e-03)	Tok/s 120777 (112613)	Loss/tok 3.2769 (3.2446)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.106 (0.100)	Data 1.19e-04 (4.84e-03)	Tok/s 120348 (112824)	Loss/tok 3.2253 (3.2379)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][70/1291]	Time 0.073 (0.098)	Data 1.11e-04 (4.17e-03)	Tok/s 106816 (112367)	Loss/tok 2.9389 (3.2447)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.138 (0.099)	Data 1.12e-04 (3.67e-03)	Tok/s 126143 (112873)	Loss/tok 3.5002 (3.2625)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.073 (0.097)	Data 1.14e-04 (3.28e-03)	Tok/s 109056 (112647)	Loss/tok 2.9744 (3.2502)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.104 (0.095)	Data 1.10e-04 (2.97e-03)	Tok/s 118417 (112231)	Loss/tok 3.3007 (3.2466)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.073 (0.096)	Data 1.10e-04 (2.71e-03)	Tok/s 106681 (112628)	Loss/tok 3.0004 (3.2486)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.073 (0.095)	Data 1.12e-04 (2.49e-03)	Tok/s 105371 (112320)	Loss/tok 3.0826 (3.2472)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.042 (0.094)	Data 1.12e-04 (2.31e-03)	Tok/s 93936 (112077)	Loss/tok 2.6001 (3.2492)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.104 (0.094)	Data 1.13e-04 (2.16e-03)	Tok/s 122383 (112286)	Loss/tok 3.3015 (3.2459)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.138 (0.094)	Data 1.15e-04 (2.02e-03)	Tok/s 127282 (112428)	Loss/tok 3.4809 (3.2455)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.073 (0.093)	Data 1.13e-04 (1.90e-03)	Tok/s 106721 (112183)	Loss/tok 3.1156 (3.2456)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.104 (0.093)	Data 1.11e-04 (1.80e-03)	Tok/s 124765 (112239)	Loss/tok 3.3103 (3.2447)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.138 (0.094)	Data 1.12e-04 (1.70e-03)	Tok/s 125623 (112702)	Loss/tok 3.4174 (3.2516)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.073 (0.094)	Data 1.17e-04 (1.62e-03)	Tok/s 105153 (112684)	Loss/tok 3.0474 (3.2551)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][200/1291]	Time 0.073 (0.094)	Data 1.14e-04 (1.55e-03)	Tok/s 107421 (112777)	Loss/tok 3.0067 (3.2550)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.138 (0.093)	Data 1.09e-04 (1.48e-03)	Tok/s 127901 (112738)	Loss/tok 3.4515 (3.2527)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.042 (0.092)	Data 1.19e-04 (1.42e-03)	Tok/s 92577 (112366)	Loss/tok 2.6978 (3.2476)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.073 (0.092)	Data 1.08e-04 (1.36e-03)	Tok/s 107593 (112258)	Loss/tok 3.1279 (3.2462)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.138 (0.092)	Data 1.10e-04 (1.31e-03)	Tok/s 126078 (112290)	Loss/tok 3.4557 (3.2470)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][250/1291]	Time 0.138 (0.092)	Data 1.10e-04 (1.26e-03)	Tok/s 127045 (112507)	Loss/tok 3.5375 (3.2542)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.073 (0.093)	Data 1.37e-04 (1.22e-03)	Tok/s 107394 (112616)	Loss/tok 3.0153 (3.2571)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.073 (0.093)	Data 1.11e-04 (1.18e-03)	Tok/s 107584 (112655)	Loss/tok 3.0502 (3.2585)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.175 (0.093)	Data 1.22e-04 (1.14e-03)	Tok/s 126651 (112639)	Loss/tok 3.7028 (3.2616)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][290/1291]	Time 0.103 (0.093)	Data 1.17e-04 (1.10e-03)	Tok/s 122030 (112493)	Loss/tok 3.2521 (3.2610)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.073 (0.093)	Data 1.11e-04 (1.07e-03)	Tok/s 104659 (112497)	Loss/tok 2.9678 (3.2632)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.176 (0.093)	Data 1.16e-04 (1.04e-03)	Tok/s 127218 (112545)	Loss/tok 3.5924 (3.2660)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.073 (0.094)	Data 1.14e-04 (1.01e-03)	Tok/s 105115 (112662)	Loss/tok 3.0944 (3.2730)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.073 (0.094)	Data 1.12e-04 (9.83e-04)	Tok/s 106767 (112770)	Loss/tok 3.0812 (3.2746)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.042 (0.094)	Data 1.16e-04 (9.58e-04)	Tok/s 93108 (112763)	Loss/tok 2.6831 (3.2749)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.138 (0.094)	Data 1.12e-04 (9.34e-04)	Tok/s 125586 (112840)	Loss/tok 3.4552 (3.2772)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.073 (0.094)	Data 1.15e-04 (9.11e-04)	Tok/s 106166 (113001)	Loss/tok 3.0088 (3.2776)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.073 (0.095)	Data 1.14e-04 (8.90e-04)	Tok/s 108200 (113124)	Loss/tok 3.0593 (3.2814)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.073 (0.094)	Data 1.13e-04 (8.69e-04)	Tok/s 105801 (113146)	Loss/tok 3.0854 (3.2802)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.138 (0.094)	Data 1.16e-04 (8.50e-04)	Tok/s 127415 (113251)	Loss/tok 3.4688 (3.2798)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.104 (0.095)	Data 1.14e-04 (8.32e-04)	Tok/s 118956 (113377)	Loss/tok 3.2671 (3.2826)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.138 (0.095)	Data 1.09e-04 (8.14e-04)	Tok/s 125785 (113418)	Loss/tok 3.4242 (3.2818)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][420/1291]	Time 0.073 (0.094)	Data 1.12e-04 (7.98e-04)	Tok/s 108601 (113324)	Loss/tok 3.0120 (3.2791)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.104 (0.094)	Data 1.14e-04 (7.82e-04)	Tok/s 120272 (113321)	Loss/tok 3.3124 (3.2785)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.176 (0.095)	Data 1.13e-04 (7.67e-04)	Tok/s 126520 (113511)	Loss/tok 3.6656 (3.2842)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.073 (0.095)	Data 1.08e-04 (7.52e-04)	Tok/s 105267 (113589)	Loss/tok 2.9607 (3.2871)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.138 (0.096)	Data 1.14e-04 (7.38e-04)	Tok/s 125243 (113751)	Loss/tok 3.5161 (3.2885)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.176 (0.096)	Data 1.14e-04 (7.25e-04)	Tok/s 128242 (113757)	Loss/tok 3.5783 (3.2894)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.104 (0.096)	Data 1.28e-04 (7.12e-04)	Tok/s 120324 (113856)	Loss/tok 3.3468 (3.2915)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.042 (0.096)	Data 1.14e-04 (7.00e-04)	Tok/s 93031 (113804)	Loss/tok 2.6813 (3.2920)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.104 (0.096)	Data 1.24e-04 (6.89e-04)	Tok/s 123203 (113751)	Loss/tok 3.2150 (3.2908)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.073 (0.096)	Data 1.21e-04 (6.77e-04)	Tok/s 107894 (113751)	Loss/tok 3.1026 (3.2909)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.104 (0.096)	Data 1.13e-04 (6.66e-04)	Tok/s 122245 (113759)	Loss/tok 3.3273 (3.2919)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.175 (0.096)	Data 1.10e-04 (6.56e-04)	Tok/s 128790 (113759)	Loss/tok 3.6155 (3.2918)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.073 (0.096)	Data 1.21e-04 (6.46e-04)	Tok/s 105531 (113826)	Loss/tok 3.0348 (3.2914)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][550/1291]	Time 0.104 (0.096)	Data 1.11e-04 (6.36e-04)	Tok/s 120578 (113752)	Loss/tok 3.2457 (3.2894)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.138 (0.095)	Data 1.10e-04 (6.27e-04)	Tok/s 124130 (113704)	Loss/tok 3.6083 (3.2895)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][570/1291]	Time 0.073 (0.096)	Data 1.10e-04 (6.18e-04)	Tok/s 105162 (113740)	Loss/tok 2.9262 (3.2894)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.104 (0.095)	Data 1.11e-04 (6.09e-04)	Tok/s 120474 (113758)	Loss/tok 3.2576 (3.2881)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.104 (0.096)	Data 1.32e-04 (6.01e-04)	Tok/s 121870 (113803)	Loss/tok 3.2783 (3.2889)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.073 (0.096)	Data 1.17e-04 (5.93e-04)	Tok/s 108406 (113917)	Loss/tok 3.0588 (3.2889)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.073 (0.095)	Data 1.13e-04 (5.85e-04)	Tok/s 107105 (113783)	Loss/tok 2.9222 (3.2870)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.104 (0.095)	Data 1.12e-04 (5.77e-04)	Tok/s 123824 (113776)	Loss/tok 3.2589 (3.2860)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.073 (0.095)	Data 1.71e-04 (5.70e-04)	Tok/s 104004 (113806)	Loss/tok 3.2260 (3.2881)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.176 (0.095)	Data 1.16e-04 (5.63e-04)	Tok/s 126463 (113768)	Loss/tok 3.5854 (3.2881)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.140 (0.095)	Data 1.15e-04 (5.57e-04)	Tok/s 125843 (113722)	Loss/tok 3.5916 (3.2892)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.073 (0.095)	Data 1.14e-04 (5.51e-04)	Tok/s 105250 (113653)	Loss/tok 3.1224 (3.2887)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.073 (0.095)	Data 1.77e-04 (5.44e-04)	Tok/s 108010 (113564)	Loss/tok 3.0040 (3.2872)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.073 (0.095)	Data 1.27e-04 (5.38e-04)	Tok/s 107187 (113629)	Loss/tok 3.1279 (3.2890)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.104 (0.095)	Data 1.17e-04 (5.33e-04)	Tok/s 121153 (113676)	Loss/tok 3.3822 (3.2901)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][700/1291]	Time 0.042 (0.095)	Data 1.20e-04 (5.27e-04)	Tok/s 89401 (113687)	Loss/tok 2.5153 (3.2891)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][710/1291]	Time 0.073 (0.095)	Data 1.10e-04 (5.21e-04)	Tok/s 107272 (113630)	Loss/tok 3.1547 (3.2880)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.073 (0.095)	Data 1.22e-04 (5.16e-04)	Tok/s 107974 (113552)	Loss/tok 3.1428 (3.2865)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.104 (0.095)	Data 2.05e-04 (5.11e-04)	Tok/s 122441 (113559)	Loss/tok 3.2363 (3.2871)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.073 (0.095)	Data 1.16e-04 (5.06e-04)	Tok/s 107684 (113527)	Loss/tok 2.9957 (3.2874)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.073 (0.095)	Data 1.75e-04 (5.01e-04)	Tok/s 107100 (113553)	Loss/tok 3.0597 (3.2863)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.104 (0.095)	Data 1.96e-04 (4.96e-04)	Tok/s 119969 (113553)	Loss/tok 3.2682 (3.2854)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.104 (0.095)	Data 1.68e-04 (4.92e-04)	Tok/s 121492 (113531)	Loss/tok 3.3120 (3.2853)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.073 (0.095)	Data 1.85e-04 (4.87e-04)	Tok/s 107904 (113555)	Loss/tok 2.9917 (3.2859)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.104 (0.095)	Data 1.26e-04 (4.83e-04)	Tok/s 120199 (113543)	Loss/tok 3.3545 (3.2854)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.073 (0.095)	Data 1.17e-04 (4.78e-04)	Tok/s 106019 (113564)	Loss/tok 3.0995 (3.2868)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.073 (0.095)	Data 1.16e-04 (4.74e-04)	Tok/s 106320 (113604)	Loss/tok 3.0335 (3.2870)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.073 (0.095)	Data 1.26e-04 (4.70e-04)	Tok/s 104303 (113559)	Loss/tok 3.0598 (3.2861)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][830/1291]	Time 0.176 (0.095)	Data 1.29e-04 (4.66e-04)	Tok/s 127024 (113555)	Loss/tok 3.6225 (3.2856)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][840/1291]	Time 0.138 (0.095)	Data 1.10e-04 (4.62e-04)	Tok/s 123224 (113559)	Loss/tok 3.5331 (3.2855)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.073 (0.095)	Data 1.14e-04 (4.58e-04)	Tok/s 106681 (113514)	Loss/tok 2.9084 (3.2846)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.138 (0.095)	Data 1.09e-04 (4.54e-04)	Tok/s 127043 (113551)	Loss/tok 3.4586 (3.2862)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.176 (0.095)	Data 1.21e-04 (4.50e-04)	Tok/s 127676 (113549)	Loss/tok 3.6396 (3.2866)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.104 (0.095)	Data 1.13e-04 (4.46e-04)	Tok/s 121709 (113531)	Loss/tok 3.2485 (3.2856)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.073 (0.094)	Data 1.21e-04 (4.42e-04)	Tok/s 105491 (113445)	Loss/tok 3.0607 (3.2838)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.073 (0.094)	Data 1.13e-04 (4.39e-04)	Tok/s 104235 (113404)	Loss/tok 3.0444 (3.2831)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.073 (0.094)	Data 1.14e-04 (4.35e-04)	Tok/s 105778 (113418)	Loss/tok 2.9600 (3.2839)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.073 (0.094)	Data 1.15e-04 (4.32e-04)	Tok/s 105311 (113445)	Loss/tok 3.1442 (3.2838)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.176 (0.094)	Data 1.11e-04 (4.28e-04)	Tok/s 126500 (113428)	Loss/tok 3.6709 (3.2837)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.042 (0.094)	Data 1.10e-04 (4.25e-04)	Tok/s 94683 (113366)	Loss/tok 2.5574 (3.2832)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.073 (0.094)	Data 1.20e-04 (4.22e-04)	Tok/s 105113 (113340)	Loss/tok 3.1669 (3.2824)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.073 (0.094)	Data 1.13e-04 (4.19e-04)	Tok/s 108277 (113332)	Loss/tok 2.9651 (3.2815)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][970/1291]	Time 0.042 (0.094)	Data 1.16e-04 (4.15e-04)	Tok/s 95930 (113367)	Loss/tok 2.6673 (3.2816)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.104 (0.094)	Data 1.17e-04 (4.12e-04)	Tok/s 121431 (113366)	Loss/tok 3.1723 (3.2820)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.09e-04)	Tok/s 121441 (113368)	Loss/tok 3.3525 (3.2817)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.073 (0.094)	Data 1.16e-04 (4.07e-04)	Tok/s 106400 (113370)	Loss/tok 3.0965 (3.2813)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.104 (0.094)	Data 1.12e-04 (4.04e-04)	Tok/s 121478 (113375)	Loss/tok 3.3200 (3.2810)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.073 (0.094)	Data 1.13e-04 (4.01e-04)	Tok/s 107539 (113323)	Loss/tok 3.1345 (3.2799)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.073 (0.094)	Data 1.16e-04 (3.98e-04)	Tok/s 104665 (113299)	Loss/tok 3.0359 (3.2790)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.95e-04)	Tok/s 107741 (113270)	Loss/tok 3.1145 (3.2783)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.104 (0.094)	Data 1.14e-04 (3.93e-04)	Tok/s 120521 (113318)	Loss/tok 3.3436 (3.2784)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1060/1291]	Time 0.176 (0.094)	Data 1.29e-04 (3.90e-04)	Tok/s 127317 (113336)	Loss/tok 3.7086 (3.2800)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.104 (0.094)	Data 1.12e-04 (3.87e-04)	Tok/s 120484 (113278)	Loss/tok 3.4090 (3.2791)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.138 (0.094)	Data 1.17e-04 (3.85e-04)	Tok/s 125600 (113313)	Loss/tok 3.3593 (3.2791)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.138 (0.094)	Data 1.16e-04 (3.82e-04)	Tok/s 127420 (113350)	Loss/tok 3.4595 (3.2812)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.80e-04)	Tok/s 106725 (113334)	Loss/tok 3.1327 (3.2807)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.138 (0.094)	Data 1.15e-04 (3.78e-04)	Tok/s 129533 (113407)	Loss/tok 3.4401 (3.2820)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.073 (0.094)	Data 1.11e-04 (3.75e-04)	Tok/s 106125 (113365)	Loss/tok 3.0767 (3.2805)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.73e-04)	Tok/s 104460 (113361)	Loss/tok 3.0179 (3.2802)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.073 (0.094)	Data 1.16e-04 (3.71e-04)	Tok/s 103967 (113358)	Loss/tok 3.0275 (3.2809)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.68e-04)	Tok/s 106673 (113312)	Loss/tok 2.9899 (3.2799)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.66e-04)	Tok/s 107309 (113278)	Loss/tok 3.1869 (3.2796)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.104 (0.094)	Data 1.29e-04 (3.64e-04)	Tok/s 118185 (113302)	Loss/tok 3.2253 (3.2792)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.138 (0.094)	Data 1.11e-04 (3.62e-04)	Tok/s 126154 (113280)	Loss/tok 3.7064 (3.2794)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1190/1291]	Time 0.104 (0.094)	Data 1.18e-04 (3.60e-04)	Tok/s 122056 (113287)	Loss/tok 3.0992 (3.2788)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.58e-04)	Tok/s 108150 (113323)	Loss/tok 3.0588 (3.2795)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.042 (0.094)	Data 1.15e-04 (3.56e-04)	Tok/s 91855 (113323)	Loss/tok 2.6355 (3.2797)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1220/1291]	Time 0.104 (0.094)	Data 1.16e-04 (3.54e-04)	Tok/s 121013 (113337)	Loss/tok 3.2829 (3.2797)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.138 (0.094)	Data 1.27e-04 (3.52e-04)	Tok/s 126187 (113331)	Loss/tok 3.5156 (3.2797)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.104 (0.094)	Data 1.12e-04 (3.50e-04)	Tok/s 118714 (113344)	Loss/tok 3.3759 (3.2792)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.138 (0.094)	Data 1.14e-04 (3.48e-04)	Tok/s 126126 (113330)	Loss/tok 3.3016 (3.2787)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.073 (0.094)	Data 1.11e-04 (3.46e-04)	Tok/s 104204 (113336)	Loss/tok 3.0580 (3.2788)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.104 (0.094)	Data 1.09e-04 (3.45e-04)	Tok/s 122999 (113376)	Loss/tok 3.2470 (3.2790)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.073 (0.094)	Data 1.11e-04 (3.43e-04)	Tok/s 108357 (113413)	Loss/tok 2.9876 (3.2797)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.043 (0.094)	Data 5.48e-05 (3.43e-04)	Tok/s 91433 (113413)	Loss/tok 2.7294 (3.2801)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590936527, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590936527, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.315 (0.315)	Decoder iters 84.0 (84.0)	Tok/s 26392 (26392)
0: Running moses detokenizer
0: BLEU(score=21.630161623795082, counts=[34321, 16657, 9226, 5337], totals=[59801, 56798, 53795, 50796], precisions=[57.39201685590542, 29.326736856931582, 17.15029277813923, 10.506732813607371], bp=0.9217139288993109, sys_len=59801, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590937532, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2163, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590937532, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2801	Test BLEU: 21.63
0: Performance: Epoch: 2	Training: 1814546 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592590937532, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590937532, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590937532, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1537319002
0: TRAIN [3][0/1291]	Time 0.414 (0.414)	Data 3.08e-01 (3.08e-01)	Tok/s 18506 (18506)	Loss/tok 2.8539 (2.8539)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.073 (0.116)	Data 1.13e-04 (2.81e-02)	Tok/s 110058 (103076)	Loss/tok 2.9414 (3.1024)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.104 (0.103)	Data 1.12e-04 (1.48e-02)	Tok/s 120348 (107831)	Loss/tok 3.2319 (3.1081)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.104 (0.102)	Data 1.14e-04 (1.00e-02)	Tok/s 120696 (109582)	Loss/tok 3.2104 (3.1553)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.073 (0.100)	Data 1.13e-04 (7.61e-03)	Tok/s 109010 (111072)	Loss/tok 2.9699 (3.1516)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.176 (0.098)	Data 1.13e-04 (6.14e-03)	Tok/s 130328 (111139)	Loss/tok 3.3911 (3.1469)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][60/1291]	Time 0.106 (0.099)	Data 1.12e-04 (5.15e-03)	Tok/s 117137 (111836)	Loss/tok 3.1816 (3.1657)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.104 (0.097)	Data 1.12e-04 (4.44e-03)	Tok/s 121144 (111747)	Loss/tok 3.1698 (3.1549)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.176 (0.097)	Data 1.17e-04 (3.91e-03)	Tok/s 127590 (112345)	Loss/tok 3.5660 (3.1701)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.104 (0.095)	Data 1.14e-04 (3.49e-03)	Tok/s 121213 (112006)	Loss/tok 3.2392 (3.1589)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.042 (0.094)	Data 1.12e-04 (3.16e-03)	Tok/s 94268 (112025)	Loss/tok 2.6254 (3.1616)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.042 (0.094)	Data 1.15e-04 (2.88e-03)	Tok/s 95967 (111937)	Loss/tok 2.5619 (3.1594)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.104 (0.094)	Data 1.14e-04 (2.65e-03)	Tok/s 123746 (112148)	Loss/tok 3.1036 (3.1580)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.104 (0.093)	Data 1.16e-04 (2.46e-03)	Tok/s 124672 (112364)	Loss/tok 3.2291 (3.1549)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.073 (0.094)	Data 1.15e-04 (2.29e-03)	Tok/s 106225 (112705)	Loss/tok 3.0582 (3.1589)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.104 (0.094)	Data 1.16e-04 (2.15e-03)	Tok/s 123701 (112735)	Loss/tok 3.1730 (3.1604)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.176 (0.094)	Data 1.15e-04 (2.02e-03)	Tok/s 124050 (112935)	Loss/tok 3.6944 (3.1662)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.104 (0.095)	Data 1.11e-04 (1.91e-03)	Tok/s 120716 (113218)	Loss/tok 3.2214 (3.1722)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.104 (0.095)	Data 1.08e-04 (1.81e-03)	Tok/s 122383 (113261)	Loss/tok 3.0892 (3.1728)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][190/1291]	Time 0.104 (0.094)	Data 1.10e-04 (1.72e-03)	Tok/s 119994 (113123)	Loss/tok 3.2570 (3.1733)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.104 (0.094)	Data 1.12e-04 (1.64e-03)	Tok/s 121047 (113133)	Loss/tok 3.1853 (3.1720)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.073 (0.094)	Data 1.19e-04 (1.57e-03)	Tok/s 110108 (113284)	Loss/tok 2.8514 (3.1761)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.073 (0.094)	Data 1.24e-04 (1.50e-03)	Tok/s 108404 (113353)	Loss/tok 3.1149 (3.1752)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.104 (0.094)	Data 1.14e-04 (1.44e-03)	Tok/s 121613 (113404)	Loss/tok 3.2199 (3.1761)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.104 (0.094)	Data 1.16e-04 (1.39e-03)	Tok/s 121160 (113335)	Loss/tok 3.0892 (3.1749)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.104 (0.095)	Data 1.14e-04 (1.34e-03)	Tok/s 121338 (113544)	Loss/tok 3.1980 (3.1773)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.104 (0.095)	Data 1.33e-04 (1.29e-03)	Tok/s 122493 (113613)	Loss/tok 3.2113 (3.1790)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.104 (0.095)	Data 1.15e-04 (1.25e-03)	Tok/s 122629 (113684)	Loss/tok 3.1725 (3.1800)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.104 (0.096)	Data 1.13e-04 (1.21e-03)	Tok/s 118853 (113796)	Loss/tok 3.2413 (3.1847)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.104 (0.095)	Data 1.08e-04 (1.17e-03)	Tok/s 123634 (113626)	Loss/tok 3.1195 (3.1835)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.073 (0.095)	Data 1.13e-04 (1.14e-03)	Tok/s 102285 (113507)	Loss/tok 2.9475 (3.1801)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][310/1291]	Time 0.044 (0.095)	Data 1.12e-04 (1.10e-03)	Tok/s 89686 (113527)	Loss/tok 2.6007 (3.1813)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][320/1291]	Time 0.073 (0.095)	Data 1.18e-04 (1.07e-03)	Tok/s 107777 (113662)	Loss/tok 2.8881 (3.1820)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.138 (0.095)	Data 1.12e-04 (1.04e-03)	Tok/s 125791 (113679)	Loss/tok 3.2973 (3.1805)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.104 (0.095)	Data 1.11e-04 (1.02e-03)	Tok/s 121814 (113691)	Loss/tok 3.2046 (3.1816)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.073 (0.095)	Data 1.13e-04 (9.90e-04)	Tok/s 108263 (113668)	Loss/tok 2.9250 (3.1803)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.042 (0.095)	Data 1.12e-04 (9.66e-04)	Tok/s 92515 (113514)	Loss/tok 2.5793 (3.1770)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.104 (0.095)	Data 1.17e-04 (9.43e-04)	Tok/s 121233 (113631)	Loss/tok 3.0331 (3.1771)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.073 (0.095)	Data 1.25e-04 (9.21e-04)	Tok/s 105720 (113607)	Loss/tok 3.0185 (3.1784)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.138 (0.095)	Data 1.16e-04 (9.01e-04)	Tok/s 126332 (113632)	Loss/tok 3.2904 (3.1807)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.073 (0.095)	Data 1.13e-04 (8.81e-04)	Tok/s 106641 (113375)	Loss/tok 2.9382 (3.1765)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.104 (0.094)	Data 1.27e-04 (8.62e-04)	Tok/s 122552 (113276)	Loss/tok 3.0899 (3.1738)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.073 (0.094)	Data 1.18e-04 (8.45e-04)	Tok/s 105316 (113312)	Loss/tok 2.9302 (3.1739)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.104 (0.095)	Data 1.18e-04 (8.28e-04)	Tok/s 121486 (113366)	Loss/tok 3.1018 (3.1748)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.104 (0.094)	Data 1.09e-04 (8.12e-04)	Tok/s 120646 (113322)	Loss/tok 3.3925 (3.1744)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][450/1291]	Time 0.073 (0.095)	Data 1.18e-04 (7.96e-04)	Tok/s 105158 (113380)	Loss/tok 3.0260 (3.1757)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.104 (0.095)	Data 1.12e-04 (7.82e-04)	Tok/s 119438 (113346)	Loss/tok 3.1387 (3.1755)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.073 (0.094)	Data 1.47e-04 (7.68e-04)	Tok/s 106595 (113226)	Loss/tok 2.9216 (3.1746)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.073 (0.094)	Data 1.15e-04 (7.54e-04)	Tok/s 105131 (113158)	Loss/tok 2.8830 (3.1726)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][490/1291]	Time 0.073 (0.094)	Data 1.20e-04 (7.41e-04)	Tok/s 108577 (113180)	Loss/tok 2.9243 (3.1731)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.138 (0.094)	Data 1.14e-04 (7.29e-04)	Tok/s 127937 (113209)	Loss/tok 3.2479 (3.1729)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.073 (0.094)	Data 1.26e-04 (7.17e-04)	Tok/s 107215 (113221)	Loss/tok 3.0377 (3.1729)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.073 (0.094)	Data 1.09e-04 (7.05e-04)	Tok/s 106692 (113161)	Loss/tok 2.9903 (3.1713)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.042 (0.094)	Data 1.11e-04 (6.94e-04)	Tok/s 96513 (113061)	Loss/tok 2.5231 (3.1690)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.042 (0.094)	Data 1.11e-04 (6.83e-04)	Tok/s 92771 (113085)	Loss/tok 2.5576 (3.1696)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.104 (0.094)	Data 1.17e-04 (6.73e-04)	Tok/s 122772 (113048)	Loss/tok 3.1912 (3.1675)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.073 (0.094)	Data 1.14e-04 (6.63e-04)	Tok/s 106959 (113073)	Loss/tok 3.0900 (3.1664)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.104 (0.094)	Data 1.15e-04 (6.54e-04)	Tok/s 119535 (113112)	Loss/tok 3.1892 (3.1665)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.104 (0.093)	Data 1.12e-04 (6.44e-04)	Tok/s 120013 (113062)	Loss/tok 3.3336 (3.1655)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.073 (0.093)	Data 1.12e-04 (6.35e-04)	Tok/s 107479 (112984)	Loss/tok 2.9670 (3.1637)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.176 (0.093)	Data 1.13e-04 (6.27e-04)	Tok/s 126449 (113016)	Loss/tok 3.5088 (3.1650)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][610/1291]	Time 0.138 (0.094)	Data 1.15e-04 (6.18e-04)	Tok/s 127207 (113110)	Loss/tok 3.3236 (3.1653)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.073 (0.093)	Data 1.15e-04 (6.10e-04)	Tok/s 104247 (113071)	Loss/tok 2.8589 (3.1647)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.073 (0.093)	Data 1.10e-04 (6.02e-04)	Tok/s 104823 (112978)	Loss/tok 2.9048 (3.1635)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.139 (0.093)	Data 1.13e-04 (5.95e-04)	Tok/s 126552 (113053)	Loss/tok 3.2559 (3.1655)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.073 (0.094)	Data 1.11e-04 (5.87e-04)	Tok/s 106245 (113054)	Loss/tok 2.8515 (3.1659)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.80e-04)	Tok/s 106818 (113074)	Loss/tok 2.9010 (3.1647)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.73e-04)	Tok/s 105060 (113098)	Loss/tok 2.9559 (3.1646)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.042 (0.094)	Data 1.11e-04 (5.66e-04)	Tok/s 93367 (113094)	Loss/tok 2.5398 (3.1644)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.073 (0.093)	Data 1.11e-04 (5.60e-04)	Tok/s 106682 (113026)	Loss/tok 3.0434 (3.1629)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.073 (0.093)	Data 1.16e-04 (5.53e-04)	Tok/s 106541 (113022)	Loss/tok 2.9521 (3.1621)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.176 (0.093)	Data 1.15e-04 (5.47e-04)	Tok/s 125684 (113046)	Loss/tok 3.6405 (3.1630)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.104 (0.094)	Data 1.29e-04 (5.41e-04)	Tok/s 123757 (113056)	Loss/tok 3.0644 (3.1626)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.073 (0.093)	Data 1.14e-04 (5.35e-04)	Tok/s 103359 (113012)	Loss/tok 2.9935 (3.1609)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][740/1291]	Time 0.073 (0.093)	Data 1.15e-04 (5.30e-04)	Tok/s 107387 (113010)	Loss/tok 2.9311 (3.1608)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.104 (0.094)	Data 1.16e-04 (5.24e-04)	Tok/s 119991 (113055)	Loss/tok 3.1326 (3.1607)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.042 (0.094)	Data 1.16e-04 (5.19e-04)	Tok/s 92583 (113036)	Loss/tok 2.5801 (3.1602)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.073 (0.094)	Data 1.13e-04 (5.13e-04)	Tok/s 107544 (113061)	Loss/tok 2.8572 (3.1598)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.073 (0.094)	Data 1.10e-04 (5.08e-04)	Tok/s 110488 (113135)	Loss/tok 2.8363 (3.1598)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.073 (0.094)	Data 1.16e-04 (5.03e-04)	Tok/s 107761 (113152)	Loss/tok 2.8660 (3.1595)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.073 (0.094)	Data 1.15e-04 (4.98e-04)	Tok/s 108931 (113194)	Loss/tok 2.9990 (3.1596)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.104 (0.094)	Data 1.18e-04 (4.94e-04)	Tok/s 122517 (113211)	Loss/tok 3.1438 (3.1596)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.042 (0.094)	Data 1.13e-04 (4.89e-04)	Tok/s 93890 (113217)	Loss/tok 2.6296 (3.1593)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.85e-04)	Tok/s 104791 (113189)	Loss/tok 3.0572 (3.1585)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.138 (0.094)	Data 1.15e-04 (4.80e-04)	Tok/s 128297 (113218)	Loss/tok 3.2759 (3.1578)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.76e-04)	Tok/s 107105 (113241)	Loss/tok 2.9979 (3.1576)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.104 (0.094)	Data 1.19e-04 (4.72e-04)	Tok/s 121259 (113300)	Loss/tok 3.1054 (3.1586)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][870/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.68e-04)	Tok/s 121127 (113364)	Loss/tok 3.1570 (3.1596)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.073 (0.094)	Data 1.13e-04 (4.64e-04)	Tok/s 106130 (113331)	Loss/tok 2.9554 (3.1585)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.60e-04)	Tok/s 105995 (113351)	Loss/tok 2.9164 (3.1579)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.138 (0.094)	Data 1.28e-04 (4.56e-04)	Tok/s 126163 (113382)	Loss/tok 3.2750 (3.1579)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.042 (0.094)	Data 1.13e-04 (4.52e-04)	Tok/s 92380 (113399)	Loss/tok 2.5096 (3.1574)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][920/1291]	Time 0.176 (0.094)	Data 1.28e-04 (4.48e-04)	Tok/s 122404 (113437)	Loss/tok 3.5366 (3.1596)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.104 (0.094)	Data 1.08e-04 (4.45e-04)	Tok/s 122243 (113463)	Loss/tok 3.0697 (3.1590)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.176 (0.094)	Data 1.17e-04 (4.41e-04)	Tok/s 127969 (113460)	Loss/tok 3.3618 (3.1582)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.104 (0.094)	Data 1.05e-04 (4.38e-04)	Tok/s 121589 (113434)	Loss/tok 3.1331 (3.1569)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.073 (0.094)	Data 1.11e-04 (4.34e-04)	Tok/s 106971 (113395)	Loss/tok 2.9265 (3.1560)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.042 (0.094)	Data 1.14e-04 (4.31e-04)	Tok/s 94122 (113377)	Loss/tok 2.4953 (3.1560)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.073 (0.094)	Data 1.09e-04 (4.28e-04)	Tok/s 109157 (113371)	Loss/tok 3.0732 (3.1557)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.138 (0.094)	Data 1.19e-04 (4.25e-04)	Tok/s 126754 (113383)	Loss/tok 3.2688 (3.1556)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.073 (0.094)	Data 1.17e-04 (4.22e-04)	Tok/s 103379 (113373)	Loss/tok 3.0033 (3.1548)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.19e-04)	Tok/s 120376 (113371)	Loss/tok 3.0918 (3.1543)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.073 (0.094)	Data 1.25e-04 (4.16e-04)	Tok/s 105416 (113367)	Loss/tok 2.9307 (3.1535)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.073 (0.094)	Data 1.19e-04 (4.13e-04)	Tok/s 103935 (113358)	Loss/tok 2.9312 (3.1529)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1040/1291]	Time 0.104 (0.094)	Data 1.25e-04 (4.10e-04)	Tok/s 120983 (113342)	Loss/tok 3.1913 (3.1527)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.07e-04)	Tok/s 120539 (113366)	Loss/tok 3.1773 (3.1527)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.042 (0.094)	Data 1.19e-04 (4.05e-04)	Tok/s 94310 (113386)	Loss/tok 2.4608 (3.1529)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.073 (0.094)	Data 1.27e-04 (4.02e-04)	Tok/s 104890 (113364)	Loss/tok 2.9018 (3.1526)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.99e-04)	Tok/s 107477 (113311)	Loss/tok 3.0283 (3.1514)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1090/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.97e-04)	Tok/s 120767 (113353)	Loss/tok 3.1015 (3.1516)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.104 (0.094)	Data 1.10e-04 (3.94e-04)	Tok/s 120113 (113319)	Loss/tok 3.1005 (3.1505)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.104 (0.094)	Data 1.22e-04 (3.92e-04)	Tok/s 121366 (113376)	Loss/tok 3.0976 (3.1511)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.104 (0.094)	Data 1.28e-04 (3.89e-04)	Tok/s 119297 (113407)	Loss/tok 3.1250 (3.1512)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.87e-04)	Tok/s 107773 (113441)	Loss/tok 2.8909 (3.1515)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.84e-04)	Tok/s 121643 (113408)	Loss/tok 3.1404 (3.1505)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.138 (0.094)	Data 1.14e-04 (3.82e-04)	Tok/s 124806 (113392)	Loss/tok 3.3879 (3.1498)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.104 (0.094)	Data 1.29e-04 (3.80e-04)	Tok/s 122678 (113414)	Loss/tok 3.1457 (3.1501)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.138 (0.094)	Data 1.12e-04 (3.78e-04)	Tok/s 126649 (113430)	Loss/tok 3.3204 (3.1505)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.073 (0.094)	Data 1.40e-04 (3.75e-04)	Tok/s 105092 (113434)	Loss/tok 3.0017 (3.1505)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.73e-04)	Tok/s 105138 (113415)	Loss/tok 2.9440 (3.1500)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.104 (0.094)	Data 1.13e-04 (3.71e-04)	Tok/s 119533 (113418)	Loss/tok 3.1513 (3.1495)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.176 (0.094)	Data 1.60e-04 (3.69e-04)	Tok/s 128631 (113404)	Loss/tok 3.2551 (3.1489)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1220/1291]	Time 0.104 (0.094)	Data 1.20e-04 (3.67e-04)	Tok/s 120443 (113432)	Loss/tok 3.1428 (3.1487)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.104 (0.094)	Data 1.72e-04 (3.65e-04)	Tok/s 119873 (113416)	Loss/tok 3.0649 (3.1478)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.63e-04)	Tok/s 107413 (113430)	Loss/tok 2.9536 (3.1480)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.138 (0.094)	Data 1.08e-04 (3.61e-04)	Tok/s 125962 (113450)	Loss/tok 3.2772 (3.1476)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.104 (0.094)	Data 1.10e-04 (3.59e-04)	Tok/s 121876 (113486)	Loss/tok 2.9624 (3.1487)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.073 (0.094)	Data 1.05e-04 (3.57e-04)	Tok/s 101754 (113443)	Loss/tok 2.9659 (3.1477)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.073 (0.094)	Data 1.09e-04 (3.55e-04)	Tok/s 106020 (113374)	Loss/tok 2.9166 (3.1471)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.073 (0.094)	Data 5.17e-05 (3.55e-04)	Tok/s 104986 (113378)	Loss/tok 2.9082 (3.1464)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592591059686, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591059686, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.412 (0.412)	Decoder iters 112.0 (112.0)	Tok/s 21894 (21894)
0: Running moses detokenizer
0: BLEU(score=24.10685277413516, counts=[37085, 18525, 10583, 6312], totals=[65310, 62307, 59305, 56307], precisions=[56.783034757311285, 29.731811834946313, 17.845038361015092, 11.209973893121637], bp=1.0, sys_len=65310, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592591060851, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24109999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591060852, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1474	Test BLEU: 24.11
0: Performance: Epoch: 3	Training: 1813447 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592591060852, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592591060852, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-19 11:24:29 AM
RESULT,RNN_TRANSLATOR,,548,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:29 AM
RESULT,RNN_TRANSLATOR,,548,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
slurmstepd: error: _is_a_lwp: open() /proc/58518/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:31 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:21 AM
