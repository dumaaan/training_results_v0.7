+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590502557, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592590502587, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592590502588, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592590502588, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592590502588, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n001
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590509799, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/13929694/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ '[' -n 7 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' -n 2 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ '[' -n 5 ']'
+ '[' 16 -gt 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
running benchmark
+ echo 'running benchmark'
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' -n 12 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:13 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592590514849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515377, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515383, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515391, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515398, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515407, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515413, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515413, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515415, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515414, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1677049750
:::MLLOG {"namespace": "", "time_ms": 1592590533091, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1677049750, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 980931216
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([246272]), new_param_packed_fragment.size()=torch.Size([246272]), master_param_fragment.size()=torch.Size([246272])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([3044864]), new_param_packed_fragment.size()=torch.Size([3044864]), master_param_fragment.size()=torch.Size([3044864])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([3948032]), new_param_packed_fragment.size()=torch.Size([3948032]), master_param_fragment.size()=torch.Size([3948032])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4982336]), new_param_packed_fragment.size()=torch.Size([4982336]), master_param_fragment.size()=torch.Size([4982336])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2756160]), new_param_packed_fragment.size()=torch.Size([2756160]), master_param_fragment.size()=torch.Size([2756160])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3698176]), new_param_packed_fragment.size()=torch.Size([3698176]), master_param_fragment.size()=torch.Size([3698176])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2188736]), new_param_packed_fragment.size()=torch.Size([2188736]), master_param_fragment.size()=torch.Size([2188736])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2302464]), new_param_packed_fragment.size()=torch.Size([2302464]), master_param_fragment.size()=torch.Size([2302464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2795008]), new_param_packed_fragment.size()=torch.Size([2795008]), master_param_fragment.size()=torch.Size([2795008])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2141696]), new_param_packed_fragment.size()=torch.Size([2141696]), master_param_fragment.size()=torch.Size([2141696])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1891840]), new_param_packed_fragment.size()=torch.Size([1891840]), master_param_fragment.size()=torch.Size([1891840])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([496128]), new_param_packed_fragment.size()=torch.Size([496128]), master_param_fragment.size()=torch.Size([496128])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1399296]), new_param_packed_fragment.size()=torch.Size([1399296]), master_param_fragment.size()=torch.Size([1399296])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3621888]), new_param_packed_fragment.size()=torch.Size([3621888]), master_param_fragment.size()=torch.Size([3621888])
model_param_fragment.size()=torch.Size([2005568]), new_param_packed_fragment.size()=torch.Size([2005568]), master_param_fragment.size()=torch.Size([2005568])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3083712]), new_param_packed_fragment.size()=torch.Size([3083712]), master_param_fragment.size()=torch.Size([3083712])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([2052608]), new_param_packed_fragment.size()=torch.Size([2052608]), master_param_fragment.size()=torch.Size([2052608])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([534976]), new_param_packed_fragment.size()=torch.Size([534976]), master_param_fragment.size()=torch.Size([534976])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592590554277, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592590554277, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592590554278, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592590554278, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592590554278, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592590558742, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592590558742, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592590558742, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592590559007, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592590559007, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592590559008, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592590559008, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592590559008, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592590559008, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592590559009, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592590559009, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592590559009, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592590559009, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592590559009, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590559009, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1591921894
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.449 (0.449)	Data 3.20e-01 (3.20e-01)	Tok/s 28624 (28624)	Loss/tok 10.5819 (10.5819)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.103 (0.124)	Data 1.13e-04 (2.92e-02)	Tok/s 122163 (106344)	Loss/tok 9.4895 (9.9157)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.103 (0.115)	Data 1.12e-04 (1.53e-02)	Tok/s 122298 (112001)	Loss/tok 9.1118 (9.5979)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.043 (0.102)	Data 1.33e-04 (1.04e-02)	Tok/s 93045 (110238)	Loss/tok 8.4838 (9.4153)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.103 (0.101)	Data 1.14e-04 (7.91e-03)	Tok/s 120881 (111400)	Loss/tok 8.5893 (9.2315)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.073 (0.098)	Data 1.00e-04 (6.38e-03)	Tok/s 106900 (110756)	Loss/tok 8.2870 (9.0986)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.103 (0.098)	Data 1.10e-04 (5.36e-03)	Tok/s 121108 (112017)	Loss/tok 8.1294 (8.9421)	LR 1.119e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][70/1291]	Time 0.137 (0.098)	Data 1.05e-04 (4.62e-03)	Tok/s 124144 (112090)	Loss/tok 8.2078 (8.8426)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.073 (0.096)	Data 1.15e-04 (4.06e-03)	Tok/s 109851 (112179)	Loss/tok 7.9503 (8.7452)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.072 (0.095)	Data 1.08e-04 (3.63e-03)	Tok/s 108996 (112484)	Loss/tok 7.8884 (8.6640)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.072 (0.095)	Data 9.89e-05 (3.28e-03)	Tok/s 107152 (112672)	Loss/tok 7.9905 (8.5907)	LR 2.746e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][110/1291]	Time 0.072 (0.096)	Data 1.03e-04 (2.99e-03)	Tok/s 104039 (113195)	Loss/tok 7.7150 (8.5367)	LR 3.378e-04
0: TRAIN [0][120/1291]	Time 0.137 (0.097)	Data 1.04e-04 (2.76e-03)	Tok/s 126723 (113493)	Loss/tok 8.0411 (8.4846)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.137 (0.096)	Data 1.71e-04 (2.55e-03)	Tok/s 127543 (113414)	Loss/tok 7.9401 (8.4350)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.073 (0.096)	Data 1.07e-04 (2.38e-03)	Tok/s 105464 (113365)	Loss/tok 7.5272 (8.3876)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.103 (0.096)	Data 1.05e-04 (2.23e-03)	Tok/s 121417 (113531)	Loss/tok 7.7515 (8.3380)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.174 (0.097)	Data 1.11e-04 (2.10e-03)	Tok/s 127735 (113685)	Loss/tok 7.7488 (8.2896)	LR 1.068e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][170/1291]	Time 0.104 (0.096)	Data 1.06e-04 (1.98e-03)	Tok/s 122209 (113639)	Loss/tok 7.4858 (8.2496)	LR 1.314e-03
0: TRAIN [0][180/1291]	Time 0.137 (0.096)	Data 1.02e-04 (1.88e-03)	Tok/s 127262 (113741)	Loss/tok 7.4214 (8.1978)	LR 1.654e-03
0: TRAIN [0][190/1291]	Time 0.104 (0.096)	Data 1.03e-04 (1.79e-03)	Tok/s 119609 (113965)	Loss/tok 7.0945 (8.1362)	LR 2.083e-03
0: TRAIN [0][200/1291]	Time 0.073 (0.096)	Data 1.03e-04 (1.70e-03)	Tok/s 106386 (114078)	Loss/tok 6.6068 (8.0754)	LR 2.622e-03
0: TRAIN [0][210/1291]	Time 0.104 (0.097)	Data 1.13e-04 (1.63e-03)	Tok/s 124707 (114190)	Loss/tok 6.7242 (8.0121)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.104 (0.097)	Data 1.03e-04 (1.56e-03)	Tok/s 123194 (114378)	Loss/tok 6.6004 (7.9404)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.104 (0.098)	Data 1.20e-04 (1.50e-03)	Tok/s 123143 (114563)	Loss/tok 6.4205 (7.8699)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.073 (0.097)	Data 1.03e-04 (1.44e-03)	Tok/s 107421 (114636)	Loss/tok 5.8488 (7.8047)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.073 (0.098)	Data 1.09e-04 (1.38e-03)	Tok/s 104897 (114716)	Loss/tok 5.7718 (7.7342)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.073 (0.097)	Data 1.07e-04 (1.34e-03)	Tok/s 106545 (114632)	Loss/tok 5.7250 (7.6740)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.174 (0.097)	Data 1.04e-04 (1.29e-03)	Tok/s 128008 (114656)	Loss/tok 6.1276 (7.6025)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.073 (0.097)	Data 1.11e-04 (1.25e-03)	Tok/s 105109 (114624)	Loss/tok 5.3725 (7.5389)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.138 (0.098)	Data 1.05e-04 (1.21e-03)	Tok/s 126615 (114692)	Loss/tok 5.8584 (7.4699)	LR 2.875e-03
0: Upscaling, new scale: 64.0
0: TRAIN [0][300/1291]	Time 0.042 (0.097)	Data 1.05e-04 (1.17e-03)	Tok/s 95861 (114512)	Loss/tok 4.4142 (7.4160)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.073 (0.097)	Data 1.05e-04 (1.14e-03)	Tok/s 105444 (114379)	Loss/tok 5.0254 (7.3591)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.073 (0.097)	Data 1.09e-04 (1.11e-03)	Tok/s 108229 (114411)	Loss/tok 5.0274 (7.2930)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.104 (0.097)	Data 1.07e-04 (1.08e-03)	Tok/s 120795 (114365)	Loss/tok 5.1982 (7.2344)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.042 (0.096)	Data 1.10e-04 (1.05e-03)	Tok/s 94372 (114118)	Loss/tok 3.8842 (7.1863)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.104 (0.096)	Data 1.05e-04 (1.02e-03)	Tok/s 122301 (114064)	Loss/tok 5.1156 (7.1302)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.072 (0.096)	Data 1.09e-04 (9.96e-04)	Tok/s 106987 (114119)	Loss/tok 4.5089 (7.0646)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.104 (0.096)	Data 1.12e-04 (9.73e-04)	Tok/s 119139 (114260)	Loss/tok 4.8561 (6.9978)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.108 (0.096)	Data 1.22e-04 (9.50e-04)	Tok/s 116941 (114090)	Loss/tok 4.8854 (6.9478)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.073 (0.095)	Data 1.09e-04 (9.29e-04)	Tok/s 106526 (113902)	Loss/tok 4.2992 (6.9001)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.104 (0.095)	Data 1.10e-04 (9.08e-04)	Tok/s 121656 (113933)	Loss/tok 4.5100 (6.8412)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.073 (0.095)	Data 1.53e-04 (8.89e-04)	Tok/s 107011 (113806)	Loss/tok 4.1757 (6.7936)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.104 (0.095)	Data 1.11e-04 (8.71e-04)	Tok/s 120098 (113912)	Loss/tok 4.3419 (6.7344)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][430/1291]	Time 0.104 (0.095)	Data 1.34e-04 (8.53e-04)	Tok/s 119012 (113829)	Loss/tok 4.5065 (6.6872)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.073 (0.095)	Data 1.07e-04 (8.37e-04)	Tok/s 104554 (113869)	Loss/tok 4.1866 (6.6325)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.042 (0.094)	Data 1.06e-04 (8.21e-04)	Tok/s 93570 (113781)	Loss/tok 3.3859 (6.5854)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.138 (0.095)	Data 1.07e-04 (8.05e-04)	Tok/s 124336 (113831)	Loss/tok 4.7360 (6.5332)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.104 (0.095)	Data 1.03e-04 (7.91e-04)	Tok/s 121908 (113904)	Loss/tok 4.4237 (6.4807)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.073 (0.095)	Data 1.09e-04 (7.77e-04)	Tok/s 105819 (113938)	Loss/tok 3.8763 (6.4312)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.042 (0.095)	Data 1.03e-04 (7.63e-04)	Tok/s 94971 (113875)	Loss/tok 3.3347 (6.3891)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.138 (0.095)	Data 1.63e-04 (7.50e-04)	Tok/s 126307 (113815)	Loss/tok 4.4051 (6.3501)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.137 (0.095)	Data 1.49e-04 (7.38e-04)	Tok/s 127395 (113814)	Loss/tok 4.3511 (6.3081)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.104 (0.095)	Data 1.62e-04 (7.26e-04)	Tok/s 119825 (113800)	Loss/tok 4.2380 (6.2694)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.104 (0.094)	Data 1.11e-04 (7.15e-04)	Tok/s 121256 (113794)	Loss/tok 4.1939 (6.2309)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.138 (0.095)	Data 1.08e-04 (7.03e-04)	Tok/s 125769 (113819)	Loss/tok 4.2971 (6.1903)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][550/1291]	Time 0.073 (0.094)	Data 1.81e-04 (6.93e-04)	Tok/s 103744 (113799)	Loss/tok 3.7408 (6.1530)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.177 (0.095)	Data 1.14e-04 (6.82e-04)	Tok/s 127522 (113811)	Loss/tok 4.2923 (6.1136)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.175 (0.094)	Data 1.03e-04 (6.73e-04)	Tok/s 127892 (113745)	Loss/tok 4.4427 (6.0810)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.073 (0.095)	Data 1.06e-04 (6.63e-04)	Tok/s 106039 (113813)	Loss/tok 3.8844 (6.0433)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.073 (0.094)	Data 1.05e-04 (6.54e-04)	Tok/s 107453 (113747)	Loss/tok 3.9232 (6.0137)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.073 (0.094)	Data 1.24e-04 (6.45e-04)	Tok/s 107227 (113780)	Loss/tok 3.7516 (5.9799)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.073 (0.094)	Data 1.18e-04 (6.36e-04)	Tok/s 107311 (113731)	Loss/tok 3.7969 (5.9501)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.074 (0.094)	Data 1.09e-04 (6.27e-04)	Tok/s 104869 (113700)	Loss/tok 3.5765 (5.9208)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.104 (0.094)	Data 1.18e-04 (6.19e-04)	Tok/s 120560 (113709)	Loss/tok 3.9259 (5.8883)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.105 (0.094)	Data 1.11e-04 (6.12e-04)	Tok/s 120676 (113712)	Loss/tok 3.8538 (5.8584)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.138 (0.094)	Data 1.16e-04 (6.04e-04)	Tok/s 128178 (113689)	Loss/tok 4.1552 (5.8294)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.104 (0.094)	Data 1.11e-04 (5.97e-04)	Tok/s 121571 (113738)	Loss/tok 3.9428 (5.7981)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.177 (0.094)	Data 1.14e-04 (5.89e-04)	Tok/s 127334 (113808)	Loss/tok 4.2497 (5.7656)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][680/1291]	Time 0.042 (0.094)	Data 1.08e-04 (5.82e-04)	Tok/s 94537 (113752)	Loss/tok 3.0656 (5.7404)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.104 (0.094)	Data 1.21e-04 (5.76e-04)	Tok/s 121461 (113828)	Loss/tok 3.8012 (5.7104)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.073 (0.095)	Data 1.73e-04 (5.69e-04)	Tok/s 106377 (113853)	Loss/tok 3.4525 (5.6819)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.105 (0.095)	Data 1.08e-04 (5.63e-04)	Tok/s 118596 (113842)	Loss/tok 3.9140 (5.6565)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.073 (0.095)	Data 1.16e-04 (5.57e-04)	Tok/s 104352 (113866)	Loss/tok 3.5714 (5.6290)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.104 (0.095)	Data 1.04e-04 (5.51e-04)	Tok/s 121720 (113858)	Loss/tok 3.8566 (5.6049)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.043 (0.095)	Data 1.01e-04 (5.45e-04)	Tok/s 94952 (113838)	Loss/tok 3.2056 (5.5827)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.073 (0.094)	Data 1.02e-04 (5.39e-04)	Tok/s 109058 (113774)	Loss/tok 3.5205 (5.5631)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.042 (0.094)	Data 1.07e-04 (5.33e-04)	Tok/s 91872 (113685)	Loss/tok 3.0323 (5.5434)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.073 (0.094)	Data 1.40e-04 (5.28e-04)	Tok/s 108922 (113674)	Loss/tok 3.5325 (5.5213)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.073 (0.095)	Data 1.08e-04 (5.22e-04)	Tok/s 105073 (113697)	Loss/tok 3.6770 (5.4959)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.139 (0.095)	Data 1.03e-04 (5.17e-04)	Tok/s 126953 (113686)	Loss/tok 4.1230 (5.4757)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.042 (0.094)	Data 1.09e-04 (5.12e-04)	Tok/s 91730 (113594)	Loss/tok 2.9085 (5.4586)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][810/1291]	Time 0.152 (0.095)	Data 1.06e-04 (5.07e-04)	Tok/s 115228 (113661)	Loss/tok 3.9335 (5.4325)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.105 (0.095)	Data 1.05e-04 (5.02e-04)	Tok/s 120887 (113647)	Loss/tok 3.8622 (5.4137)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.104 (0.095)	Data 1.10e-04 (4.97e-04)	Tok/s 121462 (113623)	Loss/tok 3.6422 (5.3947)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.042 (0.095)	Data 1.00e-04 (4.93e-04)	Tok/s 93161 (113570)	Loss/tok 2.9805 (5.3762)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.104 (0.094)	Data 1.05e-04 (4.88e-04)	Tok/s 120444 (113521)	Loss/tok 3.7723 (5.3588)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.042 (0.094)	Data 1.05e-04 (4.84e-04)	Tok/s 93607 (113455)	Loss/tok 2.9970 (5.3431)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.105 (0.094)	Data 1.09e-04 (4.79e-04)	Tok/s 119820 (113457)	Loss/tok 3.8343 (5.3245)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.043 (0.094)	Data 1.12e-04 (4.75e-04)	Tok/s 91105 (113385)	Loss/tok 2.9267 (5.3089)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.073 (0.094)	Data 1.08e-04 (4.71e-04)	Tok/s 104439 (113333)	Loss/tok 3.5253 (5.2917)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.104 (0.094)	Data 1.08e-04 (4.67e-04)	Tok/s 119927 (113370)	Loss/tok 3.6176 (5.2715)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.177 (0.094)	Data 1.23e-04 (4.63e-04)	Tok/s 127459 (113352)	Loss/tok 4.1006 (5.2543)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.152 (0.095)	Data 1.08e-04 (4.59e-04)	Tok/s 114999 (113327)	Loss/tok 3.9382 (5.2372)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.190 (0.095)	Data 1.12e-04 (4.56e-04)	Tok/s 116199 (113306)	Loss/tok 4.2237 (5.2201)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][940/1291]	Time 0.073 (0.095)	Data 1.13e-04 (4.52e-04)	Tok/s 104654 (113254)	Loss/tok 3.4457 (5.2049)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.48e-04)	Tok/s 103535 (113195)	Loss/tok 3.4699 (5.1910)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.104 (0.095)	Data 1.22e-04 (4.45e-04)	Tok/s 119853 (113175)	Loss/tok 3.7308 (5.1752)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.105 (0.095)	Data 1.12e-04 (4.41e-04)	Tok/s 120944 (113160)	Loss/tok 3.6209 (5.1594)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.104 (0.095)	Data 1.17e-04 (4.38e-04)	Tok/s 121806 (113115)	Loss/tok 3.4587 (5.1450)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.105 (0.095)	Data 1.28e-04 (4.35e-04)	Tok/s 120786 (113156)	Loss/tok 3.7335 (5.1260)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.073 (0.095)	Data 1.16e-04 (4.31e-04)	Tok/s 105845 (113149)	Loss/tok 3.3772 (5.1104)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.073 (0.095)	Data 1.06e-04 (4.28e-04)	Tok/s 105610 (113086)	Loss/tok 3.3930 (5.0983)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.073 (0.095)	Data 1.06e-04 (4.25e-04)	Tok/s 106438 (113028)	Loss/tok 3.2641 (5.0857)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.105 (0.095)	Data 1.24e-04 (4.22e-04)	Tok/s 117799 (113011)	Loss/tok 3.6527 (5.0716)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.073 (0.095)	Data 1.17e-04 (4.19e-04)	Tok/s 106736 (112995)	Loss/tok 3.5046 (5.0576)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.118 (0.095)	Data 1.07e-04 (4.16e-04)	Tok/s 105825 (112952)	Loss/tok 3.4570 (5.0445)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.117 (0.095)	Data 1.12e-04 (4.13e-04)	Tok/s 107737 (112933)	Loss/tok 3.7161 (5.0307)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1070/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.10e-04)	Tok/s 104612 (112881)	Loss/tok 3.3710 (5.0182)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.073 (0.095)	Data 1.05e-04 (4.08e-04)	Tok/s 107125 (112821)	Loss/tok 3.4103 (5.0069)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1090/1291]	Time 0.189 (0.095)	Data 1.11e-04 (4.05e-04)	Tok/s 117009 (112789)	Loss/tok 4.0855 (4.9940)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.150 (0.095)	Data 1.09e-04 (4.02e-04)	Tok/s 117463 (112761)	Loss/tok 3.8407 (4.9820)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.151 (0.095)	Data 1.08e-04 (4.00e-04)	Tok/s 115794 (112706)	Loss/tok 3.8789 (4.9714)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.151 (0.095)	Data 1.29e-04 (3.97e-04)	Tok/s 115754 (112672)	Loss/tok 3.8935 (4.9601)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.151 (0.095)	Data 1.04e-04 (3.94e-04)	Tok/s 117260 (112683)	Loss/tok 3.9238 (4.9463)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.042 (0.095)	Data 1.12e-04 (3.92e-04)	Tok/s 92817 (112631)	Loss/tok 2.8468 (4.9355)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.152 (0.096)	Data 1.14e-04 (3.90e-04)	Tok/s 116122 (112628)	Loss/tok 3.6972 (4.9225)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.042 (0.095)	Data 1.10e-04 (3.87e-04)	Tok/s 92955 (112568)	Loss/tok 2.8615 (4.9127)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.104 (0.095)	Data 1.10e-04 (3.85e-04)	Tok/s 118364 (112514)	Loss/tok 3.5836 (4.9023)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.073 (0.095)	Data 1.13e-04 (3.82e-04)	Tok/s 105761 (112517)	Loss/tok 3.3853 (4.8900)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.073 (0.095)	Data 1.14e-04 (3.80e-04)	Tok/s 106813 (112450)	Loss/tok 3.2556 (4.8803)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.104 (0.095)	Data 1.21e-04 (3.78e-04)	Tok/s 120618 (112423)	Loss/tok 3.5956 (4.8696)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.118 (0.095)	Data 1.11e-04 (3.76e-04)	Tok/s 106204 (112375)	Loss/tok 3.6384 (4.8597)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1220/1291]	Time 0.073 (0.095)	Data 1.07e-04 (3.74e-04)	Tok/s 105467 (112326)	Loss/tok 3.2745 (4.8502)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1230/1291]	Time 0.081 (0.095)	Data 1.13e-04 (3.72e-04)	Tok/s 94363 (112321)	Loss/tok 3.3380 (4.8389)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.152 (0.095)	Data 1.19e-04 (3.69e-04)	Tok/s 116436 (112283)	Loss/tok 3.7680 (4.8291)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.116 (0.096)	Data 1.10e-04 (3.67e-04)	Tok/s 107143 (112264)	Loss/tok 3.6716 (4.8187)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.042 (0.095)	Data 1.14e-04 (3.65e-04)	Tok/s 94454 (112212)	Loss/tok 2.7751 (4.8096)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.085 (0.095)	Data 1.08e-04 (3.63e-04)	Tok/s 89664 (112177)	Loss/tok 3.4432 (4.7998)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.152 (0.096)	Data 1.09e-04 (3.61e-04)	Tok/s 115377 (112159)	Loss/tok 3.8848 (4.7901)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.042 (0.095)	Data 4.77e-05 (3.61e-04)	Tok/s 93887 (112124)	Loss/tok 2.8861 (4.7808)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590682695, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590682696, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.459 (0.459)	Decoder iters 149.0 (149.0)	Tok/s 19124 (19124)
0: Running moses detokenizer
0: BLEU(score=19.767406314842585, counts=[34509, 15688, 8304, 4603], totals=[65271, 62268, 59265, 56266], precisions=[52.870340580043205, 25.194321320742596, 14.011642622120982, 8.180784132513418], bp=1.0, sys_len=65271, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590683849, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1977, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590683849, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7790	Test BLEU: 19.77
0: Performance: Epoch: 0	Training: 1793727 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592590683849, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590683849, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590683849, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 925019605
0: TRAIN [1][0/1291]	Time 0.440 (0.440)	Data 2.87e-01 (2.87e-01)	Tok/s 39059 (39059)	Loss/tok 3.6758 (3.6758)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.042 (0.125)	Data 1.08e-04 (2.62e-02)	Tok/s 87822 (105600)	Loss/tok 2.7971 (3.5145)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.139 (0.106)	Data 1.06e-04 (1.37e-02)	Tok/s 125894 (108158)	Loss/tok 3.6280 (3.4949)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.073 (0.105)	Data 1.10e-04 (9.35e-03)	Tok/s 106930 (110078)	Loss/tok 3.2577 (3.4945)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.105 (0.101)	Data 1.12e-04 (7.10e-03)	Tok/s 120641 (110470)	Loss/tok 3.4585 (3.4786)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.042 (0.098)	Data 1.08e-04 (5.73e-03)	Tok/s 93839 (109906)	Loss/tok 2.8625 (3.4638)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][60/1291]	Time 0.189 (0.099)	Data 1.06e-04 (4.81e-03)	Tok/s 118029 (109850)	Loss/tok 3.8198 (3.4706)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][70/1291]	Time 0.042 (0.099)	Data 1.08e-04 (4.14e-03)	Tok/s 92323 (109782)	Loss/tok 2.7983 (3.4852)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.073 (0.097)	Data 1.08e-04 (3.65e-03)	Tok/s 104617 (109599)	Loss/tok 3.2051 (3.4750)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.073 (0.096)	Data 1.12e-04 (3.26e-03)	Tok/s 107818 (109669)	Loss/tok 3.3169 (3.4685)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.117 (0.096)	Data 1.04e-04 (2.94e-03)	Tok/s 109152 (109538)	Loss/tok 3.4980 (3.4667)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.073 (0.096)	Data 1.63e-04 (2.69e-03)	Tok/s 104840 (109462)	Loss/tok 3.1330 (3.4693)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [1][120/1291]	Time 0.042 (0.096)	Data 1.08e-04 (2.48e-03)	Tok/s 94902 (109198)	Loss/tok 2.7366 (3.4773)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.152 (0.096)	Data 1.13e-04 (2.30e-03)	Tok/s 114961 (109149)	Loss/tok 3.7819 (3.4782)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.151 (0.096)	Data 1.09e-04 (2.14e-03)	Tok/s 117466 (109106)	Loss/tok 3.7811 (3.4873)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.073 (0.096)	Data 1.07e-04 (2.01e-03)	Tok/s 104902 (108895)	Loss/tok 3.2857 (3.4856)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.104 (0.097)	Data 1.06e-04 (1.89e-03)	Tok/s 120605 (109001)	Loss/tok 3.4377 (3.4891)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.104 (0.097)	Data 1.08e-04 (1.78e-03)	Tok/s 121473 (109025)	Loss/tok 3.5225 (3.4861)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.104 (0.097)	Data 1.10e-04 (1.69e-03)	Tok/s 121328 (109093)	Loss/tok 3.6516 (3.4940)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.191 (0.098)	Data 1.02e-04 (1.61e-03)	Tok/s 117263 (109053)	Loss/tok 3.8687 (3.4982)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.117 (0.097)	Data 1.03e-04 (1.53e-03)	Tok/s 107530 (108936)	Loss/tok 3.5331 (3.4918)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.118 (0.097)	Data 1.16e-04 (1.47e-03)	Tok/s 107872 (108940)	Loss/tok 3.5118 (3.4890)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.042 (0.097)	Data 1.09e-04 (1.40e-03)	Tok/s 95364 (108873)	Loss/tok 2.8251 (3.4879)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.188 (0.097)	Data 1.05e-04 (1.35e-03)	Tok/s 118640 (108754)	Loss/tok 3.8977 (3.4872)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.116 (0.097)	Data 1.05e-04 (1.30e-03)	Tok/s 108476 (108726)	Loss/tok 3.3483 (3.4839)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][250/1291]	Time 0.042 (0.097)	Data 1.13e-04 (1.25e-03)	Tok/s 93963 (108671)	Loss/tok 2.8133 (3.4836)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.116 (0.097)	Data 1.10e-04 (1.21e-03)	Tok/s 110474 (108618)	Loss/tok 3.5196 (3.4817)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.152 (0.097)	Data 1.20e-04 (1.17e-03)	Tok/s 115479 (108600)	Loss/tok 3.7547 (3.4851)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.073 (0.097)	Data 1.10e-04 (1.13e-03)	Tok/s 106754 (108584)	Loss/tok 3.1860 (3.4813)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.079 (0.097)	Data 1.07e-04 (1.09e-03)	Tok/s 99397 (108469)	Loss/tok 3.2024 (3.4772)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.085 (0.097)	Data 1.14e-04 (1.06e-03)	Tok/s 91039 (108365)	Loss/tok 3.1992 (3.4738)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.073 (0.097)	Data 1.15e-04 (1.03e-03)	Tok/s 106470 (108385)	Loss/tok 3.2888 (3.4754)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.075 (0.097)	Data 1.06e-04 (1.00e-03)	Tok/s 103987 (108271)	Loss/tok 3.1698 (3.4738)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.073 (0.097)	Data 1.08e-04 (9.74e-04)	Tok/s 104745 (108221)	Loss/tok 3.3663 (3.4744)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.152 (0.097)	Data 1.10e-04 (9.49e-04)	Tok/s 114763 (108238)	Loss/tok 3.6504 (3.4751)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.073 (0.097)	Data 1.11e-04 (9.25e-04)	Tok/s 103915 (108216)	Loss/tok 3.1581 (3.4785)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.104 (0.097)	Data 1.07e-04 (9.02e-04)	Tok/s 121586 (108143)	Loss/tok 3.4323 (3.4744)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.117 (0.098)	Data 1.08e-04 (8.81e-04)	Tok/s 107292 (108167)	Loss/tok 3.4940 (3.4752)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][380/1291]	Time 0.152 (0.098)	Data 1.26e-04 (8.61e-04)	Tok/s 114732 (108180)	Loss/tok 3.6423 (3.4763)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.085 (0.099)	Data 1.14e-04 (8.42e-04)	Tok/s 92608 (108288)	Loss/tok 3.1834 (3.4778)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][400/1291]	Time 0.073 (0.099)	Data 1.19e-04 (8.23e-04)	Tok/s 103649 (108256)	Loss/tok 3.3263 (3.4793)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.086 (0.099)	Data 1.01e-04 (8.06e-04)	Tok/s 89392 (108223)	Loss/tok 3.1166 (3.4775)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.085 (0.099)	Data 1.15e-04 (7.89e-04)	Tok/s 91574 (108148)	Loss/tok 3.2641 (3.4758)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.073 (0.099)	Data 1.16e-04 (7.73e-04)	Tok/s 103617 (108093)	Loss/tok 3.2544 (3.4750)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.150 (0.099)	Data 1.05e-04 (7.58e-04)	Tok/s 117341 (108070)	Loss/tok 3.5962 (3.4767)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.073 (0.099)	Data 1.07e-04 (7.44e-04)	Tok/s 105750 (108097)	Loss/tok 3.1958 (3.4800)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.042 (0.100)	Data 1.15e-04 (7.30e-04)	Tok/s 94485 (108052)	Loss/tok 2.7723 (3.4805)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.117 (0.100)	Data 1.11e-04 (7.17e-04)	Tok/s 108846 (108029)	Loss/tok 3.4691 (3.4810)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.073 (0.100)	Data 1.06e-04 (7.04e-04)	Tok/s 105348 (108060)	Loss/tok 3.1116 (3.4811)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.073 (0.100)	Data 1.10e-04 (6.92e-04)	Tok/s 104740 (107968)	Loss/tok 3.2545 (3.4785)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.116 (0.100)	Data 1.14e-04 (6.80e-04)	Tok/s 109115 (107990)	Loss/tok 3.4214 (3.4801)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.116 (0.101)	Data 1.03e-04 (6.69e-04)	Tok/s 108575 (108035)	Loss/tok 3.4561 (3.4812)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.073 (0.101)	Data 1.08e-04 (6.58e-04)	Tok/s 108444 (108011)	Loss/tok 3.2340 (3.4796)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][530/1291]	Time 0.117 (0.101)	Data 1.07e-04 (6.48e-04)	Tok/s 108980 (108047)	Loss/tok 3.3614 (3.4819)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.077 (0.101)	Data 1.06e-04 (6.38e-04)	Tok/s 102658 (108003)	Loss/tok 3.2019 (3.4807)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.117 (0.101)	Data 1.13e-04 (6.28e-04)	Tok/s 107320 (107951)	Loss/tok 3.4678 (3.4788)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.084 (0.101)	Data 1.17e-04 (6.19e-04)	Tok/s 92194 (107868)	Loss/tok 3.1974 (3.4767)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.073 (0.100)	Data 1.12e-04 (6.10e-04)	Tok/s 107313 (107795)	Loss/tok 3.0839 (3.4735)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.152 (0.100)	Data 1.34e-04 (6.02e-04)	Tok/s 114213 (107704)	Loss/tok 3.6786 (3.4734)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.105 (0.100)	Data 1.11e-04 (5.94e-04)	Tok/s 121093 (107658)	Loss/tok 3.5086 (3.4718)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.117 (0.100)	Data 1.13e-04 (5.86e-04)	Tok/s 107577 (107625)	Loss/tok 3.3512 (3.4701)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][610/1291]	Time 0.190 (0.100)	Data 1.08e-04 (5.78e-04)	Tok/s 116071 (107603)	Loss/tok 3.7046 (3.4704)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.117 (0.101)	Data 1.15e-04 (5.70e-04)	Tok/s 107344 (107613)	Loss/tok 3.3929 (3.4715)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.085 (0.101)	Data 1.09e-04 (5.63e-04)	Tok/s 91273 (107570)	Loss/tok 3.2334 (3.4695)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.153 (0.101)	Data 1.14e-04 (5.56e-04)	Tok/s 115008 (107639)	Loss/tok 3.5157 (3.4723)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.084 (0.101)	Data 1.14e-04 (5.49e-04)	Tok/s 91299 (107623)	Loss/tok 3.1891 (3.4723)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.204 (0.101)	Data 1.11e-04 (5.43e-04)	Tok/s 111470 (107568)	Loss/tok 3.6897 (3.4710)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.190 (0.101)	Data 1.07e-04 (5.36e-04)	Tok/s 117641 (107531)	Loss/tok 3.8750 (3.4710)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.117 (0.102)	Data 1.24e-04 (5.30e-04)	Tok/s 110508 (107584)	Loss/tok 3.3935 (3.4728)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.117 (0.102)	Data 1.11e-04 (5.24e-04)	Tok/s 106421 (107569)	Loss/tok 3.2767 (3.4718)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.042 (0.102)	Data 1.17e-04 (5.18e-04)	Tok/s 95730 (107544)	Loss/tok 2.8052 (3.4711)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.189 (0.102)	Data 1.09e-04 (5.12e-04)	Tok/s 118194 (107515)	Loss/tok 3.7843 (3.4711)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.117 (0.101)	Data 1.10e-04 (5.07e-04)	Tok/s 108188 (107455)	Loss/tok 3.4729 (3.4688)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.151 (0.102)	Data 1.15e-04 (5.01e-04)	Tok/s 115052 (107450)	Loss/tok 3.5992 (3.4701)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][740/1291]	Time 0.073 (0.102)	Data 1.05e-04 (4.96e-04)	Tok/s 106864 (107417)	Loss/tok 3.1889 (3.4684)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.073 (0.101)	Data 1.12e-04 (4.91e-04)	Tok/s 110031 (107406)	Loss/tok 3.1980 (3.4667)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.073 (0.101)	Data 1.16e-04 (4.86e-04)	Tok/s 107114 (107366)	Loss/tok 3.1451 (3.4647)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.151 (0.102)	Data 1.19e-04 (4.81e-04)	Tok/s 116800 (107348)	Loss/tok 3.6583 (3.4649)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.117 (0.102)	Data 1.05e-04 (4.76e-04)	Tok/s 107154 (107325)	Loss/tok 3.4527 (3.4640)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.073 (0.102)	Data 1.15e-04 (4.72e-04)	Tok/s 102468 (107288)	Loss/tok 3.2194 (3.4623)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.073 (0.101)	Data 1.12e-04 (4.67e-04)	Tok/s 107057 (107254)	Loss/tok 3.0900 (3.4612)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.153 (0.101)	Data 1.07e-04 (4.63e-04)	Tok/s 114480 (107215)	Loss/tok 3.6074 (3.4605)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.085 (0.101)	Data 1.10e-04 (4.59e-04)	Tok/s 89968 (107185)	Loss/tok 3.1944 (3.4598)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.073 (0.101)	Data 1.11e-04 (4.54e-04)	Tok/s 102567 (107145)	Loss/tok 3.2543 (3.4588)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.077 (0.101)	Data 1.08e-04 (4.50e-04)	Tok/s 97915 (107069)	Loss/tok 3.1809 (3.4568)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][850/1291]	Time 0.085 (0.101)	Data 1.05e-04 (4.46e-04)	Tok/s 91732 (107034)	Loss/tok 3.1290 (3.4557)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.118 (0.101)	Data 1.04e-04 (4.42e-04)	Tok/s 103541 (107034)	Loss/tok 3.3569 (3.4561)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.081 (0.101)	Data 1.07e-04 (4.38e-04)	Tok/s 96242 (106957)	Loss/tok 3.2814 (3.4541)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.077 (0.101)	Data 1.07e-04 (4.35e-04)	Tok/s 102389 (106927)	Loss/tok 3.1382 (3.4527)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.042 (0.101)	Data 1.08e-04 (4.31e-04)	Tok/s 94264 (106862)	Loss/tok 2.5868 (3.4505)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.073 (0.101)	Data 1.04e-04 (4.27e-04)	Tok/s 104372 (106862)	Loss/tok 3.2000 (3.4492)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.085 (0.101)	Data 1.18e-04 (4.24e-04)	Tok/s 90946 (106822)	Loss/tok 3.2060 (3.4488)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.117 (0.101)	Data 1.14e-04 (4.21e-04)	Tok/s 106020 (106809)	Loss/tok 3.4513 (3.4480)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.152 (0.101)	Data 1.14e-04 (4.17e-04)	Tok/s 114818 (106819)	Loss/tok 3.6052 (3.4478)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.117 (0.101)	Data 1.07e-04 (4.14e-04)	Tok/s 106576 (106783)	Loss/tok 3.4322 (3.4469)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.117 (0.101)	Data 1.12e-04 (4.11e-04)	Tok/s 107966 (106774)	Loss/tok 3.3420 (3.4466)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.081 (0.101)	Data 1.06e-04 (4.07e-04)	Tok/s 94117 (106749)	Loss/tok 3.0737 (3.4462)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.073 (0.101)	Data 1.09e-04 (4.04e-04)	Tok/s 107159 (106743)	Loss/tok 3.2928 (3.4469)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][980/1291]	Time 0.075 (0.101)	Data 1.13e-04 (4.01e-04)	Tok/s 101502 (106694)	Loss/tok 3.2029 (3.4460)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.165 (0.101)	Data 1.07e-04 (3.98e-04)	Tok/s 104859 (106671)	Loss/tok 3.6507 (3.4465)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.205 (0.102)	Data 1.10e-04 (3.96e-04)	Tok/s 109583 (106676)	Loss/tok 3.7533 (3.4469)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.116 (0.102)	Data 1.08e-04 (3.93e-04)	Tok/s 108938 (106685)	Loss/tok 3.4253 (3.4474)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.152 (0.102)	Data 1.11e-04 (3.90e-04)	Tok/s 115436 (106692)	Loss/tok 3.5674 (3.4465)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.117 (0.102)	Data 1.12e-04 (3.87e-04)	Tok/s 105927 (106664)	Loss/tok 3.4339 (3.4458)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.043 (0.102)	Data 1.09e-04 (3.85e-04)	Tok/s 92683 (106616)	Loss/tok 2.6927 (3.4451)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.118 (0.102)	Data 1.05e-04 (3.82e-04)	Tok/s 107034 (106606)	Loss/tok 3.3858 (3.4442)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.083 (0.102)	Data 1.08e-04 (3.79e-04)	Tok/s 94572 (106558)	Loss/tok 3.1411 (3.4430)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.080 (0.101)	Data 1.08e-04 (3.77e-04)	Tok/s 96271 (106506)	Loss/tok 3.2452 (3.4415)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.167 (0.101)	Data 1.04e-04 (3.74e-04)	Tok/s 105000 (106476)	Loss/tok 3.5473 (3.4406)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.117 (0.101)	Data 1.07e-04 (3.72e-04)	Tok/s 108393 (106445)	Loss/tok 3.3378 (3.4399)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.117 (0.101)	Data 1.11e-04 (3.69e-04)	Tok/s 109661 (106416)	Loss/tok 3.2515 (3.4392)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1110/1291]	Time 0.152 (0.101)	Data 1.12e-04 (3.67e-04)	Tok/s 115070 (106382)	Loss/tok 3.4240 (3.4380)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1120/1291]	Time 0.082 (0.101)	Data 1.17e-04 (3.65e-04)	Tok/s 94210 (106353)	Loss/tok 3.0199 (3.4370)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.118 (0.102)	Data 1.08e-04 (3.63e-04)	Tok/s 107149 (106356)	Loss/tok 3.3380 (3.4369)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.116 (0.102)	Data 1.16e-04 (3.60e-04)	Tok/s 108716 (106367)	Loss/tok 3.3618 (3.4362)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.124 (0.102)	Data 1.09e-04 (3.58e-04)	Tok/s 102012 (106352)	Loss/tok 3.2632 (3.4360)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.073 (0.102)	Data 1.05e-04 (3.56e-04)	Tok/s 106876 (106348)	Loss/tok 3.2620 (3.4362)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.077 (0.102)	Data 1.07e-04 (3.54e-04)	Tok/s 100845 (106298)	Loss/tok 3.0864 (3.4354)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.073 (0.102)	Data 1.10e-04 (3.52e-04)	Tok/s 107379 (106258)	Loss/tok 3.2427 (3.4344)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.042 (0.101)	Data 1.13e-04 (3.50e-04)	Tok/s 93777 (106212)	Loss/tok 2.7873 (3.4333)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.117 (0.101)	Data 1.08e-04 (3.48e-04)	Tok/s 108470 (106162)	Loss/tok 3.4336 (3.4323)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.073 (0.101)	Data 1.09e-04 (3.46e-04)	Tok/s 110458 (106148)	Loss/tok 3.0109 (3.4313)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.117 (0.101)	Data 1.11e-04 (3.44e-04)	Tok/s 105768 (106145)	Loss/tok 3.2987 (3.4310)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.086 (0.101)	Data 1.07e-04 (3.42e-04)	Tok/s 91836 (106112)	Loss/tok 3.1198 (3.4303)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.073 (0.101)	Data 1.14e-04 (3.40e-04)	Tok/s 106073 (106100)	Loss/tok 3.0686 (3.4292)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1250/1291]	Time 0.203 (0.101)	Data 1.06e-04 (3.38e-04)	Tok/s 112222 (106093)	Loss/tok 3.6181 (3.4288)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1260/1291]	Time 0.153 (0.101)	Data 1.05e-04 (3.36e-04)	Tok/s 114812 (106081)	Loss/tok 3.4663 (3.4287)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.118 (0.102)	Data 1.10e-04 (3.35e-04)	Tok/s 107883 (106080)	Loss/tok 3.3440 (3.4284)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.049 (0.101)	Data 1.10e-04 (3.33e-04)	Tok/s 80931 (106041)	Loss/tok 2.6969 (3.4274)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.079 (0.101)	Data 4.98e-05 (3.33e-04)	Tok/s 97644 (106028)	Loss/tok 3.0773 (3.4270)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590815542, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590815543, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.464 (0.464)	Decoder iters 149.0 (149.0)	Tok/s 19000 (19000)
0: Running moses detokenizer
0: BLEU(score=21.965572116360544, counts=[35420, 17001, 9366, 5365], totals=[64176, 61173, 58170, 55171], precisions=[55.19197207678883, 27.791672796822127, 16.101083032490976, 9.72431168548694], bp=0.9922391972087693, sys_len=64176, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590816709, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21969999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590816709, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4290	Test BLEU: 21.97
0: Performance: Epoch: 1	Training: 1697291 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592590816709, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590816709, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590816710, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3065307735
0: TRAIN [2][0/1291]	Time 0.373 (0.373)	Data 2.80e-01 (2.80e-01)	Tok/s 20856 (20856)	Loss/tok 3.1122 (3.1122)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.073 (0.115)	Data 1.08e-04 (2.55e-02)	Tok/s 106076 (102908)	Loss/tok 3.0781 (3.2399)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.104 (0.116)	Data 1.06e-04 (1.34e-02)	Tok/s 119937 (108331)	Loss/tok 3.2348 (3.3084)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.188 (0.105)	Data 1.03e-04 (9.13e-03)	Tok/s 119715 (106390)	Loss/tok 3.5543 (3.2668)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.085 (0.103)	Data 1.04e-04 (6.93e-03)	Tok/s 90487 (106284)	Loss/tok 3.0436 (3.2637)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.153 (0.098)	Data 1.12e-04 (5.59e-03)	Tok/s 113725 (105202)	Loss/tok 3.4695 (3.2379)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.117 (0.100)	Data 1.07e-04 (4.69e-03)	Tok/s 109089 (105686)	Loss/tok 3.2746 (3.2466)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.117 (0.098)	Data 1.07e-04 (4.05e-03)	Tok/s 108273 (105402)	Loss/tok 3.3226 (3.2350)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.073 (0.098)	Data 1.04e-04 (3.56e-03)	Tok/s 108029 (105509)	Loss/tok 2.9679 (3.2387)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.117 (0.099)	Data 1.07e-04 (3.18e-03)	Tok/s 108727 (105374)	Loss/tok 3.2851 (3.2431)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][100/1291]	Time 0.118 (0.100)	Data 1.09e-04 (2.88e-03)	Tok/s 107421 (105643)	Loss/tok 3.2324 (3.2514)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.085 (0.099)	Data 1.04e-04 (2.63e-03)	Tok/s 89266 (105291)	Loss/tok 2.9878 (3.2408)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.117 (0.099)	Data 1.08e-04 (2.42e-03)	Tok/s 106852 (105370)	Loss/tok 3.3264 (3.2419)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.117 (0.100)	Data 1.11e-04 (2.24e-03)	Tok/s 106405 (105484)	Loss/tok 3.3230 (3.2457)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.073 (0.099)	Data 1.02e-04 (2.09e-03)	Tok/s 106698 (105356)	Loss/tok 3.0560 (3.2442)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.042 (0.099)	Data 1.05e-04 (1.96e-03)	Tok/s 94615 (105340)	Loss/tok 2.5942 (3.2457)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.085 (0.099)	Data 1.06e-04 (1.84e-03)	Tok/s 90240 (105250)	Loss/tok 2.9748 (3.2533)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.073 (0.100)	Data 1.12e-04 (1.74e-03)	Tok/s 107227 (105460)	Loss/tok 3.0248 (3.2556)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.116 (0.101)	Data 1.05e-04 (1.65e-03)	Tok/s 107331 (105600)	Loss/tok 3.1623 (3.2600)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.042 (0.101)	Data 1.07e-04 (1.57e-03)	Tok/s 93650 (105449)	Loss/tok 2.6802 (3.2557)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.152 (0.100)	Data 1.04e-04 (1.50e-03)	Tok/s 115437 (105260)	Loss/tok 3.3883 (3.2516)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.073 (0.100)	Data 1.08e-04 (1.43e-03)	Tok/s 107188 (105274)	Loss/tok 3.3100 (3.2498)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.073 (0.100)	Data 1.09e-04 (1.37e-03)	Tok/s 106181 (105370)	Loss/tok 3.0790 (3.2530)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][230/1291]	Time 0.150 (0.102)	Data 1.11e-04 (1.32e-03)	Tok/s 117145 (105495)	Loss/tok 3.5352 (3.2580)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][240/1291]	Time 0.189 (0.102)	Data 1.07e-04 (1.27e-03)	Tok/s 117539 (105566)	Loss/tok 3.7885 (3.2640)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.191 (0.102)	Data 1.06e-04 (1.22e-03)	Tok/s 116902 (105489)	Loss/tok 3.5195 (3.2648)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.118 (0.102)	Data 1.14e-04 (1.18e-03)	Tok/s 107532 (105544)	Loss/tok 3.2836 (3.2653)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.117 (0.102)	Data 1.11e-04 (1.14e-03)	Tok/s 108574 (105445)	Loss/tok 3.3138 (3.2618)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.153 (0.102)	Data 1.07e-04 (1.10e-03)	Tok/s 114664 (105522)	Loss/tok 3.3802 (3.2666)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.118 (0.102)	Data 1.16e-04 (1.07e-03)	Tok/s 108165 (105436)	Loss/tok 3.3242 (3.2660)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.073 (0.102)	Data 1.98e-04 (1.04e-03)	Tok/s 105531 (105396)	Loss/tok 3.0698 (3.2667)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.073 (0.102)	Data 1.13e-04 (1.01e-03)	Tok/s 106050 (105450)	Loss/tok 3.1699 (3.2708)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.073 (0.102)	Data 1.12e-04 (9.79e-04)	Tok/s 104574 (105460)	Loss/tok 3.0878 (3.2696)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.151 (0.103)	Data 1.08e-04 (9.53e-04)	Tok/s 114297 (105447)	Loss/tok 3.4979 (3.2721)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.152 (0.103)	Data 1.11e-04 (9.28e-04)	Tok/s 116167 (105437)	Loss/tok 3.5263 (3.2745)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.043 (0.103)	Data 1.06e-04 (9.05e-04)	Tok/s 93215 (105373)	Loss/tok 2.7631 (3.2743)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.152 (0.103)	Data 1.05e-04 (8.83e-04)	Tok/s 114016 (105351)	Loss/tok 3.4158 (3.2759)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][370/1291]	Time 0.073 (0.103)	Data 1.09e-04 (8.62e-04)	Tok/s 103576 (105207)	Loss/tok 2.9801 (3.2786)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.118 (0.103)	Data 1.08e-04 (8.42e-04)	Tok/s 107544 (105084)	Loss/tok 3.1910 (3.2767)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.073 (0.103)	Data 1.10e-04 (8.23e-04)	Tok/s 105939 (105055)	Loss/tok 3.1188 (3.2797)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.153 (0.103)	Data 1.05e-04 (8.05e-04)	Tok/s 111073 (105015)	Loss/tok 3.5977 (3.2789)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.085 (0.102)	Data 1.04e-04 (7.88e-04)	Tok/s 90921 (104880)	Loss/tok 3.0623 (3.2780)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.190 (0.102)	Data 1.08e-04 (7.72e-04)	Tok/s 116205 (104857)	Loss/tok 3.6842 (3.2779)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.073 (0.102)	Data 1.11e-04 (7.57e-04)	Tok/s 104800 (104898)	Loss/tok 3.0580 (3.2776)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.115 (0.103)	Data 1.06e-04 (7.42e-04)	Tok/s 109330 (104963)	Loss/tok 3.1972 (3.2795)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.073 (0.103)	Data 1.03e-04 (7.28e-04)	Tok/s 107783 (104931)	Loss/tok 3.2241 (3.2789)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.120 (0.103)	Data 1.07e-04 (7.14e-04)	Tok/s 105061 (104943)	Loss/tok 3.2025 (3.2797)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.117 (0.104)	Data 1.08e-04 (7.01e-04)	Tok/s 108695 (105013)	Loss/tok 3.2646 (3.2850)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.078 (0.103)	Data 9.92e-05 (6.89e-04)	Tok/s 99896 (104876)	Loss/tok 3.0234 (3.2829)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.086 (0.103)	Data 1.06e-04 (6.77e-04)	Tok/s 89684 (104773)	Loss/tok 3.1142 (3.2815)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][500/1291]	Time 0.076 (0.103)	Data 1.06e-04 (6.66e-04)	Tok/s 101679 (104731)	Loss/tok 3.1851 (3.2800)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.119 (0.103)	Data 1.06e-04 (6.55e-04)	Tok/s 107486 (104755)	Loss/tok 3.2217 (3.2833)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.117 (0.103)	Data 1.03e-04 (6.44e-04)	Tok/s 108267 (104674)	Loss/tok 3.1520 (3.2816)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.045 (0.103)	Data 1.04e-04 (6.34e-04)	Tok/s 87489 (104660)	Loss/tok 2.6035 (3.2827)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][540/1291]	Time 0.085 (0.103)	Data 1.07e-04 (6.24e-04)	Tok/s 90196 (104661)	Loss/tok 3.0709 (3.2837)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.042 (0.103)	Data 1.07e-04 (6.15e-04)	Tok/s 94198 (104630)	Loss/tok 2.6184 (3.2830)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.042 (0.103)	Data 1.18e-04 (6.06e-04)	Tok/s 90877 (104599)	Loss/tok 2.6737 (3.2836)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.083 (0.103)	Data 1.07e-04 (5.97e-04)	Tok/s 95369 (104571)	Loss/tok 3.0638 (3.2835)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.157 (0.104)	Data 1.06e-04 (5.89e-04)	Tok/s 110555 (104628)	Loss/tok 3.5837 (3.2854)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.123 (0.104)	Data 1.05e-04 (5.81e-04)	Tok/s 103180 (104581)	Loss/tok 3.3937 (3.2850)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.117 (0.104)	Data 1.07e-04 (5.73e-04)	Tok/s 106991 (104570)	Loss/tok 3.4319 (3.2850)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.117 (0.104)	Data 1.04e-04 (5.65e-04)	Tok/s 108515 (104575)	Loss/tok 3.2300 (3.2851)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.152 (0.104)	Data 1.03e-04 (5.58e-04)	Tok/s 112915 (104534)	Loss/tok 3.4863 (3.2850)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.073 (0.103)	Data 1.01e-04 (5.51e-04)	Tok/s 106981 (104474)	Loss/tok 3.0433 (3.2832)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.081 (0.103)	Data 1.06e-04 (5.44e-04)	Tok/s 97280 (104456)	Loss/tok 3.1583 (3.2826)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.073 (0.104)	Data 1.08e-04 (5.37e-04)	Tok/s 105606 (104500)	Loss/tok 2.9830 (3.2840)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.076 (0.104)	Data 1.07e-04 (5.30e-04)	Tok/s 102249 (104427)	Loss/tok 3.1149 (3.2840)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][670/1291]	Time 0.151 (0.104)	Data 1.05e-04 (5.24e-04)	Tok/s 116059 (104422)	Loss/tok 3.4529 (3.2838)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][680/1291]	Time 0.085 (0.104)	Data 1.10e-04 (5.18e-04)	Tok/s 89707 (104418)	Loss/tok 3.1261 (3.2845)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.073 (0.104)	Data 1.06e-04 (5.12e-04)	Tok/s 106950 (104406)	Loss/tok 3.0383 (3.2842)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][700/1291]	Time 0.204 (0.104)	Data 1.03e-04 (5.06e-04)	Tok/s 108465 (104350)	Loss/tok 3.7206 (3.2854)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.152 (0.104)	Data 1.10e-04 (5.01e-04)	Tok/s 117259 (104353)	Loss/tok 3.3648 (3.2852)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.073 (0.104)	Data 1.07e-04 (4.95e-04)	Tok/s 106408 (104343)	Loss/tok 2.9525 (3.2852)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.116 (0.104)	Data 1.04e-04 (4.90e-04)	Tok/s 110286 (104336)	Loss/tok 3.2501 (3.2846)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.122 (0.104)	Data 1.05e-04 (4.85e-04)	Tok/s 103163 (104250)	Loss/tok 3.1703 (3.2831)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.117 (0.104)	Data 1.07e-04 (4.80e-04)	Tok/s 107289 (104243)	Loss/tok 3.2805 (3.2836)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.085 (0.104)	Data 1.02e-04 (4.75e-04)	Tok/s 91020 (104203)	Loss/tok 2.9760 (3.2825)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.152 (0.103)	Data 1.08e-04 (4.70e-04)	Tok/s 115411 (104143)	Loss/tok 3.5055 (3.2812)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.116 (0.103)	Data 1.11e-04 (4.65e-04)	Tok/s 107312 (104143)	Loss/tok 3.2232 (3.2810)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.073 (0.104)	Data 1.09e-04 (4.61e-04)	Tok/s 104602 (104153)	Loss/tok 3.0348 (3.2823)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.085 (0.104)	Data 1.12e-04 (4.56e-04)	Tok/s 88706 (104133)	Loss/tok 3.1344 (3.2832)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.078 (0.104)	Data 1.06e-04 (4.52e-04)	Tok/s 100332 (104092)	Loss/tok 3.0384 (3.2820)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.083 (0.103)	Data 1.13e-04 (4.48e-04)	Tok/s 94779 (104039)	Loss/tok 3.2159 (3.2813)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][830/1291]	Time 0.085 (0.103)	Data 1.11e-04 (4.44e-04)	Tok/s 92012 (104029)	Loss/tok 3.0843 (3.2806)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.118 (0.103)	Data 1.06e-04 (4.40e-04)	Tok/s 105774 (103991)	Loss/tok 3.1703 (3.2791)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.076 (0.104)	Data 1.29e-04 (4.36e-04)	Tok/s 103963 (104035)	Loss/tok 3.0013 (3.2814)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.190 (0.104)	Data 1.13e-04 (4.32e-04)	Tok/s 117023 (104047)	Loss/tok 3.7436 (3.2823)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.118 (0.104)	Data 1.05e-04 (4.29e-04)	Tok/s 108529 (104065)	Loss/tok 3.3979 (3.2822)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.189 (0.104)	Data 1.06e-04 (4.25e-04)	Tok/s 118166 (104063)	Loss/tok 3.6384 (3.2827)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.117 (0.104)	Data 1.11e-04 (4.21e-04)	Tok/s 106722 (104074)	Loss/tok 3.2642 (3.2827)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.073 (0.104)	Data 1.13e-04 (4.18e-04)	Tok/s 103873 (104068)	Loss/tok 3.1471 (3.2824)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.152 (0.104)	Data 1.03e-04 (4.14e-04)	Tok/s 114967 (104060)	Loss/tok 3.2661 (3.2822)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.117 (0.104)	Data 1.16e-04 (4.11e-04)	Tok/s 107976 (104025)	Loss/tok 3.3570 (3.2811)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.118 (0.104)	Data 1.09e-04 (4.08e-04)	Tok/s 106188 (104048)	Loss/tok 3.3201 (3.2823)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.085 (0.104)	Data 1.13e-04 (4.05e-04)	Tok/s 88685 (104038)	Loss/tok 3.0734 (3.2820)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.085 (0.104)	Data 2.13e-04 (4.02e-04)	Tok/s 89136 (104024)	Loss/tok 3.0976 (3.2823)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][960/1291]	Time 0.073 (0.104)	Data 1.05e-04 (3.99e-04)	Tok/s 106488 (104058)	Loss/tok 2.9883 (3.2825)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.153 (0.104)	Data 1.64e-04 (3.96e-04)	Tok/s 113053 (104037)	Loss/tok 3.5121 (3.2832)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][980/1291]	Time 0.153 (0.105)	Data 1.05e-04 (3.93e-04)	Tok/s 114828 (104044)	Loss/tok 3.4171 (3.2837)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.081 (0.104)	Data 1.09e-04 (3.90e-04)	Tok/s 95719 (104008)	Loss/tok 3.2110 (3.2829)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.152 (0.104)	Data 1.05e-04 (3.87e-04)	Tok/s 114838 (103980)	Loss/tok 3.4794 (3.2833)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.085 (0.104)	Data 1.08e-04 (3.85e-04)	Tok/s 91655 (103920)	Loss/tok 3.0888 (3.2821)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.117 (0.104)	Data 1.08e-04 (3.82e-04)	Tok/s 108095 (103918)	Loss/tok 3.3514 (3.2817)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1030/1291]	Time 0.190 (0.104)	Data 1.13e-04 (3.79e-04)	Tok/s 116428 (103909)	Loss/tok 3.6554 (3.2819)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.117 (0.104)	Data 1.04e-04 (3.77e-04)	Tok/s 108037 (103912)	Loss/tok 3.2147 (3.2819)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.190 (0.104)	Data 1.03e-04 (3.74e-04)	Tok/s 118177 (103909)	Loss/tok 3.6422 (3.2820)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.043 (0.104)	Data 1.07e-04 (3.72e-04)	Tok/s 92043 (103876)	Loss/tok 2.5384 (3.2812)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.073 (0.104)	Data 1.09e-04 (3.69e-04)	Tok/s 107139 (103843)	Loss/tok 3.2260 (3.2809)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.073 (0.104)	Data 1.10e-04 (3.67e-04)	Tok/s 105418 (103817)	Loss/tok 3.0238 (3.2798)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.073 (0.104)	Data 1.10e-04 (3.65e-04)	Tok/s 107684 (103762)	Loss/tok 3.2271 (3.2788)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.084 (0.104)	Data 1.16e-04 (3.62e-04)	Tok/s 93321 (103733)	Loss/tok 3.0062 (3.2778)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.154 (0.104)	Data 1.06e-04 (3.60e-04)	Tok/s 114400 (103757)	Loss/tok 3.5453 (3.2783)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.117 (0.104)	Data 1.05e-04 (3.58e-04)	Tok/s 109818 (103729)	Loss/tok 3.4416 (3.2778)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.085 (0.104)	Data 1.16e-04 (3.56e-04)	Tok/s 93166 (103715)	Loss/tok 3.1130 (3.2777)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.042 (0.104)	Data 1.05e-04 (3.54e-04)	Tok/s 91373 (103710)	Loss/tok 2.5625 (3.2773)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.073 (0.104)	Data 1.17e-04 (3.51e-04)	Tok/s 106371 (103689)	Loss/tok 3.0855 (3.2768)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1160/1291]	Time 0.117 (0.104)	Data 1.08e-04 (3.49e-04)	Tok/s 107663 (103675)	Loss/tok 3.3190 (3.2769)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.117 (0.104)	Data 1.09e-04 (3.47e-04)	Tok/s 105698 (103662)	Loss/tok 3.2526 (3.2768)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.128 (0.104)	Data 1.08e-04 (3.45e-04)	Tok/s 99372 (103647)	Loss/tok 3.3953 (3.2765)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.073 (0.104)	Data 1.07e-04 (3.43e-04)	Tok/s 110725 (103643)	Loss/tok 3.0125 (3.2761)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.117 (0.103)	Data 1.08e-04 (3.42e-04)	Tok/s 108590 (103638)	Loss/tok 3.2009 (3.2756)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.042 (0.104)	Data 1.04e-04 (3.40e-04)	Tok/s 91949 (103614)	Loss/tok 2.6588 (3.2759)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.078 (0.104)	Data 1.05e-04 (3.38e-04)	Tok/s 100827 (103635)	Loss/tok 3.0410 (3.2769)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.086 (0.104)	Data 1.06e-04 (3.36e-04)	Tok/s 90708 (103624)	Loss/tok 3.1486 (3.2775)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.073 (0.104)	Data 1.04e-04 (3.34e-04)	Tok/s 104824 (103639)	Loss/tok 3.0707 (3.2780)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.152 (0.104)	Data 1.02e-04 (3.32e-04)	Tok/s 114566 (103647)	Loss/tok 3.5359 (3.2794)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.075 (0.104)	Data 1.06e-04 (3.30e-04)	Tok/s 104411 (103632)	Loss/tok 3.1227 (3.2796)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.042 (0.104)	Data 1.04e-04 (3.29e-04)	Tok/s 94608 (103633)	Loss/tok 2.5904 (3.2794)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.082 (0.104)	Data 1.03e-04 (3.27e-04)	Tok/s 94210 (103588)	Loss/tok 3.0020 (3.2786)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1290/1291]	Time 0.118 (0.104)	Data 4.53e-05 (3.27e-04)	Tok/s 104586 (103580)	Loss/tok 3.2307 (3.2783)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590951465, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590951465, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.472 (0.472)	Decoder iters 149.0 (149.0)	Tok/s 19250 (19250)
0: Running moses detokenizer
0: BLEU(score=22.683318999389694, counts=[36669, 17927, 10067, 5899], totals=[66562, 63559, 60556, 57557], precisions=[55.089991286319524, 28.205289573467173, 16.624281656648392, 10.248970585680283], bp=1.0, sys_len=66562, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590952642, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2268, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590952642, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2813	Test BLEU: 22.68
0: Performance: Epoch: 2	Training: 1657396 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592590952642, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590952642, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590952642, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1355101639
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][0/1291]	Time 0.411 (0.411)	Data 3.08e-01 (3.08e-01)	Tok/s 30823 (30823)	Loss/tok 3.2717 (3.2717)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.139 (0.125)	Data 1.11e-04 (2.81e-02)	Tok/s 124723 (106859)	Loss/tok 3.4004 (3.1876)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.073 (0.116)	Data 1.12e-04 (1.48e-02)	Tok/s 108469 (107139)	Loss/tok 2.9164 (3.2031)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.073 (0.111)	Data 1.07e-04 (1.00e-02)	Tok/s 107700 (107258)	Loss/tok 2.9131 (3.1619)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.199 (0.111)	Data 1.11e-04 (7.61e-03)	Tok/s 112238 (106640)	Loss/tok 3.5768 (3.1836)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.052 (0.107)	Data 1.05e-04 (6.14e-03)	Tok/s 73689 (105369)	Loss/tok 2.4663 (3.1672)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.081 (0.104)	Data 1.09e-04 (5.15e-03)	Tok/s 95078 (104797)	Loss/tok 2.9656 (3.1517)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.083 (0.104)	Data 1.06e-04 (4.44e-03)	Tok/s 93036 (104553)	Loss/tok 3.0100 (3.1533)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.152 (0.104)	Data 1.09e-04 (3.90e-03)	Tok/s 115321 (104668)	Loss/tok 3.4216 (3.1653)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.189 (0.104)	Data 1.09e-04 (3.49e-03)	Tok/s 117849 (104386)	Loss/tok 3.4928 (3.1656)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.115 (0.103)	Data 1.03e-04 (3.15e-03)	Tok/s 108151 (104274)	Loss/tok 3.1357 (3.1597)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.117 (0.104)	Data 1.03e-04 (2.88e-03)	Tok/s 107976 (104256)	Loss/tok 3.1471 (3.1688)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.151 (0.103)	Data 1.18e-04 (2.65e-03)	Tok/s 114108 (104001)	Loss/tok 3.5967 (3.1732)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][130/1291]	Time 0.081 (0.104)	Data 1.10e-04 (2.46e-03)	Tok/s 95499 (104053)	Loss/tok 3.0785 (3.1759)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.073 (0.104)	Data 1.68e-04 (2.29e-03)	Tok/s 105752 (103870)	Loss/tok 2.9303 (3.1787)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.118 (0.104)	Data 1.08e-04 (2.15e-03)	Tok/s 106226 (103927)	Loss/tok 3.3170 (3.1781)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.126 (0.104)	Data 1.09e-04 (2.02e-03)	Tok/s 99841 (103746)	Loss/tok 3.1828 (3.1760)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.117 (0.104)	Data 1.08e-04 (1.91e-03)	Tok/s 107275 (103820)	Loss/tok 3.1648 (3.1766)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.085 (0.103)	Data 1.29e-04 (1.81e-03)	Tok/s 90714 (103582)	Loss/tok 2.9765 (3.1734)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.086 (0.103)	Data 1.01e-04 (1.72e-03)	Tok/s 88258 (103502)	Loss/tok 2.9841 (3.1758)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][200/1291]	Time 0.152 (0.104)	Data 1.27e-04 (1.64e-03)	Tok/s 113484 (103618)	Loss/tok 3.4259 (3.1883)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.085 (0.105)	Data 1.07e-04 (1.57e-03)	Tok/s 91022 (103596)	Loss/tok 3.0971 (3.1917)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.073 (0.104)	Data 1.08e-04 (1.50e-03)	Tok/s 107299 (103499)	Loss/tok 3.1316 (3.1914)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.203 (0.105)	Data 1.17e-04 (1.44e-03)	Tok/s 112459 (103504)	Loss/tok 3.4385 (3.1915)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.152 (0.104)	Data 1.27e-04 (1.39e-03)	Tok/s 112518 (103415)	Loss/tok 3.4126 (3.1904)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.074 (0.104)	Data 1.11e-04 (1.34e-03)	Tok/s 102770 (103396)	Loss/tok 2.9718 (3.1881)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.085 (0.104)	Data 1.19e-04 (1.29e-03)	Tok/s 88896 (103187)	Loss/tok 2.9821 (3.1866)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.152 (0.104)	Data 1.23e-04 (1.25e-03)	Tok/s 114160 (103276)	Loss/tok 3.3822 (3.1863)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.127 (0.103)	Data 1.64e-04 (1.21e-03)	Tok/s 97784 (103134)	Loss/tok 3.1996 (3.1835)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.073 (0.103)	Data 1.17e-04 (1.17e-03)	Tok/s 108240 (103189)	Loss/tok 3.0243 (3.1816)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.086 (0.103)	Data 1.17e-04 (1.13e-03)	Tok/s 90933 (103105)	Loss/tok 2.9291 (3.1801)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.195 (0.103)	Data 1.10e-04 (1.10e-03)	Tok/s 112017 (103040)	Loss/tok 3.6368 (3.1816)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.117 (0.104)	Data 1.17e-04 (1.07e-03)	Tok/s 108055 (103128)	Loss/tok 3.1612 (3.1841)	LR 1.437e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][330/1291]	Time 0.118 (0.104)	Data 1.11e-04 (1.04e-03)	Tok/s 108623 (103140)	Loss/tok 3.1122 (3.1830)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.200 (0.104)	Data 1.27e-04 (1.02e-03)	Tok/s 111876 (103065)	Loss/tok 3.6693 (3.1827)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.073 (0.103)	Data 1.70e-04 (9.90e-04)	Tok/s 102172 (103010)	Loss/tok 2.9422 (3.1820)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.118 (0.104)	Data 1.28e-04 (9.65e-04)	Tok/s 105717 (103028)	Loss/tok 3.2064 (3.1820)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.086 (0.104)	Data 1.12e-04 (9.43e-04)	Tok/s 88873 (103007)	Loss/tok 2.9527 (3.1814)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.154 (0.104)	Data 1.10e-04 (9.21e-04)	Tok/s 115061 (102997)	Loss/tok 3.2359 (3.1797)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.118 (0.104)	Data 1.08e-04 (9.00e-04)	Tok/s 107215 (103024)	Loss/tok 3.1323 (3.1799)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.167 (0.104)	Data 1.10e-04 (8.81e-04)	Tok/s 106779 (102996)	Loss/tok 3.1823 (3.1779)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.152 (0.105)	Data 1.07e-04 (8.62e-04)	Tok/s 116543 (103050)	Loss/tok 3.2896 (3.1815)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.125 (0.105)	Data 1.29e-04 (8.44e-04)	Tok/s 102318 (103014)	Loss/tok 3.0544 (3.1792)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.127 (0.104)	Data 1.08e-04 (8.28e-04)	Tok/s 97757 (102985)	Loss/tok 3.0900 (3.1774)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.042 (0.104)	Data 1.06e-04 (8.11e-04)	Tok/s 95241 (102943)	Loss/tok 2.4623 (3.1758)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.085 (0.104)	Data 1.08e-04 (7.96e-04)	Tok/s 90588 (102911)	Loss/tok 3.0191 (3.1739)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][460/1291]	Time 0.118 (0.104)	Data 1.05e-04 (7.81e-04)	Tok/s 105793 (102907)	Loss/tok 3.2189 (3.1751)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.042 (0.104)	Data 1.07e-04 (7.67e-04)	Tok/s 96299 (102850)	Loss/tok 2.5679 (3.1727)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.085 (0.104)	Data 1.09e-04 (7.53e-04)	Tok/s 89903 (102780)	Loss/tok 2.8350 (3.1713)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.152 (0.104)	Data 1.08e-04 (7.40e-04)	Tok/s 114088 (102785)	Loss/tok 3.5002 (3.1716)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.119 (0.104)	Data 1.06e-04 (7.27e-04)	Tok/s 106452 (102794)	Loss/tok 3.1996 (3.1726)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.085 (0.104)	Data 1.07e-04 (7.15e-04)	Tok/s 91375 (102712)	Loss/tok 2.9776 (3.1704)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.118 (0.104)	Data 1.10e-04 (7.03e-04)	Tok/s 107711 (102791)	Loss/tok 3.2369 (3.1709)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.117 (0.104)	Data 1.10e-04 (6.92e-04)	Tok/s 106354 (102770)	Loss/tok 3.0574 (3.1695)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.085 (0.104)	Data 1.05e-04 (6.81e-04)	Tok/s 91354 (102783)	Loss/tok 2.9382 (3.1695)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.043 (0.104)	Data 1.07e-04 (6.71e-04)	Tok/s 92857 (102719)	Loss/tok 2.6217 (3.1671)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][560/1291]	Time 0.043 (0.104)	Data 1.03e-04 (6.61e-04)	Tok/s 88792 (102677)	Loss/tok 2.5791 (3.1670)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.083 (0.104)	Data 1.04e-04 (6.51e-04)	Tok/s 93547 (102648)	Loss/tok 2.9013 (3.1662)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.081 (0.104)	Data 1.09e-04 (6.42e-04)	Tok/s 96610 (102593)	Loss/tok 2.8570 (3.1671)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.117 (0.104)	Data 1.08e-04 (6.33e-04)	Tok/s 108409 (102610)	Loss/tok 3.1493 (3.1694)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.085 (0.104)	Data 1.14e-04 (6.24e-04)	Tok/s 91394 (102553)	Loss/tok 2.9657 (3.1679)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.045 (0.104)	Data 1.09e-04 (6.16e-04)	Tok/s 89129 (102457)	Loss/tok 2.5325 (3.1668)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.152 (0.104)	Data 1.05e-04 (6.07e-04)	Tok/s 116641 (102452)	Loss/tok 3.2042 (3.1668)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.155 (0.104)	Data 1.04e-04 (6.00e-04)	Tok/s 113665 (102418)	Loss/tok 3.1998 (3.1660)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.043 (0.104)	Data 1.03e-04 (5.92e-04)	Tok/s 90440 (102382)	Loss/tok 2.5264 (3.1666)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.073 (0.104)	Data 1.09e-04 (5.84e-04)	Tok/s 104334 (102365)	Loss/tok 2.9784 (3.1661)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.086 (0.104)	Data 1.12e-04 (5.77e-04)	Tok/s 90157 (102367)	Loss/tok 3.0487 (3.1660)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.086 (0.104)	Data 1.13e-04 (5.70e-04)	Tok/s 90685 (102373)	Loss/tok 2.9295 (3.1668)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.085 (0.104)	Data 1.06e-04 (5.63e-04)	Tok/s 89715 (102348)	Loss/tok 2.9339 (3.1658)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][690/1291]	Time 0.077 (0.104)	Data 1.10e-04 (5.57e-04)	Tok/s 100565 (102299)	Loss/tok 2.9350 (3.1642)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.117 (0.104)	Data 1.08e-04 (5.50e-04)	Tok/s 108528 (102295)	Loss/tok 3.0819 (3.1636)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.085 (0.105)	Data 1.09e-04 (5.44e-04)	Tok/s 93724 (102342)	Loss/tok 2.7769 (3.1656)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.203 (0.105)	Data 1.10e-04 (5.38e-04)	Tok/s 107764 (102306)	Loss/tok 3.6895 (3.1666)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.085 (0.105)	Data 1.05e-04 (5.32e-04)	Tok/s 91889 (102242)	Loss/tok 2.8982 (3.1651)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.153 (0.104)	Data 1.10e-04 (5.26e-04)	Tok/s 114745 (102221)	Loss/tok 3.2834 (3.1637)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.165 (0.105)	Data 1.05e-04 (5.21e-04)	Tok/s 107060 (102237)	Loss/tok 3.2892 (3.1637)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.080 (0.105)	Data 1.09e-04 (5.15e-04)	Tok/s 98164 (102214)	Loss/tok 2.8260 (3.1631)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.118 (0.105)	Data 1.04e-04 (5.10e-04)	Tok/s 107281 (102180)	Loss/tok 3.1323 (3.1619)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.073 (0.105)	Data 1.06e-04 (5.05e-04)	Tok/s 104300 (102219)	Loss/tok 2.9092 (3.1627)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.117 (0.105)	Data 1.07e-04 (5.00e-04)	Tok/s 108175 (102203)	Loss/tok 3.0557 (3.1619)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.042 (0.105)	Data 1.15e-04 (4.95e-04)	Tok/s 93385 (102193)	Loss/tok 2.6416 (3.1615)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1291]	Time 0.190 (0.105)	Data 1.05e-04 (4.90e-04)	Tok/s 119497 (102206)	Loss/tok 3.3634 (3.1614)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.118 (0.105)	Data 1.11e-04 (4.86e-04)	Tok/s 107025 (102222)	Loss/tok 3.1850 (3.1619)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.133 (0.105)	Data 1.17e-04 (4.81e-04)	Tok/s 94806 (102234)	Loss/tok 3.0946 (3.1627)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.073 (0.105)	Data 1.10e-04 (4.77e-04)	Tok/s 104943 (102260)	Loss/tok 2.8920 (3.1631)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.086 (0.106)	Data 1.08e-04 (4.72e-04)	Tok/s 92997 (102268)	Loss/tok 2.9287 (3.1645)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.165 (0.106)	Data 1.11e-04 (4.68e-04)	Tok/s 106180 (102235)	Loss/tok 3.2471 (3.1639)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][870/1291]	Time 0.085 (0.105)	Data 1.07e-04 (4.64e-04)	Tok/s 88115 (102202)	Loss/tok 3.0414 (3.1637)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.042 (0.105)	Data 1.03e-04 (4.60e-04)	Tok/s 93933 (102162)	Loss/tok 2.5297 (3.1628)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.125 (0.105)	Data 1.09e-04 (4.56e-04)	Tok/s 102895 (102171)	Loss/tok 3.0844 (3.1624)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.073 (0.105)	Data 1.06e-04 (4.52e-04)	Tok/s 107239 (102153)	Loss/tok 2.9615 (3.1622)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.085 (0.105)	Data 1.07e-04 (4.48e-04)	Tok/s 91457 (102132)	Loss/tok 2.8142 (3.1617)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.117 (0.106)	Data 1.14e-04 (4.45e-04)	Tok/s 105909 (102175)	Loss/tok 3.1202 (3.1624)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.126 (0.106)	Data 1.02e-04 (4.41e-04)	Tok/s 101312 (102201)	Loss/tok 3.1467 (3.1618)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.117 (0.105)	Data 1.03e-04 (4.37e-04)	Tok/s 106951 (102175)	Loss/tok 3.2804 (3.1610)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.152 (0.106)	Data 1.07e-04 (4.34e-04)	Tok/s 115738 (102193)	Loss/tok 3.2206 (3.1619)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.084 (0.106)	Data 1.11e-04 (4.31e-04)	Tok/s 90821 (102195)	Loss/tok 2.8678 (3.1621)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.117 (0.106)	Data 1.06e-04 (4.27e-04)	Tok/s 107146 (102201)	Loss/tok 3.1988 (3.1617)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.118 (0.106)	Data 1.08e-04 (4.24e-04)	Tok/s 107167 (102167)	Loss/tok 3.0948 (3.1609)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.117 (0.106)	Data 1.11e-04 (4.21e-04)	Tok/s 107401 (102156)	Loss/tok 3.0026 (3.1596)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1000/1291]	Time 0.042 (0.106)	Data 1.08e-04 (4.18e-04)	Tok/s 92349 (102149)	Loss/tok 2.5577 (3.1594)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.076 (0.106)	Data 1.06e-04 (4.15e-04)	Tok/s 100856 (102173)	Loss/tok 2.9717 (3.1593)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.118 (0.106)	Data 1.08e-04 (4.12e-04)	Tok/s 108024 (102159)	Loss/tok 2.9281 (3.1584)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.085 (0.106)	Data 1.07e-04 (4.09e-04)	Tok/s 89430 (102127)	Loss/tok 2.8975 (3.1576)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.083 (0.106)	Data 1.09e-04 (4.06e-04)	Tok/s 96814 (102104)	Loss/tok 2.8312 (3.1563)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.085 (0.106)	Data 1.11e-04 (4.03e-04)	Tok/s 88480 (102101)	Loss/tok 2.9041 (3.1566)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.086 (0.106)	Data 1.09e-04 (4.00e-04)	Tok/s 91513 (102063)	Loss/tok 3.0429 (3.1553)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1070/1291]	Time 0.082 (0.106)	Data 1.08e-04 (3.98e-04)	Tok/s 94393 (102066)	Loss/tok 2.9420 (3.1558)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.119 (0.106)	Data 1.11e-04 (3.95e-04)	Tok/s 106053 (102059)	Loss/tok 3.0959 (3.1553)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.086 (0.106)	Data 1.08e-04 (3.92e-04)	Tok/s 90562 (102058)	Loss/tok 2.7612 (3.1548)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.153 (0.106)	Data 1.13e-04 (3.90e-04)	Tok/s 114507 (102037)	Loss/tok 3.3706 (3.1546)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.118 (0.106)	Data 1.10e-04 (3.87e-04)	Tok/s 106128 (102040)	Loss/tok 3.1309 (3.1542)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.042 (0.106)	Data 1.08e-04 (3.85e-04)	Tok/s 91033 (102001)	Loss/tok 2.5305 (3.1531)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.118 (0.106)	Data 1.11e-04 (3.82e-04)	Tok/s 106274 (101989)	Loss/tok 3.0258 (3.1526)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.082 (0.106)	Data 1.08e-04 (3.80e-04)	Tok/s 96418 (101972)	Loss/tok 2.8610 (3.1528)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.117 (0.105)	Data 1.04e-04 (3.78e-04)	Tok/s 105539 (101943)	Loss/tok 3.2016 (3.1517)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.085 (0.105)	Data 1.12e-04 (3.75e-04)	Tok/s 89964 (101894)	Loss/tok 2.9108 (3.1504)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.073 (0.105)	Data 1.09e-04 (3.73e-04)	Tok/s 106532 (101891)	Loss/tok 3.0032 (3.1508)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.190 (0.105)	Data 1.13e-04 (3.71e-04)	Tok/s 118437 (101907)	Loss/tok 3.4080 (3.1517)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.042 (0.105)	Data 1.08e-04 (3.69e-04)	Tok/s 92150 (101877)	Loss/tok 2.6065 (3.1508)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1291]	Time 0.073 (0.105)	Data 1.07e-04 (3.66e-04)	Tok/s 105606 (101858)	Loss/tok 2.9718 (3.1496)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.042 (0.105)	Data 1.13e-04 (3.64e-04)	Tok/s 93825 (101841)	Loss/tok 2.5128 (3.1487)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.081 (0.105)	Data 1.06e-04 (3.62e-04)	Tok/s 98191 (101838)	Loss/tok 2.9232 (3.1480)	LR 7.187e-04
0: TRAIN [3][1230/1291]	Time 0.073 (0.105)	Data 1.13e-04 (3.60e-04)	Tok/s 106047 (101846)	Loss/tok 2.9148 (3.1481)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.084 (0.105)	Data 1.09e-04 (3.58e-04)	Tok/s 93091 (101821)	Loss/tok 2.7847 (3.1476)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.073 (0.105)	Data 1.08e-04 (3.56e-04)	Tok/s 106667 (101864)	Loss/tok 2.8755 (3.1468)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.201 (0.105)	Data 1.05e-04 (3.54e-04)	Tok/s 111356 (101859)	Loss/tok 3.4241 (3.1475)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.085 (0.105)	Data 1.09e-04 (3.52e-04)	Tok/s 93446 (101879)	Loss/tok 2.9081 (3.1477)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.085 (0.105)	Data 1.06e-04 (3.50e-04)	Tok/s 91427 (101883)	Loss/tok 2.9801 (3.1469)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.077 (0.105)	Data 5.10e-05 (3.50e-04)	Tok/s 101126 (101891)	Loss/tok 2.9986 (3.1470)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592591089493, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591089494, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.416 (0.416)	Decoder iters 123.0 (123.0)	Tok/s 21649 (21649)
0: Running moses detokenizer
0: BLEU(score=24.317465279305146, counts=[37378, 18827, 10767, 6437], totals=[65707, 62704, 59701, 56704], precisions=[56.88587212930129, 30.025197754529216, 18.034873787708747, 11.351932844243793], bp=1.0, sys_len=65707, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592591090591, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2432, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591090592, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1492	Test BLEU: 24.32
0: Performance: Epoch: 3	Training: 1629506 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592591090592, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592591090592, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-19 11:24:58 AM
RESULT,RNN_TRANSLATOR,,585,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:24:58 AM
RESULT,RNN_TRANSLATOR,,585,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:00 AM
RESULT,RNN_TRANSLATOR,,587,nvidia,2020-06-19 11:15:13 AM
ENDING TIMING RUN AT 2020-06-19 11:25:01 AM
RESULT,RNN_TRANSLATOR,,588,nvidia,2020-06-19 11:15:13 AM
