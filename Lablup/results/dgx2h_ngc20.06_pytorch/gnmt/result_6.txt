+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590491807, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592590491838, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592590491838, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592590491838, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592590491839, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n006
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590499125, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/13929698/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' -n 13 ']'
+ DECAY_INTERVAL=506
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=192
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TEST_BATCH_SIZE=64
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' -n 5 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 9 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ TRAIN_BATCH_SIZE=192
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -n 14 ']'
+ '[' 16 -gt 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 16 -gt 1 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' -n 1 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
STARTING TIMING RUN AT 2020-06-19 11:15:02 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ LR=2.875e-3
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ NUMEPOCHS=8
+ WARMUP_STEPS=200
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' -n 2 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592590504340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504950, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504950, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504951, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504960, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504964, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504979, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590504986, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2573480854
:::MLLOG {"namespace": "", "time_ms": 1592590522911, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2573480854, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 803226146
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([2141696]), new_param_packed_fragment.size()=torch.Size([2141696]), master_param_fragment.size()=torch.Size([2141696])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3083712]), new_param_packed_fragment.size()=torch.Size([3083712]), master_param_fragment.size()=torch.Size([3083712])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([3948032]), new_param_packed_fragment.size()=torch.Size([3948032]), master_param_fragment.size()=torch.Size([3948032])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4982336]), new_param_packed_fragment.size()=torch.Size([4982336]), master_param_fragment.size()=torch.Size([4982336])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([2302464]), new_param_packed_fragment.size()=torch.Size([2302464]), master_param_fragment.size()=torch.Size([2302464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([496128]), new_param_packed_fragment.size()=torch.Size([496128]), master_param_fragment.size()=torch.Size([496128])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([2005568]), new_param_packed_fragment.size()=torch.Size([2005568]), master_param_fragment.size()=torch.Size([2005568])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([1399296]), new_param_packed_fragment.size()=torch.Size([1399296]), master_param_fragment.size()=torch.Size([1399296])
model_param_fragment.size()=torch.Size([3621888]), new_param_packed_fragment.size()=torch.Size([3621888]), master_param_fragment.size()=torch.Size([3621888])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2756160]), new_param_packed_fragment.size()=torch.Size([2756160]), master_param_fragment.size()=torch.Size([2756160])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3698176]), new_param_packed_fragment.size()=torch.Size([3698176]), master_param_fragment.size()=torch.Size([3698176])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2188736]), new_param_packed_fragment.size()=torch.Size([2188736]), master_param_fragment.size()=torch.Size([2188736])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([246272]), new_param_packed_fragment.size()=torch.Size([246272]), master_param_fragment.size()=torch.Size([246272])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2795008]), new_param_packed_fragment.size()=torch.Size([2795008]), master_param_fragment.size()=torch.Size([2795008])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3044864]), new_param_packed_fragment.size()=torch.Size([3044864]), master_param_fragment.size()=torch.Size([3044864])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([2052608]), new_param_packed_fragment.size()=torch.Size([2052608]), master_param_fragment.size()=torch.Size([2052608])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([534976]), new_param_packed_fragment.size()=torch.Size([534976]), master_param_fragment.size()=torch.Size([534976])
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1891840]), new_param_packed_fragment.size()=torch.Size([1891840]), master_param_fragment.size()=torch.Size([1891840])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592590544689, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592590544689, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592590544689, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592590544689, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592590544689, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592590549169, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592590549169, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592590549169, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592590549445, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592590549446, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592590549446, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592590549447, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592590549447, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592590549447, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592590549447, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592590549447, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592590549447, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592590549448, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592590549448, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590549448, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1594583404
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.424 (0.424)	Data 3.24e-01 (3.24e-01)	Tok/s 17492 (17492)	Loss/tok 10.6811 (10.6811)	LR 2.942e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][10/1291]	Time 0.072 (0.122)	Data 1.08e-04 (2.96e-02)	Tok/s 105223 (104623)	Loss/tok 9.4960 (10.0835)	LR 3.619e-05
0: TRAIN [0][20/1291]	Time 0.043 (0.114)	Data 1.08e-04 (1.56e-02)	Tok/s 91529 (110691)	Loss/tok 8.8463 (9.7230)	LR 4.557e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][30/1291]	Time 0.136 (0.104)	Data 1.22e-04 (1.06e-02)	Tok/s 129139 (111016)	Loss/tok 8.9893 (9.5407)	LR 5.606e-05
0: TRAIN [0][40/1291]	Time 0.136 (0.103)	Data 1.10e-04 (8.02e-03)	Tok/s 129323 (112260)	Loss/tok 8.7262 (9.3522)	LR 7.057e-05
0: TRAIN [0][50/1291]	Time 0.173 (0.103)	Data 1.12e-04 (6.47e-03)	Tok/s 128522 (113162)	Loss/tok 8.5118 (9.1752)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.072 (0.100)	Data 1.09e-04 (5.43e-03)	Tok/s 105803 (113264)	Loss/tok 8.0641 (9.0457)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.103 (0.098)	Data 1.06e-04 (4.68e-03)	Tok/s 124164 (113315)	Loss/tok 8.1118 (8.9278)	LR 1.408e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][80/1291]	Time 0.072 (0.096)	Data 1.07e-04 (4.11e-03)	Tok/s 107310 (112720)	Loss/tok 7.8422 (8.8284)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.072 (0.095)	Data 1.08e-04 (3.67e-03)	Tok/s 109228 (112570)	Loss/tok 7.7057 (8.7381)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.103 (0.094)	Data 1.13e-04 (3.32e-03)	Tok/s 120754 (112654)	Loss/tok 7.9616 (8.6595)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.103 (0.093)	Data 1.11e-04 (3.03e-03)	Tok/s 123724 (112636)	Loss/tok 8.0163 (8.5881)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.072 (0.095)	Data 1.14e-04 (2.79e-03)	Tok/s 106538 (113180)	Loss/tok 7.6969 (8.5178)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.174 (0.096)	Data 1.10e-04 (2.59e-03)	Tok/s 129170 (113892)	Loss/tok 7.9462 (8.4526)	LR 5.478e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][140/1291]	Time 0.072 (0.095)	Data 1.06e-04 (2.41e-03)	Tok/s 105143 (113663)	Loss/tok 7.7106 (8.4039)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.103 (0.096)	Data 1.11e-04 (2.26e-03)	Tok/s 121986 (113836)	Loss/tok 7.6657 (8.3543)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.103 (0.096)	Data 1.11e-04 (2.12e-03)	Tok/s 123621 (114061)	Loss/tok 7.5609 (8.3024)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.137 (0.095)	Data 1.12e-04 (2.01e-03)	Tok/s 127788 (114071)	Loss/tok 7.4277 (8.2503)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.072 (0.095)	Data 1.16e-04 (1.90e-03)	Tok/s 109973 (114152)	Loss/tok 6.9007 (8.1942)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.137 (0.095)	Data 1.13e-04 (1.81e-03)	Tok/s 127985 (114306)	Loss/tok 7.1401 (8.1279)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.072 (0.095)	Data 1.09e-04 (1.72e-03)	Tok/s 107088 (114493)	Loss/tok 6.5617 (8.0578)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.103 (0.095)	Data 1.08e-04 (1.65e-03)	Tok/s 121738 (114537)	Loss/tok 6.7029 (7.9898)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.137 (0.095)	Data 1.16e-04 (1.58e-03)	Tok/s 128665 (114511)	Loss/tok 6.5850 (7.9231)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.072 (0.095)	Data 1.14e-04 (1.51e-03)	Tok/s 105872 (114502)	Loss/tok 5.9971 (7.8557)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.073 (0.095)	Data 1.18e-04 (1.46e-03)	Tok/s 106533 (114387)	Loss/tok 5.9768 (7.7900)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.072 (0.094)	Data 1.41e-04 (1.40e-03)	Tok/s 109213 (114235)	Loss/tok 5.8385 (7.7303)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.103 (0.094)	Data 1.16e-04 (1.35e-03)	Tok/s 122757 (114145)	Loss/tok 5.9618 (7.6676)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][270/1291]	Time 0.103 (0.093)	Data 1.21e-04 (1.31e-03)	Tok/s 123140 (113955)	Loss/tok 6.0026 (7.6084)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.103 (0.093)	Data 1.16e-04 (1.27e-03)	Tok/s 117997 (113994)	Loss/tok 5.8852 (7.5427)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.072 (0.093)	Data 1.20e-04 (1.23e-03)	Tok/s 107114 (113937)	Loss/tok 5.3444 (7.4828)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.072 (0.093)	Data 1.11e-04 (1.19e-03)	Tok/s 109962 (113913)	Loss/tok 5.0411 (7.4157)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.104 (0.093)	Data 1.22e-04 (1.16e-03)	Tok/s 119615 (113881)	Loss/tok 5.4074 (7.3526)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.073 (0.093)	Data 1.17e-04 (1.12e-03)	Tok/s 105369 (113769)	Loss/tok 4.9487 (7.2914)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.073 (0.093)	Data 1.77e-04 (1.09e-03)	Tok/s 107360 (113863)	Loss/tok 4.8572 (7.2220)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.073 (0.093)	Data 1.36e-04 (1.07e-03)	Tok/s 104391 (113874)	Loss/tok 4.6354 (7.1578)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.072 (0.093)	Data 1.15e-04 (1.04e-03)	Tok/s 104154 (113908)	Loss/tok 4.7902 (7.0933)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.174 (0.094)	Data 1.25e-04 (1.01e-03)	Tok/s 128313 (114071)	Loss/tok 5.3501 (7.0142)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.137 (0.094)	Data 1.15e-04 (9.91e-04)	Tok/s 126226 (114020)	Loss/tok 4.9120 (6.9544)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.076 (0.094)	Data 1.16e-04 (9.69e-04)	Tok/s 99811 (113923)	Loss/tok 4.2859 (6.8992)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][390/1291]	Time 0.174 (0.094)	Data 1.13e-04 (9.47e-04)	Tok/s 125654 (114047)	Loss/tok 5.2556 (6.8323)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.073 (0.094)	Data 1.14e-04 (9.27e-04)	Tok/s 108110 (114109)	Loss/tok 4.2542 (6.7765)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.104 (0.094)	Data 1.20e-04 (9.08e-04)	Tok/s 121065 (114078)	Loss/tok 4.5165 (6.7244)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.042 (0.094)	Data 1.69e-04 (8.89e-04)	Tok/s 92168 (114135)	Loss/tok 3.5878 (6.6675)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.104 (0.094)	Data 1.96e-04 (8.72e-04)	Tok/s 122130 (114183)	Loss/tok 4.4183 (6.6137)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.073 (0.094)	Data 1.77e-04 (8.55e-04)	Tok/s 105866 (114046)	Loss/tok 4.0119 (6.5712)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.072 (0.093)	Data 1.70e-04 (8.40e-04)	Tok/s 107526 (114001)	Loss/tok 4.1536 (6.5249)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.103 (0.093)	Data 1.12e-04 (8.24e-04)	Tok/s 122214 (114015)	Loss/tok 4.3872 (6.4761)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.104 (0.093)	Data 1.96e-04 (8.09e-04)	Tok/s 123426 (113977)	Loss/tok 4.2874 (6.4309)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.104 (0.093)	Data 1.88e-04 (7.95e-04)	Tok/s 124336 (114010)	Loss/tok 4.2292 (6.3841)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.073 (0.093)	Data 1.18e-04 (7.82e-04)	Tok/s 103632 (113977)	Loss/tok 3.9045 (6.3413)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.103 (0.093)	Data 1.13e-04 (7.69e-04)	Tok/s 123785 (113925)	Loss/tok 4.3555 (6.3017)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.103 (0.093)	Data 1.73e-04 (7.57e-04)	Tok/s 121666 (113950)	Loss/tok 4.0174 (6.2579)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][520/1291]	Time 0.073 (0.093)	Data 1.15e-04 (7.45e-04)	Tok/s 105190 (113924)	Loss/tok 3.8543 (6.2175)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.073 (0.093)	Data 1.79e-04 (7.33e-04)	Tok/s 106605 (113934)	Loss/tok 3.8337 (6.1772)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.073 (0.093)	Data 1.46e-04 (7.22e-04)	Tok/s 107090 (113871)	Loss/tok 3.7875 (6.1425)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.104 (0.093)	Data 1.35e-04 (7.12e-04)	Tok/s 121831 (113913)	Loss/tok 4.0834 (6.1033)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.104 (0.093)	Data 1.64e-04 (7.01e-04)	Tok/s 119357 (113861)	Loss/tok 4.1241 (6.0689)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.073 (0.093)	Data 1.26e-04 (6.92e-04)	Tok/s 106625 (113826)	Loss/tok 3.7607 (6.0353)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.104 (0.093)	Data 1.18e-04 (6.82e-04)	Tok/s 122255 (113841)	Loss/tok 4.0133 (6.0006)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.175 (0.093)	Data 1.79e-04 (6.73e-04)	Tok/s 127626 (113839)	Loss/tok 4.3948 (5.9654)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.103 (0.093)	Data 1.72e-04 (6.65e-04)	Tok/s 122141 (113777)	Loss/tok 4.0917 (5.9355)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.043 (0.093)	Data 1.22e-04 (6.56e-04)	Tok/s 92282 (113718)	Loss/tok 3.1889 (5.9060)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.104 (0.093)	Data 1.63e-04 (6.48e-04)	Tok/s 123353 (113746)	Loss/tok 4.0978 (5.8743)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.073 (0.092)	Data 1.25e-04 (6.40e-04)	Tok/s 105191 (113697)	Loss/tok 3.7345 (5.8464)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.104 (0.092)	Data 1.19e-04 (6.32e-04)	Tok/s 122193 (113625)	Loss/tok 4.0845 (5.8201)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1291]	Time 0.138 (0.092)	Data 1.38e-04 (6.24e-04)	Tok/s 125326 (113643)	Loss/tok 4.1765 (5.7912)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.104 (0.093)	Data 1.16e-04 (6.16e-04)	Tok/s 121165 (113697)	Loss/tok 4.0457 (5.7598)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][670/1291]	Time 0.138 (0.093)	Data 1.17e-04 (6.09e-04)	Tok/s 127933 (113766)	Loss/tok 4.0944 (5.7278)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.104 (0.093)	Data 1.80e-04 (6.02e-04)	Tok/s 122213 (113840)	Loss/tok 3.8362 (5.6963)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.104 (0.093)	Data 1.24e-04 (5.96e-04)	Tok/s 120604 (113859)	Loss/tok 3.8551 (5.6687)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.073 (0.093)	Data 1.16e-04 (5.89e-04)	Tok/s 108788 (113880)	Loss/tok 3.6948 (5.6426)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.073 (0.093)	Data 1.16e-04 (5.82e-04)	Tok/s 104669 (113942)	Loss/tok 3.5522 (5.6132)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.073 (0.093)	Data 1.09e-04 (5.76e-04)	Tok/s 104756 (113889)	Loss/tok 3.6907 (5.5904)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.104 (0.093)	Data 1.41e-04 (5.70e-04)	Tok/s 121669 (113871)	Loss/tok 3.8762 (5.5675)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.104 (0.093)	Data 1.18e-04 (5.65e-04)	Tok/s 119405 (113893)	Loss/tok 4.0070 (5.5439)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.104 (0.094)	Data 1.21e-04 (5.59e-04)	Tok/s 121287 (113927)	Loss/tok 3.8610 (5.5191)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.104 (0.094)	Data 1.17e-04 (5.53e-04)	Tok/s 120548 (113947)	Loss/tok 3.8718 (5.4965)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.47e-04)	Tok/s 104337 (113996)	Loss/tok 3.4280 (5.4717)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.104 (0.094)	Data 1.12e-04 (5.42e-04)	Tok/s 119581 (113979)	Loss/tok 3.8730 (5.4513)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.042 (0.094)	Data 1.14e-04 (5.36e-04)	Tok/s 93707 (114009)	Loss/tok 3.0034 (5.4281)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][800/1291]	Time 0.104 (0.094)	Data 1.14e-04 (5.31e-04)	Tok/s 119734 (114062)	Loss/tok 3.7285 (5.4061)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.073 (0.094)	Data 1.12e-04 (5.26e-04)	Tok/s 106043 (114086)	Loss/tok 3.4106 (5.3846)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.042 (0.094)	Data 1.15e-04 (5.21e-04)	Tok/s 94476 (114080)	Loss/tok 3.0609 (5.3655)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.104 (0.094)	Data 1.13e-04 (5.16e-04)	Tok/s 122112 (114091)	Loss/tok 3.7158 (5.3461)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.104 (0.094)	Data 1.13e-04 (5.11e-04)	Tok/s 119410 (114164)	Loss/tok 3.6988 (5.3238)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.073 (0.094)	Data 1.15e-04 (5.07e-04)	Tok/s 107660 (114191)	Loss/tok 3.4961 (5.3045)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.104 (0.094)	Data 1.13e-04 (5.02e-04)	Tok/s 119403 (114254)	Loss/tok 3.6545 (5.2832)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.98e-04)	Tok/s 104750 (114257)	Loss/tok 3.4500 (5.2651)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.073 (0.094)	Data 1.07e-04 (4.93e-04)	Tok/s 107588 (114223)	Loss/tok 3.5808 (5.2487)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.073 (0.094)	Data 1.17e-04 (4.89e-04)	Tok/s 107607 (114189)	Loss/tok 3.5475 (5.2326)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.073 (0.094)	Data 1.11e-04 (4.85e-04)	Tok/s 105745 (114142)	Loss/tok 3.4758 (5.2172)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.138 (0.094)	Data 1.13e-04 (4.81e-04)	Tok/s 125378 (114133)	Loss/tok 3.8584 (5.2008)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.104 (0.094)	Data 1.14e-04 (4.77e-04)	Tok/s 118247 (114141)	Loss/tok 3.7776 (5.1841)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][930/1291]	Time 0.042 (0.094)	Data 1.14e-04 (4.73e-04)	Tok/s 93782 (114157)	Loss/tok 3.2220 (5.1671)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][940/1291]	Time 0.042 (0.094)	Data 1.10e-04 (4.69e-04)	Tok/s 90308 (114081)	Loss/tok 2.9754 (5.1539)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][950/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.65e-04)	Tok/s 108432 (114129)	Loss/tok 3.4427 (5.1374)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.072 (0.094)	Data 1.14e-04 (4.62e-04)	Tok/s 106590 (114115)	Loss/tok 3.4625 (5.1227)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.072 (0.094)	Data 1.25e-04 (4.58e-04)	Tok/s 108898 (114071)	Loss/tok 3.5434 (5.1091)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.073 (0.094)	Data 1.11e-04 (4.55e-04)	Tok/s 107903 (114097)	Loss/tok 3.6069 (5.0929)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.042 (0.094)	Data 1.13e-04 (4.52e-04)	Tok/s 91487 (114099)	Loss/tok 2.9413 (5.0781)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.175 (0.094)	Data 1.09e-04 (4.49e-04)	Tok/s 127029 (114095)	Loss/tok 3.9942 (5.0638)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.46e-04)	Tok/s 120967 (114056)	Loss/tok 3.6654 (5.0513)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.104 (0.094)	Data 1.14e-04 (4.42e-04)	Tok/s 121311 (114107)	Loss/tok 3.5530 (5.0354)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.042 (0.094)	Data 1.20e-04 (4.39e-04)	Tok/s 91666 (114030)	Loss/tok 2.8860 (5.0242)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.104 (0.094)	Data 1.32e-04 (4.36e-04)	Tok/s 119560 (114043)	Loss/tok 3.5998 (5.0099)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.33e-04)	Tok/s 124614 (114024)	Loss/tok 3.5607 (4.9975)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.073 (0.094)	Data 1.22e-04 (4.30e-04)	Tok/s 107549 (114036)	Loss/tok 3.3030 (4.9846)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1070/1291]	Time 0.104 (0.094)	Data 1.17e-04 (4.27e-04)	Tok/s 120570 (114066)	Loss/tok 3.4297 (4.9704)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.073 (0.094)	Data 1.29e-04 (4.24e-04)	Tok/s 106670 (114011)	Loss/tok 3.3992 (4.9598)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.104 (0.094)	Data 1.21e-04 (4.22e-04)	Tok/s 123056 (114039)	Loss/tok 3.7505 (4.9469)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.19e-04)	Tok/s 120119 (114118)	Loss/tok 3.6744 (4.9324)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.16e-04)	Tok/s 107030 (114080)	Loss/tok 3.4206 (4.9221)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.175 (0.094)	Data 1.16e-04 (4.13e-04)	Tok/s 128353 (114046)	Loss/tok 3.9458 (4.9115)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1130/1291]	Time 0.073 (0.094)	Data 1.10e-04 (4.11e-04)	Tok/s 106726 (113998)	Loss/tok 3.3917 (4.9015)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.073 (0.094)	Data 1.08e-04 (4.08e-04)	Tok/s 108033 (113954)	Loss/tok 3.4531 (4.8917)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.06e-04)	Tok/s 106707 (113951)	Loss/tok 3.4665 (4.8807)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.042 (0.094)	Data 1.10e-04 (4.03e-04)	Tok/s 92623 (113916)	Loss/tok 2.8121 (4.8704)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.042 (0.094)	Data 1.15e-04 (4.01e-04)	Tok/s 94171 (113891)	Loss/tok 2.9528 (4.8605)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.073 (0.094)	Data 1.09e-04 (3.98e-04)	Tok/s 106198 (113918)	Loss/tok 3.5230 (4.8490)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.175 (0.094)	Data 1.23e-04 (3.96e-04)	Tok/s 128892 (113965)	Loss/tok 3.9632 (4.8369)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.93e-04)	Tok/s 104769 (113972)	Loss/tok 3.3955 (4.8266)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.073 (0.094)	Data 1.17e-04 (3.91e-04)	Tok/s 106828 (113997)	Loss/tok 3.2151 (4.8153)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.138 (0.094)	Data 1.13e-04 (3.89e-04)	Tok/s 125693 (113979)	Loss/tok 3.9509 (4.8060)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.042 (0.094)	Data 1.13e-04 (3.87e-04)	Tok/s 94220 (113921)	Loss/tok 2.9325 (4.7979)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.84e-04)	Tok/s 121690 (113922)	Loss/tok 3.5234 (4.7882)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.104 (0.094)	Data 1.13e-04 (3.82e-04)	Tok/s 121538 (113918)	Loss/tok 3.5920 (4.7784)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1260/1291]	Time 0.073 (0.094)	Data 1.40e-04 (3.80e-04)	Tok/s 105357 (113866)	Loss/tok 3.2988 (4.7700)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1270/1291]	Time 0.103 (0.094)	Data 1.32e-04 (3.78e-04)	Tok/s 122361 (113898)	Loss/tok 3.6585 (4.7601)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.76e-04)	Tok/s 104246 (113872)	Loss/tok 3.2107 (4.7513)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.104 (0.094)	Data 5.34e-05 (3.76e-04)	Tok/s 120054 (113861)	Loss/tok 3.6839 (4.7424)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590670713, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590670713, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.409 (0.409)	Decoder iters 127.0 (127.0)	Tok/s 20779 (20779)
0: Running moses detokenizer
0: BLEU(score=19.590860590531626, counts=[33712, 15417, 8165, 4492], totals=[63045, 60042, 57040, 54043], precisions=[53.472916170988974, 25.67702608174278, 14.314516129032258, 8.311899783505726], bp=0.9744613618506887, sys_len=63045, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590671903, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1959, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590671903, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7409	Test BLEU: 19.59
0: Performance: Epoch: 0	Training: 1822491 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592590671904, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590671904, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590671904, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2826912512
0: TRAIN [1][0/1291]	Time 0.398 (0.398)	Data 3.26e-01 (3.26e-01)	Tok/s 19598 (19598)	Loss/tok 3.2269 (3.2269)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.104 (0.129)	Data 1.10e-04 (2.97e-02)	Tok/s 121563 (108308)	Loss/tok 3.6327 (3.5115)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.104 (0.115)	Data 1.12e-04 (1.56e-02)	Tok/s 121124 (111328)	Loss/tok 3.4934 (3.5184)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.104 (0.108)	Data 1.08e-04 (1.06e-02)	Tok/s 121530 (111168)	Loss/tok 3.4738 (3.5066)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.104 (0.107)	Data 1.13e-04 (8.06e-03)	Tok/s 121546 (111819)	Loss/tok 3.4380 (3.5181)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.042 (0.104)	Data 1.07e-04 (6.50e-03)	Tok/s 94625 (111949)	Loss/tok 2.7030 (3.5154)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.073 (0.104)	Data 1.15e-04 (5.46e-03)	Tok/s 109115 (112324)	Loss/tok 3.3480 (3.5356)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.073 (0.100)	Data 1.11e-04 (4.70e-03)	Tok/s 104532 (111696)	Loss/tok 3.2590 (3.5098)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.073 (0.099)	Data 1.16e-04 (4.14e-03)	Tok/s 104819 (111996)	Loss/tok 3.2296 (3.5041)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.103 (0.097)	Data 1.23e-04 (3.69e-03)	Tok/s 123998 (111955)	Loss/tok 3.4772 (3.4931)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.104 (0.096)	Data 1.25e-04 (3.34e-03)	Tok/s 122268 (111836)	Loss/tok 3.3976 (3.4854)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][110/1291]	Time 0.104 (0.097)	Data 1.20e-04 (3.05e-03)	Tok/s 121214 (112414)	Loss/tok 3.5414 (3.4932)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.104 (0.098)	Data 1.15e-04 (2.81e-03)	Tok/s 120677 (112987)	Loss/tok 3.4678 (3.5068)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.073 (0.099)	Data 1.17e-04 (2.60e-03)	Tok/s 106142 (113322)	Loss/tok 3.1961 (3.5109)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.104 (0.099)	Data 1.13e-04 (2.42e-03)	Tok/s 121150 (113552)	Loss/tok 3.3779 (3.5144)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.073 (0.099)	Data 1.13e-04 (2.27e-03)	Tok/s 105689 (113676)	Loss/tok 3.2807 (3.5141)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.104 (0.099)	Data 1.15e-04 (2.14e-03)	Tok/s 121266 (113763)	Loss/tok 3.2677 (3.5128)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.104 (0.099)	Data 1.13e-04 (2.02e-03)	Tok/s 120183 (113777)	Loss/tok 3.6240 (3.5180)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.104 (0.099)	Data 1.13e-04 (1.91e-03)	Tok/s 121615 (114102)	Loss/tok 3.4745 (3.5172)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.139 (0.099)	Data 1.11e-04 (1.82e-03)	Tok/s 126382 (114098)	Loss/tok 3.6901 (3.5196)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.176 (0.100)	Data 1.10e-04 (1.74e-03)	Tok/s 128304 (114295)	Loss/tok 3.8895 (3.5235)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.176 (0.100)	Data 1.16e-04 (1.66e-03)	Tok/s 125860 (114372)	Loss/tok 3.8498 (3.5276)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.104 (0.100)	Data 1.16e-04 (1.59e-03)	Tok/s 121510 (114437)	Loss/tok 3.4846 (3.5241)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.073 (0.100)	Data 1.15e-04 (1.52e-03)	Tok/s 104902 (114502)	Loss/tok 3.3201 (3.5230)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][240/1291]	Time 0.073 (0.099)	Data 1.13e-04 (1.47e-03)	Tok/s 107055 (114377)	Loss/tok 3.1851 (3.5182)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.073 (0.100)	Data 1.11e-04 (1.41e-03)	Tok/s 106196 (114480)	Loss/tok 3.1331 (3.5200)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.176 (0.099)	Data 1.15e-04 (1.36e-03)	Tok/s 125704 (114370)	Loss/tok 4.0085 (3.5188)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.176 (0.099)	Data 1.16e-04 (1.32e-03)	Tok/s 127992 (114255)	Loss/tok 3.8546 (3.5171)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.073 (0.099)	Data 1.13e-04 (1.27e-03)	Tok/s 104938 (114156)	Loss/tok 3.1789 (3.5151)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.042 (0.098)	Data 1.15e-04 (1.23e-03)	Tok/s 94846 (114007)	Loss/tok 2.8282 (3.5109)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][300/1291]	Time 0.104 (0.098)	Data 1.11e-04 (1.20e-03)	Tok/s 120559 (113935)	Loss/tok 3.5064 (3.5106)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.073 (0.098)	Data 1.16e-04 (1.16e-03)	Tok/s 106246 (114074)	Loss/tok 3.2013 (3.5089)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.104 (0.098)	Data 1.13e-04 (1.13e-03)	Tok/s 121377 (114065)	Loss/tok 3.3956 (3.5058)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.042 (0.097)	Data 1.15e-04 (1.10e-03)	Tok/s 91558 (113958)	Loss/tok 2.7799 (3.5020)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.138 (0.097)	Data 1.14e-04 (1.07e-03)	Tok/s 124431 (113856)	Loss/tok 3.6669 (3.5027)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.104 (0.096)	Data 1.15e-04 (1.04e-03)	Tok/s 121568 (113719)	Loss/tok 3.4866 (3.4979)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.104 (0.096)	Data 1.15e-04 (1.02e-03)	Tok/s 120497 (113795)	Loss/tok 3.6192 (3.4951)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.176 (0.096)	Data 1.12e-04 (9.92e-04)	Tok/s 125184 (113893)	Loss/tok 3.7785 (3.4960)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.104 (0.097)	Data 1.11e-04 (9.69e-04)	Tok/s 119956 (113926)	Loss/tok 3.3982 (3.4949)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.042 (0.096)	Data 1.11e-04 (9.47e-04)	Tok/s 92983 (113806)	Loss/tok 2.6931 (3.4920)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.073 (0.096)	Data 1.22e-04 (9.26e-04)	Tok/s 107458 (113764)	Loss/tok 3.3907 (3.4905)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.138 (0.096)	Data 1.14e-04 (9.07e-04)	Tok/s 128981 (113888)	Loss/tok 3.5740 (3.4900)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.073 (0.096)	Data 1.16e-04 (8.88e-04)	Tok/s 104766 (113816)	Loss/tok 3.3593 (3.4886)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][430/1291]	Time 0.104 (0.096)	Data 1.13e-04 (8.70e-04)	Tok/s 120673 (113790)	Loss/tok 3.5775 (3.4866)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.073 (0.096)	Data 1.17e-04 (8.53e-04)	Tok/s 106101 (113812)	Loss/tok 3.3473 (3.4883)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.073 (0.096)	Data 1.13e-04 (8.36e-04)	Tok/s 106079 (113752)	Loss/tok 3.1779 (3.4851)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.104 (0.095)	Data 1.11e-04 (8.21e-04)	Tok/s 120347 (113660)	Loss/tok 3.6331 (3.4821)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.073 (0.095)	Data 1.32e-04 (8.06e-04)	Tok/s 107476 (113599)	Loss/tok 3.2847 (3.4795)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.104 (0.095)	Data 1.11e-04 (7.92e-04)	Tok/s 121900 (113536)	Loss/tok 3.4268 (3.4767)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][490/1291]	Time 0.138 (0.095)	Data 1.24e-04 (7.78e-04)	Tok/s 124879 (113578)	Loss/tok 3.5713 (3.4763)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.104 (0.095)	Data 1.56e-04 (7.65e-04)	Tok/s 121074 (113657)	Loss/tok 3.5242 (3.4766)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.138 (0.095)	Data 1.15e-04 (7.53e-04)	Tok/s 128452 (113653)	Loss/tok 3.7836 (3.4751)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.137 (0.095)	Data 1.70e-04 (7.41e-04)	Tok/s 128048 (113648)	Loss/tok 3.6648 (3.4744)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.104 (0.095)	Data 1.18e-04 (7.29e-04)	Tok/s 121331 (113610)	Loss/tok 3.4911 (3.4725)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.073 (0.094)	Data 1.19e-04 (7.19e-04)	Tok/s 110064 (113453)	Loss/tok 3.1521 (3.4703)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.073 (0.094)	Data 1.21e-04 (7.08e-04)	Tok/s 105171 (113462)	Loss/tok 3.1300 (3.4702)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.042 (0.094)	Data 1.72e-04 (6.98e-04)	Tok/s 92831 (113476)	Loss/tok 2.8048 (3.4692)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.104 (0.094)	Data 1.71e-04 (6.88e-04)	Tok/s 123241 (113415)	Loss/tok 3.4561 (3.4682)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.042 (0.094)	Data 1.78e-04 (6.79e-04)	Tok/s 94843 (113398)	Loss/tok 2.7807 (3.4692)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.073 (0.094)	Data 1.26e-04 (6.69e-04)	Tok/s 106861 (113409)	Loss/tok 3.1070 (3.4689)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.042 (0.094)	Data 1.29e-04 (6.60e-04)	Tok/s 92239 (113424)	Loss/tok 2.7176 (3.4696)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.073 (0.094)	Data 1.36e-04 (6.52e-04)	Tok/s 107433 (113434)	Loss/tok 3.1723 (3.4698)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][620/1291]	Time 0.042 (0.094)	Data 1.36e-04 (6.44e-04)	Tok/s 96842 (113378)	Loss/tok 2.8612 (3.4682)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.104 (0.094)	Data 1.78e-04 (6.35e-04)	Tok/s 120176 (113393)	Loss/tok 3.4771 (3.4679)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.104 (0.094)	Data 1.30e-04 (6.28e-04)	Tok/s 122548 (113436)	Loss/tok 3.5018 (3.4673)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.073 (0.094)	Data 1.18e-04 (6.20e-04)	Tok/s 108435 (113436)	Loss/tok 3.0863 (3.4657)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.073 (0.094)	Data 1.44e-04 (6.13e-04)	Tok/s 106816 (113454)	Loss/tok 3.1066 (3.4667)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.138 (0.094)	Data 1.76e-04 (6.06e-04)	Tok/s 126436 (113538)	Loss/tok 3.5733 (3.4670)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.073 (0.095)	Data 1.20e-04 (5.99e-04)	Tok/s 107251 (113592)	Loss/tok 3.3019 (3.4668)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.176 (0.095)	Data 1.17e-04 (5.92e-04)	Tok/s 123563 (113615)	Loss/tok 3.8729 (3.4668)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.073 (0.094)	Data 1.12e-04 (5.85e-04)	Tok/s 107989 (113551)	Loss/tok 3.1745 (3.4647)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.073 (0.094)	Data 1.10e-04 (5.79e-04)	Tok/s 105199 (113456)	Loss/tok 3.3493 (3.4624)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.073 (0.094)	Data 1.13e-04 (5.72e-04)	Tok/s 108333 (113402)	Loss/tok 3.1012 (3.4604)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.176 (0.094)	Data 1.13e-04 (5.66e-04)	Tok/s 128471 (113491)	Loss/tok 3.6885 (3.4626)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.073 (0.094)	Data 1.15e-04 (5.60e-04)	Tok/s 108630 (113466)	Loss/tok 3.2497 (3.4621)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][750/1291]	Time 0.104 (0.094)	Data 1.13e-04 (5.54e-04)	Tok/s 120199 (113457)	Loss/tok 3.4689 (3.4619)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.073 (0.094)	Data 1.10e-04 (5.48e-04)	Tok/s 103586 (113426)	Loss/tok 3.0954 (3.4610)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.104 (0.094)	Data 1.07e-04 (5.42e-04)	Tok/s 122087 (113417)	Loss/tok 3.4195 (3.4601)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.138 (0.094)	Data 1.19e-04 (5.37e-04)	Tok/s 125021 (113352)	Loss/tok 3.6513 (3.4588)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.104 (0.094)	Data 1.10e-04 (5.32e-04)	Tok/s 121603 (113386)	Loss/tok 3.4553 (3.4583)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.073 (0.094)	Data 1.13e-04 (5.26e-04)	Tok/s 106034 (113419)	Loss/tok 3.0353 (3.4572)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.21e-04)	Tok/s 105878 (113428)	Loss/tok 3.1816 (3.4556)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.16e-04)	Tok/s 104444 (113393)	Loss/tok 3.2139 (3.4543)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.073 (0.094)	Data 1.10e-04 (5.11e-04)	Tok/s 106863 (113438)	Loss/tok 3.1547 (3.4544)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.073 (0.094)	Data 1.22e-04 (5.07e-04)	Tok/s 105835 (113422)	Loss/tok 3.2872 (3.4534)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][850/1291]	Time 0.104 (0.094)	Data 1.24e-04 (5.02e-04)	Tok/s 121618 (113480)	Loss/tok 3.4059 (3.4531)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.98e-04)	Tok/s 120389 (113439)	Loss/tok 3.4232 (3.4517)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.042 (0.094)	Data 1.19e-04 (4.93e-04)	Tok/s 92304 (113484)	Loss/tok 2.6640 (3.4505)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.138 (0.094)	Data 1.09e-04 (4.89e-04)	Tok/s 129034 (113530)	Loss/tok 3.4697 (3.4500)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.138 (0.094)	Data 1.14e-04 (4.85e-04)	Tok/s 126895 (113492)	Loss/tok 3.5899 (3.4490)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.073 (0.094)	Data 1.22e-04 (4.81e-04)	Tok/s 105026 (113517)	Loss/tok 3.1751 (3.4480)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.104 (0.094)	Data 1.11e-04 (4.77e-04)	Tok/s 121511 (113511)	Loss/tok 3.4300 (3.4468)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.176 (0.094)	Data 1.15e-04 (4.73e-04)	Tok/s 126989 (113471)	Loss/tok 3.7182 (3.4455)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.104 (0.094)	Data 1.31e-04 (4.69e-04)	Tok/s 121274 (113490)	Loss/tok 3.4425 (3.4457)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.139 (0.094)	Data 1.27e-04 (4.65e-04)	Tok/s 125156 (113546)	Loss/tok 3.4935 (3.4456)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.176 (0.094)	Data 1.15e-04 (4.61e-04)	Tok/s 129127 (113573)	Loss/tok 3.7509 (3.4462)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.073 (0.094)	Data 1.13e-04 (4.58e-04)	Tok/s 106190 (113525)	Loss/tok 3.2106 (3.4451)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][970/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.54e-04)	Tok/s 103091 (113471)	Loss/tok 3.0726 (3.4447)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.106 (0.094)	Data 1.23e-04 (4.51e-04)	Tok/s 119119 (113414)	Loss/tok 3.4322 (3.4434)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.104 (0.094)	Data 1.12e-04 (4.47e-04)	Tok/s 120956 (113410)	Loss/tok 3.5511 (3.4427)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.44e-04)	Tok/s 120937 (113447)	Loss/tok 3.3555 (3.4421)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.042 (0.094)	Data 1.13e-04 (4.41e-04)	Tok/s 93492 (113441)	Loss/tok 2.6811 (3.4417)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.176 (0.094)	Data 1.63e-04 (4.38e-04)	Tok/s 128378 (113428)	Loss/tok 3.7944 (3.4413)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.042 (0.094)	Data 1.14e-04 (4.35e-04)	Tok/s 94280 (113460)	Loss/tok 2.7281 (3.4414)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.105 (0.094)	Data 1.13e-04 (4.31e-04)	Tok/s 120462 (113473)	Loss/tok 3.3932 (3.4404)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.073 (0.094)	Data 1.16e-04 (4.28e-04)	Tok/s 108212 (113479)	Loss/tok 3.1333 (3.4398)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.176 (0.094)	Data 1.18e-04 (4.26e-04)	Tok/s 126888 (113523)	Loss/tok 3.8253 (3.4405)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.073 (0.094)	Data 1.19e-04 (4.23e-04)	Tok/s 103594 (113478)	Loss/tok 3.0616 (3.4391)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.104 (0.094)	Data 1.74e-04 (4.20e-04)	Tok/s 120907 (113529)	Loss/tok 3.3272 (3.4394)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.073 (0.094)	Data 1.20e-04 (4.17e-04)	Tok/s 105268 (113530)	Loss/tok 3.3365 (3.4390)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1100/1291]	Time 0.104 (0.094)	Data 1.22e-04 (4.15e-04)	Tok/s 121717 (113517)	Loss/tok 3.3945 (3.4376)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.073 (0.094)	Data 1.14e-04 (4.12e-04)	Tok/s 107859 (113494)	Loss/tok 3.0089 (3.4360)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.073 (0.094)	Data 1.11e-04 (4.09e-04)	Tok/s 104514 (113471)	Loss/tok 3.0295 (3.4354)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.042 (0.094)	Data 1.13e-04 (4.07e-04)	Tok/s 93521 (113473)	Loss/tok 2.7047 (3.4357)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.042 (0.094)	Data 1.13e-04 (4.04e-04)	Tok/s 96750 (113434)	Loss/tok 2.7301 (3.4344)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.01e-04)	Tok/s 121926 (113447)	Loss/tok 3.2156 (3.4331)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.042 (0.094)	Data 1.11e-04 (3.99e-04)	Tok/s 93544 (113420)	Loss/tok 2.6133 (3.4320)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.97e-04)	Tok/s 107088 (113404)	Loss/tok 3.1549 (3.4309)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.104 (0.094)	Data 1.12e-04 (3.94e-04)	Tok/s 122127 (113440)	Loss/tok 3.3055 (3.4302)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.073 (0.094)	Data 1.17e-04 (3.92e-04)	Tok/s 103589 (113429)	Loss/tok 3.1776 (3.4302)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.073 (0.094)	Data 1.21e-04 (3.90e-04)	Tok/s 105916 (113416)	Loss/tok 3.1615 (3.4288)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.176 (0.094)	Data 1.16e-04 (3.87e-04)	Tok/s 128555 (113441)	Loss/tok 3.7667 (3.4286)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.104 (0.094)	Data 1.19e-04 (3.85e-04)	Tok/s 122299 (113468)	Loss/tok 3.4523 (3.4279)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1230/1291]	Time 0.104 (0.094)	Data 1.87e-04 (3.83e-04)	Tok/s 121292 (113512)	Loss/tok 3.4607 (3.4277)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.073 (0.094)	Data 1.10e-04 (3.81e-04)	Tok/s 109279 (113508)	Loss/tok 3.1133 (3.4264)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.79e-04)	Tok/s 106987 (113460)	Loss/tok 3.1406 (3.4248)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.176 (0.094)	Data 1.16e-04 (3.77e-04)	Tok/s 125284 (113461)	Loss/tok 3.7064 (3.4251)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.104 (0.094)	Data 1.22e-04 (3.75e-04)	Tok/s 121186 (113466)	Loss/tok 3.5006 (3.4242)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.042 (0.094)	Data 1.20e-04 (3.73e-04)	Tok/s 92850 (113472)	Loss/tok 2.7463 (3.4238)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.104 (0.094)	Data 5.22e-05 (3.72e-04)	Tok/s 122060 (113532)	Loss/tok 3.3199 (3.4241)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590793918, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590793918, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.482 (0.482)	Decoder iters 149.0 (149.0)	Tok/s 19010 (19010)
0: Running moses detokenizer
0: BLEU(score=21.729533082971948, counts=[36242, 17413, 9633, 5545], totals=[66950, 63947, 60945, 57948], precisions=[54.13293502613891, 27.230362644064616, 15.806054639428993, 9.568923862773522], bp=1.0, sys_len=66950, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590795168, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2173, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590795168, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4232	Test BLEU: 21.73
0: Performance: Epoch: 1	Training: 1815627 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592590795169, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590795169, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590795169, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2698669466
0: TRAIN [2][0/1291]	Time 0.415 (0.415)	Data 3.09e-01 (3.09e-01)	Tok/s 30556 (30556)	Loss/tok 3.2960 (3.2960)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.104 (0.124)	Data 1.18e-04 (2.82e-02)	Tok/s 120376 (107396)	Loss/tok 3.3766 (3.2518)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.073 (0.101)	Data 1.14e-04 (1.48e-02)	Tok/s 107522 (107832)	Loss/tok 3.0169 (3.1946)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][30/1291]	Time 0.044 (0.100)	Data 1.15e-04 (1.01e-02)	Tok/s 88328 (109521)	Loss/tok 2.6985 (3.2419)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.104 (0.100)	Data 1.13e-04 (7.66e-03)	Tok/s 121330 (111538)	Loss/tok 3.3808 (3.2547)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.138 (0.101)	Data 1.09e-04 (6.18e-03)	Tok/s 127014 (112320)	Loss/tok 3.3076 (3.2766)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.073 (0.099)	Data 1.11e-04 (5.19e-03)	Tok/s 106769 (112734)	Loss/tok 3.2242 (3.2745)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.138 (0.100)	Data 1.11e-04 (4.47e-03)	Tok/s 126182 (112976)	Loss/tok 3.4605 (3.2838)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][80/1291]	Time 0.042 (0.100)	Data 1.11e-04 (3.93e-03)	Tok/s 94972 (113260)	Loss/tok 2.6270 (3.2964)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.073 (0.099)	Data 1.12e-04 (3.51e-03)	Tok/s 103890 (113031)	Loss/tok 2.9988 (3.2886)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.073 (0.098)	Data 1.18e-04 (3.18e-03)	Tok/s 105418 (112967)	Loss/tok 3.0711 (3.2876)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.138 (0.098)	Data 1.34e-04 (2.90e-03)	Tok/s 127992 (113161)	Loss/tok 3.4614 (3.2919)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.073 (0.097)	Data 1.28e-04 (2.67e-03)	Tok/s 106859 (113097)	Loss/tok 3.0047 (3.2857)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.073 (0.097)	Data 1.18e-04 (2.48e-03)	Tok/s 106934 (113256)	Loss/tok 2.9546 (3.2846)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.138 (0.097)	Data 1.17e-04 (2.31e-03)	Tok/s 127349 (113346)	Loss/tok 3.4851 (3.2845)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.138 (0.097)	Data 1.18e-04 (2.16e-03)	Tok/s 126658 (113426)	Loss/tok 3.4255 (3.2869)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.073 (0.096)	Data 1.15e-04 (2.04e-03)	Tok/s 106657 (113355)	Loss/tok 3.0720 (3.2837)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.104 (0.096)	Data 1.15e-04 (1.93e-03)	Tok/s 120156 (113295)	Loss/tok 3.3075 (3.2838)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.104 (0.097)	Data 1.18e-04 (1.83e-03)	Tok/s 120289 (113624)	Loss/tok 3.2736 (3.2845)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.073 (0.096)	Data 1.22e-04 (1.74e-03)	Tok/s 102142 (113466)	Loss/tok 3.1005 (3.2792)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.073 (0.096)	Data 1.13e-04 (1.66e-03)	Tok/s 106228 (113565)	Loss/tok 2.9730 (3.2790)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][210/1291]	Time 0.073 (0.096)	Data 1.18e-04 (1.58e-03)	Tok/s 108863 (113691)	Loss/tok 3.0574 (3.2827)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.104 (0.096)	Data 1.16e-04 (1.52e-03)	Tok/s 120539 (113867)	Loss/tok 3.2919 (3.2840)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.073 (0.096)	Data 1.13e-04 (1.46e-03)	Tok/s 103948 (113667)	Loss/tok 3.1448 (3.2786)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.104 (0.096)	Data 1.13e-04 (1.40e-03)	Tok/s 124228 (113664)	Loss/tok 3.2993 (3.2803)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.104 (0.095)	Data 1.20e-04 (1.35e-03)	Tok/s 120579 (113619)	Loss/tok 3.2898 (3.2773)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.104 (0.095)	Data 1.12e-04 (1.30e-03)	Tok/s 121434 (113556)	Loss/tok 3.2375 (3.2758)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.104 (0.095)	Data 1.18e-04 (1.26e-03)	Tok/s 121555 (113598)	Loss/tok 3.2230 (3.2752)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.138 (0.094)	Data 1.38e-04 (1.22e-03)	Tok/s 125188 (113379)	Loss/tok 3.4106 (3.2716)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.104 (0.094)	Data 1.17e-04 (1.18e-03)	Tok/s 123695 (113500)	Loss/tok 3.1904 (3.2724)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.073 (0.093)	Data 1.16e-04 (1.14e-03)	Tok/s 106608 (113305)	Loss/tok 3.1706 (3.2686)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.176 (0.094)	Data 1.19e-04 (1.11e-03)	Tok/s 126756 (113466)	Loss/tok 3.7573 (3.2765)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.138 (0.094)	Data 1.11e-04 (1.08e-03)	Tok/s 127280 (113557)	Loss/tok 3.5475 (3.2798)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.073 (0.094)	Data 1.09e-04 (1.05e-03)	Tok/s 106215 (113521)	Loss/tok 3.0557 (3.2765)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][340/1291]	Time 0.073 (0.094)	Data 1.08e-04 (1.02e-03)	Tok/s 104668 (113313)	Loss/tok 3.0359 (3.2737)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.138 (0.094)	Data 1.11e-04 (9.97e-04)	Tok/s 126514 (113365)	Loss/tok 3.4298 (3.2742)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.104 (0.094)	Data 1.10e-04 (9.73e-04)	Tok/s 120979 (113315)	Loss/tok 3.2967 (3.2752)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.176 (0.094)	Data 1.09e-04 (9.50e-04)	Tok/s 126301 (113397)	Loss/tok 3.5462 (3.2792)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.138 (0.094)	Data 1.12e-04 (9.28e-04)	Tok/s 126944 (113466)	Loss/tok 3.5822 (3.2808)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.104 (0.095)	Data 1.12e-04 (9.07e-04)	Tok/s 121943 (113583)	Loss/tok 3.2743 (3.2820)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.073 (0.094)	Data 1.14e-04 (8.87e-04)	Tok/s 104313 (113459)	Loss/tok 3.0106 (3.2795)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][410/1291]	Time 0.176 (0.095)	Data 1.10e-04 (8.68e-04)	Tok/s 125826 (113546)	Loss/tok 3.7754 (3.2837)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.042 (0.095)	Data 1.12e-04 (8.51e-04)	Tok/s 95266 (113518)	Loss/tok 2.7272 (3.2855)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.073 (0.095)	Data 1.09e-04 (8.34e-04)	Tok/s 104051 (113490)	Loss/tok 3.1980 (3.2864)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.073 (0.095)	Data 1.12e-04 (8.17e-04)	Tok/s 108795 (113523)	Loss/tok 3.1065 (3.2864)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.138 (0.095)	Data 1.12e-04 (8.01e-04)	Tok/s 128388 (113706)	Loss/tok 3.2812 (3.2886)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.104 (0.095)	Data 1.21e-04 (7.87e-04)	Tok/s 122932 (113700)	Loss/tok 3.2339 (3.2899)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.104 (0.095)	Data 1.16e-04 (7.72e-04)	Tok/s 120218 (113774)	Loss/tok 3.1640 (3.2904)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.104 (0.095)	Data 1.11e-04 (7.59e-04)	Tok/s 122011 (113753)	Loss/tok 3.2559 (3.2907)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.073 (0.095)	Data 1.07e-04 (7.46e-04)	Tok/s 107358 (113771)	Loss/tok 2.9240 (3.2915)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.104 (0.095)	Data 1.10e-04 (7.33e-04)	Tok/s 120451 (113721)	Loss/tok 3.2520 (3.2896)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.073 (0.095)	Data 1.14e-04 (7.21e-04)	Tok/s 104830 (113641)	Loss/tok 3.1474 (3.2880)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.104 (0.095)	Data 1.33e-04 (7.09e-04)	Tok/s 119771 (113703)	Loss/tok 3.2674 (3.2898)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.042 (0.095)	Data 1.26e-04 (6.98e-04)	Tok/s 94474 (113701)	Loss/tok 2.6319 (3.2896)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][540/1291]	Time 0.042 (0.095)	Data 1.20e-04 (6.87e-04)	Tok/s 98323 (113699)	Loss/tok 2.5434 (3.2902)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.104 (0.095)	Data 1.12e-04 (6.77e-04)	Tok/s 120218 (113712)	Loss/tok 3.2207 (3.2916)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.073 (0.095)	Data 1.17e-04 (6.67e-04)	Tok/s 108395 (113679)	Loss/tok 3.0811 (3.2910)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.073 (0.095)	Data 1.14e-04 (6.58e-04)	Tok/s 106656 (113725)	Loss/tok 3.0218 (3.2918)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.104 (0.095)	Data 1.11e-04 (6.48e-04)	Tok/s 121562 (113753)	Loss/tok 3.3015 (3.2926)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.073 (0.095)	Data 1.11e-04 (6.39e-04)	Tok/s 108030 (113710)	Loss/tok 2.9944 (3.2919)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.138 (0.095)	Data 1.15e-04 (6.30e-04)	Tok/s 125769 (113746)	Loss/tok 3.5766 (3.2924)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.042 (0.095)	Data 1.14e-04 (6.22e-04)	Tok/s 94553 (113635)	Loss/tok 2.7090 (3.2908)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.073 (0.095)	Data 1.12e-04 (6.14e-04)	Tok/s 106638 (113507)	Loss/tok 3.1203 (3.2895)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][630/1291]	Time 0.073 (0.095)	Data 1.13e-04 (6.06e-04)	Tok/s 109012 (113584)	Loss/tok 3.1031 (3.2912)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.075 (0.095)	Data 1.14e-04 (5.98e-04)	Tok/s 103779 (113543)	Loss/tok 3.2206 (3.2916)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.073 (0.095)	Data 1.11e-04 (5.91e-04)	Tok/s 106555 (113457)	Loss/tok 3.1464 (3.2898)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.104 (0.095)	Data 1.13e-04 (5.83e-04)	Tok/s 119415 (113513)	Loss/tok 3.2825 (3.2920)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.073 (0.095)	Data 1.23e-04 (5.76e-04)	Tok/s 105141 (113469)	Loss/tok 3.1603 (3.2900)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.176 (0.095)	Data 1.14e-04 (5.70e-04)	Tok/s 129032 (113510)	Loss/tok 3.6741 (3.2909)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.073 (0.095)	Data 1.08e-04 (5.63e-04)	Tok/s 102305 (113555)	Loss/tok 3.0647 (3.2913)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.105 (0.095)	Data 1.10e-04 (5.57e-04)	Tok/s 118502 (113561)	Loss/tok 3.4087 (3.2904)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.176 (0.095)	Data 1.13e-04 (5.50e-04)	Tok/s 127590 (113589)	Loss/tok 3.6212 (3.2913)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.072 (0.095)	Data 1.13e-04 (5.44e-04)	Tok/s 109937 (113485)	Loss/tok 3.0864 (3.2899)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.139 (0.095)	Data 1.10e-04 (5.38e-04)	Tok/s 124997 (113480)	Loss/tok 3.4962 (3.2890)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.104 (0.095)	Data 1.20e-04 (5.33e-04)	Tok/s 119296 (113527)	Loss/tok 3.2907 (3.2896)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.073 (0.095)	Data 1.14e-04 (5.27e-04)	Tok/s 107587 (113534)	Loss/tok 2.9960 (3.2892)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][760/1291]	Time 0.042 (0.094)	Data 1.10e-04 (5.22e-04)	Tok/s 96057 (113445)	Loss/tok 2.6291 (3.2876)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.042 (0.094)	Data 1.14e-04 (5.16e-04)	Tok/s 94415 (113410)	Loss/tok 2.6001 (3.2874)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.104 (0.094)	Data 1.15e-04 (5.11e-04)	Tok/s 122596 (113450)	Loss/tok 3.2960 (3.2872)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.073 (0.094)	Data 1.10e-04 (5.06e-04)	Tok/s 106577 (113397)	Loss/tok 3.0939 (3.2872)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.073 (0.094)	Data 1.10e-04 (5.01e-04)	Tok/s 107138 (113316)	Loss/tok 3.0222 (3.2853)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.073 (0.094)	Data 1.14e-04 (4.96e-04)	Tok/s 105417 (113361)	Loss/tok 3.1090 (3.2863)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.104 (0.094)	Data 1.07e-04 (4.92e-04)	Tok/s 120295 (113417)	Loss/tok 3.2031 (3.2876)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.073 (0.094)	Data 1.14e-04 (4.87e-04)	Tok/s 105428 (113326)	Loss/tok 3.0862 (3.2861)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.104 (0.094)	Data 1.12e-04 (4.83e-04)	Tok/s 122267 (113406)	Loss/tok 3.3570 (3.2868)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.78e-04)	Tok/s 120752 (113394)	Loss/tok 3.2339 (3.2858)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.073 (0.094)	Data 1.10e-04 (4.74e-04)	Tok/s 108230 (113421)	Loss/tok 2.9465 (3.2865)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.042 (0.094)	Data 1.11e-04 (4.70e-04)	Tok/s 93553 (113351)	Loss/tok 2.5370 (3.2858)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.104 (0.094)	Data 1.08e-04 (4.66e-04)	Tok/s 121856 (113293)	Loss/tok 3.3060 (3.2843)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][890/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.62e-04)	Tok/s 105519 (113247)	Loss/tok 2.9897 (3.2833)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.58e-04)	Tok/s 121617 (113280)	Loss/tok 3.2666 (3.2831)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.073 (0.094)	Data 1.15e-04 (4.54e-04)	Tok/s 106874 (113221)	Loss/tok 3.1065 (3.2819)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][920/1291]	Time 0.138 (0.094)	Data 1.04e-04 (4.50e-04)	Tok/s 128203 (113212)	Loss/tok 3.3746 (3.2816)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.073 (0.093)	Data 1.11e-04 (4.47e-04)	Tok/s 106656 (113203)	Loss/tok 3.1588 (3.2815)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.042 (0.093)	Data 1.72e-04 (4.43e-04)	Tok/s 95511 (113200)	Loss/tok 2.5754 (3.2812)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.104 (0.093)	Data 1.12e-04 (4.40e-04)	Tok/s 122719 (113200)	Loss/tok 3.2249 (3.2803)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.073 (0.093)	Data 1.12e-04 (4.36e-04)	Tok/s 106998 (113179)	Loss/tok 3.0861 (3.2807)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.073 (0.093)	Data 1.11e-04 (4.33e-04)	Tok/s 107957 (113182)	Loss/tok 3.1619 (3.2802)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.104 (0.094)	Data 1.10e-04 (4.30e-04)	Tok/s 123366 (113255)	Loss/tok 3.2367 (3.2804)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.104 (0.094)	Data 1.10e-04 (4.27e-04)	Tok/s 120383 (113253)	Loss/tok 3.1671 (3.2798)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.073 (0.094)	Data 1.08e-04 (4.23e-04)	Tok/s 108109 (113284)	Loss/tok 3.0401 (3.2800)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.138 (0.093)	Data 1.12e-04 (4.20e-04)	Tok/s 126708 (113212)	Loss/tok 3.4325 (3.2788)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.073 (0.093)	Data 1.17e-04 (4.17e-04)	Tok/s 105077 (113204)	Loss/tok 3.0841 (3.2788)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.073 (0.093)	Data 1.11e-04 (4.14e-04)	Tok/s 105639 (113216)	Loss/tok 3.0911 (3.2793)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.073 (0.093)	Data 1.19e-04 (4.12e-04)	Tok/s 104979 (113238)	Loss/tok 2.9870 (3.2791)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1050/1291]	Time 0.104 (0.093)	Data 1.09e-04 (4.09e-04)	Tok/s 121088 (113214)	Loss/tok 3.2955 (3.2793)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1060/1291]	Time 0.104 (0.094)	Data 1.18e-04 (4.06e-04)	Tok/s 121339 (113256)	Loss/tok 3.1949 (3.2804)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.104 (0.094)	Data 1.08e-04 (4.03e-04)	Tok/s 120606 (113244)	Loss/tok 3.3046 (3.2806)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.104 (0.094)	Data 1.11e-04 (4.01e-04)	Tok/s 119942 (113257)	Loss/tok 3.4025 (3.2804)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.98e-04)	Tok/s 119867 (113238)	Loss/tok 3.3169 (3.2802)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.042 (0.093)	Data 1.11e-04 (3.95e-04)	Tok/s 92571 (113219)	Loss/tok 2.6175 (3.2795)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1110/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.93e-04)	Tok/s 105041 (113269)	Loss/tok 3.1832 (3.2804)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.138 (0.094)	Data 1.14e-04 (3.90e-04)	Tok/s 126697 (113282)	Loss/tok 3.3994 (3.2799)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.88e-04)	Tok/s 106922 (113287)	Loss/tok 3.0524 (3.2796)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.138 (0.094)	Data 1.11e-04 (3.86e-04)	Tok/s 125525 (113363)	Loss/tok 3.5568 (3.2805)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.104 (0.094)	Data 1.13e-04 (3.83e-04)	Tok/s 119653 (113407)	Loss/tok 3.2644 (3.2808)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.81e-04)	Tok/s 104971 (113416)	Loss/tok 3.1470 (3.2804)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.104 (0.094)	Data 1.11e-04 (3.79e-04)	Tok/s 120819 (113404)	Loss/tok 3.1946 (3.2795)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.138 (0.094)	Data 1.17e-04 (3.76e-04)	Tok/s 126839 (113461)	Loss/tok 3.4093 (3.2806)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.073 (0.094)	Data 1.10e-04 (3.74e-04)	Tok/s 104838 (113378)	Loss/tok 3.0676 (3.2795)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.72e-04)	Tok/s 105994 (113422)	Loss/tok 3.0445 (3.2803)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.073 (0.094)	Data 1.10e-04 (3.70e-04)	Tok/s 107332 (113451)	Loss/tok 3.0297 (3.2809)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.138 (0.094)	Data 1.06e-04 (3.68e-04)	Tok/s 127433 (113451)	Loss/tok 3.4536 (3.2805)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.66e-04)	Tok/s 120950 (113406)	Loss/tok 3.2585 (3.2799)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1240/1291]	Time 0.104 (0.094)	Data 1.10e-04 (3.64e-04)	Tok/s 118769 (113432)	Loss/tok 3.3883 (3.2808)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.62e-04)	Tok/s 106510 (113444)	Loss/tok 3.1626 (3.2809)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.105 (0.094)	Data 1.07e-04 (3.60e-04)	Tok/s 120812 (113462)	Loss/tok 3.1492 (3.2806)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.58e-04)	Tok/s 119647 (113477)	Loss/tok 3.3044 (3.2808)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.176 (0.094)	Data 1.10e-04 (3.56e-04)	Tok/s 125363 (113431)	Loss/tok 3.6199 (3.2803)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.104 (0.094)	Data 5.10e-05 (3.56e-04)	Tok/s 119057 (113450)	Loss/tok 3.3202 (3.2803)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590917284, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590917285, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.501 (0.501)	Decoder iters 149.0 (149.0)	Tok/s 17274 (17274)
0: Running moses detokenizer
0: BLEU(score=22.781460858733357, counts=[36034, 17577, 9773, 5666], totals=[64553, 61550, 58547, 55549], precisions=[55.82079841370657, 28.55727051177904, 16.692571779937484, 10.20000360042485], bp=0.9980964030856561, sys_len=64553, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590918547, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2278, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590918547, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2803	Test BLEU: 22.78
0: Performance: Epoch: 2	Training: 1813872 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592590918547, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590918547, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590918547, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1986882062
0: TRAIN [3][0/1291]	Time 0.513 (0.513)	Data 2.93e-01 (2.93e-01)	Tok/s 43562 (43562)	Loss/tok 3.5222 (3.5222)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.073 (0.121)	Data 1.13e-04 (2.67e-02)	Tok/s 105320 (104539)	Loss/tok 3.0836 (3.1987)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.073 (0.109)	Data 1.13e-04 (1.41e-02)	Tok/s 105545 (109136)	Loss/tok 3.0235 (3.2042)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.106 (0.101)	Data 1.11e-04 (9.56e-03)	Tok/s 118053 (109524)	Loss/tok 3.1128 (3.1627)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.104 (0.104)	Data 1.13e-04 (7.25e-03)	Tok/s 120942 (111849)	Loss/tok 3.2465 (3.2111)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.138 (0.103)	Data 1.18e-04 (5.85e-03)	Tok/s 128088 (112437)	Loss/tok 3.4699 (3.2026)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.141 (0.103)	Data 1.08e-04 (4.91e-03)	Tok/s 125200 (113051)	Loss/tok 3.3420 (3.2164)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][70/1291]	Time 0.105 (0.103)	Data 1.18e-04 (4.24e-03)	Tok/s 118692 (113393)	Loss/tok 3.2466 (3.2136)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.073 (0.101)	Data 1.12e-04 (3.73e-03)	Tok/s 109093 (113166)	Loss/tok 3.0238 (3.2134)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.073 (0.101)	Data 1.26e-04 (3.33e-03)	Tok/s 107970 (113705)	Loss/tok 2.9888 (3.2124)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][100/1291]	Time 0.042 (0.100)	Data 1.80e-04 (3.01e-03)	Tok/s 94009 (113603)	Loss/tok 2.6143 (3.2122)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.073 (0.100)	Data 1.15e-04 (2.75e-03)	Tok/s 106253 (113859)	Loss/tok 3.0151 (3.2133)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.104 (0.099)	Data 1.09e-04 (2.54e-03)	Tok/s 123403 (113761)	Loss/tok 3.0515 (3.2081)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.073 (0.100)	Data 1.10e-04 (2.35e-03)	Tok/s 108555 (113873)	Loss/tok 2.9440 (3.2114)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.104 (0.101)	Data 1.16e-04 (2.19e-03)	Tok/s 121814 (114276)	Loss/tok 3.1580 (3.2161)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.138 (0.101)	Data 1.11e-04 (2.05e-03)	Tok/s 127907 (114401)	Loss/tok 3.1763 (3.2134)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.073 (0.100)	Data 1.13e-04 (1.93e-03)	Tok/s 103544 (114332)	Loss/tok 3.0355 (3.2116)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.073 (0.099)	Data 1.11e-04 (1.83e-03)	Tok/s 105405 (114083)	Loss/tok 2.9734 (3.2105)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][180/1291]	Time 0.073 (0.100)	Data 1.16e-04 (1.73e-03)	Tok/s 105742 (114175)	Loss/tok 2.9518 (3.2166)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.138 (0.100)	Data 1.12e-04 (1.65e-03)	Tok/s 126765 (114314)	Loss/tok 3.4209 (3.2161)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.073 (0.100)	Data 1.15e-04 (1.57e-03)	Tok/s 106184 (114286)	Loss/tok 2.9535 (3.2191)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.138 (0.100)	Data 1.27e-04 (1.50e-03)	Tok/s 125557 (114219)	Loss/tok 3.3486 (3.2173)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.073 (0.099)	Data 1.17e-04 (1.44e-03)	Tok/s 103054 (114193)	Loss/tok 2.9682 (3.2174)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.073 (0.099)	Data 1.17e-04 (1.38e-03)	Tok/s 107548 (114050)	Loss/tok 3.0099 (3.2118)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.073 (0.099)	Data 1.24e-04 (1.33e-03)	Tok/s 105509 (114066)	Loss/tok 2.9763 (3.2158)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.104 (0.099)	Data 1.04e-04 (1.28e-03)	Tok/s 120857 (114050)	Loss/tok 3.1093 (3.2147)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.073 (0.098)	Data 1.12e-04 (1.24e-03)	Tok/s 106452 (113889)	Loss/tok 3.0625 (3.2098)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.073 (0.098)	Data 1.16e-04 (1.20e-03)	Tok/s 103159 (114017)	Loss/tok 2.9031 (3.2091)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.176 (0.098)	Data 1.19e-04 (1.16e-03)	Tok/s 127471 (113912)	Loss/tok 3.5755 (3.2086)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.073 (0.097)	Data 1.10e-04 (1.12e-03)	Tok/s 108251 (113703)	Loss/tok 2.9327 (3.2038)	LR 1.437e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][300/1291]	Time 0.073 (0.097)	Data 1.13e-04 (1.09e-03)	Tok/s 106945 (113489)	Loss/tok 2.8109 (3.1989)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.073 (0.096)	Data 1.15e-04 (1.06e-03)	Tok/s 106849 (113399)	Loss/tok 2.9643 (3.1963)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.104 (0.096)	Data 1.20e-04 (1.03e-03)	Tok/s 120286 (113296)	Loss/tok 3.1847 (3.1922)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.104 (0.096)	Data 1.14e-04 (1.00e-03)	Tok/s 122661 (113339)	Loss/tok 3.1665 (3.1888)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.073 (0.096)	Data 1.18e-04 (9.74e-04)	Tok/s 107147 (113302)	Loss/tok 2.9691 (3.1901)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.104 (0.096)	Data 1.20e-04 (9.50e-04)	Tok/s 122322 (113236)	Loss/tok 3.1758 (3.1920)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.073 (0.095)	Data 1.12e-04 (9.26e-04)	Tok/s 107334 (113179)	Loss/tok 2.9961 (3.1904)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.104 (0.096)	Data 1.13e-04 (9.05e-04)	Tok/s 122120 (113269)	Loss/tok 3.1346 (3.1933)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.104 (0.096)	Data 1.18e-04 (8.84e-04)	Tok/s 121041 (113315)	Loss/tok 3.1864 (3.1920)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.073 (0.096)	Data 1.15e-04 (8.65e-04)	Tok/s 107639 (113392)	Loss/tok 3.0662 (3.1901)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.138 (0.096)	Data 1.21e-04 (8.46e-04)	Tok/s 127333 (113414)	Loss/tok 3.3799 (3.1897)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.104 (0.096)	Data 1.12e-04 (8.28e-04)	Tok/s 123053 (113366)	Loss/tok 3.1252 (3.1863)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.073 (0.095)	Data 1.12e-04 (8.11e-04)	Tok/s 106102 (113222)	Loss/tok 2.9428 (3.1825)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][430/1291]	Time 0.104 (0.095)	Data 1.52e-04 (7.95e-04)	Tok/s 121960 (113255)	Loss/tok 3.2004 (3.1815)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.073 (0.095)	Data 1.33e-04 (7.80e-04)	Tok/s 108533 (113213)	Loss/tok 2.8844 (3.1784)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.073 (0.095)	Data 1.16e-04 (7.65e-04)	Tok/s 106883 (113161)	Loss/tok 2.8693 (3.1754)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.073 (0.095)	Data 1.23e-04 (7.51e-04)	Tok/s 105741 (113290)	Loss/tok 3.0228 (3.1750)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.073 (0.095)	Data 1.16e-04 (7.37e-04)	Tok/s 107819 (113178)	Loss/tok 3.0037 (3.1727)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.074 (0.095)	Data 1.13e-04 (7.25e-04)	Tok/s 104963 (113201)	Loss/tok 3.0153 (3.1727)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.105 (0.095)	Data 1.13e-04 (7.12e-04)	Tok/s 119978 (113261)	Loss/tok 3.0630 (3.1733)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.074 (0.095)	Data 1.15e-04 (7.00e-04)	Tok/s 104015 (113194)	Loss/tok 2.9462 (3.1718)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.074 (0.095)	Data 1.15e-04 (6.89e-04)	Tok/s 104917 (113140)	Loss/tok 3.0412 (3.1701)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.177 (0.095)	Data 1.10e-04 (6.78e-04)	Tok/s 126699 (113153)	Loss/tok 3.3967 (3.1707)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.104 (0.095)	Data 1.12e-04 (6.67e-04)	Tok/s 120812 (113197)	Loss/tok 3.1012 (3.1700)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.074 (0.095)	Data 1.13e-04 (6.57e-04)	Tok/s 103118 (113153)	Loss/tok 3.1190 (3.1692)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.073 (0.095)	Data 1.15e-04 (6.47e-04)	Tok/s 105843 (113243)	Loss/tok 2.9964 (3.1716)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1291]	Time 0.104 (0.095)	Data 1.20e-04 (6.37e-04)	Tok/s 122113 (113183)	Loss/tok 3.1649 (3.1705)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.073 (0.095)	Data 1.15e-04 (6.28e-04)	Tok/s 106244 (113181)	Loss/tok 2.8974 (3.1695)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.073 (0.095)	Data 1.12e-04 (6.19e-04)	Tok/s 106971 (113079)	Loss/tok 3.0053 (3.1680)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.042 (0.095)	Data 1.14e-04 (6.11e-04)	Tok/s 95946 (113092)	Loss/tok 2.6828 (3.1679)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.042 (0.094)	Data 1.12e-04 (6.02e-04)	Tok/s 94369 (113043)	Loss/tok 2.6233 (3.1667)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.073 (0.094)	Data 1.17e-04 (5.94e-04)	Tok/s 105107 (112981)	Loss/tok 3.0482 (3.1643)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][620/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.87e-04)	Tok/s 108044 (113028)	Loss/tok 2.9410 (3.1658)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.042 (0.094)	Data 1.19e-04 (5.79e-04)	Tok/s 93594 (112946)	Loss/tok 2.6203 (3.1639)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.073 (0.094)	Data 1.18e-04 (5.72e-04)	Tok/s 104195 (112986)	Loss/tok 3.0485 (3.1655)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.104 (0.094)	Data 1.15e-04 (5.65e-04)	Tok/s 120073 (112953)	Loss/tok 3.1229 (3.1648)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.073 (0.094)	Data 1.13e-04 (5.58e-04)	Tok/s 105657 (112829)	Loss/tok 2.8888 (3.1626)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.073 (0.094)	Data 1.15e-04 (5.52e-04)	Tok/s 103533 (112844)	Loss/tok 2.9824 (3.1625)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.176 (0.094)	Data 1.14e-04 (5.45e-04)	Tok/s 128040 (112876)	Loss/tok 3.3635 (3.1621)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.176 (0.094)	Data 1.15e-04 (5.39e-04)	Tok/s 127563 (112818)	Loss/tok 3.3945 (3.1614)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.104 (0.094)	Data 1.16e-04 (5.33e-04)	Tok/s 119710 (112805)	Loss/tok 3.1817 (3.1609)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.073 (0.094)	Data 1.05e-04 (5.27e-04)	Tok/s 105455 (112778)	Loss/tok 2.8441 (3.1599)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.104 (0.093)	Data 1.15e-04 (5.21e-04)	Tok/s 120886 (112787)	Loss/tok 3.2982 (3.1588)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.104 (0.094)	Data 1.16e-04 (5.16e-04)	Tok/s 120875 (112854)	Loss/tok 3.1533 (3.1589)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.138 (0.094)	Data 1.17e-04 (5.10e-04)	Tok/s 126747 (112885)	Loss/tok 3.3557 (3.1587)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][750/1291]	Time 0.073 (0.094)	Data 1.20e-04 (5.05e-04)	Tok/s 104703 (112897)	Loss/tok 3.0032 (3.1590)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.073 (0.094)	Data 1.17e-04 (5.00e-04)	Tok/s 106357 (112897)	Loss/tok 3.0043 (3.1593)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.105 (0.094)	Data 1.17e-04 (4.95e-04)	Tok/s 120781 (112968)	Loss/tok 3.1375 (3.1605)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.90e-04)	Tok/s 121622 (113067)	Loss/tok 3.0568 (3.1613)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.86e-04)	Tok/s 120660 (113075)	Loss/tok 3.1321 (3.1604)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.139 (0.094)	Data 1.14e-04 (4.81e-04)	Tok/s 126322 (113083)	Loss/tok 3.2854 (3.1599)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.042 (0.094)	Data 1.19e-04 (4.76e-04)	Tok/s 95425 (113132)	Loss/tok 2.5696 (3.1607)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][820/1291]	Time 0.073 (0.094)	Data 1.17e-04 (4.72e-04)	Tok/s 108752 (113115)	Loss/tok 2.8257 (3.1599)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.104 (0.094)	Data 1.11e-04 (4.68e-04)	Tok/s 120492 (113082)	Loss/tok 3.1249 (3.1583)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.64e-04)	Tok/s 121689 (113120)	Loss/tok 3.0279 (3.1578)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.59e-04)	Tok/s 119588 (113108)	Loss/tok 3.1504 (3.1569)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.138 (0.094)	Data 1.14e-04 (4.55e-04)	Tok/s 127792 (113091)	Loss/tok 3.2849 (3.1576)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.074 (0.094)	Data 1.13e-04 (4.52e-04)	Tok/s 105687 (113049)	Loss/tok 2.9849 (3.1569)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.074 (0.094)	Data 1.12e-04 (4.48e-04)	Tok/s 105003 (113089)	Loss/tok 2.9681 (3.1570)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.074 (0.094)	Data 1.12e-04 (4.44e-04)	Tok/s 105782 (113047)	Loss/tok 2.9914 (3.1558)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.074 (0.094)	Data 1.18e-04 (4.40e-04)	Tok/s 105734 (113077)	Loss/tok 2.8850 (3.1554)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.043 (0.094)	Data 1.18e-04 (4.37e-04)	Tok/s 91777 (112997)	Loss/tok 2.5444 (3.1549)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.138 (0.094)	Data 1.18e-04 (4.34e-04)	Tok/s 127010 (112984)	Loss/tok 3.3360 (3.1542)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.073 (0.094)	Data 1.65e-04 (4.30e-04)	Tok/s 104879 (112923)	Loss/tok 2.8444 (3.1526)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.043 (0.094)	Data 1.14e-04 (4.27e-04)	Tok/s 90758 (112908)	Loss/tok 2.4545 (3.1521)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][950/1291]	Time 0.074 (0.094)	Data 1.84e-04 (4.24e-04)	Tok/s 105440 (112892)	Loss/tok 2.9807 (3.1529)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.105 (0.094)	Data 1.52e-04 (4.21e-04)	Tok/s 120701 (112887)	Loss/tok 3.1558 (3.1526)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.074 (0.094)	Data 1.13e-04 (4.18e-04)	Tok/s 106518 (112860)	Loss/tok 2.9119 (3.1516)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][980/1291]	Time 0.074 (0.094)	Data 1.25e-04 (4.15e-04)	Tok/s 105390 (112922)	Loss/tok 3.0470 (3.1532)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.074 (0.094)	Data 1.16e-04 (4.12e-04)	Tok/s 104491 (112934)	Loss/tok 3.0134 (3.1538)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.074 (0.094)	Data 1.12e-04 (4.09e-04)	Tok/s 105068 (112959)	Loss/tok 2.8478 (3.1539)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.074 (0.094)	Data 1.11e-04 (4.06e-04)	Tok/s 106525 (112947)	Loss/tok 2.8562 (3.1537)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.03e-04)	Tok/s 108140 (112937)	Loss/tok 2.9222 (3.1542)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.073 (0.094)	Data 1.13e-04 (4.01e-04)	Tok/s 108002 (112981)	Loss/tok 2.9903 (3.1548)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.105 (0.094)	Data 1.11e-04 (3.98e-04)	Tok/s 121593 (113044)	Loss/tok 3.2001 (3.1556)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.104 (0.095)	Data 1.12e-04 (3.95e-04)	Tok/s 120568 (113098)	Loss/tok 3.0959 (3.1554)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.073 (0.095)	Data 1.14e-04 (3.92e-04)	Tok/s 103466 (113114)	Loss/tok 2.9311 (3.1550)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.104 (0.095)	Data 1.10e-04 (3.90e-04)	Tok/s 121654 (113126)	Loss/tok 3.0820 (3.1551)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.87e-04)	Tok/s 105330 (113096)	Loss/tok 2.9122 (3.1541)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.073 (0.095)	Data 1.16e-04 (3.85e-04)	Tok/s 103402 (113099)	Loss/tok 2.8815 (3.1544)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.073 (0.094)	Data 1.11e-04 (3.82e-04)	Tok/s 104041 (113081)	Loss/tok 2.9998 (3.1538)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1110/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.80e-04)	Tok/s 122554 (113086)	Loss/tok 3.1107 (3.1530)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.78e-04)	Tok/s 103349 (113095)	Loss/tok 2.9286 (3.1526)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.073 (0.095)	Data 1.11e-04 (3.75e-04)	Tok/s 105421 (113133)	Loss/tok 2.9312 (3.1534)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.073 (0.095)	Data 1.10e-04 (3.73e-04)	Tok/s 107426 (113161)	Loss/tok 2.9654 (3.1535)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.073 (0.095)	Data 1.14e-04 (3.71e-04)	Tok/s 106409 (113114)	Loss/tok 2.9172 (3.1528)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.139 (0.094)	Data 1.12e-04 (3.69e-04)	Tok/s 125862 (113094)	Loss/tok 3.3176 (3.1521)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1170/1291]	Time 0.139 (0.095)	Data 1.12e-04 (3.66e-04)	Tok/s 125337 (113115)	Loss/tok 3.2794 (3.1524)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.64e-04)	Tok/s 104736 (113080)	Loss/tok 2.8105 (3.1512)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.073 (0.094)	Data 1.16e-04 (3.62e-04)	Tok/s 102825 (113058)	Loss/tok 2.9126 (3.1502)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.60e-04)	Tok/s 104966 (113099)	Loss/tok 3.1362 (3.1505)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.104 (0.094)	Data 1.11e-04 (3.58e-04)	Tok/s 120701 (113097)	Loss/tok 3.1332 (3.1499)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.139 (0.094)	Data 1.14e-04 (3.56e-04)	Tok/s 125111 (113102)	Loss/tok 3.2744 (3.1500)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.073 (0.095)	Data 1.12e-04 (3.54e-04)	Tok/s 101989 (113128)	Loss/tok 2.8544 (3.1499)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.176 (0.095)	Data 1.12e-04 (3.52e-04)	Tok/s 128032 (113113)	Loss/tok 3.3390 (3.1495)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.104 (0.094)	Data 1.14e-04 (3.50e-04)	Tok/s 121771 (113087)	Loss/tok 2.9751 (3.1487)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.042 (0.094)	Data 1.14e-04 (3.48e-04)	Tok/s 93960 (113068)	Loss/tok 2.4502 (3.1478)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.104 (0.094)	Data 1.12e-04 (3.46e-04)	Tok/s 120151 (113096)	Loss/tok 3.1500 (3.1480)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.042 (0.094)	Data 1.16e-04 (3.45e-04)	Tok/s 94040 (113072)	Loss/tok 2.5037 (3.1469)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.073 (0.094)	Data 5.36e-05 (3.45e-04)	Tok/s 108514 (113070)	Loss/tok 2.8737 (3.1464)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592591040997, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591040997, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.368 (0.368)	Decoder iters 101.0 (101.0)	Tok/s 24383 (24383)
0: Running moses detokenizer
0: BLEU(score=24.415879339163546, counts=[37185, 18773, 10780, 6406], totals=[65284, 62281, 59278, 56281], precisions=[56.95882605232522, 30.142419036303206, 18.185498835993116, 11.382171603205345], bp=1.0, sys_len=65284, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592591042062, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24420000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591042062, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1480	Test BLEU: 24.42
0: Performance: Epoch: 3	Training: 1809331 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592591042062, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592591042063, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-19 11:24:10 AM
RESULT,RNN_TRANSLATOR,,548,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:10 AM
RESULT,RNN_TRANSLATOR,,548,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
slurmstepd: error: _is_a_lwp: open() /proc/12386/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:12 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:13 AM
RESULT,RNN_TRANSLATOR,,551,nvidia,2020-06-19 11:15:02 AM
