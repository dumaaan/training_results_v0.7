+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590479772, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592590479801, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592590479801, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592590479801, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592590479802, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n054
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590487070, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/13929693/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=192
+ LR=2.875e-3
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=192
+ REMAIN_STEPS=4054
+ TEST_BATCH_SIZE=64
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -n 13 ']'
+ declare -a CMD
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ '[' -n 5 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ LR=2.875e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 3 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ '[' -n 14 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=192
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=64
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ '[' -n 4 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
running benchmark
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:14:50 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592590491971, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492612, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590492749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2549886683
:::MLLOG {"namespace": "", "time_ms": 1592590510251, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2549886683, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1391524189
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2756160]), new_param_packed_fragment.size()=torch.Size([2756160]), master_param_fragment.size()=torch.Size([2756160])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([3044864]), new_param_packed_fragment.size()=torch.Size([3044864]), master_param_fragment.size()=torch.Size([3044864])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2302464]), new_param_packed_fragment.size()=torch.Size([2302464]), master_param_fragment.size()=torch.Size([2302464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3083712]), new_param_packed_fragment.size()=torch.Size([3083712]), master_param_fragment.size()=torch.Size([3083712])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([246272]), new_param_packed_fragment.size()=torch.Size([246272]), master_param_fragment.size()=torch.Size([246272])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1891840]), new_param_packed_fragment.size()=torch.Size([1891840]), master_param_fragment.size()=torch.Size([1891840])
model_param_fragment.size()=torch.Size([496128]), new_param_packed_fragment.size()=torch.Size([496128]), master_param_fragment.size()=torch.Size([496128])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([2005568]), new_param_packed_fragment.size()=torch.Size([2005568]), master_param_fragment.size()=torch.Size([2005568])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([2052608]), new_param_packed_fragment.size()=torch.Size([2052608]), master_param_fragment.size()=torch.Size([2052608])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([534976]), new_param_packed_fragment.size()=torch.Size([534976]), master_param_fragment.size()=torch.Size([534976])
model_param_fragment.size()=torch.Size([2141696]), new_param_packed_fragment.size()=torch.Size([2141696]), master_param_fragment.size()=torch.Size([2141696])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1399296]), new_param_packed_fragment.size()=torch.Size([1399296]), master_param_fragment.size()=torch.Size([1399296])
model_param_fragment.size()=torch.Size([3621888]), new_param_packed_fragment.size()=torch.Size([3621888]), master_param_fragment.size()=torch.Size([3621888])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3948032]), new_param_packed_fragment.size()=torch.Size([3948032]), master_param_fragment.size()=torch.Size([3948032])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4982336]), new_param_packed_fragment.size()=torch.Size([4982336]), master_param_fragment.size()=torch.Size([4982336])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2795008]), new_param_packed_fragment.size()=torch.Size([2795008]), master_param_fragment.size()=torch.Size([2795008])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3698176]), new_param_packed_fragment.size()=torch.Size([3698176]), master_param_fragment.size()=torch.Size([3698176])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2188736]), new_param_packed_fragment.size()=torch.Size([2188736]), master_param_fragment.size()=torch.Size([2188736])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592590531767, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592590531768, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592590531768, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592590531768, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592590531768, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592590536183, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592590536183, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592590536184, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592590536442, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592590536443, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592590536443, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592590536444, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592590536445, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590536445, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 4230121604
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.422 (0.422)	Data 3.21e-01 (3.21e-01)	Tok/s 18310 (18310)	Loss/tok 10.6521 (10.6521)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.072 (0.140)	Data 1.10e-04 (2.92e-02)	Tok/s 108854 (110318)	Loss/tok 9.4377 (9.9529)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.103 (0.120)	Data 1.21e-04 (1.54e-02)	Tok/s 121216 (113823)	Loss/tok 9.1549 (9.6493)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.072 (0.105)	Data 1.08e-04 (1.04e-02)	Tok/s 108107 (112232)	Loss/tok 8.7913 (9.4676)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1291]	Time 0.072 (0.104)	Data 1.08e-04 (7.93e-03)	Tok/s 107053 (113001)	Loss/tok 8.5495 (9.3287)	LR 7.057e-05
0: TRAIN [0][50/1291]	Time 0.073 (0.099)	Data 1.09e-04 (6.39e-03)	Tok/s 105668 (112793)	Loss/tok 8.2777 (9.1851)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.103 (0.097)	Data 1.11e-04 (5.36e-03)	Tok/s 122098 (112524)	Loss/tok 8.1911 (9.0411)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.103 (0.096)	Data 1.12e-04 (4.63e-03)	Tok/s 118129 (112870)	Loss/tok 8.1376 (8.9042)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.072 (0.097)	Data 1.11e-04 (4.07e-03)	Tok/s 108544 (113145)	Loss/tok 7.8007 (8.7820)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.072 (0.097)	Data 1.10e-04 (3.63e-03)	Tok/s 108518 (113489)	Loss/tok 7.8002 (8.6832)	LR 2.232e-04
0: TRAIN [0][100/1291]	Time 0.042 (0.095)	Data 1.05e-04 (3.29e-03)	Tok/s 91736 (112968)	Loss/tok 7.3010 (8.6113)	LR 2.810e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][110/1291]	Time 0.103 (0.096)	Data 1.12e-04 (3.00e-03)	Tok/s 123074 (113478)	Loss/tok 8.3124 (8.5516)	LR 3.301e-04
0: TRAIN [0][120/1291]	Time 0.103 (0.096)	Data 1.07e-04 (2.76e-03)	Tok/s 122961 (113758)	Loss/tok 7.8791 (8.4963)	LR 4.156e-04
0: TRAIN [0][130/1291]	Time 0.103 (0.096)	Data 1.09e-04 (2.56e-03)	Tok/s 121602 (113542)	Loss/tok 7.7692 (8.4502)	LR 5.232e-04
0: TRAIN [0][140/1291]	Time 0.137 (0.096)	Data 1.06e-04 (2.38e-03)	Tok/s 127708 (113645)	Loss/tok 7.7975 (8.4001)	LR 6.586e-04
0: TRAIN [0][150/1291]	Time 0.073 (0.095)	Data 1.04e-04 (2.23e-03)	Tok/s 104665 (113471)	Loss/tok 7.4543 (8.3517)	LR 8.292e-04
0: TRAIN [0][160/1291]	Time 0.073 (0.094)	Data 1.05e-04 (2.10e-03)	Tok/s 109411 (113474)	Loss/tok 7.2940 (8.3030)	LR 1.044e-03
0: TRAIN [0][170/1291]	Time 0.043 (0.094)	Data 9.97e-05 (1.98e-03)	Tok/s 89945 (113224)	Loss/tok 6.3973 (8.2560)	LR 1.314e-03
0: TRAIN [0][180/1291]	Time 0.073 (0.092)	Data 1.02e-04 (1.88e-03)	Tok/s 106274 (112701)	Loss/tok 6.9346 (8.2077)	LR 1.654e-03
0: TRAIN [0][190/1291]	Time 0.137 (0.092)	Data 1.03e-04 (1.79e-03)	Tok/s 126905 (112478)	Loss/tok 7.2225 (8.1539)	LR 2.083e-03
0: TRAIN [0][200/1291]	Time 0.073 (0.091)	Data 1.01e-04 (1.70e-03)	Tok/s 106538 (112325)	Loss/tok 7.0633 (8.0994)	LR 2.622e-03
0: TRAIN [0][210/1291]	Time 0.043 (0.091)	Data 1.38e-04 (1.63e-03)	Tok/s 91568 (112401)	Loss/tok 5.5424 (8.0392)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.073 (0.092)	Data 1.07e-04 (1.56e-03)	Tok/s 106005 (112604)	Loss/tok 6.4653 (7.9713)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.103 (0.093)	Data 1.15e-04 (1.50e-03)	Tok/s 124873 (112902)	Loss/tok 6.4378 (7.8978)	LR 2.875e-03
0: Upscaling, new scale: 64.0
0: TRAIN [0][240/1291]	Time 0.073 (0.093)	Data 1.05e-04 (1.44e-03)	Tok/s 108840 (112826)	Loss/tok 6.0926 (7.8365)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.073 (0.093)	Data 1.05e-04 (1.39e-03)	Tok/s 105434 (112808)	Loss/tok 5.8914 (7.7717)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.073 (0.092)	Data 1.10e-04 (1.34e-03)	Tok/s 109311 (112859)	Loss/tok 5.8873 (7.7066)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.042 (0.092)	Data 1.13e-04 (1.29e-03)	Tok/s 96230 (112892)	Loss/tok 4.7696 (7.6399)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.104 (0.092)	Data 1.08e-04 (1.25e-03)	Tok/s 120454 (112772)	Loss/tok 5.8460 (7.5806)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.104 (0.092)	Data 1.08e-04 (1.21e-03)	Tok/s 123268 (112836)	Loss/tok 5.9349 (7.5148)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.175 (0.092)	Data 1.01e-04 (1.17e-03)	Tok/s 128152 (112800)	Loss/tok 6.0982 (7.4558)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.073 (0.091)	Data 1.03e-04 (1.14e-03)	Tok/s 107835 (112526)	Loss/tok 5.1406 (7.4066)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.104 (0.091)	Data 1.03e-04 (1.11e-03)	Tok/s 121269 (112644)	Loss/tok 5.3685 (7.3377)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.104 (0.091)	Data 1.14e-04 (1.08e-03)	Tok/s 120506 (112675)	Loss/tok 5.3338 (7.2736)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.104 (0.091)	Data 1.03e-04 (1.05e-03)	Tok/s 120566 (112696)	Loss/tok 5.1308 (7.2091)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.104 (0.091)	Data 1.11e-04 (1.02e-03)	Tok/s 120977 (112661)	Loss/tok 4.9921 (7.1481)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.104 (0.091)	Data 1.18e-04 (9.96e-04)	Tok/s 123225 (112571)	Loss/tok 5.0757 (7.0915)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][370/1291]	Time 0.104 (0.091)	Data 1.23e-04 (9.72e-04)	Tok/s 121896 (112587)	Loss/tok 4.7819 (7.0276)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.108 (0.092)	Data 1.13e-04 (9.50e-04)	Tok/s 115561 (112678)	Loss/tok 4.7712 (6.9573)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.073 (0.091)	Data 1.07e-04 (9.28e-04)	Tok/s 107852 (112638)	Loss/tok 4.4066 (6.9021)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.073 (0.092)	Data 1.13e-04 (9.07e-04)	Tok/s 108030 (112686)	Loss/tok 4.2399 (6.8397)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.073 (0.091)	Data 1.06e-04 (8.88e-04)	Tok/s 108659 (112622)	Loss/tok 4.4450 (6.7905)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.073 (0.091)	Data 1.03e-04 (8.69e-04)	Tok/s 103293 (112633)	Loss/tok 4.0440 (6.7374)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.104 (0.091)	Data 1.07e-04 (8.51e-04)	Tok/s 122784 (112711)	Loss/tok 4.4685 (6.6799)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.073 (0.091)	Data 1.02e-04 (8.35e-04)	Tok/s 105727 (112684)	Loss/tok 4.0458 (6.6276)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.073 (0.091)	Data 1.07e-04 (8.18e-04)	Tok/s 106780 (112669)	Loss/tok 3.9568 (6.5784)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.073 (0.091)	Data 1.08e-04 (8.03e-04)	Tok/s 105324 (112584)	Loss/tok 4.0450 (6.5343)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.073 (0.091)	Data 1.02e-04 (7.88e-04)	Tok/s 107637 (112580)	Loss/tok 3.8578 (6.4874)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.042 (0.091)	Data 1.02e-04 (7.74e-04)	Tok/s 93183 (112524)	Loss/tok 3.3042 (6.4453)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.104 (0.091)	Data 9.94e-05 (7.60e-04)	Tok/s 121684 (112418)	Loss/tok 4.2311 (6.4073)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][500/1291]	Time 0.104 (0.091)	Data 1.04e-04 (7.47e-04)	Tok/s 121194 (112512)	Loss/tok 4.1449 (6.3555)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.073 (0.091)	Data 1.09e-04 (7.34e-04)	Tok/s 107504 (112505)	Loss/tok 3.9880 (6.3116)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.073 (0.091)	Data 1.07e-04 (7.22e-04)	Tok/s 105672 (112549)	Loss/tok 3.8013 (6.2671)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.073 (0.091)	Data 1.00e-04 (7.11e-04)	Tok/s 105764 (112516)	Loss/tok 3.8672 (6.2295)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.104 (0.091)	Data 1.05e-04 (7.00e-04)	Tok/s 121014 (112539)	Loss/tok 4.2005 (6.1890)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.073 (0.091)	Data 1.08e-04 (6.89e-04)	Tok/s 106952 (112603)	Loss/tok 3.8383 (6.1470)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.073 (0.091)	Data 1.06e-04 (6.78e-04)	Tok/s 107174 (112693)	Loss/tok 3.8436 (6.1031)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.042 (0.091)	Data 1.04e-04 (6.68e-04)	Tok/s 93457 (112681)	Loss/tok 3.1822 (6.0671)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.073 (0.092)	Data 1.05e-04 (6.59e-04)	Tok/s 105414 (112797)	Loss/tok 3.7284 (6.0258)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.104 (0.092)	Data 1.02e-04 (6.49e-04)	Tok/s 121628 (112865)	Loss/tok 4.1008 (5.9871)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.073 (0.092)	Data 1.03e-04 (6.40e-04)	Tok/s 103379 (112931)	Loss/tok 3.9221 (5.9487)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.138 (0.092)	Data 1.06e-04 (6.31e-04)	Tok/s 127949 (112954)	Loss/tok 4.0714 (5.9150)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.042 (0.092)	Data 1.07e-04 (6.23e-04)	Tok/s 95230 (113006)	Loss/tok 3.0849 (5.8795)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][630/1291]	Time 0.103 (0.092)	Data 9.73e-05 (6.15e-04)	Tok/s 122522 (112970)	Loss/tok 3.9955 (5.8506)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.104 (0.092)	Data 1.06e-04 (6.07e-04)	Tok/s 118955 (112999)	Loss/tok 4.0425 (5.8182)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.073 (0.092)	Data 1.13e-04 (5.99e-04)	Tok/s 104929 (112931)	Loss/tok 3.7662 (5.7925)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.104 (0.092)	Data 1.06e-04 (5.92e-04)	Tok/s 121406 (112961)	Loss/tok 3.9787 (5.7617)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.073 (0.092)	Data 1.02e-04 (5.84e-04)	Tok/s 109103 (112862)	Loss/tok 3.5702 (5.7386)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.042 (0.092)	Data 1.11e-04 (5.77e-04)	Tok/s 94034 (112863)	Loss/tok 3.0999 (5.7096)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.073 (0.092)	Data 1.10e-04 (5.71e-04)	Tok/s 106978 (112872)	Loss/tok 3.6421 (5.6823)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.073 (0.092)	Data 1.08e-04 (5.64e-04)	Tok/s 108165 (112805)	Loss/tok 3.7095 (5.6591)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][710/1291]	Time 0.104 (0.092)	Data 1.07e-04 (5.58e-04)	Tok/s 121512 (112780)	Loss/tok 3.8676 (5.6347)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.139 (0.092)	Data 1.10e-04 (5.51e-04)	Tok/s 124433 (112784)	Loss/tok 4.0548 (5.6088)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.073 (0.092)	Data 1.10e-04 (5.45e-04)	Tok/s 104413 (112795)	Loss/tok 3.5864 (5.5832)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.104 (0.092)	Data 1.08e-04 (5.39e-04)	Tok/s 120715 (112799)	Loss/tok 4.0454 (5.5599)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.073 (0.092)	Data 1.13e-04 (5.34e-04)	Tok/s 105069 (112755)	Loss/tok 3.6539 (5.5384)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.073 (0.092)	Data 1.09e-04 (5.28e-04)	Tok/s 106184 (112749)	Loss/tok 3.7048 (5.5150)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.104 (0.092)	Data 1.10e-04 (5.23e-04)	Tok/s 120625 (112772)	Loss/tok 3.8359 (5.4917)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.073 (0.092)	Data 1.13e-04 (5.17e-04)	Tok/s 105179 (112740)	Loss/tok 3.4892 (5.4706)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.073 (0.092)	Data 1.05e-04 (5.12e-04)	Tok/s 109264 (112739)	Loss/tok 3.6034 (5.4497)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.073 (0.092)	Data 1.02e-04 (5.07e-04)	Tok/s 105447 (112747)	Loss/tok 3.5444 (5.4289)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.073 (0.092)	Data 1.09e-04 (5.02e-04)	Tok/s 105481 (112738)	Loss/tok 3.4085 (5.4094)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.104 (0.092)	Data 1.13e-04 (4.97e-04)	Tok/s 118248 (112769)	Loss/tok 3.9271 (5.3879)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.105 (0.093)	Data 1.06e-04 (4.93e-04)	Tok/s 120196 (112817)	Loss/tok 3.7842 (5.3664)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][840/1291]	Time 0.104 (0.093)	Data 1.07e-04 (4.88e-04)	Tok/s 119072 (112821)	Loss/tok 3.7900 (5.3463)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.074 (0.093)	Data 1.13e-04 (4.84e-04)	Tok/s 105543 (112838)	Loss/tok 3.3555 (5.3271)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.073 (0.093)	Data 1.11e-04 (4.79e-04)	Tok/s 105125 (112851)	Loss/tok 3.5383 (5.3066)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.074 (0.093)	Data 1.03e-04 (4.75e-04)	Tok/s 108073 (112842)	Loss/tok 3.6641 (5.2893)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.104 (0.093)	Data 1.05e-04 (4.71e-04)	Tok/s 120756 (112838)	Loss/tok 3.7559 (5.2717)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.104 (0.093)	Data 1.10e-04 (4.67e-04)	Tok/s 123219 (112865)	Loss/tok 3.7353 (5.2526)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.074 (0.093)	Data 1.06e-04 (4.63e-04)	Tok/s 104089 (112888)	Loss/tok 3.4777 (5.2340)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.074 (0.093)	Data 1.08e-04 (4.59e-04)	Tok/s 104322 (112847)	Loss/tok 3.4304 (5.2186)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.104 (0.093)	Data 1.16e-04 (4.55e-04)	Tok/s 121391 (112824)	Loss/tok 3.6934 (5.2023)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.074 (0.093)	Data 1.06e-04 (4.51e-04)	Tok/s 105025 (112823)	Loss/tok 3.3555 (5.1863)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.043 (0.093)	Data 1.08e-04 (4.48e-04)	Tok/s 92232 (112809)	Loss/tok 2.9320 (5.1711)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.104 (0.093)	Data 1.08e-04 (4.44e-04)	Tok/s 121833 (112877)	Loss/tok 3.6791 (5.1519)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.073 (0.093)	Data 1.08e-04 (4.41e-04)	Tok/s 105411 (112888)	Loss/tok 3.4014 (5.1358)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][970/1291]	Time 0.042 (0.093)	Data 1.12e-04 (4.37e-04)	Tok/s 94631 (112869)	Loss/tok 3.0859 (5.1217)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.073 (0.093)	Data 1.08e-04 (4.34e-04)	Tok/s 106823 (112926)	Loss/tok 3.5069 (5.1047)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.042 (0.093)	Data 1.14e-04 (4.30e-04)	Tok/s 90537 (112888)	Loss/tok 2.8410 (5.0916)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.105 (0.093)	Data 1.07e-04 (4.27e-04)	Tok/s 120505 (112881)	Loss/tok 3.5918 (5.0781)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.073 (0.093)	Data 1.08e-04 (4.24e-04)	Tok/s 108910 (112887)	Loss/tok 3.4955 (5.0641)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.139 (0.093)	Data 1.05e-04 (4.21e-04)	Tok/s 126117 (112908)	Loss/tok 3.8316 (5.0493)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.105 (0.094)	Data 1.11e-04 (4.18e-04)	Tok/s 121106 (112987)	Loss/tok 3.6859 (5.0326)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.139 (0.094)	Data 9.97e-05 (4.15e-04)	Tok/s 125844 (112992)	Loss/tok 3.8243 (5.0187)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.104 (0.093)	Data 1.03e-04 (4.12e-04)	Tok/s 119387 (112951)	Loss/tok 3.7653 (5.0070)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.104 (0.094)	Data 1.07e-04 (4.09e-04)	Tok/s 121367 (112979)	Loss/tok 3.6292 (4.9932)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.176 (0.094)	Data 1.03e-04 (4.06e-04)	Tok/s 124608 (113001)	Loss/tok 4.2570 (4.9804)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.073 (0.094)	Data 1.02e-04 (4.03e-04)	Tok/s 108243 (112982)	Loss/tok 3.3464 (4.9681)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1090/1291]	Time 0.105 (0.094)	Data 1.06e-04 (4.01e-04)	Tok/s 121953 (113024)	Loss/tok 3.6771 (4.9538)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.105 (0.094)	Data 1.03e-04 (3.98e-04)	Tok/s 121378 (113033)	Loss/tok 3.7087 (4.9418)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.073 (0.094)	Data 1.09e-04 (3.95e-04)	Tok/s 105349 (113051)	Loss/tok 3.2945 (4.9293)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.93e-04)	Tok/s 106495 (113046)	Loss/tok 3.4569 (4.9177)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1130/1291]	Time 0.178 (0.094)	Data 1.07e-04 (3.90e-04)	Tok/s 125507 (113057)	Loss/tok 3.9635 (4.9053)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.073 (0.094)	Data 1.04e-04 (3.88e-04)	Tok/s 107329 (113070)	Loss/tok 3.3298 (4.8931)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.105 (0.094)	Data 1.04e-04 (3.85e-04)	Tok/s 120254 (113105)	Loss/tok 3.4795 (4.8806)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.139 (0.094)	Data 1.09e-04 (3.83e-04)	Tok/s 126657 (113130)	Loss/tok 3.7459 (4.8688)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.176 (0.094)	Data 9.94e-05 (3.81e-04)	Tok/s 128617 (113136)	Loss/tok 3.9734 (4.8575)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.104 (0.094)	Data 1.10e-04 (3.78e-04)	Tok/s 119441 (113172)	Loss/tok 3.7743 (4.8461)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.073 (0.094)	Data 1.06e-04 (3.76e-04)	Tok/s 106520 (113160)	Loss/tok 3.2319 (4.8361)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.104 (0.094)	Data 1.06e-04 (3.74e-04)	Tok/s 120284 (113172)	Loss/tok 3.6041 (4.8251)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.042 (0.094)	Data 1.06e-04 (3.72e-04)	Tok/s 93436 (113160)	Loss/tok 2.7664 (4.8151)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.139 (0.094)	Data 1.09e-04 (3.69e-04)	Tok/s 125896 (113215)	Loss/tok 3.8305 (4.8034)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.139 (0.094)	Data 1.02e-04 (3.67e-04)	Tok/s 127790 (113200)	Loss/tok 3.7909 (4.7934)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.105 (0.094)	Data 1.09e-04 (3.65e-04)	Tok/s 122028 (113191)	Loss/tok 3.4456 (4.7840)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.104 (0.094)	Data 1.04e-04 (3.63e-04)	Tok/s 119667 (113181)	Loss/tok 3.5445 (4.7747)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1260/1291]	Time 0.177 (0.094)	Data 1.03e-04 (3.61e-04)	Tok/s 125521 (113197)	Loss/tok 3.9322 (4.7646)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.105 (0.094)	Data 1.12e-04 (3.59e-04)	Tok/s 119877 (113180)	Loss/tok 3.5265 (4.7557)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.073 (0.094)	Data 9.94e-05 (3.57e-04)	Tok/s 106402 (113202)	Loss/tok 3.4482 (4.7457)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.073 (0.094)	Data 4.98e-05 (3.57e-04)	Tok/s 105907 (113225)	Loss/tok 3.3263 (4.7360)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590658456, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590658456, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.463 (0.463)	Decoder iters 149.0 (149.0)	Tok/s 18789 (18789)
0: Running moses detokenizer
0: BLEU(score=19.33199354274059, counts=[34195, 15519, 8137, 4436], totals=[65451, 62448, 59445, 56445], precisions=[52.24519105896014, 24.8510760953113, 13.688283287072084, 7.858977765966871], bp=1.0, sys_len=65451, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590659703, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19329999999999997, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590659703, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7407	Test BLEU: 19.33
0: Performance: Epoch: 0	Training: 1811255 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592590659704, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590659704, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590659704, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1861907924
0: TRAIN [1][0/1291]	Time 0.407 (0.407)	Data 2.83e-01 (2.83e-01)	Tok/s 31100 (31100)	Loss/tok 3.5653 (3.5653)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.105 (0.131)	Data 1.11e-04 (2.58e-02)	Tok/s 119246 (106327)	Loss/tok 3.5281 (3.6089)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.138 (0.108)	Data 1.15e-04 (1.36e-02)	Tok/s 124889 (107735)	Loss/tok 3.7689 (3.5253)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][30/1291]	Time 0.073 (0.103)	Data 1.03e-04 (9.23e-03)	Tok/s 105128 (109161)	Loss/tok 3.2250 (3.5156)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.139 (0.108)	Data 1.04e-04 (7.01e-03)	Tok/s 126532 (111630)	Loss/tok 3.6147 (3.5571)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.075 (0.105)	Data 1.06e-04 (5.65e-03)	Tok/s 105685 (111892)	Loss/tok 3.2163 (3.5435)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.073 (0.103)	Data 1.05e-04 (4.74e-03)	Tok/s 106326 (112148)	Loss/tok 3.2393 (3.5269)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.073 (0.101)	Data 1.04e-04 (4.09e-03)	Tok/s 103277 (111815)	Loss/tok 3.2420 (3.5237)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.105 (0.100)	Data 1.08e-04 (3.60e-03)	Tok/s 122532 (111919)	Loss/tok 3.5094 (3.5160)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.073 (0.100)	Data 1.06e-04 (3.21e-03)	Tok/s 104164 (112000)	Loss/tok 3.1650 (3.5141)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.104 (0.100)	Data 1.10e-04 (2.91e-03)	Tok/s 120349 (112194)	Loss/tok 3.4677 (3.5149)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.139 (0.100)	Data 1.14e-04 (2.66e-03)	Tok/s 125087 (112672)	Loss/tok 3.6930 (3.5189)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.138 (0.100)	Data 1.06e-04 (2.45e-03)	Tok/s 125967 (112657)	Loss/tok 3.8187 (3.5129)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.105 (0.098)	Data 1.04e-04 (2.27e-03)	Tok/s 120002 (112381)	Loss/tok 3.5027 (3.5016)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.139 (0.101)	Data 1.29e-04 (2.12e-03)	Tok/s 126600 (112890)	Loss/tok 3.6033 (3.5219)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.105 (0.101)	Data 1.09e-04 (1.98e-03)	Tok/s 120795 (113160)	Loss/tok 3.5747 (3.5214)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][160/1291]	Time 0.073 (0.101)	Data 1.09e-04 (1.87e-03)	Tok/s 103320 (113287)	Loss/tok 3.3118 (3.5233)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.105 (0.102)	Data 1.14e-04 (1.76e-03)	Tok/s 120172 (113645)	Loss/tok 3.5230 (3.5311)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][180/1291]	Time 0.177 (0.103)	Data 1.18e-04 (1.67e-03)	Tok/s 126567 (113867)	Loss/tok 3.8415 (3.5367)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.105 (0.104)	Data 1.09e-04 (1.59e-03)	Tok/s 120044 (114228)	Loss/tok 3.4603 (3.5396)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.073 (0.104)	Data 1.08e-04 (1.52e-03)	Tok/s 103277 (114219)	Loss/tok 3.2762 (3.5392)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.104 (0.102)	Data 1.05e-04 (1.45e-03)	Tok/s 120422 (113939)	Loss/tok 3.5828 (3.5347)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.073 (0.102)	Data 1.12e-04 (1.39e-03)	Tok/s 105816 (113944)	Loss/tok 3.2335 (3.5360)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.042 (0.102)	Data 1.08e-04 (1.33e-03)	Tok/s 93085 (113861)	Loss/tok 2.7652 (3.5341)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.073 (0.101)	Data 1.05e-04 (1.28e-03)	Tok/s 105479 (113744)	Loss/tok 3.2436 (3.5327)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.042 (0.101)	Data 1.04e-04 (1.24e-03)	Tok/s 92302 (113595)	Loss/tok 2.7087 (3.5297)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.073 (0.101)	Data 1.07e-04 (1.19e-03)	Tok/s 108220 (113652)	Loss/tok 3.3880 (3.5316)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.104 (0.101)	Data 1.01e-04 (1.15e-03)	Tok/s 121973 (113745)	Loss/tok 3.3386 (3.5303)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.138 (0.100)	Data 1.04e-04 (1.12e-03)	Tok/s 127249 (113546)	Loss/tok 3.5628 (3.5266)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.105 (0.100)	Data 1.01e-04 (1.08e-03)	Tok/s 120974 (113553)	Loss/tok 3.3925 (3.5224)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.104 (0.100)	Data 1.04e-04 (1.05e-03)	Tok/s 120214 (113614)	Loss/tok 3.5046 (3.5244)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][310/1291]	Time 0.073 (0.099)	Data 1.08e-04 (1.02e-03)	Tok/s 106951 (113515)	Loss/tok 3.2868 (3.5200)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.104 (0.099)	Data 1.06e-04 (9.90e-04)	Tok/s 119792 (113475)	Loss/tok 3.4411 (3.5161)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.073 (0.099)	Data 1.06e-04 (9.63e-04)	Tok/s 105396 (113315)	Loss/tok 3.1848 (3.5111)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.105 (0.098)	Data 1.09e-04 (9.38e-04)	Tok/s 122089 (113209)	Loss/tok 3.4417 (3.5083)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.073 (0.098)	Data 1.07e-04 (9.14e-04)	Tok/s 106649 (112993)	Loss/tok 3.2560 (3.5038)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.105 (0.097)	Data 1.05e-04 (8.92e-04)	Tok/s 118104 (112969)	Loss/tok 3.3319 (3.5009)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.074 (0.098)	Data 1.07e-04 (8.71e-04)	Tok/s 109049 (113081)	Loss/tok 3.2962 (3.4993)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.105 (0.097)	Data 1.03e-04 (8.50e-04)	Tok/s 118049 (113101)	Loss/tok 3.5656 (3.4977)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.073 (0.098)	Data 1.06e-04 (8.31e-04)	Tok/s 106780 (113104)	Loss/tok 3.2275 (3.4978)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.139 (0.098)	Data 1.06e-04 (8.13e-04)	Tok/s 125982 (113202)	Loss/tok 3.5848 (3.4973)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.074 (0.098)	Data 1.05e-04 (7.96e-04)	Tok/s 105952 (113237)	Loss/tok 3.2724 (3.4939)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.043 (0.097)	Data 1.07e-04 (7.80e-04)	Tok/s 93403 (113005)	Loss/tok 2.7541 (3.4907)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.105 (0.097)	Data 1.08e-04 (7.64e-04)	Tok/s 120648 (113090)	Loss/tok 3.4711 (3.4897)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][440/1291]	Time 0.139 (0.098)	Data 1.03e-04 (7.49e-04)	Tok/s 124107 (113172)	Loss/tok 3.8837 (3.4941)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.074 (0.097)	Data 1.06e-04 (7.35e-04)	Tok/s 104675 (113086)	Loss/tok 3.1588 (3.4903)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.073 (0.097)	Data 1.04e-04 (7.21e-04)	Tok/s 104742 (113111)	Loss/tok 3.2624 (3.4890)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.074 (0.097)	Data 1.03e-04 (7.08e-04)	Tok/s 106190 (112996)	Loss/tok 3.2825 (3.4854)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.105 (0.097)	Data 1.08e-04 (6.96e-04)	Tok/s 121224 (113018)	Loss/tok 3.4649 (3.4842)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][490/1291]	Time 0.138 (0.097)	Data 1.08e-04 (6.84e-04)	Tok/s 125617 (113009)	Loss/tok 3.7906 (3.4849)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.073 (0.097)	Data 1.03e-04 (6.72e-04)	Tok/s 107265 (113008)	Loss/tok 3.2792 (3.4832)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.073 (0.097)	Data 1.05e-04 (6.61e-04)	Tok/s 105736 (112974)	Loss/tok 3.1626 (3.4821)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.073 (0.097)	Data 1.03e-04 (6.50e-04)	Tok/s 107299 (112951)	Loss/tok 3.2693 (3.4797)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.105 (0.097)	Data 1.09e-04 (6.40e-04)	Tok/s 119802 (112966)	Loss/tok 3.4304 (3.4790)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.105 (0.096)	Data 1.07e-04 (6.30e-04)	Tok/s 120533 (112876)	Loss/tok 3.3433 (3.4756)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.073 (0.096)	Data 1.08e-04 (6.21e-04)	Tok/s 104674 (112807)	Loss/tok 3.1244 (3.4735)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.073 (0.096)	Data 1.08e-04 (6.11e-04)	Tok/s 105481 (112749)	Loss/tok 3.2671 (3.4720)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.073 (0.095)	Data 1.02e-04 (6.03e-04)	Tok/s 105387 (112606)	Loss/tok 3.2181 (3.4689)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.073 (0.095)	Data 1.06e-04 (5.94e-04)	Tok/s 108236 (112565)	Loss/tok 3.1828 (3.4664)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.105 (0.095)	Data 1.12e-04 (5.86e-04)	Tok/s 120454 (112657)	Loss/tok 3.4528 (3.4684)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.073 (0.095)	Data 1.07e-04 (5.78e-04)	Tok/s 103316 (112611)	Loss/tok 3.3431 (3.4673)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.073 (0.095)	Data 1.03e-04 (5.70e-04)	Tok/s 108628 (112700)	Loss/tok 3.3263 (3.4678)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][620/1291]	Time 0.105 (0.095)	Data 1.14e-04 (5.63e-04)	Tok/s 120705 (112730)	Loss/tok 3.2614 (3.4672)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.105 (0.096)	Data 1.08e-04 (5.55e-04)	Tok/s 118976 (112824)	Loss/tok 3.5183 (3.4678)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.105 (0.096)	Data 1.06e-04 (5.48e-04)	Tok/s 120278 (112886)	Loss/tok 3.6079 (3.4667)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.140 (0.096)	Data 1.09e-04 (5.42e-04)	Tok/s 125404 (112895)	Loss/tok 3.6574 (3.4647)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.105 (0.096)	Data 1.06e-04 (5.35e-04)	Tok/s 119459 (112926)	Loss/tok 3.5316 (3.4656)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.073 (0.096)	Data 1.02e-04 (5.29e-04)	Tok/s 105588 (112924)	Loss/tok 3.1555 (3.4666)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.105 (0.096)	Data 1.07e-04 (5.22e-04)	Tok/s 121174 (112909)	Loss/tok 3.4109 (3.4651)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.105 (0.096)	Data 1.06e-04 (5.16e-04)	Tok/s 119891 (112862)	Loss/tok 3.3514 (3.4624)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.042 (0.095)	Data 1.09e-04 (5.11e-04)	Tok/s 94264 (112828)	Loss/tok 2.6738 (3.4602)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.139 (0.095)	Data 1.11e-04 (5.05e-04)	Tok/s 128168 (112802)	Loss/tok 3.5975 (3.4586)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.073 (0.095)	Data 1.14e-04 (5.00e-04)	Tok/s 107441 (112825)	Loss/tok 3.2156 (3.4587)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.94e-04)	Tok/s 104289 (112832)	Loss/tok 3.1734 (3.4581)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.042 (0.095)	Data 1.11e-04 (4.89e-04)	Tok/s 95182 (112813)	Loss/tok 2.7739 (3.4570)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][750/1291]	Time 0.105 (0.095)	Data 1.04e-04 (4.84e-04)	Tok/s 120150 (112796)	Loss/tok 3.5379 (3.4563)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][760/1291]	Time 0.139 (0.095)	Data 1.07e-04 (4.79e-04)	Tok/s 125843 (112736)	Loss/tok 3.6421 (3.4548)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.073 (0.095)	Data 1.01e-04 (4.74e-04)	Tok/s 107341 (112744)	Loss/tok 3.2501 (3.4542)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.042 (0.095)	Data 1.08e-04 (4.70e-04)	Tok/s 92931 (112714)	Loss/tok 2.6542 (3.4537)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.073 (0.095)	Data 1.14e-04 (4.65e-04)	Tok/s 103929 (112777)	Loss/tok 3.1215 (3.4546)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.074 (0.095)	Data 1.18e-04 (4.61e-04)	Tok/s 104761 (112799)	Loss/tok 3.1881 (3.4539)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.073 (0.095)	Data 1.13e-04 (4.57e-04)	Tok/s 110025 (112849)	Loss/tok 3.2176 (3.4542)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.105 (0.095)	Data 1.13e-04 (4.52e-04)	Tok/s 120260 (112862)	Loss/tok 3.3803 (3.4533)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.48e-04)	Tok/s 108167 (112920)	Loss/tok 3.2572 (3.4524)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.139 (0.095)	Data 1.35e-04 (4.44e-04)	Tok/s 123147 (112934)	Loss/tok 3.5349 (3.4511)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.105 (0.096)	Data 1.08e-04 (4.40e-04)	Tok/s 121871 (112952)	Loss/tok 3.3565 (3.4505)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.105 (0.095)	Data 1.23e-04 (4.37e-04)	Tok/s 117691 (112926)	Loss/tok 3.4522 (3.4496)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.140 (0.095)	Data 1.03e-04 (4.33e-04)	Tok/s 124701 (112963)	Loss/tok 3.5071 (3.4493)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.073 (0.095)	Data 1.09e-04 (4.29e-04)	Tok/s 106899 (112892)	Loss/tok 3.2055 (3.4471)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][890/1291]	Time 0.073 (0.095)	Data 1.07e-04 (4.26e-04)	Tok/s 106170 (112861)	Loss/tok 3.1925 (3.4456)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.073 (0.095)	Data 1.03e-04 (4.22e-04)	Tok/s 105329 (112841)	Loss/tok 3.0220 (3.4442)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.19e-04)	Tok/s 106124 (112806)	Loss/tok 3.2431 (3.4433)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.042 (0.095)	Data 1.07e-04 (4.15e-04)	Tok/s 93360 (112749)	Loss/tok 2.6655 (3.4412)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.105 (0.095)	Data 1.18e-04 (4.12e-04)	Tok/s 121201 (112710)	Loss/tok 3.4962 (3.4404)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.073 (0.095)	Data 1.05e-04 (4.09e-04)	Tok/s 106896 (112742)	Loss/tok 3.1496 (3.4398)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][950/1291]	Time 0.105 (0.095)	Data 1.06e-04 (4.05e-04)	Tok/s 120487 (112771)	Loss/tok 3.3318 (3.4388)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.104 (0.095)	Data 1.39e-04 (4.02e-04)	Tok/s 121508 (112824)	Loss/tok 3.3492 (3.4389)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.139 (0.095)	Data 1.03e-04 (3.99e-04)	Tok/s 124381 (112806)	Loss/tok 3.5956 (3.4387)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.073 (0.095)	Data 1.06e-04 (3.97e-04)	Tok/s 108341 (112747)	Loss/tok 3.1246 (3.4377)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.139 (0.095)	Data 1.10e-04 (3.94e-04)	Tok/s 125997 (112769)	Loss/tok 3.5840 (3.4381)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.042 (0.095)	Data 1.03e-04 (3.91e-04)	Tok/s 93091 (112706)	Loss/tok 2.6923 (3.4368)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.177 (0.095)	Data 1.21e-04 (3.88e-04)	Tok/s 126576 (112720)	Loss/tok 3.6944 (3.4364)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.105 (0.095)	Data 1.26e-04 (3.85e-04)	Tok/s 120456 (112712)	Loss/tok 3.4092 (3.4354)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.073 (0.094)	Data 1.05e-04 (3.83e-04)	Tok/s 106631 (112671)	Loss/tok 3.1235 (3.4340)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.073 (0.094)	Data 1.10e-04 (3.80e-04)	Tok/s 107888 (112686)	Loss/tok 3.1600 (3.4341)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.073 (0.094)	Data 1.64e-04 (3.77e-04)	Tok/s 107661 (112687)	Loss/tok 3.1460 (3.4333)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.105 (0.095)	Data 1.22e-04 (3.75e-04)	Tok/s 121836 (112721)	Loss/tok 3.3528 (3.4340)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.042 (0.095)	Data 1.03e-04 (3.72e-04)	Tok/s 93076 (112702)	Loss/tok 2.7147 (3.4333)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1080/1291]	Time 0.073 (0.095)	Data 1.06e-04 (3.70e-04)	Tok/s 105038 (112695)	Loss/tok 3.0718 (3.4322)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.073 (0.095)	Data 1.05e-04 (3.68e-04)	Tok/s 103966 (112718)	Loss/tok 3.0853 (3.4317)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.042 (0.095)	Data 1.05e-04 (3.65e-04)	Tok/s 93823 (112692)	Loss/tok 2.7000 (3.4304)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.042 (0.095)	Data 1.08e-04 (3.63e-04)	Tok/s 94499 (112674)	Loss/tok 2.7057 (3.4302)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.104 (0.095)	Data 1.11e-04 (3.61e-04)	Tok/s 120111 (112699)	Loss/tok 3.4358 (3.4302)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.073 (0.094)	Data 1.07e-04 (3.58e-04)	Tok/s 106269 (112702)	Loss/tok 3.1683 (3.4288)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1140/1291]	Time 0.139 (0.095)	Data 1.11e-04 (3.56e-04)	Tok/s 128002 (112744)	Loss/tok 3.4179 (3.4283)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1150/1291]	Time 0.177 (0.095)	Data 1.02e-04 (3.54e-04)	Tok/s 128420 (112721)	Loss/tok 3.6974 (3.4292)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.139 (0.095)	Data 1.09e-04 (3.52e-04)	Tok/s 125900 (112764)	Loss/tok 3.5422 (3.4296)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.176 (0.095)	Data 1.04e-04 (3.50e-04)	Tok/s 126654 (112795)	Loss/tok 3.6472 (3.4302)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.073 (0.095)	Data 1.09e-04 (3.48e-04)	Tok/s 107343 (112802)	Loss/tok 3.0983 (3.4292)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.073 (0.095)	Data 1.06e-04 (3.46e-04)	Tok/s 106115 (112777)	Loss/tok 3.2476 (3.4282)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.073 (0.095)	Data 1.06e-04 (3.44e-04)	Tok/s 105845 (112742)	Loss/tok 3.2188 (3.4274)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.139 (0.095)	Data 1.04e-04 (3.42e-04)	Tok/s 127157 (112759)	Loss/tok 3.4506 (3.4269)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.042 (0.095)	Data 1.05e-04 (3.40e-04)	Tok/s 92481 (112785)	Loss/tok 2.7338 (3.4268)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.177 (0.095)	Data 1.07e-04 (3.38e-04)	Tok/s 125893 (112749)	Loss/tok 3.7247 (3.4260)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.139 (0.095)	Data 1.08e-04 (3.36e-04)	Tok/s 125963 (112759)	Loss/tok 3.4613 (3.4255)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.042 (0.095)	Data 1.06e-04 (3.34e-04)	Tok/s 92133 (112776)	Loss/tok 2.6948 (3.4255)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.105 (0.095)	Data 1.08e-04 (3.32e-04)	Tok/s 118582 (112782)	Loss/tok 3.3453 (3.4245)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.073 (0.095)	Data 1.02e-04 (3.31e-04)	Tok/s 106324 (112758)	Loss/tok 3.2305 (3.4236)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1280/1291]	Time 0.105 (0.095)	Data 1.05e-04 (3.29e-04)	Tok/s 119854 (112788)	Loss/tok 3.4577 (3.4226)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.073 (0.095)	Data 4.70e-05 (3.29e-04)	Tok/s 101998 (112777)	Loss/tok 2.9729 (3.4212)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590782515, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590782515, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.450 (0.450)	Decoder iters 142.0 (142.0)	Tok/s 19523 (19523)
0: Running moses detokenizer
0: BLEU(score=21.919190609814148, counts=[35320, 17005, 9343, 5334], totals=[63606, 60603, 57600, 54601], precisions=[55.52935257680093, 28.059667013184164, 16.22048611111111, 9.769051848867237], bp=0.9833183926368186, sys_len=63606, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590783643, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2192, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590783644, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4238	Test BLEU: 21.92
0: Performance: Epoch: 1	Training: 1804238 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592590783644, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590783644, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590783644, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1076374532
0: TRAIN [2][0/1291]	Time 0.386 (0.386)	Data 2.82e-01 (2.82e-01)	Tok/s 20130 (20130)	Loss/tok 3.0803 (3.0803)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.104 (0.120)	Data 1.11e-04 (2.57e-02)	Tok/s 120380 (103835)	Loss/tok 3.1978 (3.2898)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.105 (0.104)	Data 1.06e-04 (1.35e-02)	Tok/s 120671 (107255)	Loss/tok 3.2121 (3.2462)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.105 (0.099)	Data 1.07e-04 (9.20e-03)	Tok/s 119524 (108403)	Loss/tok 3.2329 (3.2205)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.042 (0.096)	Data 1.06e-04 (6.98e-03)	Tok/s 93552 (108182)	Loss/tok 2.6482 (3.2177)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.105 (0.096)	Data 1.08e-04 (5.64e-03)	Tok/s 122865 (109573)	Loss/tok 3.1459 (3.2332)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.108 (0.096)	Data 1.12e-04 (4.73e-03)	Tok/s 117621 (110424)	Loss/tok 3.2726 (3.2367)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.073 (0.097)	Data 1.10e-04 (4.08e-03)	Tok/s 106839 (110829)	Loss/tok 3.1251 (3.2521)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.073 (0.096)	Data 1.13e-04 (3.59e-03)	Tok/s 101567 (110797)	Loss/tok 3.0552 (3.2577)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.073 (0.094)	Data 1.05e-04 (3.21e-03)	Tok/s 106835 (110554)	Loss/tok 3.0967 (3.2484)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.105 (0.095)	Data 1.08e-04 (2.91e-03)	Tok/s 119557 (110930)	Loss/tok 3.2947 (3.2548)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][110/1291]	Time 0.105 (0.093)	Data 1.22e-04 (2.65e-03)	Tok/s 121008 (110680)	Loss/tok 3.1747 (3.2470)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.074 (0.094)	Data 1.77e-04 (2.44e-03)	Tok/s 104526 (111093)	Loss/tok 3.1565 (3.2553)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.074 (0.094)	Data 1.02e-04 (2.27e-03)	Tok/s 103344 (110983)	Loss/tok 3.1664 (3.2553)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][140/1291]	Time 0.177 (0.094)	Data 1.61e-04 (2.11e-03)	Tok/s 123462 (111074)	Loss/tok 3.7248 (3.2597)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.074 (0.093)	Data 1.05e-04 (1.98e-03)	Tok/s 104730 (110996)	Loss/tok 2.9252 (3.2545)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.104 (0.092)	Data 1.04e-04 (1.87e-03)	Tok/s 119523 (110926)	Loss/tok 3.2605 (3.2485)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.139 (0.092)	Data 1.07e-04 (1.76e-03)	Tok/s 125375 (110943)	Loss/tok 3.3610 (3.2484)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.073 (0.093)	Data 1.13e-04 (1.67e-03)	Tok/s 106248 (111093)	Loss/tok 3.1049 (3.2510)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.105 (0.093)	Data 1.08e-04 (1.59e-03)	Tok/s 119774 (111113)	Loss/tok 3.4449 (3.2580)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.105 (0.093)	Data 1.05e-04 (1.52e-03)	Tok/s 120177 (111353)	Loss/tok 3.1494 (3.2583)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.073 (0.093)	Data 1.07e-04 (1.45e-03)	Tok/s 105781 (111498)	Loss/tok 3.1265 (3.2580)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.073 (0.093)	Data 1.09e-04 (1.39e-03)	Tok/s 103415 (111626)	Loss/tok 3.0929 (3.2594)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.104 (0.093)	Data 1.06e-04 (1.33e-03)	Tok/s 120989 (111702)	Loss/tok 3.3548 (3.2574)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.043 (0.094)	Data 1.14e-04 (1.28e-03)	Tok/s 93435 (111909)	Loss/tok 2.7186 (3.2608)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.074 (0.094)	Data 1.08e-04 (1.24e-03)	Tok/s 106570 (111982)	Loss/tok 3.1604 (3.2635)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.043 (0.094)	Data 1.10e-04 (1.19e-03)	Tok/s 89469 (112030)	Loss/tok 2.5518 (3.2647)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][270/1291]	Time 0.178 (0.094)	Data 1.08e-04 (1.15e-03)	Tok/s 124747 (112078)	Loss/tok 3.6675 (3.2684)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.105 (0.095)	Data 1.08e-04 (1.12e-03)	Tok/s 119491 (112258)	Loss/tok 3.2673 (3.2692)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.140 (0.095)	Data 1.04e-04 (1.08e-03)	Tok/s 123909 (112423)	Loss/tok 3.4588 (3.2761)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.105 (0.095)	Data 1.08e-04 (1.05e-03)	Tok/s 120384 (112516)	Loss/tok 3.3115 (3.2768)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.073 (0.096)	Data 1.06e-04 (1.02e-03)	Tok/s 104307 (112595)	Loss/tok 3.0992 (3.2776)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.074 (0.096)	Data 1.05e-04 (9.90e-04)	Tok/s 104341 (112637)	Loss/tok 3.1571 (3.2796)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.073 (0.095)	Data 1.07e-04 (9.63e-04)	Tok/s 106071 (112566)	Loss/tok 3.2636 (3.2782)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.073 (0.095)	Data 1.05e-04 (9.38e-04)	Tok/s 104231 (112589)	Loss/tok 3.2290 (3.2800)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.073 (0.095)	Data 1.44e-04 (9.15e-04)	Tok/s 105322 (112596)	Loss/tok 2.9952 (3.2797)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.073 (0.095)	Data 1.03e-04 (8.92e-04)	Tok/s 108659 (112465)	Loss/tok 2.9723 (3.2768)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.177 (0.095)	Data 1.10e-04 (8.71e-04)	Tok/s 127454 (112490)	Loss/tok 3.6808 (3.2791)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.105 (0.095)	Data 1.09e-04 (8.51e-04)	Tok/s 121074 (112574)	Loss/tok 3.2029 (3.2785)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.073 (0.095)	Data 1.09e-04 (8.32e-04)	Tok/s 106226 (112550)	Loss/tok 3.0409 (3.2765)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][400/1291]	Time 0.105 (0.095)	Data 1.04e-04 (8.14e-04)	Tok/s 118272 (112568)	Loss/tok 3.3743 (3.2781)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][410/1291]	Time 0.073 (0.095)	Data 1.08e-04 (7.97e-04)	Tok/s 104416 (112424)	Loss/tok 3.0250 (3.2766)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.073 (0.095)	Data 1.05e-04 (7.81e-04)	Tok/s 105799 (112536)	Loss/tok 3.1628 (3.2796)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.139 (0.095)	Data 1.13e-04 (7.65e-04)	Tok/s 124959 (112619)	Loss/tok 3.4810 (3.2810)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.105 (0.095)	Data 1.05e-04 (7.50e-04)	Tok/s 121510 (112578)	Loss/tok 3.2772 (3.2807)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.105 (0.095)	Data 1.17e-04 (7.36e-04)	Tok/s 120905 (112681)	Loss/tok 3.2506 (3.2831)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][460/1291]	Time 0.139 (0.096)	Data 1.04e-04 (7.22e-04)	Tok/s 127024 (112709)	Loss/tok 3.4244 (3.2856)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.073 (0.095)	Data 1.07e-04 (7.09e-04)	Tok/s 105088 (112644)	Loss/tok 3.0709 (3.2836)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.139 (0.096)	Data 1.18e-04 (6.97e-04)	Tok/s 125384 (112692)	Loss/tok 3.5849 (3.2870)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.073 (0.095)	Data 1.05e-04 (6.85e-04)	Tok/s 107574 (112623)	Loss/tok 3.0818 (3.2851)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.105 (0.095)	Data 1.05e-04 (6.73e-04)	Tok/s 119597 (112649)	Loss/tok 3.3988 (3.2841)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.073 (0.095)	Data 1.06e-04 (6.62e-04)	Tok/s 104821 (112736)	Loss/tok 3.2008 (3.2851)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.105 (0.095)	Data 1.05e-04 (6.51e-04)	Tok/s 121534 (112744)	Loss/tok 3.3027 (3.2845)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.139 (0.095)	Data 1.09e-04 (6.41e-04)	Tok/s 128824 (112736)	Loss/tok 3.4174 (3.2862)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.073 (0.095)	Data 1.08e-04 (6.31e-04)	Tok/s 106492 (112719)	Loss/tok 3.2054 (3.2851)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.073 (0.095)	Data 1.01e-04 (6.22e-04)	Tok/s 105135 (112681)	Loss/tok 3.1623 (3.2834)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.073 (0.095)	Data 1.05e-04 (6.12e-04)	Tok/s 102048 (112581)	Loss/tok 3.0270 (3.2828)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.105 (0.095)	Data 1.06e-04 (6.03e-04)	Tok/s 120235 (112581)	Loss/tok 3.3790 (3.2818)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.105 (0.095)	Data 1.07e-04 (5.95e-04)	Tok/s 119698 (112624)	Loss/tok 3.3320 (3.2812)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][590/1291]	Time 0.105 (0.095)	Data 1.06e-04 (5.87e-04)	Tok/s 120293 (112666)	Loss/tok 3.2559 (3.2823)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.073 (0.095)	Data 1.03e-04 (5.79e-04)	Tok/s 105583 (112588)	Loss/tok 3.0746 (3.2822)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.105 (0.095)	Data 1.07e-04 (5.71e-04)	Tok/s 120251 (112642)	Loss/tok 3.3613 (3.2834)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.105 (0.095)	Data 1.10e-04 (5.63e-04)	Tok/s 116731 (112582)	Loss/tok 3.4597 (3.2833)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.073 (0.095)	Data 1.05e-04 (5.56e-04)	Tok/s 104438 (112587)	Loss/tok 3.0758 (3.2836)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.043 (0.095)	Data 1.06e-04 (5.49e-04)	Tok/s 91062 (112590)	Loss/tok 2.5646 (3.2844)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.074 (0.094)	Data 1.11e-04 (5.42e-04)	Tok/s 104198 (112457)	Loss/tok 3.0720 (3.2837)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.105 (0.094)	Data 1.09e-04 (5.36e-04)	Tok/s 122400 (112442)	Loss/tok 3.4081 (3.2833)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.073 (0.095)	Data 1.08e-04 (5.29e-04)	Tok/s 106649 (112494)	Loss/tok 3.1650 (3.2857)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.105 (0.095)	Data 1.06e-04 (5.23e-04)	Tok/s 119990 (112550)	Loss/tok 3.3650 (3.2872)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.105 (0.095)	Data 1.05e-04 (5.17e-04)	Tok/s 121068 (112505)	Loss/tok 3.2011 (3.2862)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.106 (0.095)	Data 1.06e-04 (5.11e-04)	Tok/s 117571 (112575)	Loss/tok 3.3027 (3.2867)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.105 (0.095)	Data 1.10e-04 (5.06e-04)	Tok/s 118596 (112611)	Loss/tok 3.3713 (3.2870)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][720/1291]	Time 0.139 (0.095)	Data 1.07e-04 (5.00e-04)	Tok/s 124958 (112667)	Loss/tok 3.4464 (3.2871)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.073 (0.095)	Data 1.07e-04 (4.95e-04)	Tok/s 106901 (112673)	Loss/tok 3.1273 (3.2864)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.104 (0.095)	Data 1.08e-04 (4.89e-04)	Tok/s 120437 (112683)	Loss/tok 3.1756 (3.2861)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.073 (0.095)	Data 1.18e-04 (4.84e-04)	Tok/s 104278 (112699)	Loss/tok 3.0618 (3.2856)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.105 (0.095)	Data 1.09e-04 (4.79e-04)	Tok/s 120520 (112748)	Loss/tok 3.4010 (3.2872)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.042 (0.095)	Data 1.09e-04 (4.75e-04)	Tok/s 92818 (112738)	Loss/tok 2.7633 (3.2868)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.073 (0.095)	Data 1.09e-04 (4.70e-04)	Tok/s 103602 (112692)	Loss/tok 3.1821 (3.2861)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.073 (0.095)	Data 1.05e-04 (4.65e-04)	Tok/s 107162 (112750)	Loss/tok 3.1733 (3.2862)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.104 (0.095)	Data 1.14e-04 (4.61e-04)	Tok/s 121675 (112794)	Loss/tok 3.2824 (3.2873)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.073 (0.095)	Data 1.57e-04 (4.57e-04)	Tok/s 105762 (112750)	Loss/tok 3.1160 (3.2865)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.105 (0.095)	Data 1.10e-04 (4.53e-04)	Tok/s 119758 (112714)	Loss/tok 3.2860 (3.2861)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.073 (0.095)	Data 1.06e-04 (4.49e-04)	Tok/s 107231 (112705)	Loss/tok 3.1678 (3.2852)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.105 (0.095)	Data 1.11e-04 (4.45e-04)	Tok/s 120273 (112776)	Loss/tok 3.2100 (3.2855)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][850/1291]	Time 0.139 (0.095)	Data 1.10e-04 (4.41e-04)	Tok/s 126496 (112772)	Loss/tok 3.4221 (3.2849)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][860/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.37e-04)	Tok/s 103992 (112811)	Loss/tok 3.0339 (3.2855)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.105 (0.095)	Data 1.05e-04 (4.33e-04)	Tok/s 119903 (112766)	Loss/tok 3.3013 (3.2847)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.073 (0.095)	Data 1.13e-04 (4.30e-04)	Tok/s 105911 (112747)	Loss/tok 2.9996 (3.2838)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.073 (0.095)	Data 1.11e-04 (4.26e-04)	Tok/s 104488 (112776)	Loss/tok 2.9434 (3.2857)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.073 (0.095)	Data 1.08e-04 (4.23e-04)	Tok/s 106351 (112743)	Loss/tok 3.0475 (3.2851)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.139 (0.095)	Data 1.08e-04 (4.19e-04)	Tok/s 125636 (112724)	Loss/tok 3.5002 (3.2850)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.104 (0.095)	Data 1.04e-04 (4.16e-04)	Tok/s 119054 (112745)	Loss/tok 3.2608 (3.2853)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.073 (0.095)	Data 1.11e-04 (4.12e-04)	Tok/s 106461 (112709)	Loss/tok 3.1393 (3.2846)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.073 (0.095)	Data 1.07e-04 (4.09e-04)	Tok/s 106259 (112709)	Loss/tok 3.0862 (3.2847)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.073 (0.095)	Data 1.21e-04 (4.06e-04)	Tok/s 102039 (112697)	Loss/tok 3.0069 (3.2837)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.105 (0.095)	Data 1.15e-04 (4.03e-04)	Tok/s 118122 (112711)	Loss/tok 3.2540 (3.2834)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.073 (0.095)	Data 1.62e-04 (4.00e-04)	Tok/s 107179 (112701)	Loss/tok 3.0976 (3.2836)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][980/1291]	Time 0.073 (0.094)	Data 1.05e-04 (3.97e-04)	Tok/s 107588 (112614)	Loss/tok 3.0978 (3.2821)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.105 (0.095)	Data 1.23e-04 (3.94e-04)	Tok/s 120124 (112623)	Loss/tok 3.2076 (3.2825)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.105 (0.094)	Data 1.04e-04 (3.91e-04)	Tok/s 119754 (112604)	Loss/tok 3.2820 (3.2813)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1010/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.88e-04)	Tok/s 103318 (112613)	Loss/tok 2.9208 (3.2820)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.073 (0.095)	Data 1.08e-04 (3.86e-04)	Tok/s 104669 (112645)	Loss/tok 3.0386 (3.2815)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.073 (0.094)	Data 1.09e-04 (3.83e-04)	Tok/s 105506 (112611)	Loss/tok 3.0523 (3.2804)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.105 (0.094)	Data 1.14e-04 (3.80e-04)	Tok/s 119155 (112634)	Loss/tok 3.2571 (3.2810)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.105 (0.094)	Data 1.14e-04 (3.78e-04)	Tok/s 122323 (112615)	Loss/tok 3.1729 (3.2804)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.074 (0.094)	Data 1.10e-04 (3.75e-04)	Tok/s 102836 (112607)	Loss/tok 3.0950 (3.2804)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.140 (0.094)	Data 1.09e-04 (3.73e-04)	Tok/s 123346 (112624)	Loss/tok 3.6277 (3.2810)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.104 (0.095)	Data 1.07e-04 (3.70e-04)	Tok/s 119117 (112641)	Loss/tok 3.2640 (3.2812)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.073 (0.094)	Data 1.23e-04 (3.68e-04)	Tok/s 103994 (112617)	Loss/tok 3.1156 (3.2803)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.074 (0.095)	Data 1.12e-04 (3.66e-04)	Tok/s 104916 (112657)	Loss/tok 3.0173 (3.2807)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.043 (0.095)	Data 1.09e-04 (3.63e-04)	Tok/s 94582 (112641)	Loss/tok 2.7262 (3.2813)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.104 (0.095)	Data 1.08e-04 (3.61e-04)	Tok/s 121319 (112637)	Loss/tok 3.2889 (3.2805)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1130/1291]	Time 0.139 (0.095)	Data 1.03e-04 (3.59e-04)	Tok/s 124685 (112668)	Loss/tok 3.4246 (3.2815)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.074 (0.095)	Data 1.47e-04 (3.57e-04)	Tok/s 104965 (112627)	Loss/tok 3.0662 (3.2807)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.074 (0.095)	Data 1.03e-04 (3.55e-04)	Tok/s 102786 (112613)	Loss/tok 3.2454 (3.2802)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.105 (0.094)	Data 1.06e-04 (3.52e-04)	Tok/s 119050 (112571)	Loss/tok 3.2372 (3.2795)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.074 (0.094)	Data 1.07e-04 (3.50e-04)	Tok/s 104811 (112562)	Loss/tok 2.8563 (3.2798)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1180/1291]	Time 0.074 (0.095)	Data 1.05e-04 (3.48e-04)	Tok/s 105169 (112574)	Loss/tok 3.1300 (3.2806)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.105 (0.095)	Data 1.10e-04 (3.46e-04)	Tok/s 121197 (112617)	Loss/tok 3.3226 (3.2806)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.105 (0.095)	Data 1.09e-04 (3.44e-04)	Tok/s 119845 (112648)	Loss/tok 3.2637 (3.2812)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.178 (0.095)	Data 1.06e-04 (3.42e-04)	Tok/s 124547 (112669)	Loss/tok 3.7179 (3.2825)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.105 (0.095)	Data 1.09e-04 (3.40e-04)	Tok/s 119784 (112697)	Loss/tok 3.2588 (3.2824)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.104 (0.095)	Data 1.24e-04 (3.39e-04)	Tok/s 124043 (112707)	Loss/tok 3.1865 (3.2822)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.073 (0.095)	Data 1.18e-04 (3.37e-04)	Tok/s 104723 (112696)	Loss/tok 3.0700 (3.2822)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.105 (0.095)	Data 1.08e-04 (3.35e-04)	Tok/s 121573 (112684)	Loss/tok 3.2292 (3.2821)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.105 (0.095)	Data 1.10e-04 (3.33e-04)	Tok/s 121559 (112676)	Loss/tok 3.2701 (3.2821)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.105 (0.095)	Data 1.24e-04 (3.31e-04)	Tok/s 120578 (112635)	Loss/tok 3.4099 (3.2813)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.073 (0.095)	Data 1.22e-04 (3.30e-04)	Tok/s 106133 (112609)	Loss/tok 3.0441 (3.2812)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.073 (0.095)	Data 4.67e-05 (3.30e-04)	Tok/s 104154 (112594)	Loss/tok 3.1044 (3.2811)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590906647, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590906647, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.360 (0.360)	Decoder iters 100.0 (100.0)	Tok/s 24005 (24005)
0: Running moses detokenizer
0: BLEU(score=22.643906842612868, counts=[35731, 17410, 9697, 5653], totals=[63646, 60643, 57641, 54646], precisions=[56.14021305345191, 28.70900186336428, 16.82309467219514, 10.344764484134245], bp=0.9839469817889961, sys_len=63646, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590907699, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22640000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590907699, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2796	Test BLEU: 22.64
0: Performance: Epoch: 2	Training: 1801195 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592590907699, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590907699, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590907699, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2750369496
0: TRAIN [3][0/1291]	Time 0.380 (0.380)	Data 3.04e-01 (3.04e-01)	Tok/s 20674 (20674)	Loss/tok 2.9854 (2.9854)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.042 (0.101)	Data 1.28e-04 (2.78e-02)	Tok/s 94383 (98451)	Loss/tok 2.5142 (3.0114)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][20/1291]	Time 0.139 (0.102)	Data 1.24e-04 (1.46e-02)	Tok/s 125955 (105787)	Loss/tok 3.3550 (3.1497)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.139 (0.095)	Data 1.08e-04 (9.92e-03)	Tok/s 126040 (106628)	Loss/tok 3.3707 (3.1202)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.105 (0.092)	Data 1.06e-04 (7.53e-03)	Tok/s 120072 (107555)	Loss/tok 3.3489 (3.1190)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.105 (0.090)	Data 1.09e-04 (6.07e-03)	Tok/s 120569 (107879)	Loss/tok 3.1352 (3.1134)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][60/1291]	Time 0.075 (0.094)	Data 1.02e-04 (5.10e-03)	Tok/s 105119 (109163)	Loss/tok 2.9831 (3.1526)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.073 (0.096)	Data 1.12e-04 (4.39e-03)	Tok/s 106054 (110027)	Loss/tok 2.9853 (3.1671)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.073 (0.098)	Data 1.12e-04 (3.86e-03)	Tok/s 102991 (110686)	Loss/tok 2.9591 (3.1752)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.073 (0.098)	Data 1.06e-04 (3.45e-03)	Tok/s 104150 (110841)	Loss/tok 3.0319 (3.1830)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.074 (0.098)	Data 1.05e-04 (3.12e-03)	Tok/s 106991 (111254)	Loss/tok 2.9515 (3.1895)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.105 (0.098)	Data 1.13e-04 (2.85e-03)	Tok/s 118897 (111472)	Loss/tok 3.1433 (3.1962)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.105 (0.098)	Data 1.08e-04 (2.62e-03)	Tok/s 121611 (111631)	Loss/tok 3.1982 (3.1960)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.073 (0.098)	Data 1.10e-04 (2.43e-03)	Tok/s 105622 (111972)	Loss/tok 2.9577 (3.1921)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.105 (0.098)	Data 1.10e-04 (2.27e-03)	Tok/s 119533 (112138)	Loss/tok 3.2518 (3.1904)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][150/1291]	Time 0.177 (0.099)	Data 1.29e-04 (2.12e-03)	Tok/s 124309 (112211)	Loss/tok 3.6615 (3.2038)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.105 (0.099)	Data 1.11e-04 (2.00e-03)	Tok/s 119323 (112600)	Loss/tok 3.1666 (3.2043)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.073 (0.099)	Data 1.09e-04 (1.89e-03)	Tok/s 107673 (112537)	Loss/tok 3.0951 (3.2015)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.073 (0.098)	Data 1.13e-04 (1.79e-03)	Tok/s 103759 (112418)	Loss/tok 2.9507 (3.1966)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.178 (0.098)	Data 1.09e-04 (1.70e-03)	Tok/s 124721 (112593)	Loss/tok 3.5967 (3.2015)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.073 (0.098)	Data 1.11e-04 (1.62e-03)	Tok/s 106977 (112557)	Loss/tok 2.9790 (3.2023)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.073 (0.098)	Data 1.03e-04 (1.55e-03)	Tok/s 106305 (112639)	Loss/tok 2.9844 (3.2067)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.073 (0.098)	Data 1.23e-04 (1.49e-03)	Tok/s 102744 (112638)	Loss/tok 3.0460 (3.2085)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.139 (0.098)	Data 1.13e-04 (1.43e-03)	Tok/s 125975 (112661)	Loss/tok 3.3618 (3.2084)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.105 (0.098)	Data 1.09e-04 (1.37e-03)	Tok/s 119687 (112612)	Loss/tok 3.2495 (3.2044)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.073 (0.098)	Data 1.09e-04 (1.32e-03)	Tok/s 105452 (112608)	Loss/tok 2.9668 (3.2044)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.073 (0.097)	Data 1.08e-04 (1.28e-03)	Tok/s 105260 (112409)	Loss/tok 3.0098 (3.1999)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.139 (0.097)	Data 1.53e-04 (1.23e-03)	Tok/s 125422 (112451)	Loss/tok 3.4544 (3.1996)	LR 1.437e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][280/1291]	Time 0.105 (0.097)	Data 1.08e-04 (1.19e-03)	Tok/s 120677 (112420)	Loss/tok 3.1922 (3.1985)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.042 (0.096)	Data 1.08e-04 (1.16e-03)	Tok/s 91773 (112318)	Loss/tok 2.4616 (3.1934)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.105 (0.097)	Data 1.12e-04 (1.12e-03)	Tok/s 120589 (112424)	Loss/tok 3.0395 (3.1970)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.073 (0.097)	Data 1.18e-04 (1.09e-03)	Tok/s 104508 (112477)	Loss/tok 2.8026 (3.1956)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.105 (0.097)	Data 1.12e-04 (1.06e-03)	Tok/s 119612 (112622)	Loss/tok 3.1818 (3.1976)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.105 (0.097)	Data 1.10e-04 (1.03e-03)	Tok/s 119945 (112707)	Loss/tok 3.1212 (3.1982)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.074 (0.097)	Data 1.09e-04 (1.00e-03)	Tok/s 106610 (112633)	Loss/tok 3.0725 (3.1982)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.073 (0.097)	Data 1.05e-04 (9.76e-04)	Tok/s 104470 (112523)	Loss/tok 2.9396 (3.1953)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.139 (0.097)	Data 1.08e-04 (9.52e-04)	Tok/s 127145 (112542)	Loss/tok 3.4227 (3.1948)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.105 (0.096)	Data 1.09e-04 (9.30e-04)	Tok/s 118730 (112465)	Loss/tok 3.2823 (3.1917)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.178 (0.096)	Data 1.08e-04 (9.08e-04)	Tok/s 125869 (112491)	Loss/tok 3.5409 (3.1915)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.075 (0.097)	Data 1.11e-04 (8.88e-04)	Tok/s 104034 (112630)	Loss/tok 2.9602 (3.1923)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.073 (0.097)	Data 1.13e-04 (8.69e-04)	Tok/s 104132 (112656)	Loss/tok 2.8663 (3.1897)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][410/1291]	Time 0.073 (0.096)	Data 1.24e-04 (8.50e-04)	Tok/s 107370 (112563)	Loss/tok 2.9174 (3.1878)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.105 (0.097)	Data 1.18e-04 (8.33e-04)	Tok/s 118933 (112744)	Loss/tok 3.0981 (3.1878)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.105 (0.097)	Data 1.15e-04 (8.16e-04)	Tok/s 120910 (112853)	Loss/tok 3.1957 (3.1864)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.073 (0.097)	Data 1.16e-04 (8.01e-04)	Tok/s 106170 (112836)	Loss/tok 2.9813 (3.1861)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.073 (0.097)	Data 1.08e-04 (7.85e-04)	Tok/s 107017 (112790)	Loss/tok 2.9898 (3.1843)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.105 (0.097)	Data 1.15e-04 (7.71e-04)	Tok/s 120519 (112892)	Loss/tok 3.1915 (3.1864)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.073 (0.097)	Data 1.21e-04 (7.57e-04)	Tok/s 104988 (112802)	Loss/tok 2.8589 (3.1847)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.139 (0.097)	Data 1.17e-04 (7.43e-04)	Tok/s 124811 (112918)	Loss/tok 3.4526 (3.1859)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.073 (0.097)	Data 1.14e-04 (7.31e-04)	Tok/s 105202 (112970)	Loss/tok 3.0388 (3.1851)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.178 (0.097)	Data 1.19e-04 (7.18e-04)	Tok/s 123216 (112915)	Loss/tok 3.6837 (3.1852)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.073 (0.097)	Data 1.07e-04 (7.07e-04)	Tok/s 105245 (112863)	Loss/tok 2.8753 (3.1834)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.106 (0.097)	Data 1.05e-04 (6.95e-04)	Tok/s 121339 (112919)	Loss/tok 3.1516 (3.1828)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.073 (0.096)	Data 1.05e-04 (6.84e-04)	Tok/s 106021 (112900)	Loss/tok 2.9805 (3.1822)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][540/1291]	Time 0.139 (0.097)	Data 1.08e-04 (6.73e-04)	Tok/s 128405 (112961)	Loss/tok 3.2738 (3.1853)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.073 (0.097)	Data 1.03e-04 (6.63e-04)	Tok/s 106992 (112989)	Loss/tok 2.8662 (3.1850)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.042 (0.096)	Data 1.04e-04 (6.53e-04)	Tok/s 92412 (112773)	Loss/tok 2.4272 (3.1818)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.042 (0.096)	Data 1.03e-04 (6.44e-04)	Tok/s 93658 (112772)	Loss/tok 2.5352 (3.1811)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.104 (0.096)	Data 1.03e-04 (6.34e-04)	Tok/s 121537 (112791)	Loss/tok 3.2162 (3.1807)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.073 (0.096)	Data 1.05e-04 (6.25e-04)	Tok/s 104051 (112774)	Loss/tok 2.9868 (3.1802)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.073 (0.096)	Data 1.04e-04 (6.17e-04)	Tok/s 105140 (112740)	Loss/tok 2.9606 (3.1803)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.105 (0.096)	Data 1.09e-04 (6.08e-04)	Tok/s 119016 (112737)	Loss/tok 3.1919 (3.1797)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.073 (0.096)	Data 1.06e-04 (6.00e-04)	Tok/s 107816 (112707)	Loss/tok 3.0292 (3.1794)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.105 (0.096)	Data 1.04e-04 (5.93e-04)	Tok/s 118932 (112710)	Loss/tok 3.0242 (3.1785)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.105 (0.096)	Data 1.09e-04 (5.85e-04)	Tok/s 120826 (112712)	Loss/tok 3.1926 (3.1789)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.073 (0.096)	Data 1.07e-04 (5.78e-04)	Tok/s 104674 (112633)	Loss/tok 3.0879 (3.1775)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.105 (0.095)	Data 1.06e-04 (5.70e-04)	Tok/s 121371 (112584)	Loss/tok 3.1424 (3.1759)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][670/1291]	Time 0.105 (0.096)	Data 1.11e-04 (5.64e-04)	Tok/s 119321 (112630)	Loss/tok 3.1172 (3.1767)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.042 (0.095)	Data 1.30e-04 (5.57e-04)	Tok/s 91718 (112580)	Loss/tok 2.5455 (3.1755)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.042 (0.095)	Data 1.11e-04 (5.50e-04)	Tok/s 92738 (112527)	Loss/tok 2.4746 (3.1750)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.073 (0.095)	Data 1.08e-04 (5.44e-04)	Tok/s 105268 (112528)	Loss/tok 2.8888 (3.1735)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.105 (0.095)	Data 1.12e-04 (5.38e-04)	Tok/s 118942 (112533)	Loss/tok 3.1282 (3.1725)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.073 (0.095)	Data 1.07e-04 (5.32e-04)	Tok/s 104340 (112530)	Loss/tok 2.9750 (3.1713)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.140 (0.095)	Data 1.16e-04 (5.26e-04)	Tok/s 125477 (112631)	Loss/tok 3.1603 (3.1725)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.073 (0.095)	Data 1.05e-04 (5.20e-04)	Tok/s 104848 (112635)	Loss/tok 2.9961 (3.1731)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.043 (0.095)	Data 1.08e-04 (5.15e-04)	Tok/s 94687 (112615)	Loss/tok 2.5650 (3.1727)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.073 (0.095)	Data 1.08e-04 (5.10e-04)	Tok/s 108931 (112621)	Loss/tok 2.8914 (3.1716)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.105 (0.095)	Data 1.06e-04 (5.04e-04)	Tok/s 119925 (112589)	Loss/tok 2.9521 (3.1696)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.073 (0.095)	Data 1.09e-04 (4.99e-04)	Tok/s 106730 (112568)	Loss/tok 3.0388 (3.1690)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.073 (0.095)	Data 1.07e-04 (4.94e-04)	Tok/s 105600 (112479)	Loss/tok 2.8623 (3.1670)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][800/1291]	Time 0.073 (0.095)	Data 1.09e-04 (4.89e-04)	Tok/s 102454 (112479)	Loss/tok 3.0727 (3.1674)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.138 (0.095)	Data 1.06e-04 (4.85e-04)	Tok/s 126176 (112454)	Loss/tok 3.3428 (3.1661)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.073 (0.095)	Data 1.10e-04 (4.80e-04)	Tok/s 106439 (112528)	Loss/tok 2.9959 (3.1668)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.073 (0.095)	Data 1.04e-04 (4.76e-04)	Tok/s 105670 (112532)	Loss/tok 2.9867 (3.1659)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.105 (0.095)	Data 1.10e-04 (4.71e-04)	Tok/s 122936 (112482)	Loss/tok 3.0652 (3.1652)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.073 (0.095)	Data 1.07e-04 (4.67e-04)	Tok/s 105181 (112450)	Loss/tok 2.9661 (3.1645)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.073 (0.094)	Data 1.07e-04 (4.63e-04)	Tok/s 104988 (112405)	Loss/tok 2.8938 (3.1630)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.073 (0.094)	Data 1.05e-04 (4.59e-04)	Tok/s 107021 (112354)	Loss/tok 2.7613 (3.1614)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.105 (0.094)	Data 1.14e-04 (4.55e-04)	Tok/s 119215 (112340)	Loss/tok 3.1683 (3.1611)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][890/1291]	Time 0.073 (0.094)	Data 1.07e-04 (4.51e-04)	Tok/s 106620 (112368)	Loss/tok 3.0220 (3.1629)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.140 (0.095)	Data 1.09e-04 (4.47e-04)	Tok/s 125566 (112458)	Loss/tok 3.3065 (3.1632)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.073 (0.095)	Data 1.07e-04 (4.43e-04)	Tok/s 104014 (112429)	Loss/tok 2.8522 (3.1621)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.073 (0.095)	Data 1.04e-04 (4.39e-04)	Tok/s 106878 (112414)	Loss/tok 2.8673 (3.1619)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.105 (0.095)	Data 1.06e-04 (4.36e-04)	Tok/s 120892 (112451)	Loss/tok 3.0994 (3.1615)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.073 (0.095)	Data 1.04e-04 (4.32e-04)	Tok/s 104068 (112490)	Loss/tok 3.0132 (3.1622)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.073 (0.095)	Data 1.03e-04 (4.29e-04)	Tok/s 107530 (112459)	Loss/tok 2.8838 (3.1606)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.042 (0.095)	Data 1.12e-04 (4.26e-04)	Tok/s 93589 (112463)	Loss/tok 2.6343 (3.1609)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.140 (0.095)	Data 1.02e-04 (4.22e-04)	Tok/s 124114 (112444)	Loss/tok 3.3313 (3.1602)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.073 (0.095)	Data 1.05e-04 (4.19e-04)	Tok/s 106742 (112458)	Loss/tok 2.9535 (3.1599)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.139 (0.095)	Data 1.11e-04 (4.16e-04)	Tok/s 128413 (112485)	Loss/tok 3.1417 (3.1595)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.042 (0.095)	Data 1.06e-04 (4.13e-04)	Tok/s 92624 (112497)	Loss/tok 2.5448 (3.1601)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.042 (0.095)	Data 1.13e-04 (4.10e-04)	Tok/s 92352 (112486)	Loss/tok 2.4237 (3.1596)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1020/1291]	Time 0.140 (0.095)	Data 1.06e-04 (4.07e-04)	Tok/s 124532 (112487)	Loss/tok 3.2887 (3.1592)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.178 (0.095)	Data 1.07e-04 (4.04e-04)	Tok/s 126633 (112505)	Loss/tok 3.5100 (3.1595)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.074 (0.095)	Data 1.08e-04 (4.01e-04)	Tok/s 104756 (112506)	Loss/tok 2.9741 (3.1593)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.105 (0.095)	Data 1.08e-04 (3.98e-04)	Tok/s 120641 (112512)	Loss/tok 3.1719 (3.1592)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.105 (0.095)	Data 1.06e-04 (3.95e-04)	Tok/s 119304 (112474)	Loss/tok 3.1591 (3.1580)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.073 (0.095)	Data 1.06e-04 (3.93e-04)	Tok/s 103060 (112475)	Loss/tok 2.9027 (3.1575)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.140 (0.095)	Data 1.15e-04 (3.90e-04)	Tok/s 125557 (112483)	Loss/tok 3.3677 (3.1574)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.074 (0.095)	Data 1.06e-04 (3.87e-04)	Tok/s 107408 (112475)	Loss/tok 2.8600 (3.1565)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.073 (0.095)	Data 1.08e-04 (3.85e-04)	Tok/s 105408 (112523)	Loss/tok 3.1158 (3.1575)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.105 (0.095)	Data 1.08e-04 (3.82e-04)	Tok/s 120502 (112531)	Loss/tok 3.0260 (3.1566)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.140 (0.095)	Data 1.12e-04 (3.80e-04)	Tok/s 125290 (112581)	Loss/tok 3.2850 (3.1562)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.073 (0.095)	Data 1.10e-04 (3.78e-04)	Tok/s 106432 (112521)	Loss/tok 2.9098 (3.1547)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.073 (0.095)	Data 1.09e-04 (3.75e-04)	Tok/s 105097 (112543)	Loss/tok 2.9165 (3.1541)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1150/1291]	Time 0.105 (0.095)	Data 1.02e-04 (3.73e-04)	Tok/s 122265 (112533)	Loss/tok 3.1134 (3.1534)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.105 (0.095)	Data 1.17e-04 (3.71e-04)	Tok/s 118547 (112534)	Loss/tok 3.0300 (3.1537)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.042 (0.095)	Data 1.03e-04 (3.69e-04)	Tok/s 93618 (112458)	Loss/tok 2.5249 (3.1523)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.042 (0.095)	Data 1.16e-04 (3.66e-04)	Tok/s 92542 (112478)	Loss/tok 2.5329 (3.1521)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.140 (0.095)	Data 1.10e-04 (3.64e-04)	Tok/s 125679 (112504)	Loss/tok 3.3608 (3.1525)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.074 (0.095)	Data 1.17e-04 (3.62e-04)	Tok/s 105643 (112523)	Loss/tok 2.9439 (3.1532)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.073 (0.095)	Data 1.06e-04 (3.60e-04)	Tok/s 104134 (112489)	Loss/tok 3.1167 (3.1527)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.105 (0.095)	Data 1.04e-04 (3.58e-04)	Tok/s 117948 (112512)	Loss/tok 3.2137 (3.1533)	LR 7.187e-04
0: TRAIN [3][1230/1291]	Time 0.140 (0.095)	Data 1.07e-04 (3.56e-04)	Tok/s 123952 (112510)	Loss/tok 3.4009 (3.1532)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.073 (0.095)	Data 1.08e-04 (3.54e-04)	Tok/s 102893 (112518)	Loss/tok 2.8398 (3.1535)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.042 (0.095)	Data 1.05e-04 (3.52e-04)	Tok/s 92453 (112547)	Loss/tok 2.4561 (3.1528)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.073 (0.095)	Data 1.05e-04 (3.50e-04)	Tok/s 102882 (112534)	Loss/tok 2.8201 (3.1521)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.105 (0.095)	Data 1.04e-04 (3.48e-04)	Tok/s 118674 (112525)	Loss/tok 3.0651 (3.1518)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1280/1291]	Time 0.073 (0.095)	Data 1.07e-04 (3.46e-04)	Tok/s 106373 (112546)	Loss/tok 2.9383 (3.1513)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.073 (0.095)	Data 4.67e-05 (3.46e-04)	Tok/s 104659 (112540)	Loss/tok 2.9252 (3.1511)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592591030847, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591030848, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.398 (0.398)	Decoder iters 115.0 (115.0)	Tok/s 22593 (22593)
0: Running moses detokenizer
0: BLEU(score=24.165267027765847, counts=[37248, 18689, 10663, 6303], totals=[65456, 62453, 59450, 56454], precisions=[56.90540210217551, 29.924903527452646, 17.936080740117745, 11.16484217238814], bp=1.0, sys_len=65456, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592591031945, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24170000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591031946, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1476	Test BLEU: 24.17
0: Performance: Epoch: 3	Training: 1800480 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592591031946, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592591031946, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-19 11:24:00 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:00 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
slurmstepd: error: _is_a_lwp: open() /proc/89607/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
ENDING TIMING RUN AT 2020-06-19 11:24:02 AM
RESULT,RNN_TRANSLATOR,,552,nvidia,2020-06-19 11:14:50 AM
