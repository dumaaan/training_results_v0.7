+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590502200, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592590502230, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592590502230, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592590502230, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592590502230, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n008
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592590509419, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/13929701/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ '[' -n 6 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ '[' 16 -gt 1 ']'
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=192
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=64
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=8
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=506
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 5 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-19 11:15:12 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 16 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592590514438, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514460, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514476, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514975, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514982, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514989, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514989, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514993, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590514999, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592590515001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 802965906
:::MLLOG {"namespace": "", "time_ms": 1592590532975, "event_type": "POINT_IN_TIME", "key": "seed", "value": 802965906, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 451547720
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2795008]), new_param_packed_fragment.size()=torch.Size([2795008]), master_param_fragment.size()=torch.Size([2795008])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
0: Initializing dwu fp16 optimizer
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([2052608]), new_param_packed_fragment.size()=torch.Size([2052608]), master_param_fragment.size()=torch.Size([2052608])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([534976]), new_param_packed_fragment.size()=torch.Size([534976]), master_param_fragment.size()=torch.Size([534976])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3083712]), new_param_packed_fragment.size()=torch.Size([3083712]), master_param_fragment.size()=torch.Size([3083712])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=8192, self._block_size=160677888, self._chunk_size=80338944, self._shard_size=5021184
[0]
model_param_fragment.size()=torch.Size([3948032]), new_param_packed_fragment.size()=torch.Size([3948032]), master_param_fragment.size()=torch.Size([3948032])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4982336]), new_param_packed_fragment.size()=torch.Size([4982336]), master_param_fragment.size()=torch.Size([4982336])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3698176]), new_param_packed_fragment.size()=torch.Size([3698176]), master_param_fragment.size()=torch.Size([3698176])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2188736]), new_param_packed_fragment.size()=torch.Size([2188736]), master_param_fragment.size()=torch.Size([2188736])
model_param_fragment.size()=torch.Size([1399296]), new_param_packed_fragment.size()=torch.Size([1399296]), master_param_fragment.size()=torch.Size([1399296])
model_param_fragment.size()=torch.Size([3621888]), new_param_packed_fragment.size()=torch.Size([3621888]), master_param_fragment.size()=torch.Size([3621888])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([496128]), new_param_packed_fragment.size()=torch.Size([496128]), master_param_fragment.size()=torch.Size([496128])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([2005568]), new_param_packed_fragment.size()=torch.Size([2005568]), master_param_fragment.size()=torch.Size([2005568])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([246272]), new_param_packed_fragment.size()=torch.Size([246272]), master_param_fragment.size()=torch.Size([246272])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3044864]), new_param_packed_fragment.size()=torch.Size([3044864]), master_param_fragment.size()=torch.Size([3044864])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2141696]), new_param_packed_fragment.size()=torch.Size([2141696]), master_param_fragment.size()=torch.Size([2141696])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1891840]), new_param_packed_fragment.size()=torch.Size([1891840]), master_param_fragment.size()=torch.Size([1891840])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2756160]), new_param_packed_fragment.size()=torch.Size([2756160]), master_param_fragment.size()=torch.Size([2756160])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([5021184]), new_param_packed_fragment.size()=torch.Size([5021184]), master_param_fragment.size()=torch.Size([5021184])
model_param_fragment.size()=torch.Size([2302464]), new_param_packed_fragment.size()=torch.Size([2302464]), master_param_fragment.size()=torch.Size([2302464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592590554614, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592590554615, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592590554615, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592590554615, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592590554615, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592590559107, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592590559107, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592590559107, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592590559359, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592590559360, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592590559360, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592590559361, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592590559361, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592590559361, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592590559361, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592590559361, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592590559362, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592590559362, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592590559362, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590559362, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1076143322
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.437 (0.437)	Data 3.28e-01 (3.28e-01)	Tok/s 17757 (17757)	Loss/tok 10.6787 (10.6787)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.102 (0.137)	Data 1.15e-04 (2.99e-02)	Tok/s 125304 (111546)	Loss/tok 9.5515 (10.0450)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.103 (0.126)	Data 1.16e-04 (1.57e-02)	Tok/s 122838 (115897)	Loss/tok 9.1349 (9.6979)	LR 4.663e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][30/1291]	Time 0.136 (0.114)	Data 1.17e-04 (1.07e-02)	Tok/s 127560 (114905)	Loss/tok 8.9975 (9.5104)	LR 5.736e-05
0: TRAIN [0][40/1291]	Time 0.137 (0.111)	Data 1.14e-04 (8.11e-03)	Tok/s 127625 (115398)	Loss/tok 8.6945 (9.3261)	LR 7.222e-05
0: TRAIN [0][50/1291]	Time 0.103 (0.108)	Data 1.17e-04 (6.55e-03)	Tok/s 122839 (115424)	Loss/tok 8.3062 (9.1642)	LR 9.092e-05
0: TRAIN [0][60/1291]	Time 0.072 (0.104)	Data 1.16e-04 (5.49e-03)	Tok/s 105302 (114451)	Loss/tok 7.9237 (9.0411)	LR 1.145e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][70/1291]	Time 0.103 (0.100)	Data 1.12e-04 (4.73e-03)	Tok/s 120833 (114137)	Loss/tok 8.1408 (8.9265)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.072 (0.100)	Data 1.13e-04 (4.16e-03)	Tok/s 106512 (114404)	Loss/tok 7.7552 (8.8136)	LR 1.773e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][90/1291]	Time 0.103 (0.098)	Data 1.17e-04 (3.72e-03)	Tok/s 121040 (114240)	Loss/tok 7.9602 (8.7268)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.072 (0.097)	Data 1.22e-04 (3.36e-03)	Tok/s 106845 (113942)	Loss/tok 7.7671 (8.6525)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.072 (0.096)	Data 1.15e-04 (3.07e-03)	Tok/s 105624 (113850)	Loss/tok 7.7210 (8.5888)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.103 (0.096)	Data 1.10e-04 (2.83e-03)	Tok/s 122556 (113670)	Loss/tok 7.8695 (8.5286)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.173 (0.097)	Data 1.15e-04 (2.62e-03)	Tok/s 129539 (114099)	Loss/tok 7.9777 (8.4646)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.072 (0.096)	Data 1.14e-04 (2.44e-03)	Tok/s 107061 (114089)	Loss/tok 7.4880 (8.4133)	LR 6.897e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][150/1291]	Time 0.072 (0.097)	Data 1.14e-04 (2.29e-03)	Tok/s 105763 (114417)	Loss/tok 7.4350 (8.3624)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.072 (0.097)	Data 1.14e-04 (2.15e-03)	Tok/s 105886 (114742)	Loss/tok 7.2456 (8.3038)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.042 (0.097)	Data 1.12e-04 (2.03e-03)	Tok/s 91912 (114448)	Loss/tok 6.4564 (8.2555)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.136 (0.097)	Data 1.20e-04 (1.93e-03)	Tok/s 129974 (114630)	Loss/tok 7.5096 (8.2065)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.102 (0.098)	Data 1.09e-04 (1.83e-03)	Tok/s 122095 (115095)	Loss/tok 7.0163 (8.1436)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.136 (0.097)	Data 1.10e-04 (1.75e-03)	Tok/s 127115 (114813)	Loss/tok 7.1740 (8.0990)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.136 (0.097)	Data 1.15e-04 (1.67e-03)	Tok/s 129586 (114793)	Loss/tok 6.9819 (8.0388)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.136 (0.096)	Data 1.15e-04 (1.60e-03)	Tok/s 127513 (114679)	Loss/tok 7.1037 (7.9854)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.103 (0.096)	Data 1.13e-04 (1.53e-03)	Tok/s 121077 (114816)	Loss/tok 6.4204 (7.9209)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.042 (0.096)	Data 1.51e-04 (1.48e-03)	Tok/s 96192 (114674)	Loss/tok 5.4137 (7.8632)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.042 (0.095)	Data 1.15e-04 (1.42e-03)	Tok/s 92419 (114413)	Loss/tok 4.8512 (7.8111)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.042 (0.095)	Data 1.11e-04 (1.37e-03)	Tok/s 94113 (114472)	Loss/tok 5.0583 (7.7444)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][270/1291]	Time 0.103 (0.095)	Data 1.25e-04 (1.32e-03)	Tok/s 123850 (114427)	Loss/tok 6.1450 (7.6836)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.072 (0.095)	Data 1.17e-04 (1.28e-03)	Tok/s 105357 (114541)	Loss/tok 5.6339 (7.6176)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.072 (0.095)	Data 1.20e-04 (1.24e-03)	Tok/s 104907 (114522)	Loss/tok 5.3818 (7.5542)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.103 (0.095)	Data 1.15e-04 (1.20e-03)	Tok/s 124694 (114595)	Loss/tok 5.5667 (7.4896)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.042 (0.094)	Data 1.14e-04 (1.17e-03)	Tok/s 97157 (114406)	Loss/tok 4.5485 (7.4384)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.072 (0.094)	Data 1.17e-04 (1.14e-03)	Tok/s 108648 (114264)	Loss/tok 5.1558 (7.3841)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.072 (0.094)	Data 1.16e-04 (1.11e-03)	Tok/s 107334 (114351)	Loss/tok 4.9651 (7.3179)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.072 (0.094)	Data 1.17e-04 (1.08e-03)	Tok/s 106231 (114277)	Loss/tok 4.9077 (7.2630)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.072 (0.094)	Data 1.21e-04 (1.05e-03)	Tok/s 104785 (114326)	Loss/tok 4.6545 (7.1985)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.072 (0.094)	Data 1.61e-04 (1.02e-03)	Tok/s 106838 (114359)	Loss/tok 4.6707 (7.1338)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.136 (0.094)	Data 1.28e-04 (9.99e-04)	Tok/s 128872 (114443)	Loss/tok 5.1032 (7.0694)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.076 (0.094)	Data 1.18e-04 (9.76e-04)	Tok/s 102218 (114312)	Loss/tok 4.6609 (7.0169)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.136 (0.094)	Data 1.15e-04 (9.54e-04)	Tok/s 129091 (114333)	Loss/tok 4.8828 (6.9550)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][400/1291]	Time 0.176 (0.094)	Data 1.20e-04 (9.33e-04)	Tok/s 124650 (114368)	Loss/tok 5.1587 (6.8941)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.043 (0.094)	Data 1.12e-04 (9.13e-04)	Tok/s 92805 (114408)	Loss/tok 3.6150 (6.8340)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.074 (0.094)	Data 1.75e-04 (8.94e-04)	Tok/s 105529 (114353)	Loss/tok 4.1664 (6.7817)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.138 (0.095)	Data 1.10e-04 (8.76e-04)	Tok/s 126659 (114465)	Loss/tok 4.6445 (6.7167)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.105 (0.095)	Data 1.15e-04 (8.59e-04)	Tok/s 122665 (114517)	Loss/tok 4.3619 (6.6580)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.138 (0.095)	Data 1.14e-04 (8.42e-04)	Tok/s 125970 (114445)	Loss/tok 4.6384 (6.6099)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.104 (0.095)	Data 1.18e-04 (8.27e-04)	Tok/s 119610 (114471)	Loss/tok 4.2868 (6.5597)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.074 (0.095)	Data 1.18e-04 (8.12e-04)	Tok/s 106503 (114432)	Loss/tok 3.9498 (6.5136)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.104 (0.095)	Data 1.15e-04 (7.97e-04)	Tok/s 121623 (114420)	Loss/tok 4.1723 (6.4652)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.138 (0.095)	Data 1.18e-04 (7.83e-04)	Tok/s 126983 (114455)	Loss/tok 4.3878 (6.4166)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.105 (0.095)	Data 1.13e-04 (7.70e-04)	Tok/s 121048 (114538)	Loss/tok 4.2189 (6.3638)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.105 (0.095)	Data 1.24e-04 (7.57e-04)	Tok/s 118982 (114470)	Loss/tok 4.2209 (6.3230)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.074 (0.095)	Data 1.16e-04 (7.45e-04)	Tok/s 108977 (114430)	Loss/tok 3.8262 (6.2833)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][530/1291]	Time 0.105 (0.095)	Data 1.69e-04 (7.33e-04)	Tok/s 120777 (114326)	Loss/tok 4.2386 (6.2481)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.074 (0.095)	Data 1.12e-04 (7.22e-04)	Tok/s 105081 (114273)	Loss/tok 3.8038 (6.2100)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.074 (0.095)	Data 1.11e-04 (7.11e-04)	Tok/s 106585 (114070)	Loss/tok 3.7648 (6.1819)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.074 (0.094)	Data 1.77e-04 (7.01e-04)	Tok/s 108134 (114056)	Loss/tok 3.8384 (6.1442)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.074 (0.094)	Data 1.22e-04 (6.91e-04)	Tok/s 106612 (114004)	Loss/tok 3.7388 (6.1098)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.104 (0.094)	Data 1.29e-04 (6.82e-04)	Tok/s 118860 (113976)	Loss/tok 4.1071 (6.0764)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.074 (0.094)	Data 1.13e-04 (6.72e-04)	Tok/s 102990 (113902)	Loss/tok 3.7765 (6.0448)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.073 (0.094)	Data 1.18e-04 (6.63e-04)	Tok/s 106776 (113832)	Loss/tok 3.7258 (6.0130)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.137 (0.094)	Data 1.15e-04 (6.55e-04)	Tok/s 127721 (113822)	Loss/tok 4.2691 (5.9814)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.104 (0.094)	Data 1.17e-04 (6.46e-04)	Tok/s 118765 (113854)	Loss/tok 3.9655 (5.9479)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.103 (0.094)	Data 1.20e-04 (6.38e-04)	Tok/s 122095 (113823)	Loss/tok 3.9180 (5.9187)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.042 (0.094)	Data 1.29e-04 (6.30e-04)	Tok/s 96934 (113857)	Loss/tok 2.9588 (5.8873)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.072 (0.094)	Data 1.44e-04 (6.23e-04)	Tok/s 106097 (113925)	Loss/tok 3.7418 (5.8539)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][660/1291]	Time 0.073 (0.094)	Data 1.12e-04 (6.15e-04)	Tok/s 107514 (113960)	Loss/tok 3.5947 (5.8239)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][670/1291]	Time 0.103 (0.094)	Data 1.71e-04 (6.08e-04)	Tok/s 120652 (113988)	Loss/tok 4.0022 (5.7935)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.103 (0.094)	Data 1.21e-04 (6.01e-04)	Tok/s 121518 (113987)	Loss/tok 3.8507 (5.7660)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.072 (0.094)	Data 1.54e-04 (5.94e-04)	Tok/s 107034 (113966)	Loss/tok 3.6894 (5.7413)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.104 (0.094)	Data 2.03e-04 (5.88e-04)	Tok/s 121867 (113975)	Loss/tok 3.9564 (5.7137)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.137 (0.094)	Data 1.16e-04 (5.81e-04)	Tok/s 128674 (113994)	Loss/tok 4.0726 (5.6874)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.073 (0.094)	Data 1.13e-04 (5.75e-04)	Tok/s 105936 (113981)	Loss/tok 3.6018 (5.6629)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.042 (0.094)	Data 1.13e-04 (5.69e-04)	Tok/s 94949 (113973)	Loss/tok 3.0704 (5.6366)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.073 (0.094)	Data 2.08e-04 (5.63e-04)	Tok/s 105821 (113947)	Loss/tok 3.6279 (5.6146)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.073 (0.094)	Data 1.16e-04 (5.57e-04)	Tok/s 106856 (113933)	Loss/tok 3.4774 (5.5921)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.042 (0.094)	Data 1.82e-04 (5.52e-04)	Tok/s 93961 (113864)	Loss/tok 2.9976 (5.5724)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.104 (0.094)	Data 1.14e-04 (5.46e-04)	Tok/s 122058 (113906)	Loss/tok 3.7664 (5.5480)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.103 (0.094)	Data 1.67e-04 (5.41e-04)	Tok/s 123519 (113934)	Loss/tok 3.8358 (5.5246)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.073 (0.094)	Data 1.19e-04 (5.36e-04)	Tok/s 105623 (113958)	Loss/tok 3.5094 (5.5026)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][800/1291]	Time 0.042 (0.094)	Data 1.17e-04 (5.31e-04)	Tok/s 94644 (113874)	Loss/tok 3.0761 (5.4852)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.072 (0.094)	Data 1.18e-04 (5.26e-04)	Tok/s 107317 (113871)	Loss/tok 3.5525 (5.4644)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.042 (0.094)	Data 1.19e-04 (5.21e-04)	Tok/s 94585 (113795)	Loss/tok 2.9974 (5.4476)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.104 (0.094)	Data 1.14e-04 (5.16e-04)	Tok/s 119733 (113794)	Loss/tok 3.8012 (5.4278)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.103 (0.094)	Data 1.16e-04 (5.12e-04)	Tok/s 121640 (113875)	Loss/tok 3.8044 (5.4049)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.073 (0.094)	Data 1.13e-04 (5.07e-04)	Tok/s 106339 (113882)	Loss/tok 3.5895 (5.3855)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.103 (0.094)	Data 1.79e-04 (5.03e-04)	Tok/s 122419 (113887)	Loss/tok 3.7236 (5.3667)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.042 (0.094)	Data 1.11e-04 (4.99e-04)	Tok/s 92711 (113810)	Loss/tok 2.8164 (5.3514)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.104 (0.093)	Data 1.10e-04 (4.94e-04)	Tok/s 123169 (113731)	Loss/tok 3.8449 (5.3363)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.073 (0.093)	Data 1.25e-04 (4.90e-04)	Tok/s 106014 (113682)	Loss/tok 3.6427 (5.3206)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.042 (0.093)	Data 1.18e-04 (4.86e-04)	Tok/s 93975 (113651)	Loss/tok 2.9822 (5.3046)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.073 (0.093)	Data 1.18e-04 (4.82e-04)	Tok/s 107465 (113612)	Loss/tok 3.6499 (5.2888)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.073 (0.093)	Data 1.16e-04 (4.78e-04)	Tok/s 105835 (113596)	Loss/tok 3.4561 (5.2725)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][930/1291]	Time 0.103 (0.093)	Data 1.17e-04 (4.75e-04)	Tok/s 120965 (113565)	Loss/tok 3.8494 (5.2576)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.137 (0.093)	Data 1.32e-04 (4.71e-04)	Tok/s 126512 (113591)	Loss/tok 4.0117 (5.2396)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.103 (0.093)	Data 1.15e-04 (4.68e-04)	Tok/s 121962 (113663)	Loss/tok 3.7033 (5.2210)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][960/1291]	Time 0.104 (0.093)	Data 1.21e-04 (4.64e-04)	Tok/s 119040 (113640)	Loss/tok 3.7984 (5.2068)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.138 (0.093)	Data 1.16e-04 (4.61e-04)	Tok/s 126653 (113664)	Loss/tok 3.9718 (5.1906)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.074 (0.093)	Data 1.74e-04 (4.57e-04)	Tok/s 103902 (113657)	Loss/tok 3.4024 (5.1747)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.138 (0.093)	Data 1.24e-04 (4.54e-04)	Tok/s 126058 (113697)	Loss/tok 3.9293 (5.1575)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1000/1291]	Time 0.074 (0.094)	Data 1.10e-04 (4.51e-04)	Tok/s 103547 (113714)	Loss/tok 3.3977 (5.1419)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.105 (0.094)	Data 1.28e-04 (4.48e-04)	Tok/s 119693 (113696)	Loss/tok 3.5551 (5.1279)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.074 (0.094)	Data 1.15e-04 (4.44e-04)	Tok/s 105396 (113677)	Loss/tok 3.2789 (5.1138)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.043 (0.094)	Data 1.18e-04 (4.41e-04)	Tok/s 92492 (113675)	Loss/tok 2.9951 (5.0996)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.105 (0.094)	Data 1.17e-04 (4.38e-04)	Tok/s 118950 (113700)	Loss/tok 3.7555 (5.0848)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.105 (0.094)	Data 1.70e-04 (4.35e-04)	Tok/s 121134 (113705)	Loss/tok 3.6794 (5.0708)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.105 (0.094)	Data 1.15e-04 (4.33e-04)	Tok/s 119801 (113719)	Loss/tok 3.7075 (5.0564)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.105 (0.094)	Data 1.10e-04 (4.30e-04)	Tok/s 119309 (113711)	Loss/tok 3.6687 (5.0431)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.074 (0.094)	Data 1.78e-04 (4.27e-04)	Tok/s 106671 (113692)	Loss/tok 3.3698 (5.0309)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.074 (0.094)	Data 1.11e-04 (4.24e-04)	Tok/s 102418 (113729)	Loss/tok 3.4196 (5.0162)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.074 (0.094)	Data 1.23e-04 (4.22e-04)	Tok/s 106153 (113717)	Loss/tok 3.3637 (5.0037)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.138 (0.094)	Data 1.16e-04 (4.19e-04)	Tok/s 126798 (113679)	Loss/tok 3.9597 (4.9925)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1120/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.17e-04)	Tok/s 118783 (113719)	Loss/tok 3.5383 (4.9791)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.043 (0.094)	Data 1.23e-04 (4.14e-04)	Tok/s 89362 (113684)	Loss/tok 2.8240 (4.9679)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.139 (0.094)	Data 1.14e-04 (4.12e-04)	Tok/s 125140 (113664)	Loss/tok 3.8468 (4.9568)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.138 (0.094)	Data 1.17e-04 (4.09e-04)	Tok/s 127317 (113661)	Loss/tok 3.7305 (4.9452)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.138 (0.094)	Data 1.21e-04 (4.07e-04)	Tok/s 127552 (113674)	Loss/tok 3.8362 (4.9326)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.139 (0.094)	Data 1.19e-04 (4.05e-04)	Tok/s 127037 (113674)	Loss/tok 3.6906 (4.9209)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.074 (0.094)	Data 1.18e-04 (4.02e-04)	Tok/s 101927 (113645)	Loss/tok 3.2938 (4.9101)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.043 (0.094)	Data 1.20e-04 (4.00e-04)	Tok/s 94580 (113616)	Loss/tok 2.8617 (4.8999)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.074 (0.094)	Data 1.13e-04 (3.97e-04)	Tok/s 102982 (113583)	Loss/tok 3.4912 (4.8900)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.105 (0.094)	Data 1.24e-04 (3.95e-04)	Tok/s 118792 (113608)	Loss/tok 3.5719 (4.8782)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.043 (0.094)	Data 1.14e-04 (3.93e-04)	Tok/s 93152 (113575)	Loss/tok 2.8653 (4.8684)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.074 (0.094)	Data 1.15e-04 (3.91e-04)	Tok/s 103603 (113508)	Loss/tok 3.2593 (4.8599)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.104 (0.094)	Data 1.16e-04 (3.89e-04)	Tok/s 119941 (113474)	Loss/tok 3.5756 (4.8504)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1250/1291]	Time 0.074 (0.094)	Data 1.15e-04 (3.87e-04)	Tok/s 101689 (113442)	Loss/tok 3.3025 (4.8408)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.105 (0.094)	Data 1.71e-04 (3.85e-04)	Tok/s 121729 (113440)	Loss/tok 3.5394 (4.8304)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1270/1291]	Time 0.176 (0.094)	Data 1.24e-04 (3.83e-04)	Tok/s 127744 (113452)	Loss/tok 3.9120 (4.8193)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.81e-04)	Tok/s 119559 (113483)	Loss/tok 3.5767 (4.8084)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.104 (0.094)	Data 5.15e-05 (3.81e-04)	Tok/s 122606 (113494)	Loss/tok 3.5350 (4.7987)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590681025, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590681025, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.414 (0.414)	Decoder iters 112.0 (112.0)	Tok/s 21324 (21324)
0: Running moses detokenizer
0: BLEU(score=19.41154535674057, counts=[33943, 15349, 8068, 4399], totals=[64589, 61586, 58583, 55585], precisions=[52.55229218597594, 24.922872081317184, 13.771913353703292, 7.914005577044167], bp=0.9986539281807943, sys_len=64589, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590682309, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1941, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590682309, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7970	Test BLEU: 19.41
0: Performance: Epoch: 0	Training: 1815747 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592590682310, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590682310, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590682310, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1104771557
0: TRAIN [1][0/1291]	Time 0.413 (0.413)	Data 3.41e-01 (3.41e-01)	Tok/s 18831 (18831)	Loss/tok 3.2504 (3.2504)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.104 (0.124)	Data 1.11e-04 (3.11e-02)	Tok/s 122058 (105683)	Loss/tok 3.4889 (3.5280)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.136 (0.110)	Data 1.29e-04 (1.64e-02)	Tok/s 127915 (110539)	Loss/tok 3.7847 (3.5163)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.103 (0.101)	Data 1.88e-04 (1.11e-02)	Tok/s 123701 (110553)	Loss/tok 3.4940 (3.4891)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.175 (0.101)	Data 1.17e-04 (8.43e-03)	Tok/s 127763 (112086)	Loss/tok 3.8060 (3.5055)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.137 (0.103)	Data 1.11e-04 (6.80e-03)	Tok/s 127275 (113612)	Loss/tok 3.6506 (3.5328)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.137 (0.102)	Data 1.12e-04 (5.71e-03)	Tok/s 125734 (113451)	Loss/tok 3.7711 (3.5300)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.072 (0.101)	Data 1.17e-04 (4.92e-03)	Tok/s 105781 (113465)	Loss/tok 3.3110 (3.5391)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.104 (0.101)	Data 1.12e-04 (4.33e-03)	Tok/s 120679 (113932)	Loss/tok 3.5157 (3.5363)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.072 (0.099)	Data 1.10e-04 (3.86e-03)	Tok/s 105500 (113690)	Loss/tok 3.2580 (3.5295)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.072 (0.098)	Data 1.21e-04 (3.49e-03)	Tok/s 104530 (113583)	Loss/tok 3.3271 (3.5166)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][110/1291]	Time 0.072 (0.098)	Data 1.16e-04 (3.19e-03)	Tok/s 106875 (113841)	Loss/tok 3.2116 (3.5171)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.072 (0.098)	Data 1.17e-04 (2.93e-03)	Tok/s 106932 (114004)	Loss/tok 3.3064 (3.5175)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.072 (0.097)	Data 1.16e-04 (2.72e-03)	Tok/s 106151 (113782)	Loss/tok 3.2461 (3.5106)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.072 (0.096)	Data 1.83e-04 (2.53e-03)	Tok/s 106655 (113552)	Loss/tok 3.2822 (3.5060)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.174 (0.096)	Data 1.16e-04 (2.37e-03)	Tok/s 127821 (113681)	Loss/tok 3.8757 (3.5085)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.073 (0.096)	Data 1.35e-04 (2.23e-03)	Tok/s 107523 (113727)	Loss/tok 3.2681 (3.5053)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.104 (0.095)	Data 1.14e-04 (2.11e-03)	Tok/s 117756 (113532)	Loss/tok 3.4364 (3.4992)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.174 (0.095)	Data 1.11e-04 (2.00e-03)	Tok/s 127707 (113518)	Loss/tok 3.8056 (3.4982)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.073 (0.094)	Data 1.16e-04 (1.90e-03)	Tok/s 108004 (113362)	Loss/tok 3.3149 (3.4948)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.073 (0.094)	Data 1.17e-04 (1.81e-03)	Tok/s 104034 (113352)	Loss/tok 3.3179 (3.4924)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.072 (0.094)	Data 1.16e-04 (1.73e-03)	Tok/s 111053 (113368)	Loss/tok 3.3994 (3.4910)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.103 (0.093)	Data 1.20e-04 (1.66e-03)	Tok/s 118868 (113296)	Loss/tok 3.5327 (3.4865)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.072 (0.093)	Data 1.17e-04 (1.59e-03)	Tok/s 105474 (113345)	Loss/tok 3.3168 (3.4827)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][240/1291]	Time 0.072 (0.093)	Data 1.17e-04 (1.53e-03)	Tok/s 108692 (113367)	Loss/tok 3.2009 (3.4828)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][250/1291]	Time 0.073 (0.094)	Data 1.14e-04 (1.47e-03)	Tok/s 108288 (113581)	Loss/tok 3.1924 (3.4837)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.072 (0.094)	Data 1.22e-04 (1.42e-03)	Tok/s 108420 (113720)	Loss/tok 3.3012 (3.4864)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.042 (0.094)	Data 1.23e-04 (1.37e-03)	Tok/s 92019 (113604)	Loss/tok 2.8328 (3.4832)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.137 (0.094)	Data 1.14e-04 (1.33e-03)	Tok/s 127416 (113639)	Loss/tok 3.5549 (3.4816)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.072 (0.094)	Data 1.19e-04 (1.29e-03)	Tok/s 109211 (113677)	Loss/tok 3.2856 (3.4810)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.072 (0.094)	Data 1.17e-04 (1.25e-03)	Tok/s 107883 (113725)	Loss/tok 3.3096 (3.4820)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.137 (0.094)	Data 1.16e-04 (1.21e-03)	Tok/s 127687 (113847)	Loss/tok 3.5633 (3.4813)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.072 (0.094)	Data 1.15e-04 (1.18e-03)	Tok/s 105781 (113856)	Loss/tok 3.4051 (3.4786)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.072 (0.094)	Data 1.36e-04 (1.15e-03)	Tok/s 106726 (113802)	Loss/tok 3.2587 (3.4752)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.072 (0.094)	Data 1.18e-04 (1.12e-03)	Tok/s 106668 (113945)	Loss/tok 3.2238 (3.4798)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][350/1291]	Time 0.073 (0.094)	Data 1.13e-04 (1.09e-03)	Tok/s 108197 (113992)	Loss/tok 3.1694 (3.4808)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.042 (0.094)	Data 1.16e-04 (1.06e-03)	Tok/s 94767 (113975)	Loss/tok 2.8271 (3.4818)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.103 (0.094)	Data 1.16e-04 (1.04e-03)	Tok/s 121347 (113923)	Loss/tok 3.5578 (3.4820)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.073 (0.094)	Data 1.18e-04 (1.01e-03)	Tok/s 107096 (114006)	Loss/tok 3.2869 (3.4823)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.073 (0.094)	Data 1.17e-04 (9.89e-04)	Tok/s 104251 (113946)	Loss/tok 3.2895 (3.4812)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.103 (0.094)	Data 1.20e-04 (9.67e-04)	Tok/s 119979 (113993)	Loss/tok 3.4412 (3.4805)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.042 (0.094)	Data 1.16e-04 (9.46e-04)	Tok/s 94325 (113792)	Loss/tok 2.6354 (3.4761)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.072 (0.094)	Data 1.13e-04 (9.27e-04)	Tok/s 108513 (113773)	Loss/tok 3.2621 (3.4765)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.103 (0.094)	Data 1.19e-04 (9.08e-04)	Tok/s 123661 (113782)	Loss/tok 3.4107 (3.4757)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.103 (0.094)	Data 1.13e-04 (8.90e-04)	Tok/s 122026 (113856)	Loss/tok 3.4558 (3.4785)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.072 (0.093)	Data 1.18e-04 (8.73e-04)	Tok/s 107861 (113795)	Loss/tok 3.1608 (3.4751)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.072 (0.094)	Data 1.15e-04 (8.56e-04)	Tok/s 109149 (113884)	Loss/tok 3.3036 (3.4745)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.104 (0.093)	Data 1.10e-04 (8.41e-04)	Tok/s 121886 (113818)	Loss/tok 3.3757 (3.4711)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][480/1291]	Time 0.072 (0.093)	Data 1.22e-04 (8.25e-04)	Tok/s 105733 (113825)	Loss/tok 3.3266 (3.4721)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.073 (0.093)	Data 1.16e-04 (8.11e-04)	Tok/s 107347 (113843)	Loss/tok 3.1552 (3.4711)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.073 (0.093)	Data 1.17e-04 (7.97e-04)	Tok/s 106893 (113789)	Loss/tok 3.3350 (3.4714)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.073 (0.093)	Data 1.18e-04 (7.84e-04)	Tok/s 103779 (113783)	Loss/tok 3.2259 (3.4706)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.073 (0.093)	Data 1.30e-04 (7.71e-04)	Tok/s 106986 (113706)	Loss/tok 3.2458 (3.4698)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.073 (0.093)	Data 1.17e-04 (7.58e-04)	Tok/s 103394 (113713)	Loss/tok 3.0853 (3.4688)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.174 (0.093)	Data 1.17e-04 (7.47e-04)	Tok/s 129177 (113766)	Loss/tok 3.8022 (3.4684)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.073 (0.093)	Data 1.11e-04 (7.35e-04)	Tok/s 104224 (113781)	Loss/tok 3.3720 (3.4677)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.104 (0.093)	Data 1.12e-04 (7.24e-04)	Tok/s 121564 (113751)	Loss/tok 3.5027 (3.4658)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.074 (0.093)	Data 1.16e-04 (7.14e-04)	Tok/s 102203 (113818)	Loss/tok 3.3254 (3.4653)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.105 (0.093)	Data 1.17e-04 (7.03e-04)	Tok/s 119570 (113905)	Loss/tok 3.4627 (3.4668)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.074 (0.093)	Data 1.19e-04 (6.94e-04)	Tok/s 103891 (113820)	Loss/tok 3.1254 (3.4647)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.139 (0.094)	Data 1.16e-04 (6.84e-04)	Tok/s 127014 (113847)	Loss/tok 3.6641 (3.4661)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][610/1291]	Time 0.105 (0.094)	Data 1.14e-04 (6.75e-04)	Tok/s 119815 (113850)	Loss/tok 3.5263 (3.4650)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.105 (0.094)	Data 1.15e-04 (6.66e-04)	Tok/s 120099 (113872)	Loss/tok 3.4716 (3.4649)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][630/1291]	Time 0.074 (0.094)	Data 1.23e-04 (6.57e-04)	Tok/s 104245 (113825)	Loss/tok 3.1790 (3.4654)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.074 (0.094)	Data 1.16e-04 (6.49e-04)	Tok/s 103972 (113810)	Loss/tok 3.2343 (3.4658)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.105 (0.094)	Data 1.11e-04 (6.41e-04)	Tok/s 119553 (113801)	Loss/tok 3.6213 (3.4675)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.105 (0.094)	Data 1.17e-04 (6.33e-04)	Tok/s 121927 (113771)	Loss/tok 3.5955 (3.4664)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.074 (0.094)	Data 1.19e-04 (6.25e-04)	Tok/s 102952 (113614)	Loss/tok 3.1642 (3.4638)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.138 (0.094)	Data 1.15e-04 (6.18e-04)	Tok/s 127810 (113594)	Loss/tok 3.5862 (3.4628)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.105 (0.094)	Data 1.18e-04 (6.10e-04)	Tok/s 120146 (113629)	Loss/tok 3.5907 (3.4643)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.074 (0.094)	Data 1.14e-04 (6.03e-04)	Tok/s 106732 (113522)	Loss/tok 3.3340 (3.4619)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.105 (0.094)	Data 1.16e-04 (5.96e-04)	Tok/s 118957 (113519)	Loss/tok 3.6721 (3.4612)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.074 (0.094)	Data 1.15e-04 (5.90e-04)	Tok/s 103949 (113469)	Loss/tok 3.2388 (3.4606)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.105 (0.093)	Data 1.17e-04 (5.83e-04)	Tok/s 118935 (113329)	Loss/tok 3.4114 (3.4578)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.138 (0.093)	Data 1.12e-04 (5.77e-04)	Tok/s 126243 (113333)	Loss/tok 3.5936 (3.4581)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.043 (0.093)	Data 1.23e-04 (5.71e-04)	Tok/s 90546 (113279)	Loss/tok 2.7360 (3.4561)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][760/1291]	Time 0.043 (0.093)	Data 1.19e-04 (5.65e-04)	Tok/s 93067 (113278)	Loss/tok 2.8170 (3.4564)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.105 (0.093)	Data 1.19e-04 (5.59e-04)	Tok/s 120304 (113293)	Loss/tok 3.5135 (3.4565)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.074 (0.094)	Data 1.11e-04 (5.53e-04)	Tok/s 106567 (113325)	Loss/tok 3.1602 (3.4569)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.139 (0.094)	Data 1.13e-04 (5.48e-04)	Tok/s 127014 (113398)	Loss/tok 3.6078 (3.4580)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.139 (0.094)	Data 1.11e-04 (5.42e-04)	Tok/s 124224 (113480)	Loss/tok 3.6552 (3.4588)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.103 (0.094)	Data 1.13e-04 (5.37e-04)	Tok/s 120340 (113445)	Loss/tok 3.4605 (3.4580)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.104 (0.094)	Data 1.09e-04 (5.32e-04)	Tok/s 122232 (113421)	Loss/tok 3.5121 (3.4564)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.103 (0.094)	Data 1.21e-04 (5.27e-04)	Tok/s 119626 (113414)	Loss/tok 3.4663 (3.4554)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.073 (0.094)	Data 1.15e-04 (5.22e-04)	Tok/s 109145 (113388)	Loss/tok 3.1862 (3.4537)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.104 (0.094)	Data 1.14e-04 (5.17e-04)	Tok/s 121502 (113355)	Loss/tok 3.3445 (3.4518)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.104 (0.094)	Data 1.13e-04 (5.12e-04)	Tok/s 120516 (113370)	Loss/tok 3.4061 (3.4513)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.103 (0.094)	Data 1.11e-04 (5.08e-04)	Tok/s 120700 (113444)	Loss/tok 3.3280 (3.4518)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.072 (0.094)	Data 1.14e-04 (5.03e-04)	Tok/s 106834 (113469)	Loss/tok 3.1994 (3.4523)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][890/1291]	Time 0.073 (0.094)	Data 1.15e-04 (4.99e-04)	Tok/s 109071 (113480)	Loss/tok 3.2382 (3.4515)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][900/1291]	Time 0.137 (0.094)	Data 1.18e-04 (4.95e-04)	Tok/s 128120 (113509)	Loss/tok 3.5428 (3.4530)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.91e-04)	Tok/s 107354 (113458)	Loss/tok 3.0560 (3.4514)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.042 (0.094)	Data 1.14e-04 (4.86e-04)	Tok/s 93951 (113417)	Loss/tok 2.7230 (3.4501)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.175 (0.094)	Data 1.12e-04 (4.82e-04)	Tok/s 126609 (113407)	Loss/tok 3.9157 (3.4505)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.073 (0.094)	Data 1.15e-04 (4.79e-04)	Tok/s 106384 (113418)	Loss/tok 3.1513 (3.4506)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.103 (0.094)	Data 1.18e-04 (4.75e-04)	Tok/s 121503 (113399)	Loss/tok 3.3375 (3.4486)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.042 (0.094)	Data 1.17e-04 (4.71e-04)	Tok/s 93349 (113361)	Loss/tok 2.7599 (3.4474)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.67e-04)	Tok/s 123772 (113375)	Loss/tok 3.3944 (3.4465)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.176 (0.094)	Data 1.15e-04 (4.64e-04)	Tok/s 124553 (113398)	Loss/tok 3.7709 (3.4467)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.073 (0.094)	Data 1.12e-04 (4.60e-04)	Tok/s 107277 (113365)	Loss/tok 3.1393 (3.4449)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.57e-04)	Tok/s 105404 (113368)	Loss/tok 3.2763 (3.4445)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.072 (0.093)	Data 1.18e-04 (4.53e-04)	Tok/s 109246 (113326)	Loss/tok 3.2452 (3.4431)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1020/1291]	Time 0.103 (0.093)	Data 1.13e-04 (4.50e-04)	Tok/s 120643 (113336)	Loss/tok 3.5786 (3.4423)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.073 (0.093)	Data 1.08e-04 (4.47e-04)	Tok/s 105645 (113322)	Loss/tok 3.2439 (3.4411)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.176 (0.093)	Data 1.20e-04 (4.44e-04)	Tok/s 129132 (113356)	Loss/tok 3.6303 (3.4415)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1050/1291]	Time 0.104 (0.093)	Data 1.19e-04 (4.41e-04)	Tok/s 121981 (113370)	Loss/tok 3.2882 (3.4410)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.104 (0.094)	Data 1.18e-04 (4.37e-04)	Tok/s 121556 (113430)	Loss/tok 3.2852 (3.4405)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.175 (0.094)	Data 1.18e-04 (4.34e-04)	Tok/s 127362 (113435)	Loss/tok 3.7560 (3.4409)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.175 (0.094)	Data 1.16e-04 (4.31e-04)	Tok/s 126659 (113442)	Loss/tok 3.8404 (3.4414)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.104 (0.094)	Data 1.08e-04 (4.29e-04)	Tok/s 122884 (113458)	Loss/tok 3.3477 (3.4407)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.26e-04)	Tok/s 122326 (113441)	Loss/tok 3.3220 (3.4396)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.104 (0.094)	Data 1.12e-04 (4.23e-04)	Tok/s 119857 (113438)	Loss/tok 3.4703 (3.4391)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.137 (0.094)	Data 1.13e-04 (4.20e-04)	Tok/s 129503 (113471)	Loss/tok 3.4469 (3.4392)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.138 (0.094)	Data 1.14e-04 (4.17e-04)	Tok/s 127472 (113494)	Loss/tok 3.6863 (3.4392)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.073 (0.094)	Data 1.14e-04 (4.15e-04)	Tok/s 106584 (113484)	Loss/tok 3.1296 (3.4384)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.137 (0.094)	Data 1.14e-04 (4.12e-04)	Tok/s 126276 (113501)	Loss/tok 3.6599 (3.4380)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.10e-04)	Tok/s 123384 (113510)	Loss/tok 3.3407 (3.4365)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.137 (0.094)	Data 1.17e-04 (4.07e-04)	Tok/s 128440 (113474)	Loss/tok 3.4720 (3.4356)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1180/1291]	Time 0.073 (0.094)	Data 1.41e-04 (4.05e-04)	Tok/s 107378 (113542)	Loss/tok 3.2337 (3.4354)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.104 (0.094)	Data 1.16e-04 (4.02e-04)	Tok/s 118963 (113545)	Loss/tok 3.5611 (3.4345)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.137 (0.094)	Data 1.15e-04 (4.00e-04)	Tok/s 126379 (113562)	Loss/tok 3.6416 (3.4341)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.104 (0.094)	Data 1.19e-04 (3.97e-04)	Tok/s 123811 (113557)	Loss/tok 3.3722 (3.4334)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.104 (0.094)	Data 1.16e-04 (3.95e-04)	Tok/s 119417 (113619)	Loss/tok 3.3995 (3.4335)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1230/1291]	Time 0.137 (0.094)	Data 1.31e-04 (3.93e-04)	Tok/s 126442 (113641)	Loss/tok 3.4863 (3.4338)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.103 (0.094)	Data 1.19e-04 (3.91e-04)	Tok/s 123038 (113652)	Loss/tok 3.3654 (3.4328)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.103 (0.094)	Data 1.19e-04 (3.89e-04)	Tok/s 121165 (113617)	Loss/tok 3.4833 (3.4326)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.176 (0.094)	Data 1.21e-04 (3.86e-04)	Tok/s 124430 (113631)	Loss/tok 3.9448 (3.4329)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.84e-04)	Tok/s 109065 (113623)	Loss/tok 3.1404 (3.4322)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.103 (0.094)	Data 1.14e-04 (3.82e-04)	Tok/s 122713 (113569)	Loss/tok 3.3980 (3.4309)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.073 (0.094)	Data 5.29e-05 (3.82e-04)	Tok/s 105790 (113578)	Loss/tok 3.2077 (3.4312)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590804171, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590804172, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.398 (0.398)	Decoder iters 117.0 (117.0)	Tok/s 22480 (22480)
0: Running moses detokenizer
0: BLEU(score=21.910374205798608, counts=[35460, 16999, 9326, 5340], totals=[64636, 61633, 58630, 55631], precisions=[54.861068135404416, 27.58100368309185, 15.906532491898346, 9.598964606064964], bp=0.9993813412719036, sys_len=64636, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590805315, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2191, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590805315, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4324	Test BLEU: 21.91
0: Performance: Epoch: 1	Training: 1817811 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592590805315, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592590805315, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590805315, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3348146714
0: TRAIN [2][0/1291]	Time 0.416 (0.416)	Data 2.98e-01 (2.98e-01)	Tok/s 30335 (30335)	Loss/tok 3.3633 (3.3633)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.137 (0.124)	Data 1.12e-04 (2.72e-02)	Tok/s 129656 (108247)	Loss/tok 3.2892 (3.2467)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.103 (0.107)	Data 1.09e-04 (1.43e-02)	Tok/s 124025 (110292)	Loss/tok 3.1073 (3.2310)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.103 (0.103)	Data 1.11e-04 (9.74e-03)	Tok/s 121380 (112196)	Loss/tok 3.3132 (3.2455)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.104 (0.100)	Data 1.15e-04 (7.39e-03)	Tok/s 120937 (112592)	Loss/tok 3.3460 (3.2474)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.072 (0.098)	Data 1.11e-04 (5.97e-03)	Tok/s 104918 (112679)	Loss/tok 3.1606 (3.2514)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.106 (0.096)	Data 1.15e-04 (5.01e-03)	Tok/s 116627 (112722)	Loss/tok 3.3294 (3.2515)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][70/1291]	Time 0.104 (0.096)	Data 1.13e-04 (4.32e-03)	Tok/s 120795 (112798)	Loss/tok 3.3645 (3.2547)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.103 (0.096)	Data 1.13e-04 (3.80e-03)	Tok/s 119270 (113225)	Loss/tok 3.3559 (3.2565)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.103 (0.097)	Data 1.15e-04 (3.39e-03)	Tok/s 121867 (113743)	Loss/tok 3.3307 (3.2755)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.072 (0.095)	Data 1.28e-04 (3.07e-03)	Tok/s 108366 (113450)	Loss/tok 2.9643 (3.2710)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.072 (0.095)	Data 1.13e-04 (2.80e-03)	Tok/s 106024 (113457)	Loss/tok 3.0565 (3.2711)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.104 (0.094)	Data 1.15e-04 (2.58e-03)	Tok/s 120951 (113078)	Loss/tok 3.2985 (3.2669)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.042 (0.092)	Data 1.39e-04 (2.39e-03)	Tok/s 93624 (112428)	Loss/tok 2.5626 (3.2550)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.073 (0.092)	Data 1.17e-04 (2.23e-03)	Tok/s 108542 (112478)	Loss/tok 3.1267 (3.2574)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.073 (0.091)	Data 1.14e-04 (2.09e-03)	Tok/s 105006 (112381)	Loss/tok 3.1227 (3.2563)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.137 (0.091)	Data 1.34e-04 (1.97e-03)	Tok/s 127186 (112557)	Loss/tok 3.4851 (3.2589)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.072 (0.093)	Data 1.17e-04 (1.86e-03)	Tok/s 106564 (112839)	Loss/tok 3.1400 (3.2718)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.104 (0.093)	Data 1.15e-04 (1.77e-03)	Tok/s 122110 (112949)	Loss/tok 3.2444 (3.2715)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.073 (0.092)	Data 1.19e-04 (1.68e-03)	Tok/s 106045 (112907)	Loss/tok 3.0688 (3.2700)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][200/1291]	Time 0.072 (0.093)	Data 1.18e-04 (1.60e-03)	Tok/s 109220 (113010)	Loss/tok 2.9731 (3.2722)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.073 (0.093)	Data 1.20e-04 (1.53e-03)	Tok/s 107353 (113236)	Loss/tok 3.0258 (3.2771)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.072 (0.092)	Data 1.18e-04 (1.47e-03)	Tok/s 106263 (113098)	Loss/tok 3.1999 (3.2723)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.138 (0.093)	Data 1.24e-04 (1.41e-03)	Tok/s 127997 (113191)	Loss/tok 3.3899 (3.2724)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.072 (0.093)	Data 1.19e-04 (1.36e-03)	Tok/s 105283 (113259)	Loss/tok 3.0285 (3.2711)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.103 (0.093)	Data 1.22e-04 (1.31e-03)	Tok/s 121421 (113312)	Loss/tok 3.3771 (3.2694)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.137 (0.093)	Data 1.16e-04 (1.26e-03)	Tok/s 126016 (113532)	Loss/tok 3.4834 (3.2736)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.072 (0.093)	Data 1.22e-04 (1.22e-03)	Tok/s 106784 (113556)	Loss/tok 3.0517 (3.2727)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.073 (0.092)	Data 1.27e-04 (1.18e-03)	Tok/s 106865 (113396)	Loss/tok 3.1440 (3.2691)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.138 (0.092)	Data 1.24e-04 (1.14e-03)	Tok/s 126418 (113317)	Loss/tok 3.4378 (3.2656)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.138 (0.092)	Data 1.25e-04 (1.11e-03)	Tok/s 127413 (113295)	Loss/tok 3.4210 (3.2686)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.137 (0.092)	Data 1.34e-04 (1.08e-03)	Tok/s 127652 (113301)	Loss/tok 3.5243 (3.2696)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][320/1291]	Time 0.073 (0.092)	Data 1.12e-04 (1.05e-03)	Tok/s 106705 (113277)	Loss/tok 3.0858 (3.2698)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][330/1291]	Time 0.104 (0.093)	Data 1.22e-04 (1.02e-03)	Tok/s 122720 (113307)	Loss/tok 3.3285 (3.2768)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.104 (0.093)	Data 1.18e-04 (9.95e-04)	Tok/s 120599 (113324)	Loss/tok 3.1183 (3.2759)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.072 (0.093)	Data 1.43e-04 (9.70e-04)	Tok/s 108164 (113525)	Loss/tok 3.0726 (3.2793)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.073 (0.093)	Data 1.39e-04 (9.47e-04)	Tok/s 106277 (113565)	Loss/tok 3.0410 (3.2795)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.137 (0.093)	Data 1.17e-04 (9.24e-04)	Tok/s 129159 (113616)	Loss/tok 3.4531 (3.2795)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.103 (0.093)	Data 1.17e-04 (9.03e-04)	Tok/s 123950 (113738)	Loss/tok 3.2158 (3.2807)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.072 (0.094)	Data 1.54e-04 (8.83e-04)	Tok/s 104910 (113784)	Loss/tok 3.0132 (3.2833)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.073 (0.094)	Data 1.19e-04 (8.64e-04)	Tok/s 107954 (113801)	Loss/tok 3.1665 (3.2844)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.073 (0.094)	Data 1.15e-04 (8.46e-04)	Tok/s 104421 (113786)	Loss/tok 3.0941 (3.2845)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.104 (0.094)	Data 1.20e-04 (8.29e-04)	Tok/s 120179 (113907)	Loss/tok 3.2918 (3.2858)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.104 (0.094)	Data 1.15e-04 (8.12e-04)	Tok/s 122051 (113996)	Loss/tok 3.2148 (3.2854)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.138 (0.094)	Data 1.10e-04 (7.97e-04)	Tok/s 127044 (113979)	Loss/tok 3.5367 (3.2850)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.103 (0.094)	Data 1.18e-04 (7.82e-04)	Tok/s 122708 (114006)	Loss/tok 3.2195 (3.2826)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][460/1291]	Time 0.042 (0.094)	Data 1.15e-04 (7.67e-04)	Tok/s 96320 (114023)	Loss/tok 2.6500 (3.2829)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.103 (0.094)	Data 1.15e-04 (7.54e-04)	Tok/s 122253 (113982)	Loss/tok 3.4786 (3.2823)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.104 (0.093)	Data 1.21e-04 (7.41e-04)	Tok/s 121752 (113931)	Loss/tok 3.2486 (3.2817)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.103 (0.093)	Data 1.14e-04 (7.28e-04)	Tok/s 123744 (113921)	Loss/tok 3.2511 (3.2806)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.103 (0.093)	Data 1.39e-04 (7.16e-04)	Tok/s 122327 (113914)	Loss/tok 3.3239 (3.2805)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.103 (0.093)	Data 1.20e-04 (7.04e-04)	Tok/s 121741 (114004)	Loss/tok 3.3020 (3.2817)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.073 (0.093)	Data 1.26e-04 (6.93e-04)	Tok/s 107246 (113986)	Loss/tok 3.0558 (3.2824)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.103 (0.093)	Data 1.17e-04 (6.82e-04)	Tok/s 121387 (113968)	Loss/tok 3.2820 (3.2806)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.073 (0.093)	Data 1.25e-04 (6.72e-04)	Tok/s 105645 (114008)	Loss/tok 2.9899 (3.2818)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.072 (0.093)	Data 1.19e-04 (6.62e-04)	Tok/s 106426 (114027)	Loss/tok 3.0890 (3.2817)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.073 (0.093)	Data 1.12e-04 (6.52e-04)	Tok/s 109801 (114023)	Loss/tok 3.0613 (3.2813)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][570/1291]	Time 0.073 (0.093)	Data 1.21e-04 (6.43e-04)	Tok/s 105813 (113972)	Loss/tok 3.2263 (3.2810)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.072 (0.093)	Data 1.26e-04 (6.34e-04)	Tok/s 107491 (113990)	Loss/tok 3.0720 (3.2804)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.073 (0.093)	Data 1.13e-04 (6.26e-04)	Tok/s 104778 (113948)	Loss/tok 3.1559 (3.2790)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.073 (0.093)	Data 1.13e-04 (6.17e-04)	Tok/s 107796 (114072)	Loss/tok 3.0865 (3.2828)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.072 (0.094)	Data 1.18e-04 (6.09e-04)	Tok/s 106018 (114074)	Loss/tok 2.9890 (3.2845)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.137 (0.094)	Data 1.19e-04 (6.02e-04)	Tok/s 126970 (114126)	Loss/tok 3.4554 (3.2851)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.072 (0.094)	Data 1.14e-04 (5.94e-04)	Tok/s 107850 (114167)	Loss/tok 3.1255 (3.2871)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.104 (0.094)	Data 1.12e-04 (5.87e-04)	Tok/s 120551 (114182)	Loss/tok 3.3301 (3.2865)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.044 (0.094)	Data 1.85e-04 (5.80e-04)	Tok/s 85894 (114074)	Loss/tok 2.5696 (3.2852)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.042 (0.093)	Data 1.78e-04 (5.73e-04)	Tok/s 91619 (113986)	Loss/tok 2.5478 (3.2845)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.073 (0.093)	Data 1.23e-04 (5.66e-04)	Tok/s 105120 (113958)	Loss/tok 3.0350 (3.2857)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.137 (0.094)	Data 2.03e-04 (5.60e-04)	Tok/s 126303 (114031)	Loss/tok 3.5539 (3.2876)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.073 (0.093)	Data 1.13e-04 (5.54e-04)	Tok/s 106745 (113998)	Loss/tok 3.2848 (3.2864)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][700/1291]	Time 0.042 (0.093)	Data 1.19e-04 (5.47e-04)	Tok/s 96378 (113978)	Loss/tok 2.6259 (3.2859)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.072 (0.093)	Data 1.17e-04 (5.41e-04)	Tok/s 107800 (113931)	Loss/tok 3.2196 (3.2850)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.073 (0.093)	Data 1.17e-04 (5.36e-04)	Tok/s 106974 (113899)	Loss/tok 3.1349 (3.2845)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.042 (0.093)	Data 1.78e-04 (5.30e-04)	Tok/s 95396 (113854)	Loss/tok 2.6492 (3.2832)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.042 (0.093)	Data 1.20e-04 (5.25e-04)	Tok/s 94383 (113811)	Loss/tok 2.6432 (3.2828)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.072 (0.093)	Data 1.32e-04 (5.19e-04)	Tok/s 106716 (113707)	Loss/tok 3.1425 (3.2825)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.073 (0.093)	Data 1.21e-04 (5.14e-04)	Tok/s 106426 (113733)	Loss/tok 3.1296 (3.2831)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.104 (0.093)	Data 1.15e-04 (5.09e-04)	Tok/s 121928 (113644)	Loss/tok 3.5242 (3.2815)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.073 (0.093)	Data 1.14e-04 (5.04e-04)	Tok/s 108993 (113723)	Loss/tok 3.0335 (3.2860)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.103 (0.093)	Data 1.25e-04 (5.00e-04)	Tok/s 121561 (113794)	Loss/tok 3.3232 (3.2869)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.104 (0.093)	Data 1.75e-04 (4.95e-04)	Tok/s 120410 (113744)	Loss/tok 3.3933 (3.2876)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.138 (0.093)	Data 1.18e-04 (4.91e-04)	Tok/s 126389 (113698)	Loss/tok 3.4605 (3.2867)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][820/1291]	Time 0.104 (0.093)	Data 1.22e-04 (4.86e-04)	Tok/s 118532 (113714)	Loss/tok 3.3950 (3.2876)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.042 (0.093)	Data 1.11e-04 (4.82e-04)	Tok/s 93593 (113690)	Loss/tok 2.5915 (3.2886)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.042 (0.093)	Data 1.16e-04 (4.78e-04)	Tok/s 93575 (113621)	Loss/tok 2.7744 (3.2881)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][850/1291]	Time 0.072 (0.093)	Data 1.14e-04 (4.74e-04)	Tok/s 107640 (113604)	Loss/tok 3.0055 (3.2885)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.104 (0.093)	Data 1.28e-04 (4.70e-04)	Tok/s 122102 (113608)	Loss/tok 3.2187 (3.2886)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.073 (0.093)	Data 1.13e-04 (4.66e-04)	Tok/s 103804 (113664)	Loss/tok 3.1138 (3.2901)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][880/1291]	Time 0.073 (0.093)	Data 1.21e-04 (4.62e-04)	Tok/s 106447 (113645)	Loss/tok 3.0732 (3.2897)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.138 (0.093)	Data 1.17e-04 (4.58e-04)	Tok/s 128087 (113664)	Loss/tok 3.3220 (3.2889)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.073 (0.093)	Data 1.17e-04 (4.54e-04)	Tok/s 103252 (113715)	Loss/tok 3.0422 (3.2909)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.103 (0.093)	Data 1.67e-04 (4.51e-04)	Tok/s 121372 (113715)	Loss/tok 3.2030 (3.2903)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.137 (0.093)	Data 1.15e-04 (4.47e-04)	Tok/s 126377 (113686)	Loss/tok 3.5278 (3.2898)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.104 (0.093)	Data 1.17e-04 (4.43e-04)	Tok/s 122751 (113701)	Loss/tok 3.2042 (3.2901)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.073 (0.093)	Data 1.17e-04 (4.40e-04)	Tok/s 107069 (113725)	Loss/tok 3.1102 (3.2897)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.175 (0.093)	Data 1.24e-04 (4.37e-04)	Tok/s 128359 (113702)	Loss/tok 3.7032 (3.2912)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.104 (0.093)	Data 1.14e-04 (4.33e-04)	Tok/s 119648 (113676)	Loss/tok 3.4134 (3.2909)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.072 (0.093)	Data 1.16e-04 (4.30e-04)	Tok/s 105642 (113688)	Loss/tok 3.0569 (3.2904)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.105 (0.093)	Data 1.21e-04 (4.27e-04)	Tok/s 119660 (113715)	Loss/tok 3.1889 (3.2907)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.074 (0.093)	Data 1.15e-04 (4.24e-04)	Tok/s 102813 (113728)	Loss/tok 3.1325 (3.2913)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1000/1291]	Time 0.176 (0.094)	Data 1.24e-04 (4.21e-04)	Tok/s 126056 (113760)	Loss/tok 3.6430 (3.2932)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.105 (0.094)	Data 1.16e-04 (4.18e-04)	Tok/s 118945 (113783)	Loss/tok 3.2886 (3.2932)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.074 (0.094)	Data 1.13e-04 (4.15e-04)	Tok/s 106257 (113761)	Loss/tok 2.9730 (3.2926)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.074 (0.094)	Data 1.09e-04 (4.12e-04)	Tok/s 102785 (113732)	Loss/tok 3.1520 (3.2922)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.074 (0.094)	Data 1.11e-04 (4.09e-04)	Tok/s 104807 (113667)	Loss/tok 3.1231 (3.2919)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.105 (0.094)	Data 1.17e-04 (4.06e-04)	Tok/s 119990 (113628)	Loss/tok 3.1412 (3.2908)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.105 (0.094)	Data 1.16e-04 (4.03e-04)	Tok/s 119790 (113625)	Loss/tok 3.2284 (3.2907)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.074 (0.093)	Data 1.15e-04 (4.01e-04)	Tok/s 106745 (113579)	Loss/tok 2.9827 (3.2900)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.138 (0.094)	Data 1.15e-04 (3.98e-04)	Tok/s 125676 (113604)	Loss/tok 3.5223 (3.2908)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.175 (0.094)	Data 1.27e-04 (3.95e-04)	Tok/s 129320 (113634)	Loss/tok 3.4866 (3.2912)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.138 (0.094)	Data 1.15e-04 (3.93e-04)	Tok/s 128052 (113661)	Loss/tok 3.3700 (3.2923)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.104 (0.094)	Data 1.11e-04 (3.90e-04)	Tok/s 120583 (113675)	Loss/tok 3.2696 (3.2916)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.88e-04)	Tok/s 123079 (113699)	Loss/tok 3.2127 (3.2911)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1130/1291]	Time 0.104 (0.094)	Data 1.20e-04 (3.85e-04)	Tok/s 121187 (113704)	Loss/tok 3.2957 (3.2909)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.042 (0.094)	Data 1.18e-04 (3.83e-04)	Tok/s 91907 (113654)	Loss/tok 2.6186 (3.2898)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1150/1291]	Time 0.175 (0.094)	Data 1.33e-04 (3.81e-04)	Tok/s 127596 (113654)	Loss/tok 3.6970 (3.2906)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.138 (0.094)	Data 1.30e-04 (3.79e-04)	Tok/s 128558 (113672)	Loss/tok 3.4785 (3.2911)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.073 (0.094)	Data 1.24e-04 (3.77e-04)	Tok/s 109209 (113692)	Loss/tok 3.0759 (3.2910)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1180/1291]	Time 0.103 (0.094)	Data 1.22e-04 (3.74e-04)	Tok/s 120841 (113732)	Loss/tok 3.2960 (3.2915)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.104 (0.094)	Data 1.13e-04 (3.72e-04)	Tok/s 121428 (113748)	Loss/tok 3.1708 (3.2909)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.073 (0.094)	Data 1.14e-04 (3.70e-04)	Tok/s 104357 (113758)	Loss/tok 3.0735 (3.2908)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.174 (0.094)	Data 1.15e-04 (3.68e-04)	Tok/s 128372 (113752)	Loss/tok 3.5410 (3.2905)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.137 (0.094)	Data 1.13e-04 (3.66e-04)	Tok/s 128267 (113727)	Loss/tok 3.4012 (3.2894)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.64e-04)	Tok/s 106688 (113718)	Loss/tok 3.0949 (3.2894)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.104 (0.094)	Data 1.14e-04 (3.62e-04)	Tok/s 121467 (113705)	Loss/tok 3.3431 (3.2887)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.073 (0.094)	Data 1.13e-04 (3.60e-04)	Tok/s 103885 (113660)	Loss/tok 3.1619 (3.2884)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.073 (0.094)	Data 1.20e-04 (3.58e-04)	Tok/s 106237 (113649)	Loss/tok 3.2075 (3.2884)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.104 (0.094)	Data 1.18e-04 (3.57e-04)	Tok/s 119857 (113676)	Loss/tok 3.2592 (3.2885)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.073 (0.094)	Data 1.18e-04 (3.55e-04)	Tok/s 106311 (113699)	Loss/tok 3.1475 (3.2893)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.072 (0.094)	Data 5.58e-05 (3.55e-04)	Tok/s 107410 (113713)	Loss/tok 2.9853 (3.2893)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592590927116, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590927117, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.371 (0.371)	Decoder iters 106.0 (106.0)	Tok/s 23791 (23791)
0: Running moses detokenizer
0: BLEU(score=22.9517869224222, counts=[36093, 17661, 9846, 5751], totals=[64329, 61326, 58323, 55324], precisions=[56.106888028727326, 28.798552000782703, 16.881847641582223, 10.395126888872822], bp=0.9946203765608636, sys_len=64329, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592590928183, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22949999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590928183, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2861	Test BLEU: 22.95
0: Performance: Epoch: 2	Training: 1818796 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592590928183, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592590928183, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592590928183, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2232435624
0: TRAIN [3][0/1291]	Time 0.386 (0.386)	Data 3.01e-01 (3.01e-01)	Tok/s 20257 (20257)	Loss/tok 2.9682 (2.9682)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.072 (0.107)	Data 1.22e-04 (2.75e-02)	Tok/s 106880 (100920)	Loss/tok 2.9496 (3.0391)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][20/1291]	Time 0.073 (0.100)	Data 1.18e-04 (1.44e-02)	Tok/s 107672 (107713)	Loss/tok 2.9887 (3.0636)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.073 (0.098)	Data 1.13e-04 (9.83e-03)	Tok/s 107364 (109651)	Loss/tok 3.0958 (3.1212)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][40/1291]	Time 0.072 (0.100)	Data 1.18e-04 (7.46e-03)	Tok/s 106486 (111104)	Loss/tok 3.0146 (3.1721)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.072 (0.098)	Data 1.19e-04 (6.02e-03)	Tok/s 107463 (112084)	Loss/tok 2.8526 (3.1656)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.105 (0.101)	Data 1.27e-04 (5.06e-03)	Tok/s 118672 (113258)	Loss/tok 3.1611 (3.2030)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.072 (0.100)	Data 1.15e-04 (4.36e-03)	Tok/s 108992 (113155)	Loss/tok 3.0537 (3.2000)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.137 (0.100)	Data 1.30e-04 (3.84e-03)	Tok/s 128140 (113600)	Loss/tok 3.4167 (3.2106)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.073 (0.099)	Data 1.71e-04 (3.43e-03)	Tok/s 105506 (113690)	Loss/tok 2.9930 (3.2062)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.073 (0.100)	Data 2.02e-04 (3.11e-03)	Tok/s 106728 (114153)	Loss/tok 2.9738 (3.2150)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.073 (0.100)	Data 1.22e-04 (2.84e-03)	Tok/s 107186 (114004)	Loss/tok 3.0410 (3.2120)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.073 (0.098)	Data 1.16e-04 (2.61e-03)	Tok/s 104574 (113748)	Loss/tok 3.0243 (3.2084)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.042 (0.097)	Data 1.17e-04 (2.42e-03)	Tok/s 95718 (113391)	Loss/tok 2.6217 (3.2009)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.104 (0.097)	Data 1.15e-04 (2.26e-03)	Tok/s 122112 (113398)	Loss/tok 3.2128 (3.2011)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.103 (0.098)	Data 1.18e-04 (2.12e-03)	Tok/s 123408 (113801)	Loss/tok 3.3409 (3.2111)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.042 (0.097)	Data 1.56e-04 (2.00e-03)	Tok/s 91272 (113633)	Loss/tok 2.6445 (3.2073)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][170/1291]	Time 0.073 (0.097)	Data 1.16e-04 (1.89e-03)	Tok/s 107274 (113712)	Loss/tok 3.0252 (3.2098)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.137 (0.097)	Data 1.15e-04 (1.79e-03)	Tok/s 128015 (113844)	Loss/tok 3.3019 (3.2101)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.042 (0.097)	Data 1.27e-04 (1.71e-03)	Tok/s 93452 (113780)	Loss/tok 2.7113 (3.2125)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.073 (0.096)	Data 1.15e-04 (1.63e-03)	Tok/s 106959 (113831)	Loss/tok 2.9683 (3.2105)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.137 (0.096)	Data 1.17e-04 (1.56e-03)	Tok/s 129309 (113896)	Loss/tok 3.3317 (3.2127)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.072 (0.096)	Data 1.21e-04 (1.49e-03)	Tok/s 108488 (113891)	Loss/tok 3.0433 (3.2091)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.073 (0.096)	Data 1.61e-04 (1.43e-03)	Tok/s 107668 (113854)	Loss/tok 3.0035 (3.2092)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.073 (0.096)	Data 1.16e-04 (1.38e-03)	Tok/s 108388 (113947)	Loss/tok 3.0092 (3.2054)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.103 (0.095)	Data 1.17e-04 (1.33e-03)	Tok/s 122223 (113780)	Loss/tok 3.2310 (3.2036)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.073 (0.095)	Data 1.15e-04 (1.28e-03)	Tok/s 106322 (113838)	Loss/tok 3.0703 (3.2007)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.073 (0.095)	Data 1.17e-04 (1.24e-03)	Tok/s 107040 (113727)	Loss/tok 3.1225 (3.1958)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.073 (0.095)	Data 1.16e-04 (1.20e-03)	Tok/s 106158 (113717)	Loss/tok 3.0085 (3.1929)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.073 (0.094)	Data 1.14e-04 (1.16e-03)	Tok/s 107230 (113537)	Loss/tok 2.9616 (3.1898)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][300/1291]	Time 0.137 (0.094)	Data 1.16e-04 (1.13e-03)	Tok/s 128064 (113597)	Loss/tok 3.2675 (3.1915)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.073 (0.094)	Data 1.78e-04 (1.10e-03)	Tok/s 107440 (113487)	Loss/tok 2.9808 (3.1879)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.104 (0.093)	Data 1.41e-04 (1.07e-03)	Tok/s 122521 (113316)	Loss/tok 3.1476 (3.1829)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.137 (0.094)	Data 1.32e-04 (1.04e-03)	Tok/s 126241 (113397)	Loss/tok 3.3611 (3.1861)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.104 (0.094)	Data 1.20e-04 (1.01e-03)	Tok/s 124734 (113527)	Loss/tok 3.1294 (3.1865)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.073 (0.094)	Data 1.14e-04 (9.86e-04)	Tok/s 105637 (113477)	Loss/tok 3.0783 (3.1844)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.137 (0.094)	Data 1.13e-04 (9.62e-04)	Tok/s 127702 (113546)	Loss/tok 3.4092 (3.1860)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.104 (0.094)	Data 1.15e-04 (9.40e-04)	Tok/s 119515 (113600)	Loss/tok 3.2144 (3.1849)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.175 (0.094)	Data 1.12e-04 (9.19e-04)	Tok/s 129100 (113654)	Loss/tok 3.5288 (3.1848)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.073 (0.094)	Data 1.98e-04 (8.99e-04)	Tok/s 106814 (113565)	Loss/tok 2.8240 (3.1832)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.073 (0.094)	Data 1.17e-04 (8.79e-04)	Tok/s 107682 (113522)	Loss/tok 3.0267 (3.1818)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.073 (0.094)	Data 1.80e-04 (8.61e-04)	Tok/s 106171 (113641)	Loss/tok 2.8696 (3.1828)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.073 (0.094)	Data 1.13e-04 (8.44e-04)	Tok/s 106173 (113517)	Loss/tok 2.9818 (3.1793)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][430/1291]	Time 0.138 (0.094)	Data 1.10e-04 (8.28e-04)	Tok/s 127832 (113740)	Loss/tok 3.3854 (3.1806)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.104 (0.094)	Data 1.22e-04 (8.12e-04)	Tok/s 121553 (113665)	Loss/tok 3.1744 (3.1790)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.104 (0.094)	Data 1.79e-04 (7.97e-04)	Tok/s 121998 (113563)	Loss/tok 3.1146 (3.1758)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.073 (0.094)	Data 1.96e-04 (7.83e-04)	Tok/s 106795 (113523)	Loss/tok 3.0688 (3.1754)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.138 (0.094)	Data 1.26e-04 (7.69e-04)	Tok/s 127934 (113680)	Loss/tok 3.3174 (3.1774)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.138 (0.094)	Data 1.14e-04 (7.56e-04)	Tok/s 126764 (113673)	Loss/tok 3.2928 (3.1757)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.104 (0.094)	Data 1.13e-04 (7.43e-04)	Tok/s 120659 (113651)	Loss/tok 3.2373 (3.1745)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.176 (0.094)	Data 1.17e-04 (7.30e-04)	Tok/s 126944 (113718)	Loss/tok 3.3615 (3.1751)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.073 (0.094)	Data 1.20e-04 (7.18e-04)	Tok/s 106701 (113834)	Loss/tok 2.9717 (3.1755)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.104 (0.094)	Data 1.83e-04 (7.07e-04)	Tok/s 120835 (113926)	Loss/tok 3.1908 (3.1765)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.042 (0.094)	Data 1.13e-04 (6.96e-04)	Tok/s 97912 (113923)	Loss/tok 2.5736 (3.1746)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.104 (0.095)	Data 1.86e-04 (6.86e-04)	Tok/s 123180 (114015)	Loss/tok 3.1341 (3.1765)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.073 (0.094)	Data 1.62e-04 (6.76e-04)	Tok/s 106151 (114008)	Loss/tok 3.0132 (3.1746)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1291]	Time 0.103 (0.095)	Data 1.72e-04 (6.66e-04)	Tok/s 119792 (114061)	Loss/tok 3.1670 (3.1729)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.138 (0.094)	Data 1.90e-04 (6.57e-04)	Tok/s 128447 (114039)	Loss/tok 3.2274 (3.1732)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][580/1291]	Time 0.072 (0.094)	Data 1.26e-04 (6.48e-04)	Tok/s 109133 (113968)	Loss/tok 2.8496 (3.1717)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.072 (0.094)	Data 1.17e-04 (6.39e-04)	Tok/s 107563 (113975)	Loss/tok 2.9127 (3.1723)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.073 (0.094)	Data 1.83e-04 (6.31e-04)	Tok/s 102957 (113976)	Loss/tok 3.0018 (3.1721)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.073 (0.094)	Data 1.72e-04 (6.23e-04)	Tok/s 106259 (113968)	Loss/tok 2.8147 (3.1708)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.072 (0.094)	Data 1.16e-04 (6.15e-04)	Tok/s 105942 (113958)	Loss/tok 3.0759 (3.1704)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.104 (0.094)	Data 1.84e-04 (6.08e-04)	Tok/s 121798 (113905)	Loss/tok 3.0350 (3.1686)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.137 (0.094)	Data 1.72e-04 (6.00e-04)	Tok/s 126919 (113943)	Loss/tok 3.3628 (3.1704)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.104 (0.094)	Data 1.13e-04 (5.93e-04)	Tok/s 119320 (113954)	Loss/tok 3.1544 (3.1704)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.073 (0.094)	Data 1.15e-04 (5.86e-04)	Tok/s 107465 (113923)	Loss/tok 2.9673 (3.1683)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.073 (0.094)	Data 1.29e-04 (5.79e-04)	Tok/s 105153 (113957)	Loss/tok 2.9949 (3.1689)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.042 (0.094)	Data 1.25e-04 (5.73e-04)	Tok/s 92253 (113878)	Loss/tok 2.5713 (3.1674)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.072 (0.094)	Data 1.26e-04 (5.66e-04)	Tok/s 107386 (113867)	Loss/tok 2.9556 (3.1663)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.073 (0.094)	Data 1.18e-04 (5.60e-04)	Tok/s 105304 (113986)	Loss/tok 3.0215 (3.1683)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][710/1291]	Time 0.103 (0.094)	Data 1.13e-04 (5.54e-04)	Tok/s 122472 (114001)	Loss/tok 3.1450 (3.1694)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.103 (0.094)	Data 1.21e-04 (5.48e-04)	Tok/s 120074 (113971)	Loss/tok 3.2093 (3.1692)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.042 (0.094)	Data 1.14e-04 (5.42e-04)	Tok/s 95754 (113942)	Loss/tok 2.7020 (3.1682)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.137 (0.094)	Data 1.13e-04 (5.36e-04)	Tok/s 126935 (113949)	Loss/tok 3.3901 (3.1683)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.073 (0.094)	Data 1.16e-04 (5.31e-04)	Tok/s 106858 (113989)	Loss/tok 2.9178 (3.1690)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.176 (0.094)	Data 1.15e-04 (5.25e-04)	Tok/s 128236 (113950)	Loss/tok 3.3256 (3.1692)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.073 (0.094)	Data 1.18e-04 (5.20e-04)	Tok/s 107421 (113904)	Loss/tok 2.8723 (3.1674)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.104 (0.094)	Data 1.14e-04 (5.15e-04)	Tok/s 121504 (113945)	Loss/tok 3.1393 (3.1677)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.104 (0.094)	Data 1.14e-04 (5.10e-04)	Tok/s 123315 (113933)	Loss/tok 3.1764 (3.1672)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.073 (0.094)	Data 1.14e-04 (5.05e-04)	Tok/s 107103 (113924)	Loss/tok 2.9860 (3.1661)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.072 (0.094)	Data 1.15e-04 (5.00e-04)	Tok/s 108538 (113970)	Loss/tok 2.9711 (3.1662)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.073 (0.094)	Data 1.28e-04 (4.96e-04)	Tok/s 105757 (113948)	Loss/tok 2.9230 (3.1645)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.91e-04)	Tok/s 119431 (114009)	Loss/tok 3.1509 (3.1646)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][840/1291]	Time 0.073 (0.094)	Data 1.16e-04 (4.87e-04)	Tok/s 107023 (113946)	Loss/tok 2.9657 (3.1631)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][850/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.83e-04)	Tok/s 121057 (113991)	Loss/tok 3.1918 (3.1645)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.78e-04)	Tok/s 120941 (114000)	Loss/tok 3.1776 (3.1638)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.042 (0.094)	Data 1.17e-04 (4.74e-04)	Tok/s 93322 (113960)	Loss/tok 2.5473 (3.1625)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.072 (0.094)	Data 1.27e-04 (4.70e-04)	Tok/s 108555 (113951)	Loss/tok 2.9287 (3.1623)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.042 (0.094)	Data 1.15e-04 (4.66e-04)	Tok/s 97950 (113899)	Loss/tok 2.5335 (3.1616)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.073 (0.094)	Data 1.17e-04 (4.62e-04)	Tok/s 105387 (113941)	Loss/tok 3.0016 (3.1615)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.042 (0.094)	Data 1.12e-04 (4.59e-04)	Tok/s 94210 (113942)	Loss/tok 2.6560 (3.1610)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.55e-04)	Tok/s 122414 (114010)	Loss/tok 2.9903 (3.1613)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.104 (0.094)	Data 1.19e-04 (4.51e-04)	Tok/s 120656 (114024)	Loss/tok 3.1941 (3.1616)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.138 (0.094)	Data 1.14e-04 (4.48e-04)	Tok/s 128444 (114039)	Loss/tok 3.2794 (3.1613)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.073 (0.094)	Data 1.23e-04 (4.44e-04)	Tok/s 106102 (114017)	Loss/tok 2.8960 (3.1607)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.41e-04)	Tok/s 106408 (113940)	Loss/tok 2.9337 (3.1592)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.103 (0.094)	Data 1.15e-04 (4.37e-04)	Tok/s 121699 (113935)	Loss/tok 3.0744 (3.1587)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][980/1291]	Time 0.073 (0.094)	Data 1.17e-04 (4.34e-04)	Tok/s 105482 (113926)	Loss/tok 3.0591 (3.1590)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.104 (0.094)	Data 1.15e-04 (4.31e-04)	Tok/s 120542 (113924)	Loss/tok 3.2366 (3.1586)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.042 (0.094)	Data 1.17e-04 (4.28e-04)	Tok/s 90635 (113942)	Loss/tok 2.5179 (3.1582)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.073 (0.094)	Data 1.18e-04 (4.25e-04)	Tok/s 105563 (113929)	Loss/tok 2.9245 (3.1575)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.042 (0.094)	Data 1.18e-04 (4.22e-04)	Tok/s 92047 (113890)	Loss/tok 2.5634 (3.1566)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.073 (0.094)	Data 1.10e-04 (4.19e-04)	Tok/s 108345 (113836)	Loss/tok 2.9376 (3.1553)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.104 (0.094)	Data 1.13e-04 (4.16e-04)	Tok/s 121228 (113794)	Loss/tok 3.1556 (3.1543)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.042 (0.094)	Data 1.17e-04 (4.13e-04)	Tok/s 93697 (113732)	Loss/tok 2.5088 (3.1528)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.073 (0.093)	Data 1.12e-04 (4.10e-04)	Tok/s 104808 (113690)	Loss/tok 2.7879 (3.1515)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.175 (0.094)	Data 1.14e-04 (4.08e-04)	Tok/s 128485 (113719)	Loss/tok 3.4019 (3.1515)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.138 (0.094)	Data 1.15e-04 (4.05e-04)	Tok/s 128224 (113771)	Loss/tok 3.3296 (3.1524)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.104 (0.094)	Data 1.11e-04 (4.02e-04)	Tok/s 122747 (113699)	Loss/tok 3.2070 (3.1509)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.042 (0.094)	Data 1.10e-04 (4.00e-04)	Tok/s 93547 (113734)	Loss/tok 2.6370 (3.1505)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1110/1291]	Time 0.137 (0.094)	Data 1.23e-04 (3.97e-04)	Tok/s 127894 (113759)	Loss/tok 3.2227 (3.1509)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.104 (0.094)	Data 1.25e-04 (3.95e-04)	Tok/s 122043 (113779)	Loss/tok 3.1818 (3.1505)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.073 (0.094)	Data 1.16e-04 (3.92e-04)	Tok/s 107206 (113777)	Loss/tok 3.0087 (3.1497)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.073 (0.094)	Data 1.25e-04 (3.90e-04)	Tok/s 108786 (113773)	Loss/tok 2.8535 (3.1487)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.87e-04)	Tok/s 108769 (113777)	Loss/tok 2.9112 (3.1490)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.073 (0.094)	Data 1.15e-04 (3.85e-04)	Tok/s 105909 (113831)	Loss/tok 2.9349 (3.1491)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.042 (0.094)	Data 1.15e-04 (3.83e-04)	Tok/s 91691 (113846)	Loss/tok 2.3742 (3.1489)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.104 (0.094)	Data 1.25e-04 (3.80e-04)	Tok/s 122903 (113845)	Loss/tok 3.0627 (3.1481)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.137 (0.094)	Data 1.14e-04 (3.78e-04)	Tok/s 126950 (113827)	Loss/tok 3.3445 (3.1478)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.104 (0.094)	Data 1.16e-04 (3.76e-04)	Tok/s 120149 (113846)	Loss/tok 3.1483 (3.1482)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.74e-04)	Tok/s 105039 (113847)	Loss/tok 2.9468 (3.1478)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.104 (0.094)	Data 1.15e-04 (3.72e-04)	Tok/s 121736 (113786)	Loss/tok 3.0825 (3.1473)	LR 7.187e-04
0: TRAIN [3][1230/1291]	Time 0.104 (0.094)	Data 1.14e-04 (3.70e-04)	Tok/s 121422 (113799)	Loss/tok 3.1911 (3.1471)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1240/1291]	Time 0.073 (0.094)	Data 1.12e-04 (3.68e-04)	Tok/s 107586 (113781)	Loss/tok 2.9000 (3.1466)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.104 (0.094)	Data 1.11e-04 (3.66e-04)	Tok/s 122154 (113814)	Loss/tok 3.1024 (3.1461)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.138 (0.094)	Data 1.16e-04 (3.64e-04)	Tok/s 125523 (113853)	Loss/tok 3.2005 (3.1466)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.104 (0.094)	Data 1.18e-04 (3.62e-04)	Tok/s 120158 (113857)	Loss/tok 3.1871 (3.1457)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.073 (0.094)	Data 1.17e-04 (3.60e-04)	Tok/s 106367 (113858)	Loss/tok 2.9262 (3.1454)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.042 (0.094)	Data 5.72e-05 (3.60e-04)	Tok/s 92859 (113833)	Loss/tok 2.5576 (3.1448)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592591049865, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591049865, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.382 (0.382)	Decoder iters 110.0 (110.0)	Tok/s 23384 (23384)
0: Running moses detokenizer
0: BLEU(score=24.01562401622205, counts=[36954, 18467, 10487, 6199], totals=[65028, 62025, 59022, 56024], precisions=[56.82782801254844, 29.77347843611447, 17.767950933550203, 11.064900756818506], bp=1.0, sys_len=65028, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592591050979, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2402, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592591050979, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1531	Test BLEU: 24.02
0: Performance: Epoch: 3	Training: 1819870 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592591050979, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592591050980, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-19 11:24:19 AM
RESULT,RNN_TRANSLATOR,,547,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:19 AM
RESULT,RNN_TRANSLATOR,,547,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:21 AM
RESULT,RNN_TRANSLATOR,,549,nvidia,2020-06-19 11:15:12 AM
ENDING TIMING RUN AT 2020-06-19 11:24:22 AM
RESULT,RNN_TRANSLATOR,,550,nvidia,2020-06-19 11:15:12 AM
