+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592453789409, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592453789449, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592453789450, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592453789450, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592453789450, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0052
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592453795600, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/13842443/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ LR=2.875e-3
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TRAIN_BATCH_SIZE=384
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=128
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=506
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ NUMEPOCHS=8
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ declare -a CMD
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:37 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592453799692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453799931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1485482203
:::MLLOG {"namespace": "", "time_ms": 1592453808280, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1485482203, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3138246199
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592453822425, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592453822426, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592453822426, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592453822426, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592453822426, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592453824080, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592453824080, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592453824080, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592453824365, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592453824366, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592453824366, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592453824366, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592453824366, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592453824366, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592453824367, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592453824367, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592453824367, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592453824367, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592453824367, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453824367, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1287439625
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.315 (0.315)	Data 2.37e-01 (2.37e-01)	Tok/s 48642 (48642)	Loss/tok 10.6144 (10.6144)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.133 (0.114)	Data 1.07e-04 (2.16e-02)	Tok/s 265743 (228257)	Loss/tok 9.5985 (9.9710)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.098 (0.099)	Data 1.12e-04 (1.14e-02)	Tok/s 259300 (236544)	Loss/tok 9.1608 (9.6614)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.172 (0.101)	Data 1.16e-04 (7.75e-03)	Tok/s 259279 (240048)	Loss/tok 9.0423 (9.4376)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.036 (0.096)	Data 1.10e-04 (5.89e-03)	Tok/s 224991 (240004)	Loss/tok 8.1989 (9.2836)	LR 7.390e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][50/1291]	Time 0.035 (0.091)	Data 1.09e-04 (4.76e-03)	Tok/s 225606 (239888)	Loss/tok 8.0807 (9.1489)	LR 9.092e-05
0: TRAIN [0][60/1291]	Time 0.066 (0.089)	Data 1.14e-04 (4.00e-03)	Tok/s 236110 (240543)	Loss/tok 8.1916 (9.0085)	LR 1.145e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][70/1291]	Time 0.098 (0.088)	Data 1.12e-04 (3.45e-03)	Tok/s 254325 (240660)	Loss/tok 9.1206 (8.9824)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.099 (0.088)	Data 1.10e-04 (3.04e-03)	Tok/s 257183 (241157)	Loss/tok 8.0546 (8.8931)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.098 (0.088)	Data 1.15e-04 (2.72e-03)	Tok/s 254893 (241758)	Loss/tok 7.9809 (8.7916)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.066 (0.088)	Data 1.12e-04 (2.46e-03)	Tok/s 235453 (242404)	Loss/tok 7.7378 (8.6977)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.066 (0.087)	Data 1.09e-04 (2.25e-03)	Tok/s 237583 (242061)	Loss/tok 7.7864 (8.6366)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.088)	Data 1.13e-04 (2.07e-03)	Tok/s 230212 (242488)	Loss/tok 7.8047 (8.5711)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.098 (0.087)	Data 1.12e-04 (1.92e-03)	Tok/s 253375 (242591)	Loss/tok 7.8149 (8.5191)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.134 (0.088)	Data 1.15e-04 (1.79e-03)	Tok/s 260460 (242835)	Loss/tok 7.8785 (8.4630)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.066 (0.087)	Data 1.20e-04 (1.68e-03)	Tok/s 237270 (243148)	Loss/tok 7.5374 (8.4099)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.087)	Data 1.11e-04 (1.58e-03)	Tok/s 232148 (243092)	Loss/tok 7.2420 (8.3564)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.066 (0.087)	Data 1.10e-04 (1.50e-03)	Tok/s 236932 (243003)	Loss/tok 7.1102 (8.3012)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.066 (0.087)	Data 1.32e-04 (1.42e-03)	Tok/s 230952 (243104)	Loss/tok 6.9536 (8.2408)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.098 (0.086)	Data 1.14e-04 (1.35e-03)	Tok/s 255974 (242889)	Loss/tok 7.0315 (8.1859)	LR 2.181e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][200/1291]	Time 0.066 (0.086)	Data 1.12e-04 (1.29e-03)	Tok/s 234262 (242981)	Loss/tok 6.7195 (8.1222)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.099 (0.086)	Data 1.12e-04 (1.24e-03)	Tok/s 253383 (243179)	Loss/tok 6.7470 (8.0544)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.099 (0.087)	Data 1.14e-04 (1.18e-03)	Tok/s 253542 (243225)	Loss/tok 6.6222 (7.9854)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.098 (0.087)	Data 1.11e-04 (1.14e-03)	Tok/s 254339 (243441)	Loss/tok 6.5057 (7.9168)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.087)	Data 1.10e-04 (1.10e-03)	Tok/s 233559 (243680)	Loss/tok 5.9983 (7.8438)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.066 (0.087)	Data 1.07e-04 (1.06e-03)	Tok/s 237591 (243713)	Loss/tok 5.9724 (7.7820)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.088)	Data 1.13e-04 (1.02e-03)	Tok/s 236273 (243766)	Loss/tok 5.8021 (7.7144)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.066 (0.087)	Data 1.12e-04 (9.88e-04)	Tok/s 235146 (243452)	Loss/tok 5.6212 (7.6590)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.035 (0.087)	Data 1.11e-04 (9.57e-04)	Tok/s 223583 (243451)	Loss/tok 4.5825 (7.5931)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.098 (0.088)	Data 1.18e-04 (9.28e-04)	Tok/s 255518 (243590)	Loss/tok 5.7901 (7.5185)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.066 (0.087)	Data 1.14e-04 (9.01e-04)	Tok/s 236299 (243608)	Loss/tok 5.3773 (7.4576)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.066 (0.087)	Data 1.11e-04 (8.76e-04)	Tok/s 231861 (243544)	Loss/tok 5.2558 (7.4011)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.066 (0.087)	Data 1.11e-04 (8.52e-04)	Tok/s 237600 (243505)	Loss/tok 5.0971 (7.3406)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][330/1291]	Time 0.066 (0.087)	Data 1.16e-04 (8.29e-04)	Tok/s 234078 (243640)	Loss/tok 4.9593 (7.2750)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.099 (0.087)	Data 1.13e-04 (8.09e-04)	Tok/s 253210 (243687)	Loss/tok 5.1541 (7.2132)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.134 (0.087)	Data 1.13e-04 (7.89e-04)	Tok/s 260585 (243847)	Loss/tok 5.2206 (7.1436)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.099 (0.087)	Data 1.17e-04 (7.70e-04)	Tok/s 255258 (243947)	Loss/tok 4.8969 (7.0803)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.066 (0.087)	Data 1.11e-04 (7.52e-04)	Tok/s 235787 (243840)	Loss/tok 4.4840 (7.0251)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.099 (0.087)	Data 1.15e-04 (7.36e-04)	Tok/s 255424 (243870)	Loss/tok 4.6664 (6.9638)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.066 (0.087)	Data 1.12e-04 (7.20e-04)	Tok/s 231457 (244037)	Loss/tok 4.3694 (6.8985)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.066 (0.087)	Data 1.10e-04 (7.05e-04)	Tok/s 230626 (243869)	Loss/tok 4.3454 (6.8461)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.134 (0.088)	Data 1.11e-04 (6.90e-04)	Tok/s 262138 (244065)	Loss/tok 4.7691 (6.7823)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.066 (0.087)	Data 1.10e-04 (6.76e-04)	Tok/s 236165 (243939)	Loss/tok 4.1871 (6.7353)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.066 (0.087)	Data 1.24e-04 (6.63e-04)	Tok/s 233503 (243936)	Loss/tok 4.2026 (6.6796)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.133 (0.088)	Data 1.10e-04 (6.51e-04)	Tok/s 262217 (244008)	Loss/tok 4.7505 (6.6279)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.099 (0.088)	Data 1.15e-04 (6.39e-04)	Tok/s 254250 (243942)	Loss/tok 4.3704 (6.5784)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][460/1291]	Time 0.099 (0.088)	Data 1.13e-04 (6.28e-04)	Tok/s 256245 (243979)	Loss/tok 4.3991 (6.5271)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][470/1291]	Time 0.035 (0.088)	Data 1.12e-04 (6.17e-04)	Tok/s 225153 (244087)	Loss/tok 3.4135 (6.4709)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.099 (0.088)	Data 1.10e-04 (6.07e-04)	Tok/s 257016 (244027)	Loss/tok 4.2060 (6.4300)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.099 (0.088)	Data 1.12e-04 (5.97e-04)	Tok/s 254907 (244072)	Loss/tok 4.2946 (6.3832)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.098 (0.088)	Data 1.11e-04 (5.87e-04)	Tok/s 253227 (244178)	Loss/tok 4.2202 (6.3345)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.066 (0.088)	Data 1.08e-04 (5.78e-04)	Tok/s 236664 (244242)	Loss/tok 3.8374 (6.2897)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.088)	Data 1.09e-04 (5.69e-04)	Tok/s 237589 (244184)	Loss/tok 3.8285 (6.2526)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.173 (0.089)	Data 1.12e-04 (5.60e-04)	Tok/s 258303 (244345)	Loss/tok 4.5955 (6.2053)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.035 (0.088)	Data 1.12e-04 (5.52e-04)	Tok/s 229229 (244318)	Loss/tok 3.3380 (6.1708)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.066 (0.088)	Data 1.19e-04 (5.44e-04)	Tok/s 236175 (244237)	Loss/tok 3.8831 (6.1371)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.098 (0.088)	Data 1.18e-04 (5.36e-04)	Tok/s 256353 (244272)	Loss/tok 4.1973 (6.1014)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.098 (0.088)	Data 1.08e-04 (5.29e-04)	Tok/s 256094 (244293)	Loss/tok 4.1283 (6.0673)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.066 (0.088)	Data 1.12e-04 (5.22e-04)	Tok/s 236705 (244268)	Loss/tok 3.7365 (6.0328)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.035 (0.088)	Data 1.17e-04 (5.15e-04)	Tok/s 224917 (244320)	Loss/tok 3.1504 (5.9973)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][600/1291]	Time 0.035 (0.088)	Data 1.12e-04 (5.08e-04)	Tok/s 226610 (244255)	Loss/tok 3.1154 (5.9665)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.099 (0.088)	Data 1.10e-04 (5.02e-04)	Tok/s 253989 (244205)	Loss/tok 4.0728 (5.9368)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.067 (0.088)	Data 1.12e-04 (4.95e-04)	Tok/s 237276 (244148)	Loss/tok 3.7037 (5.9082)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.134 (0.088)	Data 1.12e-04 (4.89e-04)	Tok/s 260831 (244208)	Loss/tok 4.1584 (5.8744)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.83e-04)	Tok/s 231080 (244205)	Loss/tok 3.7228 (5.8438)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.066 (0.088)	Data 1.12e-04 (4.78e-04)	Tok/s 234322 (244178)	Loss/tok 3.6204 (5.8155)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.066 (0.088)	Data 1.10e-04 (4.72e-04)	Tok/s 230914 (244154)	Loss/tok 3.7239 (5.7876)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.172 (0.088)	Data 1.11e-04 (4.67e-04)	Tok/s 257717 (244198)	Loss/tok 4.3539 (5.7572)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.098 (0.088)	Data 1.24e-04 (4.62e-04)	Tok/s 255899 (244220)	Loss/tok 3.9756 (5.7295)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.134 (0.088)	Data 1.11e-04 (4.57e-04)	Tok/s 262926 (244251)	Loss/tok 4.0539 (5.7017)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.066 (0.088)	Data 1.25e-04 (4.52e-04)	Tok/s 235122 (244176)	Loss/tok 3.6272 (5.6794)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.47e-04)	Tok/s 233997 (244186)	Loss/tok 3.5805 (5.6522)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.066 (0.088)	Data 1.11e-04 (4.42e-04)	Tok/s 233015 (244164)	Loss/tok 3.6298 (5.6282)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][730/1291]	Time 0.134 (0.088)	Data 1.12e-04 (4.38e-04)	Tok/s 260351 (244103)	Loss/tok 4.1275 (5.6055)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.067 (0.088)	Data 1.13e-04 (4.34e-04)	Tok/s 232146 (244216)	Loss/tok 3.6888 (5.5774)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.035 (0.088)	Data 1.17e-04 (4.29e-04)	Tok/s 226212 (244190)	Loss/tok 3.0468 (5.5549)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.067 (0.088)	Data 1.14e-04 (4.25e-04)	Tok/s 229741 (244218)	Loss/tok 3.6121 (5.5315)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.099 (0.088)	Data 1.19e-04 (4.21e-04)	Tok/s 251244 (244248)	Loss/tok 3.8236 (5.5091)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.135 (0.089)	Data 1.16e-04 (4.17e-04)	Tok/s 259294 (244307)	Loss/tok 4.0715 (5.4860)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.066 (0.089)	Data 1.12e-04 (4.13e-04)	Tok/s 232397 (244316)	Loss/tok 3.5633 (5.4644)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.100 (0.088)	Data 1.12e-04 (4.09e-04)	Tok/s 251796 (244287)	Loss/tok 3.9197 (5.4456)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.099 (0.088)	Data 1.15e-04 (4.06e-04)	Tok/s 254988 (244280)	Loss/tok 3.8866 (5.4262)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.099 (0.088)	Data 1.13e-04 (4.02e-04)	Tok/s 251844 (244216)	Loss/tok 3.8594 (5.4080)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.99e-04)	Tok/s 256921 (244287)	Loss/tok 3.7199 (5.3855)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.067 (0.088)	Data 1.14e-04 (3.95e-04)	Tok/s 229754 (244312)	Loss/tok 3.6187 (5.3652)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.92e-04)	Tok/s 233510 (244302)	Loss/tok 3.4509 (5.3469)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][860/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.89e-04)	Tok/s 232876 (244277)	Loss/tok 3.5470 (5.3288)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][870/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.86e-04)	Tok/s 252528 (244296)	Loss/tok 3.8112 (5.3103)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.83e-04)	Tok/s 235185 (244336)	Loss/tok 3.5926 (5.2911)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.80e-04)	Tok/s 255835 (244289)	Loss/tok 3.7664 (5.2750)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.066 (0.088)	Data 1.08e-04 (3.77e-04)	Tok/s 233540 (244222)	Loss/tok 3.5079 (5.2598)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.74e-04)	Tok/s 256787 (244187)	Loss/tok 3.7813 (5.2445)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.71e-04)	Tok/s 220850 (244175)	Loss/tok 2.9801 (5.2286)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.68e-04)	Tok/s 233266 (244118)	Loss/tok 3.4663 (5.2136)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.067 (0.088)	Data 1.14e-04 (3.66e-04)	Tok/s 232914 (244102)	Loss/tok 3.4248 (5.1982)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.63e-04)	Tok/s 254385 (244138)	Loss/tok 3.8305 (5.1811)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.036 (0.088)	Data 1.15e-04 (3.60e-04)	Tok/s 222752 (244149)	Loss/tok 2.9262 (5.1660)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.099 (0.088)	Data 1.17e-04 (3.58e-04)	Tok/s 251970 (244164)	Loss/tok 3.6690 (5.1501)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.174 (0.088)	Data 1.16e-04 (3.55e-04)	Tok/s 256569 (244186)	Loss/tok 4.0285 (5.1339)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.067 (0.088)	Data 1.11e-04 (3.53e-04)	Tok/s 231889 (244159)	Loss/tok 3.4799 (5.1193)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1000/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.51e-04)	Tok/s 232427 (244141)	Loss/tok 3.4464 (5.1053)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1010/1291]	Time 0.100 (0.088)	Data 1.13e-04 (3.48e-04)	Tok/s 252185 (244161)	Loss/tok 3.6831 (5.0910)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.46e-04)	Tok/s 254281 (244164)	Loss/tok 3.6276 (5.0768)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.099 (0.088)	Data 1.14e-04 (3.44e-04)	Tok/s 253308 (244193)	Loss/tok 3.6190 (5.0628)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.066 (0.088)	Data 1.25e-04 (3.41e-04)	Tok/s 233686 (244176)	Loss/tok 3.4690 (5.0497)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.100 (0.088)	Data 1.13e-04 (3.39e-04)	Tok/s 252037 (244207)	Loss/tok 3.6434 (5.0352)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.37e-04)	Tok/s 233417 (244153)	Loss/tok 3.3364 (5.0237)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.036 (0.088)	Data 1.10e-04 (3.35e-04)	Tok/s 224581 (244198)	Loss/tok 3.0044 (5.0101)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.066 (0.088)	Data 1.09e-04 (3.33e-04)	Tok/s 235162 (244161)	Loss/tok 3.4480 (4.9987)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.174 (0.088)	Data 1.13e-04 (3.31e-04)	Tok/s 257931 (244206)	Loss/tok 3.9401 (4.9848)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.29e-04)	Tok/s 256328 (244183)	Loss/tok 3.6219 (4.9734)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.27e-04)	Tok/s 252467 (244112)	Loss/tok 3.7649 (4.9632)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.25e-04)	Tok/s 254926 (244157)	Loss/tok 3.6518 (4.9502)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1130/1291]	Time 0.067 (0.088)	Data 1.29e-04 (3.23e-04)	Tok/s 233418 (244163)	Loss/tok 3.3985 (4.9383)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.21e-04)	Tok/s 254446 (244129)	Loss/tok 3.5276 (4.9272)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.100 (0.088)	Data 1.15e-04 (3.20e-04)	Tok/s 253804 (244196)	Loss/tok 3.7293 (4.9141)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.134 (0.088)	Data 1.10e-04 (3.18e-04)	Tok/s 263515 (244236)	Loss/tok 3.8665 (4.9019)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.174 (0.088)	Data 1.12e-04 (3.16e-04)	Tok/s 258447 (244232)	Loss/tok 4.0284 (4.8908)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.173 (0.088)	Data 1.14e-04 (3.14e-04)	Tok/s 259824 (244255)	Loss/tok 3.9367 (4.8790)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.13e-04)	Tok/s 253076 (244235)	Loss/tok 3.6324 (4.8683)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.174 (0.088)	Data 1.32e-04 (3.11e-04)	Tok/s 257830 (244216)	Loss/tok 3.9085 (4.8574)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.099 (0.088)	Data 1.05e-04 (3.09e-04)	Tok/s 255581 (244223)	Loss/tok 3.6238 (4.8467)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.066 (0.088)	Data 1.07e-04 (3.08e-04)	Tok/s 239377 (244156)	Loss/tok 3.3721 (4.8378)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.036 (0.088)	Data 1.02e-04 (3.06e-04)	Tok/s 223869 (244140)	Loss/tok 2.8412 (4.8279)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.135 (0.088)	Data 1.08e-04 (3.05e-04)	Tok/s 259168 (244145)	Loss/tok 3.7905 (4.8179)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.135 (0.088)	Data 1.05e-04 (3.03e-04)	Tok/s 257392 (244171)	Loss/tok 3.7951 (4.8070)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1260/1291]	Time 0.099 (0.088)	Data 1.04e-04 (3.01e-04)	Tok/s 256933 (244139)	Loss/tok 3.5664 (4.7979)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.100 (0.088)	Data 1.02e-04 (3.00e-04)	Tok/s 252427 (244119)	Loss/tok 3.4996 (4.7884)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1280/1291]	Time 0.134 (0.088)	Data 1.15e-04 (2.98e-04)	Tok/s 261356 (244090)	Loss/tok 3.8055 (4.7796)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.098 (0.088)	Data 4.58e-05 (2.99e-04)	Tok/s 255652 (244072)	Loss/tok 3.6588 (4.7701)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592453939038, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453939038, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.506 (0.506)	Decoder iters 149.0 (149.0)	Tok/s 32473 (32473)
0: Running moses detokenizer
0: BLEU(score=19.44060039611277, counts=[34574, 15680, 8291, 4560], totals=[66141, 63138, 60136, 57139], precisions=[52.27317397680712, 24.83448953086889, 13.787082612744445, 7.98053868636133], bp=1.0, sys_len=66141, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592453941013, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19440000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453941013, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7717	Test BLEU: 19.44
0: Performance: Epoch: 0	Training: 1952306 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592453941013, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453941013, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453941013, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3845219382
0: TRAIN [1][0/1291]	Time 0.267 (0.267)	Data 1.92e-01 (1.92e-01)	Tok/s 58606 (58606)	Loss/tok 3.1816 (3.1816)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.099 (0.088)	Data 1.11e-04 (1.75e-02)	Tok/s 253964 (222365)	Loss/tok 3.4870 (3.3169)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.100 (0.092)	Data 1.13e-04 (9.24e-03)	Tok/s 253319 (234129)	Loss/tok 3.5149 (3.4328)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.066 (0.091)	Data 1.26e-04 (6.30e-03)	Tok/s 238712 (237535)	Loss/tok 3.2759 (3.4552)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.035 (0.092)	Data 1.13e-04 (4.79e-03)	Tok/s 220322 (239515)	Loss/tok 2.8467 (3.4787)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.095)	Data 1.18e-04 (3.87e-03)	Tok/s 233134 (241261)	Loss/tok 3.3123 (3.5138)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.067 (0.094)	Data 1.14e-04 (3.26e-03)	Tok/s 232840 (241424)	Loss/tok 3.2605 (3.5163)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.035 (0.096)	Data 1.10e-04 (2.81e-03)	Tok/s 222838 (242343)	Loss/tok 2.8184 (3.5358)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.099 (0.097)	Data 1.11e-04 (2.48e-03)	Tok/s 254468 (243239)	Loss/tok 3.5146 (3.5478)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.067 (0.094)	Data 1.11e-04 (2.22e-03)	Tok/s 227147 (242171)	Loss/tok 3.3110 (3.5362)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.067 (0.095)	Data 1.10e-04 (2.01e-03)	Tok/s 228433 (242514)	Loss/tok 3.2664 (3.5448)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.174 (0.095)	Data 1.10e-04 (1.84e-03)	Tok/s 254876 (242734)	Loss/tok 3.8935 (3.5441)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][120/1291]	Time 0.100 (0.096)	Data 1.42e-04 (1.70e-03)	Tok/s 253215 (243365)	Loss/tok 3.4800 (3.5488)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.100 (0.095)	Data 1.09e-04 (1.58e-03)	Tok/s 251472 (243391)	Loss/tok 3.5938 (3.5464)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.099 (0.094)	Data 1.10e-04 (1.48e-03)	Tok/s 255488 (243228)	Loss/tok 3.5458 (3.5420)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.035 (0.093)	Data 1.13e-04 (1.39e-03)	Tok/s 220302 (242768)	Loss/tok 2.7600 (3.5320)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.067 (0.092)	Data 1.08e-04 (1.31e-03)	Tok/s 231676 (242612)	Loss/tok 3.2829 (3.5273)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.066 (0.092)	Data 1.12e-04 (1.24e-03)	Tok/s 233598 (242564)	Loss/tok 3.2445 (3.5238)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.067 (0.091)	Data 1.13e-04 (1.18e-03)	Tok/s 232866 (242375)	Loss/tok 3.2265 (3.5210)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.035 (0.091)	Data 1.10e-04 (1.12e-03)	Tok/s 223398 (242286)	Loss/tok 2.8194 (3.5145)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.100 (0.090)	Data 1.12e-04 (1.07e-03)	Tok/s 248618 (242059)	Loss/tok 3.4229 (3.5083)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.066 (0.090)	Data 1.12e-04 (1.03e-03)	Tok/s 237170 (242307)	Loss/tok 3.2779 (3.5104)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.067 (0.090)	Data 1.10e-04 (9.84e-04)	Tok/s 229982 (242166)	Loss/tok 3.3061 (3.5062)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][230/1291]	Time 0.066 (0.090)	Data 1.14e-04 (9.46e-04)	Tok/s 238256 (242257)	Loss/tok 3.1871 (3.5074)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.134 (0.090)	Data 1.09e-04 (9.12e-04)	Tok/s 258459 (242369)	Loss/tok 3.6931 (3.5043)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.099 (0.090)	Data 1.16e-04 (8.80e-04)	Tok/s 253704 (242509)	Loss/tok 3.4641 (3.5025)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.066 (0.090)	Data 1.13e-04 (8.50e-04)	Tok/s 234492 (242696)	Loss/tok 3.2374 (3.5063)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.066 (0.091)	Data 1.34e-04 (8.23e-04)	Tok/s 232761 (242940)	Loss/tok 3.2555 (3.5058)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.101 (0.090)	Data 1.11e-04 (7.98e-04)	Tok/s 247484 (242473)	Loss/tok 3.5242 (3.5010)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.101 (0.090)	Data 1.13e-04 (7.74e-04)	Tok/s 251374 (242514)	Loss/tok 3.4593 (3.5018)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.068 (0.090)	Data 1.13e-04 (7.52e-04)	Tok/s 225647 (242325)	Loss/tok 3.2930 (3.5020)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.136 (0.091)	Data 1.09e-04 (7.32e-04)	Tok/s 256539 (242416)	Loss/tok 3.7896 (3.5024)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.037 (0.090)	Data 1.13e-04 (7.12e-04)	Tok/s 212686 (242217)	Loss/tok 2.7547 (3.4995)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.035 (0.091)	Data 1.16e-04 (6.94e-04)	Tok/s 227468 (242224)	Loss/tok 2.8616 (3.5013)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.174 (0.091)	Data 1.10e-04 (6.77e-04)	Tok/s 257011 (242443)	Loss/tok 3.7482 (3.5020)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.099 (0.091)	Data 1.11e-04 (6.61e-04)	Tok/s 253989 (242544)	Loss/tok 3.4933 (3.5016)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][360/1291]	Time 0.135 (0.091)	Data 1.11e-04 (6.46e-04)	Tok/s 259736 (242603)	Loss/tok 3.5676 (3.4994)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.135 (0.091)	Data 1.12e-04 (6.31e-04)	Tok/s 259306 (242592)	Loss/tok 3.7033 (3.4970)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.066 (0.090)	Data 1.09e-04 (6.18e-04)	Tok/s 235890 (242533)	Loss/tok 3.2493 (3.4941)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.066 (0.090)	Data 1.09e-04 (6.05e-04)	Tok/s 236894 (242533)	Loss/tok 3.2059 (3.4932)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.099 (0.091)	Data 1.12e-04 (5.93e-04)	Tok/s 255773 (242635)	Loss/tok 3.4620 (3.4940)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.066 (0.091)	Data 1.12e-04 (5.81e-04)	Tok/s 230854 (242693)	Loss/tok 3.2390 (3.4934)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.091)	Data 1.11e-04 (5.70e-04)	Tok/s 229659 (242851)	Loss/tok 3.2282 (3.4938)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.099 (0.091)	Data 1.12e-04 (5.59e-04)	Tok/s 256650 (242873)	Loss/tok 3.4203 (3.4915)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.135 (0.091)	Data 1.13e-04 (5.49e-04)	Tok/s 260058 (243082)	Loss/tok 3.5828 (3.4922)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.066 (0.091)	Data 1.10e-04 (5.39e-04)	Tok/s 231466 (243113)	Loss/tok 3.3386 (3.4908)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.067 (0.091)	Data 1.08e-04 (5.30e-04)	Tok/s 229524 (243044)	Loss/tok 3.2136 (3.4880)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.066 (0.090)	Data 1.11e-04 (5.21e-04)	Tok/s 230911 (242917)	Loss/tok 3.2137 (3.4849)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.13e-04)	Tok/s 234318 (242989)	Loss/tok 3.1980 (3.4847)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][490/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.05e-04)	Tok/s 255852 (242974)	Loss/tok 3.4807 (3.4842)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][500/1291]	Time 0.066 (0.090)	Data 1.09e-04 (4.97e-04)	Tok/s 232748 (242892)	Loss/tok 3.1524 (3.4833)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.89e-04)	Tok/s 234197 (242878)	Loss/tok 3.2259 (3.4824)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.136 (0.090)	Data 1.11e-04 (4.82e-04)	Tok/s 258511 (242879)	Loss/tok 3.6485 (3.4823)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][530/1291]	Time 0.100 (0.090)	Data 1.28e-04 (4.75e-04)	Tok/s 251915 (242905)	Loss/tok 3.4235 (3.4816)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.066 (0.090)	Data 1.14e-04 (4.69e-04)	Tok/s 231785 (242807)	Loss/tok 3.1621 (3.4794)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.100 (0.090)	Data 1.12e-04 (4.62e-04)	Tok/s 252554 (242829)	Loss/tok 3.5200 (3.4778)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.099 (0.090)	Data 1.08e-04 (4.56e-04)	Tok/s 257595 (242921)	Loss/tok 3.3495 (3.4781)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.50e-04)	Tok/s 253885 (242890)	Loss/tok 3.3808 (3.4757)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.066 (0.090)	Data 1.13e-04 (4.44e-04)	Tok/s 233808 (242834)	Loss/tok 3.1291 (3.4741)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.100 (0.090)	Data 1.11e-04 (4.38e-04)	Tok/s 250821 (242707)	Loss/tok 3.4319 (3.4716)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.173 (0.090)	Data 1.15e-04 (4.33e-04)	Tok/s 258862 (242679)	Loss/tok 3.8012 (3.4721)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.28e-04)	Tok/s 231113 (242629)	Loss/tok 3.2002 (3.4700)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.23e-04)	Tok/s 250724 (242557)	Loss/tok 3.4519 (3.4682)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.136 (0.090)	Data 1.14e-04 (4.18e-04)	Tok/s 257693 (242721)	Loss/tok 3.6792 (3.4686)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.066 (0.090)	Data 1.10e-04 (4.13e-04)	Tok/s 230481 (242722)	Loss/tok 3.2093 (3.4678)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][650/1291]	Time 0.099 (0.090)	Data 1.12e-04 (4.08e-04)	Tok/s 254151 (242756)	Loss/tok 3.4197 (3.4668)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.066 (0.089)	Data 1.10e-04 (4.04e-04)	Tok/s 235786 (242667)	Loss/tok 3.1320 (3.4648)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.100 (0.089)	Data 1.09e-04 (4.00e-04)	Tok/s 253724 (242704)	Loss/tok 3.4396 (3.4644)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.95e-04)	Tok/s 257266 (242714)	Loss/tok 3.4080 (3.4652)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.91e-04)	Tok/s 229816 (242646)	Loss/tok 3.2062 (3.4635)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.87e-04)	Tok/s 254584 (242553)	Loss/tok 3.4291 (3.4625)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][710/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.83e-04)	Tok/s 234932 (242636)	Loss/tok 3.1350 (3.4625)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.80e-04)	Tok/s 259823 (242727)	Loss/tok 3.5964 (3.4627)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.76e-04)	Tok/s 236046 (242753)	Loss/tok 3.1840 (3.4618)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.134 (0.089)	Data 1.12e-04 (3.72e-04)	Tok/s 259930 (242772)	Loss/tok 3.5952 (3.4619)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.69e-04)	Tok/s 254623 (242783)	Loss/tok 3.4271 (3.4615)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.090)	Data 1.20e-04 (3.66e-04)	Tok/s 230699 (242865)	Loss/tok 3.1893 (3.4646)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.090)	Data 1.10e-04 (3.62e-04)	Tok/s 241151 (242923)	Loss/tok 3.1467 (3.4647)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.173 (0.090)	Data 1.11e-04 (3.59e-04)	Tok/s 256431 (242913)	Loss/tok 3.8655 (3.4649)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.099 (0.090)	Data 1.13e-04 (3.56e-04)	Tok/s 252785 (242936)	Loss/tok 3.3736 (3.4649)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.174 (0.090)	Data 1.11e-04 (3.53e-04)	Tok/s 257074 (242916)	Loss/tok 3.8678 (3.4639)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.50e-04)	Tok/s 230670 (242907)	Loss/tok 3.1819 (3.4627)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.066 (0.090)	Data 1.12e-04 (3.47e-04)	Tok/s 233747 (242899)	Loss/tok 3.1339 (3.4609)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.099 (0.090)	Data 1.11e-04 (3.44e-04)	Tok/s 253241 (242870)	Loss/tok 3.3472 (3.4592)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][840/1291]	Time 0.134 (0.090)	Data 1.13e-04 (3.42e-04)	Tok/s 258890 (242912)	Loss/tok 3.5620 (3.4587)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][850/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.39e-04)	Tok/s 251383 (242937)	Loss/tok 3.4411 (3.4584)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.36e-04)	Tok/s 231065 (242961)	Loss/tok 3.1154 (3.4583)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.099 (0.090)	Data 1.09e-04 (3.34e-04)	Tok/s 252891 (242961)	Loss/tok 3.4734 (3.4570)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.035 (0.090)	Data 1.08e-04 (3.31e-04)	Tok/s 222911 (242990)	Loss/tok 2.6803 (3.4563)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.135 (0.090)	Data 1.10e-04 (3.29e-04)	Tok/s 259393 (242964)	Loss/tok 3.6523 (3.4548)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.26e-04)	Tok/s 235265 (242862)	Loss/tok 3.2019 (3.4530)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.24e-04)	Tok/s 231552 (242791)	Loss/tok 3.2265 (3.4515)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.22e-04)	Tok/s 236294 (242770)	Loss/tok 3.1859 (3.4497)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.19e-04)	Tok/s 253331 (242790)	Loss/tok 3.3920 (3.4497)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.173 (0.089)	Data 1.10e-04 (3.17e-04)	Tok/s 259992 (242851)	Loss/tok 3.7663 (3.4503)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.099 (0.089)	Data 1.23e-04 (3.15e-04)	Tok/s 254802 (242933)	Loss/tok 3.3131 (3.4495)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.13e-04)	Tok/s 255593 (242941)	Loss/tok 3.4554 (3.4485)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.11e-04)	Tok/s 236715 (242981)	Loss/tok 3.2824 (3.4479)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][980/1291]	Time 0.174 (0.090)	Data 1.13e-04 (3.09e-04)	Tok/s 257723 (243011)	Loss/tok 3.7830 (3.4492)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][990/1291]	Time 0.066 (0.090)	Data 1.26e-04 (3.07e-04)	Tok/s 234568 (243011)	Loss/tok 3.1050 (3.4492)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.05e-04)	Tok/s 232069 (243059)	Loss/tok 3.1397 (3.4486)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.134 (0.090)	Data 1.12e-04 (3.03e-04)	Tok/s 258911 (243061)	Loss/tok 3.5893 (3.4479)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.066 (0.090)	Data 1.12e-04 (3.01e-04)	Tok/s 231723 (243031)	Loss/tok 3.1834 (3.4466)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.067 (0.090)	Data 1.12e-04 (2.99e-04)	Tok/s 234733 (243031)	Loss/tok 3.1926 (3.4466)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.135 (0.090)	Data 1.11e-04 (2.98e-04)	Tok/s 260948 (243028)	Loss/tok 3.5250 (3.4464)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.96e-04)	Tok/s 221637 (242979)	Loss/tok 2.7218 (3.4449)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.94e-04)	Tok/s 231641 (242920)	Loss/tok 3.1148 (3.4431)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.92e-04)	Tok/s 232442 (242925)	Loss/tok 3.0536 (3.4427)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.91e-04)	Tok/s 248622 (242939)	Loss/tok 3.3948 (3.4421)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.099 (0.090)	Data 1.10e-04 (2.89e-04)	Tok/s 257194 (242982)	Loss/tok 3.4406 (3.4429)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.067 (0.090)	Data 1.14e-04 (2.88e-04)	Tok/s 228133 (242977)	Loss/tok 3.1016 (3.4421)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1110/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.86e-04)	Tok/s 222239 (242902)	Loss/tok 2.7355 (3.4404)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.84e-04)	Tok/s 232245 (242858)	Loss/tok 3.1049 (3.4391)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.83e-04)	Tok/s 235051 (242845)	Loss/tok 3.1714 (3.4379)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.81e-04)	Tok/s 253813 (242790)	Loss/tok 3.3342 (3.4364)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.80e-04)	Tok/s 254765 (242815)	Loss/tok 3.3966 (3.4367)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.136 (0.089)	Data 1.12e-04 (2.79e-04)	Tok/s 259004 (242818)	Loss/tok 3.5955 (3.4361)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1170/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.77e-04)	Tok/s 232562 (242841)	Loss/tok 3.1539 (3.4366)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.76e-04)	Tok/s 231739 (242814)	Loss/tok 3.2025 (3.4356)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.100 (0.089)	Data 1.19e-04 (2.74e-04)	Tok/s 249523 (242798)	Loss/tok 3.3883 (3.4342)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.035 (0.089)	Data 1.11e-04 (2.73e-04)	Tok/s 218561 (242832)	Loss/tok 2.7105 (3.4336)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.135 (0.089)	Data 1.16e-04 (2.72e-04)	Tok/s 262760 (242804)	Loss/tok 3.5566 (3.4333)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.135 (0.089)	Data 1.16e-04 (2.70e-04)	Tok/s 259744 (242843)	Loss/tok 3.5404 (3.4332)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.69e-04)	Tok/s 234513 (242793)	Loss/tok 3.1392 (3.4317)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.036 (0.089)	Data 1.13e-04 (2.68e-04)	Tok/s 226283 (242772)	Loss/tok 2.6640 (3.4310)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.67e-04)	Tok/s 233277 (242710)	Loss/tok 3.1451 (3.4297)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.65e-04)	Tok/s 232466 (242707)	Loss/tok 3.1523 (3.4295)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.035 (0.089)	Data 1.11e-04 (2.64e-04)	Tok/s 223709 (242666)	Loss/tok 2.6305 (3.4284)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.63e-04)	Tok/s 254749 (242736)	Loss/tok 3.3375 (3.4281)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.066 (0.089)	Data 4.39e-05 (2.65e-04)	Tok/s 232311 (242761)	Loss/tok 3.1416 (3.4278)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592454056346, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454056346, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.475 (0.475)	Decoder iters 149.0 (149.0)	Tok/s 34860 (34860)
0: Running moses detokenizer
0: BLEU(score=21.98806690034008, counts=[35788, 17113, 9460, 5461], totals=[65251, 62248, 59246, 56248], precisions=[54.84666901656679, 27.49164631795399, 15.96732268845154, 9.708789645854075], bp=1.0, sys_len=65251, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454058289, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21989999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454058289, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4290	Test BLEU: 21.99
0: Performance: Epoch: 1	Training: 1942369 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592454058290, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454058290, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592454058290, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3390226705
0: Upscaling, new scale: 4096.0
0: TRAIN [2][0/1291]	Time 0.288 (0.288)	Data 1.88e-01 (1.88e-01)	Tok/s 54327 (54327)	Loss/tok 3.0701 (3.0701)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.099 (0.111)	Data 1.12e-04 (1.72e-02)	Tok/s 255807 (228837)	Loss/tok 3.2424 (3.2861)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.099 (0.101)	Data 1.13e-04 (9.05e-03)	Tok/s 253165 (234851)	Loss/tok 3.2799 (3.2981)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.067 (0.087)	Data 1.24e-04 (6.16e-03)	Tok/s 235081 (233813)	Loss/tok 3.1230 (3.2390)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.067 (0.085)	Data 1.07e-04 (4.69e-03)	Tok/s 236357 (236153)	Loss/tok 3.0379 (3.2293)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.067 (0.086)	Data 1.24e-04 (3.79e-03)	Tok/s 230281 (237485)	Loss/tok 3.0437 (3.2294)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.102 (0.088)	Data 1.13e-04 (3.19e-03)	Tok/s 248139 (238805)	Loss/tok 3.2327 (3.2473)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.134 (0.086)	Data 1.10e-04 (2.76e-03)	Tok/s 258565 (239118)	Loss/tok 3.4353 (3.2433)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][80/1291]	Time 0.066 (0.086)	Data 1.13e-04 (2.43e-03)	Tok/s 238572 (239364)	Loss/tok 3.1276 (3.2500)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.085)	Data 1.13e-04 (2.18e-03)	Tok/s 234312 (239545)	Loss/tok 3.0766 (3.2473)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.066 (0.087)	Data 1.08e-04 (1.97e-03)	Tok/s 234387 (240416)	Loss/tok 3.1012 (3.2721)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.134 (0.089)	Data 1.10e-04 (1.80e-03)	Tok/s 261148 (240899)	Loss/tok 3.4296 (3.2897)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.066 (0.088)	Data 1.13e-04 (1.66e-03)	Tok/s 233666 (240946)	Loss/tok 3.0655 (3.2845)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.099 (0.088)	Data 1.09e-04 (1.55e-03)	Tok/s 256566 (241313)	Loss/tok 3.1884 (3.2826)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.035 (0.088)	Data 1.11e-04 (1.44e-03)	Tok/s 227516 (241493)	Loss/tok 2.6871 (3.2806)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.067 (0.089)	Data 1.13e-04 (1.36e-03)	Tok/s 233562 (241888)	Loss/tok 3.1293 (3.2845)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.067 (0.088)	Data 1.10e-04 (1.28e-03)	Tok/s 229838 (241658)	Loss/tok 3.0506 (3.2811)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.099 (0.088)	Data 1.12e-04 (1.21e-03)	Tok/s 254078 (241925)	Loss/tok 3.3189 (3.2867)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.035 (0.089)	Data 1.13e-04 (1.15e-03)	Tok/s 222130 (241789)	Loss/tok 2.6695 (3.2938)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.066 (0.088)	Data 1.14e-04 (1.10e-03)	Tok/s 230691 (241598)	Loss/tok 3.1148 (3.2889)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.066 (0.088)	Data 1.12e-04 (1.05e-03)	Tok/s 231297 (241888)	Loss/tok 3.0462 (3.2902)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][210/1291]	Time 0.135 (0.088)	Data 1.14e-04 (1.00e-03)	Tok/s 258324 (241881)	Loss/tok 3.4829 (3.2901)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.175 (0.088)	Data 1.10e-04 (9.63e-04)	Tok/s 256494 (241923)	Loss/tok 3.7186 (3.2908)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.099 (0.088)	Data 1.13e-04 (9.26e-04)	Tok/s 251687 (242034)	Loss/tok 3.2661 (3.2885)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.066 (0.089)	Data 1.10e-04 (8.93e-04)	Tok/s 230681 (242340)	Loss/tok 3.1289 (3.2950)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.099 (0.089)	Data 1.18e-04 (8.62e-04)	Tok/s 254999 (242685)	Loss/tok 3.3284 (3.2983)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.134 (0.089)	Data 1.11e-04 (8.33e-04)	Tok/s 259258 (242514)	Loss/tok 3.5601 (3.2968)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.067 (0.089)	Data 1.13e-04 (8.06e-04)	Tok/s 236947 (242610)	Loss/tok 2.9640 (3.3002)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.067 (0.089)	Data 1.11e-04 (7.82e-04)	Tok/s 236559 (242671)	Loss/tok 3.1155 (3.3013)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.134 (0.089)	Data 1.08e-04 (7.59e-04)	Tok/s 260637 (242460)	Loss/tok 3.4955 (3.2998)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.067 (0.088)	Data 1.12e-04 (7.37e-04)	Tok/s 233678 (242369)	Loss/tok 3.0215 (3.2977)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.135 (0.088)	Data 1.19e-04 (7.17e-04)	Tok/s 260747 (242348)	Loss/tok 3.4145 (3.2965)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.099 (0.088)	Data 1.11e-04 (6.98e-04)	Tok/s 253924 (242350)	Loss/tok 3.2783 (3.2967)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.066 (0.088)	Data 1.10e-04 (6.80e-04)	Tok/s 235922 (242394)	Loss/tok 3.0468 (3.2948)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][340/1291]	Time 0.099 (0.088)	Data 1.13e-04 (6.64e-04)	Tok/s 253322 (242420)	Loss/tok 3.3169 (3.2928)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.036 (0.088)	Data 1.10e-04 (6.48e-04)	Tok/s 222380 (242340)	Loss/tok 2.7062 (3.2921)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.100 (0.087)	Data 1.11e-04 (6.33e-04)	Tok/s 248478 (242215)	Loss/tok 3.3509 (3.2898)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][370/1291]	Time 0.134 (0.087)	Data 1.13e-04 (6.19e-04)	Tok/s 257457 (242154)	Loss/tok 3.5322 (3.2899)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.067 (0.087)	Data 1.12e-04 (6.06e-04)	Tok/s 232043 (242171)	Loss/tok 3.1350 (3.2910)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.066 (0.087)	Data 1.16e-04 (5.93e-04)	Tok/s 232081 (242067)	Loss/tok 3.0382 (3.2885)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.174 (0.088)	Data 1.10e-04 (5.81e-04)	Tok/s 255785 (242132)	Loss/tok 3.6915 (3.2924)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.098 (0.087)	Data 1.10e-04 (5.70e-04)	Tok/s 253582 (242191)	Loss/tok 3.3577 (3.2918)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.135 (0.088)	Data 1.07e-04 (5.59e-04)	Tok/s 257225 (242235)	Loss/tok 3.4858 (3.2930)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.067 (0.087)	Data 1.12e-04 (5.49e-04)	Tok/s 235697 (242149)	Loss/tok 3.1480 (3.2908)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.067 (0.087)	Data 1.10e-04 (5.39e-04)	Tok/s 233356 (242007)	Loss/tok 3.0704 (3.2887)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.135 (0.087)	Data 1.13e-04 (5.29e-04)	Tok/s 257750 (242169)	Loss/tok 3.5031 (3.2906)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.100 (0.088)	Data 1.10e-04 (5.20e-04)	Tok/s 252205 (242328)	Loss/tok 3.3032 (3.2902)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.099 (0.088)	Data 1.13e-04 (5.12e-04)	Tok/s 254039 (242412)	Loss/tok 3.3004 (3.2905)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.036 (0.088)	Data 1.10e-04 (5.03e-04)	Tok/s 224186 (242373)	Loss/tok 2.7701 (3.2905)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.099 (0.087)	Data 1.13e-04 (4.96e-04)	Tok/s 257802 (242327)	Loss/tok 3.2698 (3.2890)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][500/1291]	Time 0.137 (0.088)	Data 1.13e-04 (4.88e-04)	Tok/s 253402 (242353)	Loss/tok 3.4513 (3.2912)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.135 (0.088)	Data 1.15e-04 (4.81e-04)	Tok/s 258361 (242297)	Loss/tok 3.5041 (3.2919)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.067 (0.088)	Data 1.11e-04 (4.74e-04)	Tok/s 232877 (242298)	Loss/tok 3.1966 (3.2909)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.087)	Data 1.17e-04 (4.67e-04)	Tok/s 232754 (242215)	Loss/tok 3.0208 (3.2885)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.175 (0.088)	Data 1.13e-04 (4.60e-04)	Tok/s 256368 (242254)	Loss/tok 3.5877 (3.2901)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][550/1291]	Time 0.174 (0.088)	Data 1.18e-04 (4.54e-04)	Tok/s 259033 (242253)	Loss/tok 3.6728 (3.2925)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.067 (0.088)	Data 1.08e-04 (4.48e-04)	Tok/s 231770 (242294)	Loss/tok 3.0830 (3.2921)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.067 (0.088)	Data 1.09e-04 (4.42e-04)	Tok/s 234178 (242219)	Loss/tok 3.0642 (3.2925)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][580/1291]	Time 0.067 (0.088)	Data 1.12e-04 (4.36e-04)	Tok/s 232686 (242262)	Loss/tok 3.1275 (3.2937)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.099 (0.088)	Data 1.23e-04 (4.31e-04)	Tok/s 252843 (242261)	Loss/tok 3.3201 (3.2920)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.099 (0.087)	Data 1.11e-04 (4.25e-04)	Tok/s 251764 (242274)	Loss/tok 3.2649 (3.2911)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.066 (0.087)	Data 1.11e-04 (4.20e-04)	Tok/s 231630 (242205)	Loss/tok 3.1104 (3.2905)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.087)	Data 1.13e-04 (4.15e-04)	Tok/s 232011 (242268)	Loss/tok 3.0779 (3.2901)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.066 (0.087)	Data 1.11e-04 (4.11e-04)	Tok/s 236306 (242240)	Loss/tok 3.0121 (3.2884)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.099 (0.087)	Data 1.16e-04 (4.06e-04)	Tok/s 255126 (242254)	Loss/tok 3.3072 (3.2877)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.099 (0.087)	Data 1.13e-04 (4.02e-04)	Tok/s 257096 (242277)	Loss/tok 3.1702 (3.2866)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.067 (0.087)	Data 1.13e-04 (3.97e-04)	Tok/s 233524 (242322)	Loss/tok 3.0594 (3.2872)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.099 (0.087)	Data 1.17e-04 (3.93e-04)	Tok/s 255220 (242289)	Loss/tok 3.2609 (3.2862)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.067 (0.087)	Data 1.10e-04 (3.89e-04)	Tok/s 232608 (242311)	Loss/tok 3.0641 (3.2873)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.066 (0.088)	Data 1.17e-04 (3.85e-04)	Tok/s 236851 (242374)	Loss/tok 3.0585 (3.2878)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][700/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.81e-04)	Tok/s 233468 (242405)	Loss/tok 3.1758 (3.2890)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.135 (0.088)	Data 1.14e-04 (3.77e-04)	Tok/s 260240 (242475)	Loss/tok 3.3781 (3.2907)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.100 (0.088)	Data 1.14e-04 (3.74e-04)	Tok/s 253964 (242505)	Loss/tok 3.2117 (3.2918)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.135 (0.088)	Data 1.12e-04 (3.70e-04)	Tok/s 261463 (242625)	Loss/tok 3.4332 (3.2940)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.66e-04)	Tok/s 233440 (242637)	Loss/tok 3.0800 (3.2934)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.099 (0.088)	Data 1.10e-04 (3.63e-04)	Tok/s 256422 (242680)	Loss/tok 3.2544 (3.2924)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.136 (0.088)	Data 1.12e-04 (3.60e-04)	Tok/s 258092 (242697)	Loss/tok 3.4742 (3.2917)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.100 (0.088)	Data 1.16e-04 (3.57e-04)	Tok/s 252129 (242691)	Loss/tok 3.3433 (3.2922)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.099 (0.088)	Data 1.15e-04 (3.54e-04)	Tok/s 255637 (242686)	Loss/tok 3.3759 (3.2918)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.51e-04)	Tok/s 235147 (242733)	Loss/tok 3.0911 (3.2912)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.48e-04)	Tok/s 220387 (242765)	Loss/tok 2.6946 (3.2934)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.099 (0.089)	Data 1.18e-04 (3.45e-04)	Tok/s 254883 (242803)	Loss/tok 3.2030 (3.2932)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.42e-04)	Tok/s 235028 (242844)	Loss/tok 3.0908 (3.2942)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][830/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.39e-04)	Tok/s 252169 (242884)	Loss/tok 3.3123 (3.2961)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.37e-04)	Tok/s 234309 (242873)	Loss/tok 3.1075 (3.2957)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.34e-04)	Tok/s 233111 (242954)	Loss/tok 3.1333 (3.2976)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.31e-04)	Tok/s 234706 (242984)	Loss/tok 3.0985 (3.2965)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.29e-04)	Tok/s 233780 (242967)	Loss/tok 3.1005 (3.2961)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.098 (0.089)	Data 1.13e-04 (3.27e-04)	Tok/s 255876 (242944)	Loss/tok 3.2916 (3.2957)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.24e-04)	Tok/s 233507 (242879)	Loss/tok 3.0881 (3.2954)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.22e-04)	Tok/s 239804 (242909)	Loss/tok 3.0610 (3.2957)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.20e-04)	Tok/s 238901 (242943)	Loss/tok 3.0749 (3.2966)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.173 (0.089)	Data 1.15e-04 (3.17e-04)	Tok/s 257620 (242897)	Loss/tok 3.6442 (3.2962)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.15e-04)	Tok/s 252796 (242865)	Loss/tok 3.4275 (3.2955)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.135 (0.089)	Data 1.14e-04 (3.13e-04)	Tok/s 259279 (242859)	Loss/tok 3.4772 (3.2949)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.11e-04)	Tok/s 260239 (242964)	Loss/tok 3.4225 (3.2963)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][960/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.09e-04)	Tok/s 258661 (242980)	Loss/tok 3.4369 (3.2956)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 232322 (242946)	Loss/tok 2.9525 (3.2947)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.05e-04)	Tok/s 229092 (242943)	Loss/tok 3.1248 (3.2950)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.174 (0.089)	Data 1.19e-04 (3.03e-04)	Tok/s 255457 (242936)	Loss/tok 3.6890 (3.2950)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.01e-04)	Tok/s 235233 (242868)	Loss/tok 2.9454 (3.2936)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.99e-04)	Tok/s 233878 (242845)	Loss/tok 3.1213 (3.2941)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.100 (0.089)	Data 1.16e-04 (2.97e-04)	Tok/s 250275 (242822)	Loss/tok 3.2575 (3.2938)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.100 (0.089)	Data 1.17e-04 (2.96e-04)	Tok/s 253552 (242802)	Loss/tok 3.3341 (3.2933)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.94e-04)	Tok/s 236469 (242806)	Loss/tok 3.0386 (3.2923)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.92e-04)	Tok/s 220954 (242785)	Loss/tok 2.6229 (3.2921)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.035 (0.089)	Data 1.13e-04 (2.90e-04)	Tok/s 224491 (242780)	Loss/tok 2.5982 (3.2912)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.035 (0.089)	Data 1.13e-04 (2.89e-04)	Tok/s 222028 (242818)	Loss/tok 2.6195 (3.2906)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.87e-04)	Tok/s 236042 (242817)	Loss/tok 3.0438 (3.2914)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1090/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.86e-04)	Tok/s 230164 (242848)	Loss/tok 3.1755 (3.2913)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.84e-04)	Tok/s 252258 (242838)	Loss/tok 3.2461 (3.2914)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.82e-04)	Tok/s 253397 (242808)	Loss/tok 3.2226 (3.2904)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.81e-04)	Tok/s 233095 (242811)	Loss/tok 3.1198 (3.2901)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1130/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.80e-04)	Tok/s 232149 (242862)	Loss/tok 3.0761 (3.2900)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.78e-04)	Tok/s 232073 (242828)	Loss/tok 3.0395 (3.2903)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.066 (0.088)	Data 1.11e-04 (2.77e-04)	Tok/s 231790 (242778)	Loss/tok 3.0716 (3.2892)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.76e-04)	Tok/s 254767 (242812)	Loss/tok 3.3005 (3.2896)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.135 (0.089)	Data 1.16e-04 (2.74e-04)	Tok/s 260642 (242848)	Loss/tok 3.3872 (3.2897)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.067 (0.089)	Data 1.23e-04 (2.73e-04)	Tok/s 232532 (242889)	Loss/tok 2.9426 (3.2896)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.71e-04)	Tok/s 232916 (242898)	Loss/tok 3.0810 (3.2895)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.70e-04)	Tok/s 230375 (242854)	Loss/tok 3.1196 (3.2886)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.135 (0.089)	Data 1.11e-04 (2.69e-04)	Tok/s 262796 (242840)	Loss/tok 3.3773 (3.2883)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.174 (0.089)	Data 1.12e-04 (2.68e-04)	Tok/s 259490 (242834)	Loss/tok 3.5582 (3.2880)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.66e-04)	Tok/s 235530 (242898)	Loss/tok 3.0465 (3.2888)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.65e-04)	Tok/s 230870 (242868)	Loss/tok 3.0209 (3.2879)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1250/1291]	Time 0.066 (0.089)	Data 1.19e-04 (2.64e-04)	Tok/s 231718 (242840)	Loss/tok 3.0265 (3.2871)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.63e-04)	Tok/s 232809 (242852)	Loss/tok 3.0857 (3.2864)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.61e-04)	Tok/s 255395 (242853)	Loss/tok 3.2689 (3.2855)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1280/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.60e-04)	Tok/s 259416 (242870)	Loss/tok 3.4351 (3.2856)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.134 (0.089)	Data 4.48e-05 (2.62e-04)	Tok/s 259998 (242930)	Loss/tok 3.4197 (3.2859)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592454173608, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454173608, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.421 (0.421)	Decoder iters 120.0 (120.0)	Tok/s 38698 (38698)
0: Running moses detokenizer
0: BLEU(score=22.68789129638945, counts=[36102, 17574, 9783, 5669], totals=[64964, 61961, 58958, 55959], precisions=[55.572316975555694, 28.36300253385194, 16.593168017911054, 10.130631355099268], bp=1.0, sys_len=64964, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454175496, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22690000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454175497, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2854	Test BLEU: 22.69
0: Performance: Epoch: 2	Training: 1943208 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592454175497, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454175497, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592454175497, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1102709019
0: TRAIN [3][0/1291]	Time 0.327 (0.327)	Data 2.28e-01 (2.28e-01)	Tok/s 76667 (76667)	Loss/tok 3.1528 (3.1528)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.100 (0.114)	Data 1.14e-04 (2.09e-02)	Tok/s 252844 (232053)	Loss/tok 3.1195 (3.1437)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.100 (0.096)	Data 1.10e-04 (1.10e-02)	Tok/s 250765 (233307)	Loss/tok 3.1993 (3.1375)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.099 (0.102)	Data 1.10e-04 (7.47e-03)	Tok/s 257066 (238238)	Loss/tok 3.1946 (3.2084)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.036 (0.097)	Data 1.15e-04 (5.68e-03)	Tok/s 221838 (238836)	Loss/tok 2.6192 (3.1963)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.035 (0.094)	Data 1.14e-04 (4.59e-03)	Tok/s 219592 (238362)	Loss/tok 2.4853 (3.1923)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.102 (0.094)	Data 1.09e-04 (3.85e-03)	Tok/s 248357 (239744)	Loss/tok 3.2785 (3.2044)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.173 (0.096)	Data 1.23e-04 (3.33e-03)	Tok/s 257904 (240898)	Loss/tok 3.5551 (3.2137)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.099 (0.098)	Data 1.13e-04 (2.93e-03)	Tok/s 254096 (242202)	Loss/tok 3.1963 (3.2281)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.099 (0.099)	Data 1.15e-04 (2.62e-03)	Tok/s 253362 (242737)	Loss/tok 3.2264 (3.2349)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.099 (0.097)	Data 1.12e-04 (2.37e-03)	Tok/s 254488 (242803)	Loss/tok 3.2188 (3.2282)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.066 (0.095)	Data 1.12e-04 (2.17e-03)	Tok/s 231866 (242421)	Loss/tok 3.0309 (3.2224)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][120/1291]	Time 0.067 (0.095)	Data 1.13e-04 (2.00e-03)	Tok/s 229862 (242569)	Loss/tok 2.9115 (3.2151)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.067 (0.094)	Data 1.12e-04 (1.85e-03)	Tok/s 233941 (242226)	Loss/tok 3.0559 (3.2110)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][140/1291]	Time 0.174 (0.095)	Data 1.13e-04 (1.73e-03)	Tok/s 254093 (242602)	Loss/tok 3.6052 (3.2197)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.099 (0.095)	Data 1.13e-04 (1.62e-03)	Tok/s 255846 (243008)	Loss/tok 3.2409 (3.2214)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.175 (0.094)	Data 1.18e-04 (1.53e-03)	Tok/s 254633 (242619)	Loss/tok 3.5979 (3.2187)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.066 (0.093)	Data 1.16e-04 (1.45e-03)	Tok/s 237039 (242317)	Loss/tok 2.9962 (3.2135)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.100 (0.093)	Data 1.10e-04 (1.37e-03)	Tok/s 249152 (242331)	Loss/tok 3.3059 (3.2171)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.099 (0.093)	Data 1.14e-04 (1.31e-03)	Tok/s 254754 (242615)	Loss/tok 3.3184 (3.2219)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.067 (0.092)	Data 1.13e-04 (1.25e-03)	Tok/s 231708 (242242)	Loss/tok 2.9983 (3.2158)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.099 (0.092)	Data 1.19e-04 (1.19e-03)	Tok/s 254318 (242195)	Loss/tok 3.2253 (3.2131)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.067 (0.091)	Data 1.16e-04 (1.15e-03)	Tok/s 227936 (242090)	Loss/tok 3.0404 (3.2090)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.066 (0.091)	Data 1.10e-04 (1.10e-03)	Tok/s 234990 (241967)	Loss/tok 3.0136 (3.2047)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.100 (0.090)	Data 1.10e-04 (1.06e-03)	Tok/s 251311 (241806)	Loss/tok 3.1533 (3.1997)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.066 (0.091)	Data 1.13e-04 (1.02e-03)	Tok/s 233905 (242049)	Loss/tok 2.9943 (3.2040)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.067 (0.091)	Data 1.13e-04 (9.87e-04)	Tok/s 233437 (241944)	Loss/tok 2.9648 (3.2027)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][270/1291]	Time 0.100 (0.091)	Data 1.13e-04 (9.55e-04)	Tok/s 251293 (242131)	Loss/tok 3.1245 (3.2007)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.100 (0.090)	Data 1.15e-04 (9.25e-04)	Tok/s 252225 (242100)	Loss/tok 3.2484 (3.1991)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.100 (0.090)	Data 1.13e-04 (8.97e-04)	Tok/s 255851 (242083)	Loss/tok 3.2031 (3.1963)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.100 (0.090)	Data 1.11e-04 (8.71e-04)	Tok/s 255547 (242085)	Loss/tok 3.1561 (3.1925)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.135 (0.090)	Data 1.12e-04 (8.46e-04)	Tok/s 258670 (242182)	Loss/tok 3.4487 (3.1949)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.136 (0.090)	Data 1.16e-04 (8.23e-04)	Tok/s 254811 (242123)	Loss/tok 3.4004 (3.1936)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.100 (0.090)	Data 1.10e-04 (8.02e-04)	Tok/s 253736 (242243)	Loss/tok 3.2663 (3.1949)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.067 (0.090)	Data 1.14e-04 (7.82e-04)	Tok/s 229601 (242329)	Loss/tok 2.9794 (3.1936)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.067 (0.090)	Data 1.09e-04 (7.63e-04)	Tok/s 232237 (242392)	Loss/tok 2.9685 (3.1953)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.100 (0.090)	Data 1.10e-04 (7.45e-04)	Tok/s 252482 (242294)	Loss/tok 3.2162 (3.1926)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.035 (0.089)	Data 1.12e-04 (7.28e-04)	Tok/s 222561 (242212)	Loss/tok 2.6178 (3.1909)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.067 (0.090)	Data 1.16e-04 (7.12e-04)	Tok/s 231406 (242391)	Loss/tok 3.0292 (3.1923)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.066 (0.090)	Data 1.14e-04 (6.96e-04)	Tok/s 234953 (242490)	Loss/tok 3.0959 (3.1917)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][400/1291]	Time 0.067 (0.090)	Data 1.13e-04 (6.82e-04)	Tok/s 229995 (242458)	Loss/tok 3.0153 (3.1908)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.174 (0.090)	Data 1.11e-04 (6.68e-04)	Tok/s 257662 (242677)	Loss/tok 3.4835 (3.1911)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.100 (0.090)	Data 1.14e-04 (6.55e-04)	Tok/s 249753 (242688)	Loss/tok 3.2198 (3.1893)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.066 (0.090)	Data 1.09e-04 (6.42e-04)	Tok/s 234246 (242668)	Loss/tok 3.0193 (3.1880)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.100 (0.090)	Data 1.12e-04 (6.30e-04)	Tok/s 251499 (242784)	Loss/tok 3.1233 (3.1898)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.067 (0.090)	Data 1.15e-04 (6.19e-04)	Tok/s 237677 (242821)	Loss/tok 2.9392 (3.1883)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.066 (0.090)	Data 1.24e-04 (6.08e-04)	Tok/s 233569 (242684)	Loss/tok 2.8950 (3.1848)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.135 (0.090)	Data 1.16e-04 (5.97e-04)	Tok/s 257559 (242718)	Loss/tok 3.3169 (3.1840)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.87e-04)	Tok/s 231359 (242790)	Loss/tok 2.9967 (3.1848)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.066 (0.090)	Data 1.14e-04 (5.78e-04)	Tok/s 235938 (242725)	Loss/tok 3.0447 (3.1833)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.035 (0.090)	Data 1.09e-04 (5.68e-04)	Tok/s 224887 (242704)	Loss/tok 2.6418 (3.1822)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.066 (0.090)	Data 1.18e-04 (5.59e-04)	Tok/s 231398 (242793)	Loss/tok 3.0015 (3.1823)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.067 (0.090)	Data 1.13e-04 (5.51e-04)	Tok/s 226298 (242741)	Loss/tok 3.0335 (3.1829)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][530/1291]	Time 0.100 (0.090)	Data 1.13e-04 (5.43e-04)	Tok/s 255173 (242683)	Loss/tok 3.2138 (3.1817)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.036 (0.090)	Data 1.10e-04 (5.35e-04)	Tok/s 221970 (242738)	Loss/tok 2.5743 (3.1815)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.068 (0.090)	Data 1.17e-04 (5.27e-04)	Tok/s 227791 (242606)	Loss/tok 2.8974 (3.1798)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.176 (0.090)	Data 1.13e-04 (5.20e-04)	Tok/s 252189 (242489)	Loss/tok 3.4565 (3.1794)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.066 (0.089)	Data 1.16e-04 (5.13e-04)	Tok/s 232066 (242291)	Loss/tok 3.0371 (3.1767)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.176 (0.089)	Data 1.15e-04 (5.06e-04)	Tok/s 255919 (242259)	Loss/tok 3.5024 (3.1767)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.099 (0.089)	Data 1.16e-04 (4.99e-04)	Tok/s 256700 (242172)	Loss/tok 3.1648 (3.1745)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.099 (0.089)	Data 1.16e-04 (4.93e-04)	Tok/s 251209 (242308)	Loss/tok 3.1962 (3.1755)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.066 (0.089)	Data 1.11e-04 (4.87e-04)	Tok/s 231286 (242164)	Loss/tok 3.0705 (3.1735)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.135 (0.089)	Data 1.18e-04 (4.81e-04)	Tok/s 258921 (242225)	Loss/tok 3.2295 (3.1728)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.75e-04)	Tok/s 233366 (242138)	Loss/tok 2.9972 (3.1716)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.100 (0.088)	Data 1.09e-04 (4.69e-04)	Tok/s 251639 (242150)	Loss/tok 3.0406 (3.1700)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.066 (0.088)	Data 1.12e-04 (4.64e-04)	Tok/s 230370 (242145)	Loss/tok 2.9063 (3.1700)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][660/1291]	Time 0.067 (0.088)	Data 1.38e-04 (4.58e-04)	Tok/s 233413 (242159)	Loss/tok 2.8981 (3.1688)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.134 (0.089)	Data 1.16e-04 (4.53e-04)	Tok/s 258998 (242191)	Loss/tok 3.3673 (3.1707)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.137 (0.089)	Data 1.23e-04 (4.48e-04)	Tok/s 255777 (242267)	Loss/tok 3.3939 (3.1725)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.099 (0.088)	Data 1.10e-04 (4.43e-04)	Tok/s 254955 (242128)	Loss/tok 3.0920 (3.1699)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.099 (0.088)	Data 1.11e-04 (4.39e-04)	Tok/s 252112 (242047)	Loss/tok 3.1931 (3.1686)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.067 (0.088)	Data 1.16e-04 (4.34e-04)	Tok/s 231975 (242042)	Loss/tok 2.9568 (3.1679)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.136 (0.088)	Data 1.13e-04 (4.30e-04)	Tok/s 254878 (242135)	Loss/tok 3.3184 (3.1682)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.134 (0.088)	Data 1.14e-04 (4.26e-04)	Tok/s 261674 (242096)	Loss/tok 3.3589 (3.1684)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.175 (0.088)	Data 1.09e-04 (4.21e-04)	Tok/s 254693 (242081)	Loss/tok 3.5507 (3.1683)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.067 (0.088)	Data 1.13e-04 (4.17e-04)	Tok/s 234932 (242019)	Loss/tok 2.8710 (3.1674)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.175 (0.088)	Data 1.17e-04 (4.13e-04)	Tok/s 256273 (242066)	Loss/tok 3.3737 (3.1681)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.035 (0.088)	Data 1.12e-04 (4.09e-04)	Tok/s 224135 (242034)	Loss/tok 2.5495 (3.1662)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][780/1291]	Time 0.067 (0.088)	Data 1.12e-04 (4.06e-04)	Tok/s 232601 (242020)	Loss/tok 2.8786 (3.1661)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.099 (0.088)	Data 1.13e-04 (4.02e-04)	Tok/s 256818 (241966)	Loss/tok 3.1093 (3.1644)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.036 (0.088)	Data 1.21e-04 (3.98e-04)	Tok/s 223254 (242010)	Loss/tok 2.4614 (3.1643)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.035 (0.088)	Data 1.14e-04 (3.95e-04)	Tok/s 224860 (241961)	Loss/tok 2.5820 (3.1634)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.066 (0.088)	Data 1.09e-04 (3.91e-04)	Tok/s 234983 (241962)	Loss/tok 2.9344 (3.1631)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.88e-04)	Tok/s 235207 (242001)	Loss/tok 3.0012 (3.1624)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.035 (0.088)	Data 1.12e-04 (3.85e-04)	Tok/s 223511 (242006)	Loss/tok 2.5964 (3.1617)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.174 (0.088)	Data 1.13e-04 (3.82e-04)	Tok/s 258256 (242051)	Loss/tok 3.4274 (3.1618)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.100 (0.088)	Data 1.11e-04 (3.78e-04)	Tok/s 252071 (242117)	Loss/tok 3.0743 (3.1628)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.099 (0.088)	Data 1.14e-04 (3.75e-04)	Tok/s 255422 (242108)	Loss/tok 3.1571 (3.1618)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.100 (0.088)	Data 1.16e-04 (3.72e-04)	Tok/s 254438 (242110)	Loss/tok 3.1985 (3.1619)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.135 (0.088)	Data 1.11e-04 (3.69e-04)	Tok/s 260325 (242172)	Loss/tok 3.2513 (3.1619)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.088)	Data 1.16e-04 (3.67e-04)	Tok/s 232872 (242169)	Loss/tok 3.0359 (3.1612)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][910/1291]	Time 0.067 (0.088)	Data 1.08e-04 (3.64e-04)	Tok/s 236470 (242110)	Loss/tok 2.9064 (3.1600)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.136 (0.088)	Data 1.17e-04 (3.61e-04)	Tok/s 260059 (242091)	Loss/tok 3.2343 (3.1595)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.067 (0.088)	Data 1.09e-04 (3.59e-04)	Tok/s 231581 (242103)	Loss/tok 2.8994 (3.1598)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.100 (0.088)	Data 1.15e-04 (3.56e-04)	Tok/s 249898 (242141)	Loss/tok 3.0785 (3.1598)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.173 (0.089)	Data 1.18e-04 (3.53e-04)	Tok/s 252554 (242203)	Loss/tok 3.6038 (3.1614)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.035 (0.089)	Data 1.24e-04 (3.51e-04)	Tok/s 222775 (242166)	Loss/tok 2.6219 (3.1611)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.48e-04)	Tok/s 236683 (242107)	Loss/tok 2.9343 (3.1597)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.088)	Data 1.16e-04 (3.46e-04)	Tok/s 229424 (242075)	Loss/tok 2.9442 (3.1589)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.036 (0.088)	Data 1.19e-04 (3.44e-04)	Tok/s 223917 (242088)	Loss/tok 2.5717 (3.1594)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.41e-04)	Tok/s 252838 (242126)	Loss/tok 3.0874 (3.1585)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.035 (0.089)	Data 1.14e-04 (3.39e-04)	Tok/s 222840 (242103)	Loss/tok 2.5084 (3.1588)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.37e-04)	Tok/s 235089 (242096)	Loss/tok 2.9208 (3.1580)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.135 (0.089)	Data 1.12e-04 (3.35e-04)	Tok/s 258991 (242135)	Loss/tok 3.3332 (3.1591)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1040/1291]	Time 0.036 (0.089)	Data 1.33e-04 (3.33e-04)	Tok/s 220793 (242156)	Loss/tok 2.4996 (3.1595)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.099 (0.089)	Data 1.19e-04 (3.31e-04)	Tok/s 250676 (242114)	Loss/tok 3.2599 (3.1586)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.136 (0.089)	Data 1.10e-04 (3.29e-04)	Tok/s 257361 (242169)	Loss/tok 3.2688 (3.1585)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.067 (0.089)	Data 1.18e-04 (3.27e-04)	Tok/s 229610 (242157)	Loss/tok 2.9766 (3.1581)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.25e-04)	Tok/s 254288 (242220)	Loss/tok 3.0573 (3.1588)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.23e-04)	Tok/s 233020 (242216)	Loss/tok 2.8811 (3.1585)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.21e-04)	Tok/s 233934 (242248)	Loss/tok 2.9285 (3.1578)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.035 (0.089)	Data 1.12e-04 (3.19e-04)	Tok/s 219196 (242179)	Loss/tok 2.5132 (3.1565)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.100 (0.089)	Data 1.20e-04 (3.17e-04)	Tok/s 249962 (242260)	Loss/tok 3.1280 (3.1567)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.099 (0.089)	Data 1.23e-04 (3.15e-04)	Tok/s 255077 (242331)	Loss/tok 3.1958 (3.1572)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.136 (0.089)	Data 1.16e-04 (3.14e-04)	Tok/s 258207 (242318)	Loss/tok 3.3497 (3.1568)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.12e-04)	Tok/s 254600 (242372)	Loss/tok 3.0709 (3.1564)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.10e-04)	Tok/s 234222 (242383)	Loss/tok 2.9992 (3.1559)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1170/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.09e-04)	Tok/s 253160 (242404)	Loss/tok 3.0834 (3.1559)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.07e-04)	Tok/s 236309 (242431)	Loss/tok 2.9497 (3.1556)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.05e-04)	Tok/s 231945 (242361)	Loss/tok 2.9706 (3.1543)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.136 (0.089)	Data 1.13e-04 (3.04e-04)	Tok/s 258063 (242371)	Loss/tok 3.2432 (3.1541)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.02e-04)	Tok/s 251506 (242371)	Loss/tok 3.1219 (3.1533)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.01e-04)	Tok/s 236963 (242370)	Loss/tok 2.8274 (3.1525)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.99e-04)	Tok/s 254979 (242368)	Loss/tok 3.1137 (3.1520)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.98e-04)	Tok/s 229425 (242376)	Loss/tok 2.9639 (3.1513)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.099 (0.089)	Data 1.21e-04 (2.96e-04)	Tok/s 253900 (242405)	Loss/tok 3.1048 (3.1508)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.067 (0.089)	Data 1.21e-04 (2.95e-04)	Tok/s 226826 (242378)	Loss/tok 2.9115 (3.1501)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.93e-04)	Tok/s 234167 (242374)	Loss/tok 2.9805 (3.1497)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.175 (0.089)	Data 1.15e-04 (2.92e-04)	Tok/s 254790 (242451)	Loss/tok 3.5635 (3.1506)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.066 (0.089)	Data 4.63e-05 (2.93e-04)	Tok/s 231851 (242441)	Loss/tok 2.9281 (3.1506)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592454291038, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592454291038, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.497 (0.497)	Decoder iters 149.0 (149.0)	Tok/s 32800 (32800)
0: Running moses detokenizer
0: BLEU(score=24.13455408138696, counts=[36994, 18534, 10595, 6307], totals=[65216, 62213, 59210, 56213], precisions=[56.72534347399411, 29.791201195891535, 17.89393683499409, 11.219824595734083], bp=1.0, sys_len=65216, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454292975, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2413, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592454292975, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1512	Test BLEU: 24.13
0: Performance: Epoch: 3	Training: 1939762 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592454292976, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592454292976, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:24:59 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:37 PM
ENDING TIMING RUN AT 2020-06-17 09:25:00 PM
RESULT,RNN_TRANSLATOR,,503,nvidia,2020-06-17 09:16:37 PM
