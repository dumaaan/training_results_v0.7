+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592453836927, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592453836965, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592453836966, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592453836966, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592453836966, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0059
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592453842949, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/13842445/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DECAY_INTERVAL=506
+ TARGET=24.0
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ REMAIN_STEPS=4054
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
+ echo 'running benchmark'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:17:25 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592453847098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847339, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847343, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847353, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453847390, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2709877259
:::MLLOG {"namespace": "", "time_ms": 1592453854948, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2709877259, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1272551375
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592453869091, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592453869092, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592453869092, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592453869092, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592453869092, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592453870731, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592453870731, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592453870731, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592453870997, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592453870998, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592453870998, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592453870999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592453870999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592453870999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592453870999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592453870999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592453870999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592453871000, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592453871000, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453871000, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2811599975
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.305 (0.305)	Data 2.23e-01 (2.23e-01)	Tok/s 51927 (51927)	Loss/tok 10.5803 (10.5803)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.132 (0.113)	Data 1.37e-04 (2.04e-02)	Tok/s 267277 (228085)	Loss/tok 9.5649 (9.9091)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.098 (0.103)	Data 1.22e-04 (1.07e-02)	Tok/s 255899 (238770)	Loss/tok 9.1456 (9.6078)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.098 (0.104)	Data 1.13e-04 (7.32e-03)	Tok/s 258128 (242283)	Loss/tok 8.8159 (9.4012)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.098 (0.100)	Data 1.18e-04 (5.56e-03)	Tok/s 260595 (243680)	Loss/tok 8.6204 (9.2457)	LR 7.222e-05
0: TRAIN [0][50/1291]	Time 0.066 (0.098)	Data 1.13e-04 (4.49e-03)	Tok/s 232958 (244332)	Loss/tok 8.2706 (9.1022)	LR 9.092e-05
0: TRAIN [0][60/1291]	Time 0.098 (0.096)	Data 1.26e-04 (3.77e-03)	Tok/s 256909 (245011)	Loss/tok 8.1508 (8.9676)	LR 1.145e-04
0: TRAIN [0][70/1291]	Time 0.133 (0.096)	Data 1.11e-04 (3.26e-03)	Tok/s 260928 (245905)	Loss/tok 8.0903 (8.8398)	LR 1.441e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][80/1291]	Time 0.098 (0.094)	Data 1.13e-04 (2.87e-03)	Tok/s 258350 (246033)	Loss/tok 8.6394 (8.7495)	LR 1.773e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][90/1291]	Time 0.066 (0.094)	Data 1.12e-04 (2.57e-03)	Tok/s 232753 (246182)	Loss/tok 7.8366 (8.7082)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.066 (0.093)	Data 1.10e-04 (2.33e-03)	Tok/s 234973 (246095)	Loss/tok 7.7768 (8.6345)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.133 (0.092)	Data 1.13e-04 (2.13e-03)	Tok/s 262351 (246148)	Loss/tok 8.0339 (8.5714)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.171 (0.094)	Data 1.13e-04 (1.96e-03)	Tok/s 259752 (246565)	Loss/tok 8.1908 (8.5154)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.098 (0.092)	Data 1.13e-04 (1.82e-03)	Tok/s 257323 (246376)	Loss/tok 7.7677 (8.4676)	LR 5.478e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][140/1291]	Time 0.066 (0.092)	Data 1.16e-04 (1.70e-03)	Tok/s 237672 (246481)	Loss/tok 8.1466 (8.4362)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.066 (0.090)	Data 1.12e-04 (1.59e-03)	Tok/s 237569 (245850)	Loss/tok 7.5613 (8.3986)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.134 (0.091)	Data 1.15e-04 (1.50e-03)	Tok/s 262699 (246235)	Loss/tok 7.6631 (8.3473)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.066 (0.091)	Data 1.16e-04 (1.42e-03)	Tok/s 236796 (246398)	Loss/tok 7.2267 (8.2937)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.134 (0.091)	Data 1.10e-04 (1.35e-03)	Tok/s 261411 (246428)	Loss/tok 7.3075 (8.2367)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.098 (0.090)	Data 1.08e-04 (1.28e-03)	Tok/s 255484 (246136)	Loss/tok 7.0882 (8.1845)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.098 (0.090)	Data 1.13e-04 (1.22e-03)	Tok/s 255949 (246132)	Loss/tok 7.2809 (8.1219)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.099 (0.090)	Data 1.10e-04 (1.17e-03)	Tok/s 256244 (246167)	Loss/tok 6.7381 (8.0637)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.066 (0.090)	Data 1.15e-04 (1.12e-03)	Tok/s 234195 (246115)	Loss/tok 6.4013 (8.0034)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.099 (0.090)	Data 1.11e-04 (1.08e-03)	Tok/s 256355 (246216)	Loss/tok 6.5225 (7.9363)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.090)	Data 1.13e-04 (1.04e-03)	Tok/s 232101 (246239)	Loss/tok 5.9357 (7.8701)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.066 (0.090)	Data 1.13e-04 (1.00e-03)	Tok/s 236962 (246237)	Loss/tok 5.8824 (7.8013)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.098 (0.090)	Data 1.23e-04 (9.69e-04)	Tok/s 257804 (246117)	Loss/tok 6.0496 (7.7355)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][270/1291]	Time 0.173 (0.091)	Data 1.13e-04 (9.38e-04)	Tok/s 258100 (246335)	Loss/tok 6.1939 (7.6588)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.098 (0.091)	Data 1.16e-04 (9.09e-04)	Tok/s 256718 (246361)	Loss/tok 5.7186 (7.5934)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.067 (0.091)	Data 1.14e-04 (8.81e-04)	Tok/s 230996 (246345)	Loss/tok 5.3890 (7.5281)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.099 (0.091)	Data 1.15e-04 (8.56e-04)	Tok/s 256921 (246241)	Loss/tok 5.6276 (7.4689)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.099 (0.091)	Data 1.11e-04 (8.32e-04)	Tok/s 253203 (246316)	Loss/tok 5.5000 (7.4041)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.067 (0.090)	Data 1.13e-04 (8.10e-04)	Tok/s 231702 (246163)	Loss/tok 5.0016 (7.3473)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.066 (0.090)	Data 1.11e-04 (7.88e-04)	Tok/s 232145 (246290)	Loss/tok 4.7675 (7.2759)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.099 (0.090)	Data 1.11e-04 (7.69e-04)	Tok/s 254339 (246199)	Loss/tok 5.2415 (7.2205)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.066 (0.090)	Data 1.12e-04 (7.50e-04)	Tok/s 233873 (246253)	Loss/tok 4.6151 (7.1562)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.134 (0.090)	Data 1.11e-04 (7.32e-04)	Tok/s 260795 (246385)	Loss/tok 5.0869 (7.0882)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.066 (0.090)	Data 1.14e-04 (7.16e-04)	Tok/s 229503 (246241)	Loss/tok 4.5302 (7.0351)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.067 (0.091)	Data 1.15e-04 (7.00e-04)	Tok/s 231119 (246282)	Loss/tok 4.3364 (6.9652)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.067 (0.090)	Data 1.12e-04 (6.85e-04)	Tok/s 231154 (246152)	Loss/tok 4.3193 (6.9140)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][400/1291]	Time 0.135 (0.091)	Data 1.10e-04 (6.71e-04)	Tok/s 260725 (246149)	Loss/tok 4.7714 (6.8478)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.173 (0.091)	Data 1.13e-04 (6.57e-04)	Tok/s 256896 (246156)	Loss/tok 4.9615 (6.7896)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.099 (0.091)	Data 1.18e-04 (6.44e-04)	Tok/s 252887 (245960)	Loss/tok 4.5130 (6.7447)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.099 (0.091)	Data 1.10e-04 (6.32e-04)	Tok/s 254934 (246089)	Loss/tok 4.4673 (6.6848)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.099 (0.091)	Data 1.14e-04 (6.20e-04)	Tok/s 256688 (246040)	Loss/tok 4.5401 (6.6344)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.067 (0.091)	Data 1.12e-04 (6.09e-04)	Tok/s 228775 (245909)	Loss/tok 4.1891 (6.5915)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.066 (0.090)	Data 1.12e-04 (5.98e-04)	Tok/s 233963 (245730)	Loss/tok 3.9048 (6.5489)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.135 (0.090)	Data 1.13e-04 (5.88e-04)	Tok/s 257956 (245764)	Loss/tok 4.4634 (6.4997)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.099 (0.090)	Data 1.09e-04 (5.78e-04)	Tok/s 256873 (245662)	Loss/tok 4.3252 (6.4604)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.067 (0.090)	Data 1.11e-04 (5.68e-04)	Tok/s 232415 (245621)	Loss/tok 3.9288 (6.4168)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.067 (0.090)	Data 1.10e-04 (5.59e-04)	Tok/s 228055 (245654)	Loss/tok 3.9304 (6.3730)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.035 (0.090)	Data 1.10e-04 (5.51e-04)	Tok/s 224667 (245496)	Loss/tok 3.3455 (6.3379)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.173 (0.090)	Data 1.08e-04 (5.42e-04)	Tok/s 258611 (245504)	Loss/tok 4.6873 (6.2956)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][530/1291]	Time 0.099 (0.090)	Data 1.13e-04 (5.34e-04)	Tok/s 254726 (245488)	Loss/tok 4.0878 (6.2566)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.099 (0.089)	Data 1.14e-04 (5.26e-04)	Tok/s 253908 (245424)	Loss/tok 4.0697 (6.2204)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.134 (0.090)	Data 1.12e-04 (5.19e-04)	Tok/s 260740 (245553)	Loss/tok 4.3584 (6.1770)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.066 (0.090)	Data 1.08e-04 (5.11e-04)	Tok/s 230167 (245469)	Loss/tok 3.8641 (6.1418)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.134 (0.090)	Data 1.16e-04 (5.05e-04)	Tok/s 256880 (245472)	Loss/tok 4.1897 (6.1054)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.066 (0.090)	Data 1.09e-04 (4.98e-04)	Tok/s 233998 (245462)	Loss/tok 3.7326 (6.0693)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.134 (0.090)	Data 1.11e-04 (4.92e-04)	Tok/s 258364 (245582)	Loss/tok 4.3594 (6.0297)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.099 (0.090)	Data 1.15e-04 (4.85e-04)	Tok/s 254976 (245578)	Loss/tok 4.0387 (5.9972)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.173 (0.090)	Data 1.08e-04 (4.79e-04)	Tok/s 254886 (245617)	Loss/tok 4.4974 (5.9590)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.73e-04)	Tok/s 230714 (245573)	Loss/tok 3.6805 (5.9280)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.066 (0.090)	Data 1.12e-04 (4.68e-04)	Tok/s 233922 (245574)	Loss/tok 3.7846 (5.8964)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.099 (0.091)	Data 1.14e-04 (4.62e-04)	Tok/s 254636 (245643)	Loss/tok 3.9606 (5.8623)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1291]	Time 0.099 (0.091)	Data 1.11e-04 (4.57e-04)	Tok/s 253287 (245701)	Loss/tok 4.1533 (5.8298)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.099 (0.091)	Data 1.13e-04 (4.52e-04)	Tok/s 257133 (245621)	Loss/tok 4.0158 (5.8048)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.036 (0.090)	Data 1.13e-04 (4.47e-04)	Tok/s 224317 (245554)	Loss/tok 3.0623 (5.7791)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.066 (0.090)	Data 1.13e-04 (4.42e-04)	Tok/s 235659 (245470)	Loss/tok 3.6244 (5.7540)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.37e-04)	Tok/s 235104 (245336)	Loss/tok 3.6397 (5.7328)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.32e-04)	Tok/s 233809 (245266)	Loss/tok 3.7564 (5.7094)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.28e-04)	Tok/s 232963 (245140)	Loss/tok 3.5774 (5.6892)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.24e-04)	Tok/s 232741 (245113)	Loss/tok 3.6242 (5.6657)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.135 (0.090)	Data 1.12e-04 (4.19e-04)	Tok/s 257815 (245230)	Loss/tok 3.9956 (5.6369)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.16e-04)	Tok/s 235268 (245117)	Loss/tok 3.6847 (5.6170)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.12e-04)	Tok/s 228586 (245071)	Loss/tok 3.6906 (5.5952)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.066 (0.089)	Data 1.13e-04 (4.08e-04)	Tok/s 230385 (245056)	Loss/tok 3.5814 (5.5723)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.036 (0.089)	Data 1.12e-04 (4.04e-04)	Tok/s 224624 (245032)	Loss/tok 3.0253 (5.5507)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1291]	Time 0.036 (0.089)	Data 1.33e-04 (4.00e-04)	Tok/s 222630 (244916)	Loss/tok 3.0637 (5.5342)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][790/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.97e-04)	Tok/s 234466 (244963)	Loss/tok 3.6155 (5.5091)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.93e-04)	Tok/s 234079 (244972)	Loss/tok 3.6142 (5.4868)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.90e-04)	Tok/s 229689 (244962)	Loss/tok 3.4945 (5.4659)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.133 (0.089)	Data 1.11e-04 (3.86e-04)	Tok/s 260621 (244998)	Loss/tok 3.9727 (5.4429)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.83e-04)	Tok/s 254996 (244906)	Loss/tok 3.8940 (5.4266)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.80e-04)	Tok/s 233189 (244864)	Loss/tok 3.5803 (5.4078)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.036 (0.089)	Data 1.12e-04 (3.77e-04)	Tok/s 221763 (244816)	Loss/tok 2.9607 (5.3891)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.098 (0.089)	Data 1.12e-04 (3.74e-04)	Tok/s 257945 (244887)	Loss/tok 3.6741 (5.3672)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.71e-04)	Tok/s 252846 (244934)	Loss/tok 3.7462 (5.3467)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.68e-04)	Tok/s 232516 (244884)	Loss/tok 3.4780 (5.3296)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.65e-04)	Tok/s 234740 (244796)	Loss/tok 3.4808 (5.3136)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.098 (0.089)	Data 1.13e-04 (3.62e-04)	Tok/s 254649 (244746)	Loss/tok 3.8376 (5.2976)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.100 (0.089)	Data 1.32e-04 (3.60e-04)	Tok/s 252343 (244654)	Loss/tok 3.7497 (5.2827)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][920/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.57e-04)	Tok/s 234691 (244654)	Loss/tok 3.4295 (5.2654)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.036 (0.089)	Data 1.17e-04 (3.54e-04)	Tok/s 220521 (244563)	Loss/tok 2.9076 (5.2513)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.52e-04)	Tok/s 253885 (244578)	Loss/tok 3.7649 (5.2341)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.49e-04)	Tok/s 234960 (244539)	Loss/tok 3.3661 (5.2192)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.098 (0.089)	Data 1.12e-04 (3.47e-04)	Tok/s 258472 (244554)	Loss/tok 3.6899 (5.2025)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.067 (0.089)	Data 1.19e-04 (3.44e-04)	Tok/s 231818 (244487)	Loss/tok 3.4118 (5.1883)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.42e-04)	Tok/s 235664 (244454)	Loss/tok 3.5321 (5.1735)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.40e-04)	Tok/s 223365 (244368)	Loss/tok 2.9932 (5.1609)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.38e-04)	Tok/s 252601 (244356)	Loss/tok 3.7459 (5.1464)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.35e-04)	Tok/s 235448 (244359)	Loss/tok 3.4366 (5.1320)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.33e-04)	Tok/s 232237 (244343)	Loss/tok 3.4147 (5.1177)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.100 (0.088)	Data 1.21e-04 (3.31e-04)	Tok/s 252891 (244317)	Loss/tok 3.7431 (5.1041)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.29e-04)	Tok/s 229621 (244309)	Loss/tok 3.4916 (5.0901)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1050/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.27e-04)	Tok/s 239268 (244243)	Loss/tok 3.4122 (5.0784)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1060/1291]	Time 0.173 (0.088)	Data 1.15e-04 (3.25e-04)	Tok/s 258830 (244251)	Loss/tok 3.9826 (5.0642)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.098 (0.088)	Data 1.36e-04 (3.23e-04)	Tok/s 253627 (244286)	Loss/tok 3.6597 (5.0500)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.21e-04)	Tok/s 235001 (244250)	Loss/tok 3.3617 (5.0380)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.098 (0.088)	Data 1.10e-04 (3.19e-04)	Tok/s 257165 (244213)	Loss/tok 3.5775 (5.0261)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.17e-04)	Tok/s 231297 (244198)	Loss/tok 3.3828 (5.0136)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.035 (0.088)	Data 1.13e-04 (3.16e-04)	Tok/s 225125 (244260)	Loss/tok 2.9326 (4.9995)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.035 (0.088)	Data 1.10e-04 (3.14e-04)	Tok/s 223777 (244222)	Loss/tok 2.9279 (4.9885)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.066 (0.088)	Data 1.10e-04 (3.12e-04)	Tok/s 233717 (244198)	Loss/tok 3.3476 (4.9772)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.036 (0.088)	Data 1.15e-04 (3.10e-04)	Tok/s 219388 (244201)	Loss/tok 2.8433 (4.9651)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.09e-04)	Tok/s 256934 (244207)	Loss/tok 3.6256 (4.9535)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.066 (0.088)	Data 1.14e-04 (3.07e-04)	Tok/s 234403 (244130)	Loss/tok 3.3723 (4.9438)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.05e-04)	Tok/s 239207 (244104)	Loss/tok 3.3839 (4.9328)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.04e-04)	Tok/s 233773 (244141)	Loss/tok 3.3739 (4.9206)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1190/1291]	Time 0.172 (0.088)	Data 1.17e-04 (3.02e-04)	Tok/s 259499 (244173)	Loss/tok 3.9983 (4.9081)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1200/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.01e-04)	Tok/s 235132 (244186)	Loss/tok 3.4354 (4.8967)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.099 (0.088)	Data 1.16e-04 (2.99e-04)	Tok/s 256860 (244207)	Loss/tok 3.6007 (4.8856)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.099 (0.088)	Data 1.12e-04 (2.98e-04)	Tok/s 253543 (244156)	Loss/tok 3.5951 (4.8761)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.134 (0.088)	Data 1.11e-04 (2.96e-04)	Tok/s 259883 (244155)	Loss/tok 3.7469 (4.8652)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.067 (0.088)	Data 1.13e-04 (2.95e-04)	Tok/s 229333 (244097)	Loss/tok 3.2437 (4.8559)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.066 (0.088)	Data 1.12e-04 (2.93e-04)	Tok/s 231882 (244102)	Loss/tok 3.4094 (4.8455)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.067 (0.088)	Data 1.12e-04 (2.92e-04)	Tok/s 230752 (244085)	Loss/tok 3.4349 (4.8356)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.099 (0.088)	Data 1.10e-04 (2.90e-04)	Tok/s 251899 (244147)	Loss/tok 3.6812 (4.8243)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.066 (0.088)	Data 1.37e-04 (2.89e-04)	Tok/s 230827 (244113)	Loss/tok 3.2730 (4.8155)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.066 (0.088)	Data 4.39e-05 (2.90e-04)	Tok/s 235195 (244141)	Loss/tok 3.3004 (4.8046)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592453985648, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453985649, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.484 (0.484)	Decoder iters 149.0 (149.0)	Tok/s 32047 (32047)
0: Running moses detokenizer
0: BLEU(score=19.46566945730241, counts=[33808, 15335, 8095, 4430], totals=[63242, 60239, 57236, 54238], precisions=[53.458144903703236, 25.456929895914605, 14.143196589559018, 8.167705298867952], bp=0.9775803366012199, sys_len=63242, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592453987788, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19469999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453987788, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.8043	Test BLEU: 19.47
0: Performance: Epoch: 0	Training: 1952646 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592453987788, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453987788, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453987788, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 704517457
0: TRAIN [1][0/1291]	Time 0.286 (0.286)	Data 2.03e-01 (2.03e-01)	Tok/s 54107 (54107)	Loss/tok 3.3012 (3.3012)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.099 (0.115)	Data 1.15e-04 (1.86e-02)	Tok/s 252921 (228866)	Loss/tok 3.6573 (3.5740)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.135 (0.098)	Data 1.23e-04 (9.79e-03)	Tok/s 260166 (234518)	Loss/tok 3.6871 (3.5071)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.099 (0.092)	Data 1.18e-04 (6.67e-03)	Tok/s 253652 (237053)	Loss/tok 3.4734 (3.4720)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][40/1291]	Time 0.100 (0.089)	Data 1.12e-04 (5.07e-03)	Tok/s 250615 (236801)	Loss/tok 3.4605 (3.4704)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][50/1291]	Time 0.067 (0.088)	Data 1.13e-04 (4.10e-03)	Tok/s 232528 (237396)	Loss/tok 3.2162 (3.4819)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.066 (0.085)	Data 1.14e-04 (3.45e-03)	Tok/s 230136 (236738)	Loss/tok 3.2872 (3.4599)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][70/1291]	Time 0.134 (0.087)	Data 1.10e-04 (2.98e-03)	Tok/s 262002 (237958)	Loss/tok 3.7335 (3.4830)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.62e-03)	Tok/s 234615 (239394)	Loss/tok 3.3031 (3.5020)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.35e-03)	Tok/s 227056 (239901)	Loss/tok 2.7533 (3.4952)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.134 (0.089)	Data 1.19e-04 (2.13e-03)	Tok/s 260583 (240455)	Loss/tok 3.7661 (3.5023)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.173 (0.091)	Data 1.40e-04 (1.95e-03)	Tok/s 261400 (241245)	Loss/tok 3.7419 (3.5072)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.066 (0.090)	Data 1.23e-04 (1.79e-03)	Tok/s 236563 (241225)	Loss/tok 3.3162 (3.5063)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.099 (0.090)	Data 1.11e-04 (1.67e-03)	Tok/s 253700 (241525)	Loss/tok 3.4307 (3.5073)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.099 (0.091)	Data 1.15e-04 (1.56e-03)	Tok/s 256801 (241984)	Loss/tok 3.4983 (3.5167)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.135 (0.091)	Data 1.32e-04 (1.46e-03)	Tok/s 260634 (242324)	Loss/tok 3.7507 (3.5174)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.099 (0.091)	Data 1.10e-04 (1.38e-03)	Tok/s 253908 (242405)	Loss/tok 3.5910 (3.5172)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.068 (0.091)	Data 1.10e-04 (1.30e-03)	Tok/s 227142 (242046)	Loss/tok 3.2640 (3.5171)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.036 (0.090)	Data 1.13e-04 (1.24e-03)	Tok/s 224310 (241833)	Loss/tok 2.8066 (3.5121)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.099 (0.090)	Data 1.12e-04 (1.18e-03)	Tok/s 252052 (241873)	Loss/tok 3.5567 (3.5128)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][200/1291]	Time 0.174 (0.090)	Data 1.08e-04 (1.13e-03)	Tok/s 253842 (241716)	Loss/tok 3.8549 (3.5083)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.099 (0.090)	Data 5.91e-04 (1.08e-03)	Tok/s 256237 (241736)	Loss/tok 3.4800 (3.5063)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.066 (0.089)	Data 1.08e-04 (1.04e-03)	Tok/s 230978 (241608)	Loss/tok 3.2147 (3.5001)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.067 (0.089)	Data 1.15e-04 (9.96e-04)	Tok/s 234473 (241656)	Loss/tok 3.2134 (3.4956)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.066 (0.089)	Data 1.10e-04 (9.60e-04)	Tok/s 237862 (241766)	Loss/tok 3.2185 (3.4933)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.067 (0.088)	Data 1.12e-04 (9.26e-04)	Tok/s 233837 (241590)	Loss/tok 3.2000 (3.4866)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.066 (0.088)	Data 1.15e-04 (8.96e-04)	Tok/s 234415 (241503)	Loss/tok 3.3156 (3.4866)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.067 (0.088)	Data 1.10e-04 (8.67e-04)	Tok/s 231099 (241613)	Loss/tok 3.1188 (3.4867)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.066 (0.088)	Data 1.09e-04 (8.40e-04)	Tok/s 231399 (241701)	Loss/tok 3.2706 (3.4895)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.135 (0.088)	Data 1.17e-04 (8.15e-04)	Tok/s 259136 (241870)	Loss/tok 3.7612 (3.4919)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.068 (0.088)	Data 1.14e-04 (7.92e-04)	Tok/s 228248 (241806)	Loss/tok 3.2530 (3.4912)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.102 (0.089)	Data 1.13e-04 (7.70e-04)	Tok/s 248780 (241769)	Loss/tok 3.4972 (3.4914)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][320/1291]	Time 0.068 (0.089)	Data 1.09e-04 (7.49e-04)	Tok/s 226322 (241546)	Loss/tok 3.2299 (3.4887)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.101 (0.089)	Data 1.13e-04 (7.30e-04)	Tok/s 250680 (241519)	Loss/tok 3.5027 (3.4887)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.069 (0.089)	Data 1.10e-04 (7.12e-04)	Tok/s 224844 (241369)	Loss/tok 3.2099 (3.4870)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.102 (0.088)	Data 1.13e-04 (6.95e-04)	Tok/s 243411 (241117)	Loss/tok 3.4508 (3.4843)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.068 (0.088)	Data 1.16e-04 (6.79e-04)	Tok/s 227180 (241018)	Loss/tok 3.1954 (3.4822)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.176 (0.088)	Data 1.11e-04 (6.64e-04)	Tok/s 253358 (240797)	Loss/tok 3.8801 (3.4811)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.136 (0.088)	Data 1.11e-04 (6.49e-04)	Tok/s 258134 (240566)	Loss/tok 3.6725 (3.4790)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.101 (0.088)	Data 1.13e-04 (6.36e-04)	Tok/s 249763 (240491)	Loss/tok 3.5452 (3.4786)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][400/1291]	Time 0.069 (0.088)	Data 1.13e-04 (6.23e-04)	Tok/s 224655 (240459)	Loss/tok 3.2278 (3.4778)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.134 (0.089)	Data 1.13e-04 (6.10e-04)	Tok/s 262017 (240567)	Loss/tok 3.5988 (3.4790)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.98e-04)	Tok/s 229226 (240672)	Loss/tok 3.1653 (3.4793)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.099 (0.089)	Data 1.34e-04 (5.88e-04)	Tok/s 253179 (240634)	Loss/tok 3.5251 (3.4778)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.067 (0.089)	Data 1.14e-04 (5.77e-04)	Tok/s 232347 (240777)	Loss/tok 3.2496 (3.4774)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.67e-04)	Tok/s 233433 (240930)	Loss/tok 3.1902 (3.4788)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.036 (0.089)	Data 1.20e-04 (5.57e-04)	Tok/s 218062 (240984)	Loss/tok 2.7941 (3.4778)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.099 (0.089)	Data 1.10e-04 (5.47e-04)	Tok/s 255252 (240958)	Loss/tok 3.4370 (3.4760)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.066 (0.088)	Data 1.21e-04 (5.38e-04)	Tok/s 233951 (240942)	Loss/tok 3.1848 (3.4734)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.100 (0.088)	Data 1.12e-04 (5.30e-04)	Tok/s 252847 (241038)	Loss/tok 3.4903 (3.4732)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.174 (0.089)	Data 1.13e-04 (5.22e-04)	Tok/s 255639 (241194)	Loss/tok 3.8384 (3.4756)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.036 (0.089)	Data 1.13e-04 (5.14e-04)	Tok/s 217850 (241131)	Loss/tok 2.7717 (3.4732)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][520/1291]	Time 0.135 (0.088)	Data 1.36e-04 (5.06e-04)	Tok/s 257489 (241085)	Loss/tok 3.6700 (3.4729)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.099 (0.089)	Data 1.13e-04 (4.99e-04)	Tok/s 254081 (241196)	Loss/tok 3.5483 (3.4731)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.088)	Data 1.14e-04 (4.91e-04)	Tok/s 235016 (241104)	Loss/tok 3.2514 (3.4705)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.135 (0.088)	Data 1.13e-04 (4.85e-04)	Tok/s 259937 (241160)	Loss/tok 3.6164 (3.4699)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.173 (0.089)	Data 1.24e-04 (4.78e-04)	Tok/s 259716 (241238)	Loss/tok 3.7202 (3.4706)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.067 (0.088)	Data 1.13e-04 (4.72e-04)	Tok/s 230886 (241208)	Loss/tok 3.2084 (3.4684)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.135 (0.089)	Data 1.28e-04 (4.65e-04)	Tok/s 259960 (241290)	Loss/tok 3.6502 (3.4689)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.099 (0.089)	Data 1.18e-04 (4.59e-04)	Tok/s 252796 (241431)	Loss/tok 3.4772 (3.4715)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.54e-04)	Tok/s 230032 (241521)	Loss/tok 3.2917 (3.4712)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.099 (0.089)	Data 1.16e-04 (4.48e-04)	Tok/s 254960 (241611)	Loss/tok 3.3898 (3.4698)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.100 (0.089)	Data 1.11e-04 (4.43e-04)	Tok/s 251630 (241635)	Loss/tok 3.4825 (3.4684)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.135 (0.089)	Data 1.12e-04 (4.38e-04)	Tok/s 260752 (241610)	Loss/tok 3.6343 (3.4681)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.099 (0.089)	Data 1.22e-04 (4.33e-04)	Tok/s 253383 (241689)	Loss/tok 3.4907 (3.4717)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][650/1291]	Time 0.135 (0.090)	Data 1.11e-04 (4.28e-04)	Tok/s 260437 (241774)	Loss/tok 3.6118 (3.4731)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.173 (0.090)	Data 1.11e-04 (4.23e-04)	Tok/s 258022 (241852)	Loss/tok 3.7871 (3.4729)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.035 (0.090)	Data 1.09e-04 (4.18e-04)	Tok/s 222055 (241887)	Loss/tok 2.8328 (3.4716)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.14e-04)	Tok/s 231853 (241814)	Loss/tok 3.1744 (3.4696)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.09e-04)	Tok/s 230906 (241815)	Loss/tok 3.0974 (3.4679)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.05e-04)	Tok/s 234336 (241855)	Loss/tok 3.2765 (3.4681)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.01e-04)	Tok/s 232433 (241879)	Loss/tok 3.1941 (3.4675)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.97e-04)	Tok/s 255714 (241908)	Loss/tok 3.4351 (3.4668)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.067 (0.089)	Data 1.22e-04 (3.93e-04)	Tok/s 232997 (241969)	Loss/tok 3.2054 (3.4672)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.135 (0.090)	Data 1.15e-04 (3.89e-04)	Tok/s 257986 (242077)	Loss/tok 3.6118 (3.4662)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.099 (0.090)	Data 1.12e-04 (3.86e-04)	Tok/s 254283 (242135)	Loss/tok 3.3575 (3.4660)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.82e-04)	Tok/s 227764 (242117)	Loss/tok 3.1322 (3.4642)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.79e-04)	Tok/s 232312 (242027)	Loss/tok 3.2080 (3.4624)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][780/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.75e-04)	Tok/s 233043 (242062)	Loss/tok 3.1949 (3.4622)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.72e-04)	Tok/s 237325 (242058)	Loss/tok 3.2474 (3.4609)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.69e-04)	Tok/s 233042 (242015)	Loss/tok 3.1155 (3.4590)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.66e-04)	Tok/s 253650 (242019)	Loss/tok 3.3599 (3.4576)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][820/1291]	Time 0.066 (0.089)	Data 1.19e-04 (3.63e-04)	Tok/s 234798 (242139)	Loss/tok 3.1449 (3.4592)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.135 (0.089)	Data 1.23e-04 (3.60e-04)	Tok/s 258668 (242063)	Loss/tok 3.6095 (3.4577)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.57e-04)	Tok/s 232511 (242067)	Loss/tok 3.1094 (3.4566)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.174 (0.089)	Data 1.13e-04 (3.54e-04)	Tok/s 258840 (242121)	Loss/tok 3.8040 (3.4586)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.51e-04)	Tok/s 252801 (242116)	Loss/tok 3.5532 (3.4577)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.136 (0.089)	Data 1.11e-04 (3.48e-04)	Tok/s 257505 (242113)	Loss/tok 3.6179 (3.4572)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.099 (0.089)	Data 1.22e-04 (3.46e-04)	Tok/s 256350 (242072)	Loss/tok 3.3667 (3.4554)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.134 (0.089)	Data 1.13e-04 (3.43e-04)	Tok/s 262231 (242058)	Loss/tok 3.6450 (3.4542)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.41e-04)	Tok/s 233933 (242037)	Loss/tok 3.1706 (3.4531)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.036 (0.089)	Data 1.15e-04 (3.38e-04)	Tok/s 221448 (241966)	Loss/tok 2.7021 (3.4516)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.066 (0.089)	Data 1.35e-04 (3.36e-04)	Tok/s 229260 (241938)	Loss/tok 3.1522 (3.4508)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.100 (0.089)	Data 1.16e-04 (3.33e-04)	Tok/s 251772 (242010)	Loss/tok 3.4472 (3.4512)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.31e-04)	Tok/s 252598 (242039)	Loss/tok 3.4374 (3.4501)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][950/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.29e-04)	Tok/s 253491 (242058)	Loss/tok 3.3378 (3.4500)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.27e-04)	Tok/s 251517 (242015)	Loss/tok 3.3783 (3.4484)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.24e-04)	Tok/s 221606 (241970)	Loss/tok 2.7583 (3.4468)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.136 (0.089)	Data 1.23e-04 (3.22e-04)	Tok/s 256559 (242049)	Loss/tok 3.5048 (3.4458)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.20e-04)	Tok/s 231287 (242024)	Loss/tok 3.1917 (3.4445)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.18e-04)	Tok/s 233513 (242014)	Loss/tok 3.1354 (3.4433)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1010/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.16e-04)	Tok/s 254945 (242139)	Loss/tok 3.4052 (3.4453)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.14e-04)	Tok/s 233095 (242051)	Loss/tok 3.2030 (3.4434)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.12e-04)	Tok/s 235171 (242067)	Loss/tok 3.1457 (3.4429)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.10e-04)	Tok/s 232830 (242051)	Loss/tok 3.2689 (3.4424)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.08e-04)	Tok/s 235273 (241995)	Loss/tok 3.2392 (3.4408)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.174 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 256491 (242017)	Loss/tok 3.7389 (3.4412)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.067 (0.088)	Data 1.17e-04 (3.05e-04)	Tok/s 226801 (241985)	Loss/tok 3.1849 (3.4401)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.067 (0.088)	Data 1.11e-04 (3.03e-04)	Tok/s 234889 (241989)	Loss/tok 3.2789 (3.4398)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.01e-04)	Tok/s 258413 (241972)	Loss/tok 3.4031 (3.4384)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.099 (0.089)	Data 1.19e-04 (3.00e-04)	Tok/s 252800 (242030)	Loss/tok 3.4045 (3.4389)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 233059 (242061)	Loss/tok 3.1038 (3.4386)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.96e-04)	Tok/s 229688 (242083)	Loss/tok 3.1179 (3.4376)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.95e-04)	Tok/s 232605 (242145)	Loss/tok 3.2143 (3.4376)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1140/1291]	Time 0.134 (0.089)	Data 1.18e-04 (2.93e-04)	Tok/s 260548 (242127)	Loss/tok 3.5870 (3.4368)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.92e-04)	Tok/s 234206 (242130)	Loss/tok 3.1071 (3.4362)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.066 (0.089)	Data 1.20e-04 (2.90e-04)	Tok/s 230937 (242125)	Loss/tok 3.2163 (3.4352)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1170/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.88e-04)	Tok/s 234638 (242183)	Loss/tok 3.1577 (3.4344)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.87e-04)	Tok/s 255251 (242213)	Loss/tok 3.2604 (3.4341)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.134 (0.089)	Data 1.10e-04 (2.86e-04)	Tok/s 256777 (242256)	Loss/tok 3.5816 (3.4348)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1200/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.84e-04)	Tok/s 254138 (242311)	Loss/tok 3.3823 (3.4350)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.83e-04)	Tok/s 252278 (242343)	Loss/tok 3.3470 (3.4349)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.81e-04)	Tok/s 252835 (242360)	Loss/tok 3.3493 (3.4345)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.134 (0.089)	Data 1.19e-04 (2.80e-04)	Tok/s 263624 (242346)	Loss/tok 3.4212 (3.4335)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.79e-04)	Tok/s 253948 (242371)	Loss/tok 3.3903 (3.4334)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.77e-04)	Tok/s 235008 (242326)	Loss/tok 3.1441 (3.4325)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.76e-04)	Tok/s 227074 (242327)	Loss/tok 3.1442 (3.4323)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.75e-04)	Tok/s 253432 (242307)	Loss/tok 3.3737 (3.4312)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.134 (0.089)	Data 1.16e-04 (2.73e-04)	Tok/s 260915 (242352)	Loss/tok 3.5155 (3.4308)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.099 (0.089)	Data 4.48e-05 (2.75e-04)	Tok/s 253912 (242298)	Loss/tok 3.3471 (3.4293)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592454103345, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454103345, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.491 (0.491)	Decoder iters 149.0 (149.0)	Tok/s 33490 (33490)
0: Running moses detokenizer
0: BLEU(score=21.885553379970673, counts=[35958, 17369, 9598, 5542], totals=[66282, 63279, 60276, 57278], precisions=[54.250022630578435, 27.448284580982634, 15.92341893954476, 9.67561716540382], bp=1.0, sys_len=66282, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454105309, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2189, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454105309, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4294	Test BLEU: 21.89
0: Performance: Epoch: 1	Training: 1937839 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592454105310, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454105310, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592454105310, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1181959647
0: TRAIN [2][0/1291]	Time 0.282 (0.282)	Data 1.82e-01 (1.82e-01)	Tok/s 55184 (55184)	Loss/tok 3.0602 (3.0602)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.099 (0.117)	Data 1.15e-04 (1.66e-02)	Tok/s 255600 (228642)	Loss/tok 3.2441 (3.3423)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.066 (0.101)	Data 1.18e-04 (8.76e-03)	Tok/s 234455 (235027)	Loss/tok 3.0495 (3.2709)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.135 (0.101)	Data 1.14e-04 (5.97e-03)	Tok/s 258184 (239031)	Loss/tok 3.5889 (3.2982)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][40/1291]	Time 0.135 (0.100)	Data 1.12e-04 (4.54e-03)	Tok/s 259576 (240142)	Loss/tok 3.5034 (3.3062)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.135 (0.098)	Data 1.50e-04 (3.68e-03)	Tok/s 258195 (240961)	Loss/tok 3.4980 (3.3111)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.070 (0.093)	Data 1.12e-04 (3.09e-03)	Tok/s 218947 (239608)	Loss/tok 3.0148 (3.2849)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.100 (0.094)	Data 1.16e-04 (2.67e-03)	Tok/s 249580 (240741)	Loss/tok 3.2750 (3.2914)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.099 (0.094)	Data 1.14e-04 (2.36e-03)	Tok/s 255375 (241271)	Loss/tok 3.2699 (3.2922)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.136 (0.094)	Data 1.19e-04 (2.11e-03)	Tok/s 258196 (241595)	Loss/tok 3.4126 (3.2934)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.136 (0.093)	Data 1.12e-04 (1.92e-03)	Tok/s 255343 (241785)	Loss/tok 3.4961 (3.2908)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.135 (0.094)	Data 1.17e-04 (1.76e-03)	Tok/s 260334 (242438)	Loss/tok 3.3669 (3.2954)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.099 (0.094)	Data 1.12e-04 (1.63e-03)	Tok/s 254198 (242753)	Loss/tok 3.2381 (3.2986)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.067 (0.093)	Data 1.11e-04 (1.51e-03)	Tok/s 232336 (242655)	Loss/tok 3.0798 (3.2980)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.099 (0.094)	Data 1.14e-04 (1.41e-03)	Tok/s 251379 (243267)	Loss/tok 3.2703 (3.3040)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.067 (0.093)	Data 1.17e-04 (1.33e-03)	Tok/s 231751 (242936)	Loss/tok 3.0761 (3.2995)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.036 (0.093)	Data 1.13e-04 (1.25e-03)	Tok/s 224551 (242917)	Loss/tok 2.5986 (3.2953)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][170/1291]	Time 0.067 (0.091)	Data 1.17e-04 (1.18e-03)	Tok/s 231174 (242711)	Loss/tok 3.0795 (3.2898)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][180/1291]	Time 0.067 (0.092)	Data 1.16e-04 (1.13e-03)	Tok/s 230919 (242960)	Loss/tok 3.0047 (3.2956)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.067 (0.092)	Data 1.13e-04 (1.07e-03)	Tok/s 232829 (242847)	Loss/tok 3.0343 (3.2924)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.067 (0.091)	Data 1.17e-04 (1.02e-03)	Tok/s 229291 (242611)	Loss/tok 3.0523 (3.2874)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.067 (0.090)	Data 1.11e-04 (9.81e-04)	Tok/s 232433 (242261)	Loss/tok 3.1305 (3.2821)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.036 (0.090)	Data 1.10e-04 (9.42e-04)	Tok/s 221419 (242191)	Loss/tok 2.7120 (3.2820)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.134 (0.089)	Data 1.37e-04 (9.06e-04)	Tok/s 263860 (241924)	Loss/tok 3.4993 (3.2786)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.135 (0.089)	Data 1.13e-04 (8.73e-04)	Tok/s 260520 (242062)	Loss/tok 3.4976 (3.2791)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.135 (0.089)	Data 1.15e-04 (8.43e-04)	Tok/s 259847 (242135)	Loss/tok 3.4996 (3.2795)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.099 (0.089)	Data 1.12e-04 (8.15e-04)	Tok/s 255188 (242215)	Loss/tok 3.3777 (3.2799)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.134 (0.090)	Data 1.12e-04 (7.89e-04)	Tok/s 259634 (242300)	Loss/tok 3.4859 (3.2846)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.067 (0.089)	Data 1.13e-04 (7.65e-04)	Tok/s 229817 (242234)	Loss/tok 3.0106 (3.2820)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.100 (0.089)	Data 1.14e-04 (7.43e-04)	Tok/s 248270 (242214)	Loss/tok 3.3145 (3.2806)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.036 (0.089)	Data 1.12e-04 (7.22e-04)	Tok/s 220631 (242333)	Loss/tok 2.6576 (3.2824)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][310/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.03e-04)	Tok/s 234560 (242481)	Loss/tok 2.9264 (3.2830)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.067 (0.089)	Data 1.37e-04 (6.84e-04)	Tok/s 231861 (242343)	Loss/tok 3.1381 (3.2823)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][330/1291]	Time 0.099 (0.089)	Data 1.11e-04 (6.67e-04)	Tok/s 250791 (242280)	Loss/tok 3.2949 (3.2816)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.099 (0.089)	Data 1.14e-04 (6.51e-04)	Tok/s 254828 (242254)	Loss/tok 3.1993 (3.2786)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.100 (0.089)	Data 1.14e-04 (6.36e-04)	Tok/s 247647 (242161)	Loss/tok 3.2660 (3.2777)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.067 (0.089)	Data 1.11e-04 (6.22e-04)	Tok/s 236348 (242162)	Loss/tok 3.0865 (3.2778)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.100 (0.089)	Data 1.12e-04 (6.08e-04)	Tok/s 253758 (242271)	Loss/tok 3.1895 (3.2785)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.067 (0.089)	Data 1.13e-04 (5.95e-04)	Tok/s 230468 (242203)	Loss/tok 3.0893 (3.2798)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.036 (0.089)	Data 1.13e-04 (5.83e-04)	Tok/s 222508 (242168)	Loss/tok 2.7876 (3.2803)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.067 (0.088)	Data 1.16e-04 (5.71e-04)	Tok/s 232423 (242070)	Loss/tok 3.1176 (3.2777)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.099 (0.088)	Data 1.17e-04 (5.60e-04)	Tok/s 249058 (241936)	Loss/tok 3.3659 (3.2770)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][420/1291]	Time 0.174 (0.089)	Data 1.12e-04 (5.50e-04)	Tok/s 255619 (242035)	Loss/tok 3.6506 (3.2820)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.100 (0.089)	Data 1.15e-04 (5.40e-04)	Tok/s 253598 (242078)	Loss/tok 3.1843 (3.2843)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.035 (0.089)	Data 1.20e-04 (5.30e-04)	Tok/s 224485 (241989)	Loss/tok 2.6765 (3.2878)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.21e-04)	Tok/s 234682 (242054)	Loss/tok 3.1018 (3.2898)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.067 (0.089)	Data 1.19e-04 (5.12e-04)	Tok/s 237045 (242008)	Loss/tok 3.0784 (3.2898)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.135 (0.090)	Data 1.16e-04 (5.04e-04)	Tok/s 258378 (242183)	Loss/tok 3.3869 (3.2911)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.135 (0.090)	Data 1.12e-04 (4.95e-04)	Tok/s 258600 (242194)	Loss/tok 3.5175 (3.2918)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.066 (0.090)	Data 1.16e-04 (4.88e-04)	Tok/s 233443 (242163)	Loss/tok 3.0723 (3.2921)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.135 (0.090)	Data 1.14e-04 (4.80e-04)	Tok/s 261746 (242158)	Loss/tok 3.3748 (3.2914)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.174 (0.090)	Data 1.16e-04 (4.73e-04)	Tok/s 256747 (242230)	Loss/tok 3.6420 (3.2912)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.66e-04)	Tok/s 236475 (242255)	Loss/tok 3.0671 (3.2897)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.60e-04)	Tok/s 234984 (242326)	Loss/tok 3.0512 (3.2893)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][540/1291]	Time 0.035 (0.089)	Data 1.11e-04 (4.53e-04)	Tok/s 220498 (242297)	Loss/tok 2.6462 (3.2885)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][550/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.47e-04)	Tok/s 233694 (242384)	Loss/tok 3.0909 (3.2911)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.067 (0.090)	Data 1.18e-04 (4.41e-04)	Tok/s 233599 (242395)	Loss/tok 3.1123 (3.2923)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.067 (0.090)	Data 1.12e-04 (4.36e-04)	Tok/s 232959 (242261)	Loss/tok 3.0683 (3.2900)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.036 (0.089)	Data 1.17e-04 (4.30e-04)	Tok/s 224313 (242213)	Loss/tok 2.5816 (3.2886)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.135 (0.089)	Data 1.27e-04 (4.25e-04)	Tok/s 258971 (242238)	Loss/tok 3.4481 (3.2897)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.20e-04)	Tok/s 231682 (242325)	Loss/tok 3.1476 (3.2899)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.15e-04)	Tok/s 231603 (242336)	Loss/tok 3.0946 (3.2894)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.10e-04)	Tok/s 226534 (242317)	Loss/tok 3.0453 (3.2892)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.135 (0.089)	Data 1.11e-04 (4.05e-04)	Tok/s 260702 (242219)	Loss/tok 3.3857 (3.2874)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.174 (0.089)	Data 1.11e-04 (4.01e-04)	Tok/s 259138 (242292)	Loss/tok 3.5280 (3.2887)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.96e-04)	Tok/s 231780 (242344)	Loss/tok 3.1362 (3.2887)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.92e-04)	Tok/s 252645 (242258)	Loss/tok 3.2627 (3.2878)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.88e-04)	Tok/s 228847 (242268)	Loss/tok 3.1238 (3.2883)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][680/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.84e-04)	Tok/s 230180 (242299)	Loss/tok 3.0954 (3.2884)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.80e-04)	Tok/s 225842 (242230)	Loss/tok 2.7270 (3.2874)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.066 (0.089)	Data 1.17e-04 (3.76e-04)	Tok/s 233896 (242204)	Loss/tok 3.0882 (3.2880)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.036 (0.089)	Data 1.35e-04 (3.73e-04)	Tok/s 225612 (242134)	Loss/tok 2.7135 (3.2871)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.69e-04)	Tok/s 230762 (242058)	Loss/tok 3.1085 (3.2864)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.65e-04)	Tok/s 254466 (242009)	Loss/tok 3.3478 (3.2865)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.134 (0.089)	Data 1.12e-04 (3.62e-04)	Tok/s 257452 (242080)	Loss/tok 3.4890 (3.2879)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.067 (0.089)	Data 1.34e-04 (3.59e-04)	Tok/s 233887 (242104)	Loss/tok 3.0953 (3.2891)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.135 (0.089)	Data 1.15e-04 (3.56e-04)	Tok/s 258993 (242091)	Loss/tok 3.5375 (3.2885)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.53e-04)	Tok/s 255078 (242078)	Loss/tok 3.2309 (3.2877)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.50e-04)	Tok/s 232095 (242121)	Loss/tok 3.0599 (3.2882)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.47e-04)	Tok/s 254227 (242088)	Loss/tok 3.2474 (3.2867)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.067 (0.089)	Data 1.21e-04 (3.44e-04)	Tok/s 230603 (242040)	Loss/tok 3.0878 (3.2855)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][810/1291]	Time 0.135 (0.089)	Data 1.27e-04 (3.41e-04)	Tok/s 259420 (241998)	Loss/tok 3.4732 (3.2848)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][820/1291]	Time 0.036 (0.088)	Data 1.15e-04 (3.38e-04)	Tok/s 221587 (241934)	Loss/tok 2.6245 (3.2844)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.134 (0.088)	Data 1.11e-04 (3.35e-04)	Tok/s 261451 (241939)	Loss/tok 3.4749 (3.2841)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.067 (0.088)	Data 1.13e-04 (3.33e-04)	Tok/s 227603 (241947)	Loss/tok 3.0605 (3.2831)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.30e-04)	Tok/s 231116 (242019)	Loss/tok 3.0791 (3.2842)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.135 (0.089)	Data 1.09e-04 (3.28e-04)	Tok/s 261653 (242105)	Loss/tok 3.4321 (3.2845)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.136 (0.089)	Data 1.19e-04 (3.25e-04)	Tok/s 257219 (242200)	Loss/tok 3.3969 (3.2842)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.23e-04)	Tok/s 248698 (242293)	Loss/tok 3.3806 (3.2857)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.21e-04)	Tok/s 235280 (242190)	Loss/tok 3.0457 (3.2842)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.136 (0.089)	Data 1.12e-04 (3.18e-04)	Tok/s 254432 (242148)	Loss/tok 3.4831 (3.2842)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.16e-04)	Tok/s 230969 (242119)	Loss/tok 3.1228 (3.2832)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.067 (0.089)	Data 1.17e-04 (3.14e-04)	Tok/s 231721 (242110)	Loss/tok 3.0386 (3.2831)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.067 (0.089)	Data 1.21e-04 (3.12e-04)	Tok/s 226250 (242113)	Loss/tok 3.0488 (3.2843)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.10e-04)	Tok/s 231753 (242124)	Loss/tok 3.0074 (3.2837)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][950/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.08e-04)	Tok/s 255083 (242189)	Loss/tok 3.2579 (3.2839)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.06e-04)	Tok/s 253519 (242185)	Loss/tok 3.2580 (3.2827)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.04e-04)	Tok/s 234304 (242175)	Loss/tok 3.1414 (3.2824)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][980/1291]	Time 0.175 (0.089)	Data 1.12e-04 (3.02e-04)	Tok/s 257171 (242267)	Loss/tok 3.6556 (3.2833)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.135 (0.089)	Data 1.15e-04 (3.00e-04)	Tok/s 260838 (242330)	Loss/tok 3.3547 (3.2842)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 232522 (242298)	Loss/tok 3.0695 (3.2831)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.96e-04)	Tok/s 228987 (242333)	Loss/tok 3.1600 (3.2836)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.134 (0.089)	Data 1.16e-04 (2.95e-04)	Tok/s 262167 (242458)	Loss/tok 3.3767 (3.2843)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.136 (0.089)	Data 1.18e-04 (2.93e-04)	Tok/s 256721 (242500)	Loss/tok 3.4775 (3.2851)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.91e-04)	Tok/s 230646 (242432)	Loss/tok 2.9572 (3.2841)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1050/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.89e-04)	Tok/s 219384 (242448)	Loss/tok 2.6008 (3.2845)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.099 (0.089)	Data 1.17e-04 (2.88e-04)	Tok/s 251727 (242500)	Loss/tok 3.2352 (3.2848)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.86e-04)	Tok/s 257911 (242564)	Loss/tok 3.4778 (3.2855)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.85e-04)	Tok/s 233702 (242523)	Loss/tok 3.1002 (3.2847)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.83e-04)	Tok/s 231757 (242509)	Loss/tok 3.0605 (3.2847)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.136 (0.089)	Data 1.12e-04 (2.82e-04)	Tok/s 257914 (242486)	Loss/tok 3.3649 (3.2842)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.099 (0.089)	Data 1.14e-04 (2.80e-04)	Tok/s 253228 (242529)	Loss/tok 3.2243 (3.2835)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.79e-04)	Tok/s 232128 (242512)	Loss/tok 3.0350 (3.2829)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.067 (0.089)	Data 1.34e-04 (2.77e-04)	Tok/s 232149 (242554)	Loss/tok 3.0856 (3.2835)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.76e-04)	Tok/s 235711 (242540)	Loss/tok 3.0207 (3.2836)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.74e-04)	Tok/s 253536 (242556)	Loss/tok 3.2191 (3.2844)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.099 (0.089)	Data 1.16e-04 (2.73e-04)	Tok/s 255998 (242565)	Loss/tok 3.1959 (3.2849)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.067 (0.089)	Data 1.18e-04 (2.72e-04)	Tok/s 227773 (242550)	Loss/tok 3.0844 (3.2846)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1180/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.70e-04)	Tok/s 225409 (242578)	Loss/tok 2.6390 (3.2845)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.69e-04)	Tok/s 259676 (242582)	Loss/tok 3.4643 (3.2854)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.067 (0.089)	Data 1.29e-04 (2.68e-04)	Tok/s 227919 (242547)	Loss/tok 3.1113 (3.2848)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.67e-04)	Tok/s 233792 (242569)	Loss/tok 2.9866 (3.2842)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.65e-04)	Tok/s 231467 (242523)	Loss/tok 3.0336 (3.2831)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.64e-04)	Tok/s 233299 (242436)	Loss/tok 3.1373 (3.2823)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.63e-04)	Tok/s 236205 (242397)	Loss/tok 3.1010 (3.2825)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.62e-04)	Tok/s 252124 (242432)	Loss/tok 3.3306 (3.2830)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.100 (0.089)	Data 1.14e-04 (2.61e-04)	Tok/s 251173 (242404)	Loss/tok 3.2318 (3.2821)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.175 (0.089)	Data 1.16e-04 (2.59e-04)	Tok/s 255467 (242424)	Loss/tok 3.6309 (3.2825)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.099 (0.089)	Data 1.21e-04 (2.58e-04)	Tok/s 253529 (242442)	Loss/tok 3.2502 (3.2818)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.099 (0.089)	Data 4.77e-05 (2.60e-04)	Tok/s 254294 (242408)	Loss/tok 3.2103 (3.2811)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592454220803, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454220804, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.465 (0.465)	Decoder iters 140.0 (140.0)	Tok/s 34908 (34908)
0: Running moses detokenizer
0: BLEU(score=22.70244878913912, counts=[35888, 17535, 9732, 5640], totals=[64247, 61244, 58242, 55245], precisions=[55.859417560353016, 28.63137613480504, 16.709591016792007, 10.209068693999457], bp=0.9933448894095914, sys_len=64247, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454222712, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22699999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454222712, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2822	Test BLEU: 22.70
0: Performance: Epoch: 2	Training: 1939070 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592454222712, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454222712, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592454222712, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 581989725
0: TRAIN [3][0/1291]	Time 0.270 (0.270)	Data 1.93e-01 (1.93e-01)	Tok/s 56558 (56558)	Loss/tok 2.9798 (2.9798)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][10/1291]	Time 0.067 (0.116)	Data 1.13e-04 (1.76e-02)	Tok/s 230324 (230473)	Loss/tok 3.0198 (3.2298)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.067 (0.102)	Data 1.16e-04 (9.30e-03)	Tok/s 232505 (236403)	Loss/tok 2.9974 (3.1908)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][30/1291]	Time 0.136 (0.105)	Data 1.19e-04 (6.34e-03)	Tok/s 255120 (240754)	Loss/tok 3.4233 (3.2346)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.067 (0.098)	Data 1.22e-04 (4.82e-03)	Tok/s 234263 (240089)	Loss/tok 2.9144 (3.2079)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.036 (0.095)	Data 1.12e-04 (3.90e-03)	Tok/s 225144 (240156)	Loss/tok 2.6714 (3.1992)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.099 (0.095)	Data 1.26e-04 (3.28e-03)	Tok/s 254057 (240979)	Loss/tok 3.2250 (3.1985)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.136 (0.092)	Data 1.18e-04 (2.83e-03)	Tok/s 261312 (240376)	Loss/tok 3.2727 (3.1831)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.036 (0.091)	Data 1.13e-04 (2.50e-03)	Tok/s 219949 (240433)	Loss/tok 2.5411 (3.1788)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.067 (0.090)	Data 1.35e-04 (2.24e-03)	Tok/s 232807 (240531)	Loss/tok 3.0350 (3.1761)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.03e-03)	Tok/s 251258 (241022)	Loss/tok 3.1499 (3.1714)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.067 (0.090)	Data 1.15e-04 (1.85e-03)	Tok/s 231363 (241394)	Loss/tok 2.9637 (3.1785)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.135 (0.089)	Data 1.08e-04 (1.71e-03)	Tok/s 259934 (241485)	Loss/tok 3.3889 (3.1764)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.067 (0.089)	Data 1.12e-04 (1.59e-03)	Tok/s 227681 (241571)	Loss/tok 3.0791 (3.1778)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.099 (0.091)	Data 1.09e-04 (1.48e-03)	Tok/s 253856 (242354)	Loss/tok 3.2278 (3.1848)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.100 (0.091)	Data 1.11e-04 (1.39e-03)	Tok/s 254460 (242236)	Loss/tok 3.1923 (3.1910)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][160/1291]	Time 0.067 (0.091)	Data 1.33e-04 (1.31e-03)	Tok/s 231659 (242200)	Loss/tok 3.0721 (3.1886)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][170/1291]	Time 0.038 (0.090)	Data 1.12e-04 (1.24e-03)	Tok/s 204698 (241801)	Loss/tok 2.6240 (3.1863)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.099 (0.091)	Data 1.09e-04 (1.18e-03)	Tok/s 252326 (242276)	Loss/tok 3.1584 (3.1933)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.12e-03)	Tok/s 235285 (242394)	Loss/tok 2.9850 (3.1918)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.066 (0.091)	Data 1.07e-04 (1.07e-03)	Tok/s 235228 (242487)	Loss/tok 3.0204 (3.1926)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.099 (0.090)	Data 1.11e-04 (1.03e-03)	Tok/s 253695 (242189)	Loss/tok 3.2585 (3.1904)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.100 (0.090)	Data 1.10e-04 (9.87e-04)	Tok/s 253182 (241994)	Loss/tok 3.2159 (3.1875)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.067 (0.089)	Data 1.13e-04 (9.50e-04)	Tok/s 232067 (241625)	Loss/tok 2.9153 (3.1884)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.067 (0.090)	Data 1.14e-04 (9.15e-04)	Tok/s 233881 (241597)	Loss/tok 2.9666 (3.1929)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.067 (0.090)	Data 1.12e-04 (8.83e-04)	Tok/s 232096 (241567)	Loss/tok 2.9943 (3.1948)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.099 (0.090)	Data 1.11e-04 (8.54e-04)	Tok/s 255499 (241574)	Loss/tok 3.1860 (3.1930)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.067 (0.089)	Data 1.19e-04 (8.26e-04)	Tok/s 227773 (241331)	Loss/tok 2.9751 (3.1890)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.036 (0.089)	Data 1.13e-04 (8.01e-04)	Tok/s 217627 (241263)	Loss/tok 2.5378 (3.1863)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][290/1291]	Time 0.100 (0.090)	Data 1.12e-04 (7.77e-04)	Tok/s 251951 (241525)	Loss/tok 3.1921 (3.1899)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.55e-04)	Tok/s 236916 (241462)	Loss/tok 2.9389 (3.1923)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.066 (0.089)	Data 1.25e-04 (7.35e-04)	Tok/s 229877 (241263)	Loss/tok 2.9378 (3.1898)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.15e-04)	Tok/s 228896 (241422)	Loss/tok 2.9778 (3.1939)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.135 (0.090)	Data 1.12e-04 (6.97e-04)	Tok/s 259309 (241611)	Loss/tok 3.3687 (3.1946)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.100 (0.090)	Data 1.12e-04 (6.80e-04)	Tok/s 249761 (241692)	Loss/tok 3.2420 (3.1928)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.036 (0.090)	Data 1.12e-04 (6.64e-04)	Tok/s 222290 (241540)	Loss/tok 2.5482 (3.1913)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.067 (0.089)	Data 1.11e-04 (6.48e-04)	Tok/s 230707 (241407)	Loss/tok 3.0119 (3.1891)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.067 (0.090)	Data 1.10e-04 (6.34e-04)	Tok/s 229321 (241495)	Loss/tok 2.9764 (3.1885)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.066 (0.089)	Data 1.11e-04 (6.20e-04)	Tok/s 236881 (241313)	Loss/tok 2.8888 (3.1849)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.100 (0.089)	Data 1.13e-04 (6.07e-04)	Tok/s 251525 (241401)	Loss/tok 3.1230 (3.1845)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.135 (0.089)	Data 1.12e-04 (5.95e-04)	Tok/s 255170 (241369)	Loss/tok 3.3543 (3.1832)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.100 (0.089)	Data 1.14e-04 (5.83e-04)	Tok/s 251132 (241426)	Loss/tok 3.2233 (3.1841)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][420/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.72e-04)	Tok/s 233742 (241457)	Loss/tok 3.0124 (3.1820)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.135 (0.090)	Data 1.11e-04 (5.62e-04)	Tok/s 260332 (241682)	Loss/tok 3.3976 (3.1859)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.100 (0.089)	Data 1.14e-04 (5.51e-04)	Tok/s 254829 (241636)	Loss/tok 3.2181 (3.1834)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.101 (0.089)	Data 1.13e-04 (5.42e-04)	Tok/s 247594 (241569)	Loss/tok 3.1910 (3.1810)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.135 (0.089)	Data 1.11e-04 (5.32e-04)	Tok/s 259542 (241520)	Loss/tok 3.2774 (3.1795)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.24e-04)	Tok/s 231321 (241648)	Loss/tok 2.9668 (3.1795)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.15e-04)	Tok/s 249909 (241770)	Loss/tok 3.1909 (3.1801)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.036 (0.089)	Data 1.14e-04 (5.07e-04)	Tok/s 220236 (241718)	Loss/tok 2.5299 (3.1783)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.99e-04)	Tok/s 235400 (241793)	Loss/tok 2.9530 (3.1785)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.91e-04)	Tok/s 231273 (241633)	Loss/tok 2.9633 (3.1755)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.067 (0.088)	Data 1.15e-04 (4.84e-04)	Tok/s 230097 (241505)	Loss/tok 2.9964 (3.1737)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.036 (0.088)	Data 1.12e-04 (4.77e-04)	Tok/s 222394 (241448)	Loss/tok 2.5244 (3.1714)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.036 (0.088)	Data 1.11e-04 (4.70e-04)	Tok/s 218413 (241374)	Loss/tok 2.4850 (3.1701)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][550/1291]	Time 0.173 (0.088)	Data 1.15e-04 (4.64e-04)	Tok/s 256715 (241468)	Loss/tok 3.4591 (3.1721)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.58e-04)	Tok/s 233426 (241548)	Loss/tok 3.0616 (3.1723)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.174 (0.089)	Data 1.15e-04 (4.52e-04)	Tok/s 256177 (241637)	Loss/tok 3.5357 (3.1733)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.46e-04)	Tok/s 232948 (241638)	Loss/tok 2.9507 (3.1734)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.40e-04)	Tok/s 228331 (241716)	Loss/tok 2.9938 (3.1750)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.35e-04)	Tok/s 229692 (241627)	Loss/tok 2.9736 (3.1727)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.100 (0.089)	Data 1.13e-04 (4.30e-04)	Tok/s 253325 (241694)	Loss/tok 3.1889 (3.1720)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.24e-04)	Tok/s 227967 (241654)	Loss/tok 3.0611 (3.1711)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.135 (0.089)	Data 1.13e-04 (4.20e-04)	Tok/s 260733 (241733)	Loss/tok 3.3190 (3.1718)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.136 (0.089)	Data 1.18e-04 (4.15e-04)	Tok/s 258213 (241788)	Loss/tok 3.3815 (3.1731)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.100 (0.089)	Data 1.15e-04 (4.10e-04)	Tok/s 255936 (241870)	Loss/tok 3.1876 (3.1738)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.067 (0.089)	Data 1.39e-04 (4.06e-04)	Tok/s 232490 (241887)	Loss/tok 2.9965 (3.1742)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.174 (0.089)	Data 1.13e-04 (4.01e-04)	Tok/s 257905 (241892)	Loss/tok 3.5961 (3.1758)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][680/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.97e-04)	Tok/s 250674 (241860)	Loss/tok 3.1990 (3.1745)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.93e-04)	Tok/s 232979 (241815)	Loss/tok 3.0010 (3.1730)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.174 (0.089)	Data 1.13e-04 (3.89e-04)	Tok/s 255961 (241887)	Loss/tok 3.3955 (3.1750)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.85e-04)	Tok/s 250930 (241853)	Loss/tok 3.1756 (3.1737)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.089)	Data 1.25e-04 (3.82e-04)	Tok/s 229938 (241822)	Loss/tok 2.9241 (3.1735)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.100 (0.089)	Data 1.14e-04 (3.78e-04)	Tok/s 250357 (241798)	Loss/tok 3.1229 (3.1717)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.74e-04)	Tok/s 249179 (241838)	Loss/tok 3.1831 (3.1706)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.71e-04)	Tok/s 251048 (241808)	Loss/tok 3.1107 (3.1694)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.067 (0.089)	Data 1.30e-04 (3.68e-04)	Tok/s 234277 (241794)	Loss/tok 3.0447 (3.1681)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.64e-04)	Tok/s 255994 (241745)	Loss/tok 3.0554 (3.1663)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.61e-04)	Tok/s 237445 (241745)	Loss/tok 2.8876 (3.1664)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.135 (0.089)	Data 1.12e-04 (3.58e-04)	Tok/s 256911 (241747)	Loss/tok 3.4010 (3.1661)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.100 (0.089)	Data 1.24e-04 (3.55e-04)	Tok/s 253822 (241683)	Loss/tok 3.1206 (3.1645)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][810/1291]	Time 0.136 (0.089)	Data 1.34e-04 (3.52e-04)	Tok/s 255295 (241693)	Loss/tok 3.3468 (3.1644)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.036 (0.089)	Data 1.18e-04 (3.49e-04)	Tok/s 219388 (241673)	Loss/tok 2.5742 (3.1647)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.134 (0.088)	Data 1.11e-04 (3.46e-04)	Tok/s 260111 (241624)	Loss/tok 3.3465 (3.1635)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.068 (0.088)	Data 1.12e-04 (3.43e-04)	Tok/s 227620 (241475)	Loss/tok 2.9497 (3.1622)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.101 (0.088)	Data 1.32e-04 (3.41e-04)	Tok/s 249627 (241475)	Loss/tok 3.1463 (3.1621)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.068 (0.088)	Data 1.14e-04 (3.38e-04)	Tok/s 225072 (241369)	Loss/tok 2.9238 (3.1611)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.102 (0.088)	Data 1.21e-04 (3.36e-04)	Tok/s 246393 (241384)	Loss/tok 3.1592 (3.1608)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.068 (0.088)	Data 1.13e-04 (3.33e-04)	Tok/s 230412 (241298)	Loss/tok 3.0973 (3.1600)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.138 (0.088)	Data 1.12e-04 (3.31e-04)	Tok/s 252271 (241264)	Loss/tok 3.4488 (3.1601)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.068 (0.088)	Data 1.12e-04 (3.28e-04)	Tok/s 224564 (241233)	Loss/tok 2.9373 (3.1599)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.101 (0.088)	Data 1.13e-04 (3.26e-04)	Tok/s 250252 (241230)	Loss/tok 3.1611 (3.1596)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.099 (0.089)	Data 1.18e-04 (3.23e-04)	Tok/s 254169 (241327)	Loss/tok 3.1465 (3.1614)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][930/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.21e-04)	Tok/s 255202 (241369)	Loss/tok 3.1194 (3.1604)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.19e-04)	Tok/s 232202 (241337)	Loss/tok 2.9986 (3.1595)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.17e-04)	Tok/s 252852 (241399)	Loss/tok 3.1217 (3.1600)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.135 (0.089)	Data 1.11e-04 (3.15e-04)	Tok/s 258138 (241403)	Loss/tok 3.3017 (3.1595)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.13e-04)	Tok/s 234152 (241382)	Loss/tok 3.0200 (3.1586)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.11e-04)	Tok/s 220344 (241377)	Loss/tok 2.5936 (3.1582)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.100 (0.089)	Data 1.32e-04 (3.09e-04)	Tok/s 251520 (241376)	Loss/tok 3.1436 (3.1583)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.099 (0.088)	Data 1.14e-04 (3.07e-04)	Tok/s 254064 (241375)	Loss/tok 3.0716 (3.1573)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.088)	Data 1.09e-04 (3.05e-04)	Tok/s 231074 (241377)	Loss/tok 2.9429 (3.1570)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.03e-04)	Tok/s 256430 (241463)	Loss/tok 3.0848 (3.1568)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.01e-04)	Tok/s 252440 (241490)	Loss/tok 3.0835 (3.1566)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.99e-04)	Tok/s 252342 (241541)	Loss/tok 3.1380 (3.1570)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.98e-04)	Tok/s 255150 (241641)	Loss/tok 3.0844 (3.1578)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1060/1291]	Time 0.136 (0.089)	Data 1.15e-04 (2.96e-04)	Tok/s 256658 (241688)	Loss/tok 3.2532 (3.1583)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.94e-04)	Tok/s 224644 (241712)	Loss/tok 2.9610 (3.1578)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.135 (0.089)	Data 1.19e-04 (2.92e-04)	Tok/s 259753 (241724)	Loss/tok 3.2095 (3.1571)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.135 (0.089)	Data 1.10e-04 (2.91e-04)	Tok/s 258358 (241814)	Loss/tok 3.1859 (3.1570)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.099 (0.089)	Data 1.19e-04 (2.89e-04)	Tok/s 254155 (241825)	Loss/tok 3.0818 (3.1568)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.88e-04)	Tok/s 235434 (241783)	Loss/tok 2.9592 (3.1561)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.100 (0.089)	Data 1.17e-04 (2.86e-04)	Tok/s 248528 (241846)	Loss/tok 3.2123 (3.1567)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.85e-04)	Tok/s 255925 (241918)	Loss/tok 3.1599 (3.1571)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.099 (0.090)	Data 1.12e-04 (2.83e-04)	Tok/s 254647 (241946)	Loss/tok 3.1212 (3.1573)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.82e-04)	Tok/s 217321 (241899)	Loss/tok 2.5355 (3.1563)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.80e-04)	Tok/s 218608 (241893)	Loss/tok 2.5055 (3.1555)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.136 (0.089)	Data 1.11e-04 (2.79e-04)	Tok/s 254190 (241925)	Loss/tok 3.3013 (3.1551)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.135 (0.089)	Data 1.18e-04 (2.78e-04)	Tok/s 261930 (241933)	Loss/tok 3.2144 (3.1558)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1190/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.76e-04)	Tok/s 261058 (241969)	Loss/tok 3.2479 (3.1554)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.100 (0.090)	Data 1.14e-04 (2.75e-04)	Tok/s 255183 (242013)	Loss/tok 3.0962 (3.1550)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.74e-04)	Tok/s 252270 (241963)	Loss/tok 3.1369 (3.1539)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.72e-04)	Tok/s 252245 (241980)	Loss/tok 3.1254 (3.1536)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.71e-04)	Tok/s 252289 (241974)	Loss/tok 3.1632 (3.1535)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.036 (0.089)	Data 1.15e-04 (2.70e-04)	Tok/s 222088 (241937)	Loss/tok 2.5103 (3.1524)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.68e-04)	Tok/s 225869 (241880)	Loss/tok 2.8564 (3.1514)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.67e-04)	Tok/s 230155 (241851)	Loss/tok 3.0061 (3.1506)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.66e-04)	Tok/s 231638 (241821)	Loss/tok 2.9456 (3.1501)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.136 (0.089)	Data 1.19e-04 (2.65e-04)	Tok/s 259117 (241851)	Loss/tok 3.2491 (3.1505)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.173 (0.089)	Data 4.10e-05 (2.66e-04)	Tok/s 257489 (241808)	Loss/tok 3.4344 (3.1505)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592454338492, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592454338492, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.393 (0.393)	Decoder iters 99.0 (99.0)	Tok/s 41727 (41727)
0: Running moses detokenizer
0: BLEU(score=24.330934011233513, counts=[37207, 18774, 10737, 6389], totals=[65404, 62401, 59398, 56402], precisions=[56.88795792306281, 30.08605631320011, 18.076366207616417, 11.32761249601078], bp=1.0, sys_len=65404, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454340434, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2433, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592454340434, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1496	Test BLEU: 24.33
0: Performance: Epoch: 3	Training: 1934028 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592454340434, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592454340434, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-17 09:25:46 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:46 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:47 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:47 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:47 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:47 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:47 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:17:25 PM
ENDING TIMING RUN AT 2020-06-17 09:25:47 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:17:25 PM
