+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592446400549, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592446400588, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592446400588, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592446400588, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592446400588, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0267
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592446406747, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/13842440/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ LR=2.875e-3
+ '[' -n 7 ']'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=384
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ TEST_BATCH_SIZE=128
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ declare -a CMD
Using TCMalloc
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=8
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ MATH=fp16
+ declare -a CMD
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-17 07:13:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592446410902, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446410947, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446411031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446411044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446411155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446411160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446411167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446411186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 662076006
:::MLLOG {"namespace": "", "time_ms": 1592446418795, "event_type": "POINT_IN_TIME", "key": "seed", "value": 662076006, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2906837765
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592446432940, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592446432941, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592446432941, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592446432941, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592446432941, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592446434607, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592446434607, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592446434607, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592446434909, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592446434909, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592446434910, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592446434910, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592446434910, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592446434910, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592446434910, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592446434910, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592446434911, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592446434911, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592446434911, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446434911, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2745424330
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.335 (0.335)	Data 2.29e-01 (2.29e-01)	Tok/s 75488 (75488)	Loss/tok 10.5942 (10.5942)	LR 2.942e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/1291]	Time 0.066 (0.106)	Data 1.12e-04 (2.09e-02)	Tok/s 237074 (225986)	Loss/tok 9.4371 (9.9503)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.066 (0.102)	Data 1.14e-04 (1.10e-02)	Tok/s 233554 (235927)	Loss/tok 9.0784 (9.6365)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.172 (0.099)	Data 1.11e-04 (7.49e-03)	Tok/s 262679 (238623)	Loss/tok 9.0529 (9.4225)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.066 (0.097)	Data 1.10e-04 (5.69e-03)	Tok/s 232880 (240829)	Loss/tok 8.4453 (9.2519)	LR 7.390e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.098 (0.097)	Data 1.12e-04 (4.60e-03)	Tok/s 255543 (241828)	Loss/tok 8.3057 (9.1014)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.098 (0.093)	Data 1.19e-04 (3.86e-03)	Tok/s 256521 (242163)	Loss/tok 8.1748 (8.9805)	LR 1.119e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][70/1291]	Time 0.065 (0.093)	Data 1.13e-04 (3.33e-03)	Tok/s 240963 (242652)	Loss/tok 8.0826 (8.8961)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.035 (0.089)	Data 1.13e-04 (2.94e-03)	Tok/s 224035 (241717)	Loss/tok 7.4304 (8.8218)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.066 (0.090)	Data 1.08e-04 (2.63e-03)	Tok/s 234232 (242567)	Loss/tok 7.8477 (8.7258)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.066 (0.090)	Data 1.13e-04 (2.38e-03)	Tok/s 234024 (243036)	Loss/tok 7.7582 (8.6461)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.133 (0.090)	Data 1.12e-04 (2.17e-03)	Tok/s 266182 (243574)	Loss/tok 8.0313 (8.5786)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.090)	Data 1.11e-04 (2.00e-03)	Tok/s 235740 (243747)	Loss/tok 7.6052 (8.5177)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.098 (0.090)	Data 1.09e-04 (1.86e-03)	Tok/s 256961 (244043)	Loss/tok 7.8104 (8.4609)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.035 (0.091)	Data 1.11e-04 (1.73e-03)	Tok/s 224224 (244397)	Loss/tok 6.9104 (8.4037)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.035 (0.090)	Data 1.08e-04 (1.63e-03)	Tok/s 225186 (244346)	Loss/tok 6.5687 (8.3547)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.090)	Data 1.09e-04 (1.53e-03)	Tok/s 232688 (244394)	Loss/tok 7.1656 (8.3020)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.066 (0.089)	Data 1.13e-04 (1.45e-03)	Tok/s 232627 (244346)	Loss/tok 7.0682 (8.2457)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.066 (0.089)	Data 1.12e-04 (1.38e-03)	Tok/s 234828 (244447)	Loss/tok 6.8527 (8.1868)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.066 (0.089)	Data 1.11e-04 (1.31e-03)	Tok/s 233219 (244549)	Loss/tok 6.6184 (8.1213)	LR 2.181e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][200/1291]	Time 0.035 (0.088)	Data 1.10e-04 (1.25e-03)	Tok/s 220385 (244181)	Loss/tok 5.8778 (8.0679)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.088)	Data 1.12e-04 (1.20e-03)	Tok/s 235441 (244271)	Loss/tok 6.4898 (8.0041)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.035 (0.088)	Data 1.08e-04 (1.15e-03)	Tok/s 227518 (244464)	Loss/tok 5.6471 (7.9396)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.035 (0.088)	Data 1.10e-04 (1.10e-03)	Tok/s 225546 (244508)	Loss/tok 5.4014 (7.8729)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.088)	Data 1.08e-04 (1.06e-03)	Tok/s 234385 (244237)	Loss/tok 6.0321 (7.8167)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.066 (0.087)	Data 1.12e-04 (1.02e-03)	Tok/s 234043 (244291)	Loss/tok 5.8223 (7.7508)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.098 (0.088)	Data 1.12e-04 (9.88e-04)	Tok/s 253918 (244466)	Loss/tok 6.0306 (7.6774)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.098 (0.088)	Data 1.12e-04 (9.56e-04)	Tok/s 256585 (244583)	Loss/tok 5.8178 (7.6060)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.035 (0.087)	Data 1.12e-04 (9.26e-04)	Tok/s 230209 (244300)	Loss/tok 4.5353 (7.5496)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.098 (0.087)	Data 1.10e-04 (8.98e-04)	Tok/s 253787 (244391)	Loss/tok 5.6161 (7.4787)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.134 (0.088)	Data 1.11e-04 (8.72e-04)	Tok/s 263702 (244432)	Loss/tok 5.6478 (7.4073)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.133 (0.088)	Data 1.11e-04 (8.47e-04)	Tok/s 261058 (244529)	Loss/tok 5.4966 (7.3341)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][320/1291]	Time 0.066 (0.088)	Data 1.12e-04 (8.25e-04)	Tok/s 234717 (244516)	Loss/tok 4.9414 (7.2703)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.134 (0.088)	Data 1.15e-04 (8.03e-04)	Tok/s 261812 (244590)	Loss/tok 5.5891 (7.2069)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.066 (0.089)	Data 1.13e-04 (7.83e-04)	Tok/s 234555 (244834)	Loss/tok 4.7327 (7.1288)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.098 (0.090)	Data 1.11e-04 (7.64e-04)	Tok/s 257169 (244908)	Loss/tok 4.9185 (7.0585)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.066 (0.089)	Data 1.11e-04 (7.46e-04)	Tok/s 234781 (244756)	Loss/tok 4.6016 (7.0048)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.066 (0.089)	Data 1.45e-04 (7.29e-04)	Tok/s 232060 (244590)	Loss/tok 4.6194 (6.9533)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.134 (0.089)	Data 1.13e-04 (7.12e-04)	Tok/s 261687 (244659)	Loss/tok 4.8715 (6.8875)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.036 (0.089)	Data 1.14e-04 (6.97e-04)	Tok/s 221700 (244484)	Loss/tok 3.7144 (6.8399)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.067 (0.089)	Data 1.14e-04 (6.83e-04)	Tok/s 231803 (244400)	Loss/tok 4.4165 (6.7836)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.066 (0.089)	Data 1.12e-04 (6.69e-04)	Tok/s 230498 (244351)	Loss/tok 4.1164 (6.7306)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.066 (0.089)	Data 1.06e-04 (6.55e-04)	Tok/s 232234 (244447)	Loss/tok 4.2575 (6.6720)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.134 (0.089)	Data 1.08e-04 (6.43e-04)	Tok/s 260566 (244365)	Loss/tok 4.7276 (6.6259)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.099 (0.089)	Data 1.18e-04 (6.31e-04)	Tok/s 256003 (244404)	Loss/tok 4.2659 (6.5742)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][450/1291]	Time 0.067 (0.088)	Data 1.19e-04 (6.19e-04)	Tok/s 235446 (244410)	Loss/tok 4.0395 (6.5254)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.066 (0.088)	Data 1.30e-04 (6.08e-04)	Tok/s 233968 (244360)	Loss/tok 3.8327 (6.4800)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.099 (0.088)	Data 1.13e-04 (5.98e-04)	Tok/s 252790 (244219)	Loss/tok 4.2607 (6.4404)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.036 (0.088)	Data 1.13e-04 (5.88e-04)	Tok/s 222856 (244099)	Loss/tok 3.2743 (6.3997)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.066 (0.088)	Data 1.08e-04 (5.78e-04)	Tok/s 236807 (244331)	Loss/tok 3.9637 (6.3429)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.099 (0.088)	Data 1.10e-04 (5.69e-04)	Tok/s 254824 (244276)	Loss/tok 4.2456 (6.3043)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.066 (0.088)	Data 1.11e-04 (5.60e-04)	Tok/s 239617 (244275)	Loss/tok 3.9067 (6.2611)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.099 (0.088)	Data 1.22e-04 (5.51e-04)	Tok/s 251577 (244413)	Loss/tok 4.0247 (6.2156)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.066 (0.088)	Data 1.09e-04 (5.43e-04)	Tok/s 231694 (244283)	Loss/tok 3.9153 (6.1802)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.066 (0.088)	Data 1.11e-04 (5.35e-04)	Tok/s 233952 (244194)	Loss/tok 3.8942 (6.1461)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.099 (0.088)	Data 1.10e-04 (5.27e-04)	Tok/s 253154 (244278)	Loss/tok 4.1075 (6.1050)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.173 (0.088)	Data 1.12e-04 (5.20e-04)	Tok/s 257091 (244260)	Loss/tok 4.4370 (6.0674)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.066 (0.088)	Data 1.10e-04 (5.13e-04)	Tok/s 234588 (244287)	Loss/tok 3.7464 (6.0308)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][580/1291]	Time 0.035 (0.088)	Data 1.10e-04 (5.06e-04)	Tok/s 226977 (244241)	Loss/tok 3.3453 (5.9982)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][590/1291]	Time 0.100 (0.088)	Data 1.13e-04 (4.99e-04)	Tok/s 253948 (244143)	Loss/tok 4.1363 (5.9689)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.098 (0.088)	Data 1.10e-04 (4.92e-04)	Tok/s 255676 (244272)	Loss/tok 4.0118 (5.9321)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.099 (0.088)	Data 1.10e-04 (4.86e-04)	Tok/s 258128 (244272)	Loss/tok 3.9732 (5.9018)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.134 (0.088)	Data 1.12e-04 (4.80e-04)	Tok/s 267051 (244251)	Loss/tok 4.1387 (5.8722)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.067 (0.088)	Data 1.09e-04 (4.74e-04)	Tok/s 229092 (244190)	Loss/tok 3.5954 (5.8435)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.133 (0.087)	Data 1.09e-04 (4.69e-04)	Tok/s 262257 (244056)	Loss/tok 4.2288 (5.8187)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.099 (0.088)	Data 1.08e-04 (4.63e-04)	Tok/s 256482 (244112)	Loss/tok 4.0296 (5.7862)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.035 (0.088)	Data 1.05e-04 (4.58e-04)	Tok/s 222907 (244076)	Loss/tok 3.0167 (5.7579)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.099 (0.087)	Data 1.09e-04 (4.53e-04)	Tok/s 253403 (243996)	Loss/tok 4.0295 (5.7340)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.099 (0.087)	Data 1.08e-04 (4.48e-04)	Tok/s 253953 (243953)	Loss/tok 3.9683 (5.7089)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.067 (0.087)	Data 1.11e-04 (4.43e-04)	Tok/s 233605 (243835)	Loss/tok 3.6486 (5.6865)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.099 (0.087)	Data 1.10e-04 (4.38e-04)	Tok/s 257260 (243833)	Loss/tok 3.8115 (5.6612)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.099 (0.087)	Data 1.17e-04 (4.34e-04)	Tok/s 253172 (243851)	Loss/tok 3.9154 (5.6343)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][720/1291]	Time 0.134 (0.087)	Data 1.30e-04 (4.29e-04)	Tok/s 260759 (243891)	Loss/tok 4.1456 (5.6081)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.099 (0.088)	Data 1.10e-04 (4.25e-04)	Tok/s 254052 (244041)	Loss/tok 3.8043 (5.5783)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.067 (0.088)	Data 1.12e-04 (4.20e-04)	Tok/s 232929 (244008)	Loss/tok 3.5603 (5.5549)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.067 (0.087)	Data 1.09e-04 (4.16e-04)	Tok/s 229886 (243991)	Loss/tok 3.5451 (5.5325)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][760/1291]	Time 0.066 (0.088)	Data 1.14e-04 (4.12e-04)	Tok/s 240304 (244013)	Loss/tok 3.6007 (5.5086)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.066 (0.087)	Data 1.09e-04 (4.08e-04)	Tok/s 232534 (243953)	Loss/tok 3.5770 (5.4886)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.098 (0.087)	Data 1.20e-04 (4.05e-04)	Tok/s 254760 (243971)	Loss/tok 3.9694 (5.4680)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.066 (0.087)	Data 1.12e-04 (4.01e-04)	Tok/s 234926 (243961)	Loss/tok 3.5768 (5.4473)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.035 (0.088)	Data 1.09e-04 (3.97e-04)	Tok/s 225741 (244090)	Loss/tok 3.0042 (5.4218)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.066 (0.088)	Data 1.34e-04 (3.94e-04)	Tok/s 231388 (244027)	Loss/tok 3.5732 (5.4042)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.098 (0.088)	Data 1.33e-04 (3.90e-04)	Tok/s 256798 (244041)	Loss/tok 3.8277 (5.3840)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.173 (0.087)	Data 1.08e-04 (3.87e-04)	Tok/s 260048 (243979)	Loss/tok 4.1246 (5.3666)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.035 (0.087)	Data 1.11e-04 (3.84e-04)	Tok/s 224043 (243907)	Loss/tok 2.9374 (5.3501)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.135 (0.087)	Data 1.09e-04 (3.81e-04)	Tok/s 259287 (243885)	Loss/tok 4.0461 (5.3325)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.135 (0.087)	Data 1.11e-04 (3.77e-04)	Tok/s 260620 (243888)	Loss/tok 3.9375 (5.3130)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.066 (0.087)	Data 1.12e-04 (3.74e-04)	Tok/s 229065 (243866)	Loss/tok 3.5081 (5.2945)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.71e-04)	Tok/s 237725 (243952)	Loss/tok 3.5102 (5.2741)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][890/1291]	Time 0.100 (0.088)	Data 1.11e-04 (3.68e-04)	Tok/s 256996 (243971)	Loss/tok 3.7537 (5.2565)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.66e-04)	Tok/s 233577 (243974)	Loss/tok 3.4705 (5.2394)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.63e-04)	Tok/s 232927 (243954)	Loss/tok 3.4091 (5.2224)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.100 (0.088)	Data 1.10e-04 (3.60e-04)	Tok/s 254230 (243959)	Loss/tok 3.7378 (5.2059)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.57e-04)	Tok/s 254483 (244017)	Loss/tok 3.7224 (5.1882)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.55e-04)	Tok/s 230595 (243960)	Loss/tok 3.5126 (5.1735)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.135 (0.088)	Data 1.10e-04 (3.52e-04)	Tok/s 260414 (244025)	Loss/tok 3.8860 (5.1557)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.135 (0.088)	Data 1.15e-04 (3.50e-04)	Tok/s 258869 (244024)	Loss/tok 3.8979 (5.1396)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.174 (0.088)	Data 1.11e-04 (3.47e-04)	Tok/s 255116 (244073)	Loss/tok 4.0265 (5.1229)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.066 (0.088)	Data 1.14e-04 (3.45e-04)	Tok/s 233781 (244106)	Loss/tok 3.4058 (5.1067)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.099 (0.088)	Data 1.08e-04 (3.43e-04)	Tok/s 251863 (244063)	Loss/tok 3.7210 (5.0937)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.40e-04)	Tok/s 255805 (244060)	Loss/tok 3.6779 (5.0805)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.099 (0.088)	Data 1.08e-04 (3.38e-04)	Tok/s 251530 (244053)	Loss/tok 3.7138 (5.0666)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1020/1291]	Time 0.135 (0.088)	Data 1.13e-04 (3.36e-04)	Tok/s 255342 (244039)	Loss/tok 3.9188 (5.0533)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.067 (0.088)	Data 1.11e-04 (3.34e-04)	Tok/s 233736 (244065)	Loss/tok 3.4416 (5.0382)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.31e-04)	Tok/s 252021 (244078)	Loss/tok 3.6813 (5.0244)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.29e-04)	Tok/s 255500 (244129)	Loss/tok 3.6417 (5.0096)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.27e-04)	Tok/s 232381 (244164)	Loss/tok 3.4668 (4.9959)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1070/1291]	Time 0.066 (0.088)	Data 1.09e-04 (3.25e-04)	Tok/s 234805 (244150)	Loss/tok 3.4400 (4.9832)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1080/1291]	Time 0.134 (0.088)	Data 1.12e-04 (3.23e-04)	Tok/s 262447 (244161)	Loss/tok 3.7485 (4.9699)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.21e-04)	Tok/s 235547 (244142)	Loss/tok 3.3071 (4.9579)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.135 (0.088)	Data 1.09e-04 (3.19e-04)	Tok/s 260313 (244154)	Loss/tok 3.8824 (4.9457)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.135 (0.089)	Data 1.08e-04 (3.18e-04)	Tok/s 258901 (244219)	Loss/tok 3.8390 (4.9325)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.16e-04)	Tok/s 254309 (244263)	Loss/tok 3.6869 (4.9194)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.14e-04)	Tok/s 255229 (244268)	Loss/tok 3.6168 (4.9076)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.12e-04)	Tok/s 255377 (244311)	Loss/tok 3.6160 (4.8945)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.035 (0.089)	Data 1.13e-04 (3.10e-04)	Tok/s 221765 (244254)	Loss/tok 2.8506 (4.8845)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.09e-04)	Tok/s 237581 (244195)	Loss/tok 3.2939 (4.8750)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.100 (0.089)	Data 1.18e-04 (3.07e-04)	Tok/s 251262 (244185)	Loss/tok 3.5655 (4.8647)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.067 (0.088)	Data 1.08e-04 (3.05e-04)	Tok/s 232293 (244151)	Loss/tok 3.3651 (4.8551)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.04e-04)	Tok/s 232670 (244135)	Loss/tok 3.2582 (4.8446)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1200/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.02e-04)	Tok/s 255011 (244131)	Loss/tok 3.6169 (4.8341)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.036 (0.088)	Data 1.11e-04 (3.01e-04)	Tok/s 221211 (244072)	Loss/tok 2.8212 (4.8244)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.174 (0.088)	Data 1.08e-04 (2.99e-04)	Tok/s 259041 (244050)	Loss/tok 3.9243 (4.8146)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.066 (0.088)	Data 1.10e-04 (2.98e-04)	Tok/s 232453 (243983)	Loss/tok 3.3353 (4.8059)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.066 (0.088)	Data 1.14e-04 (2.96e-04)	Tok/s 234122 (244019)	Loss/tok 3.4174 (4.7952)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.099 (0.088)	Data 1.09e-04 (2.95e-04)	Tok/s 256270 (244005)	Loss/tok 3.6014 (4.7861)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.066 (0.088)	Data 1.28e-04 (2.93e-04)	Tok/s 230446 (243990)	Loss/tok 3.3812 (4.7764)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.067 (0.088)	Data 1.11e-04 (2.92e-04)	Tok/s 229693 (243950)	Loss/tok 3.3834 (4.7678)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.067 (0.088)	Data 1.11e-04 (2.90e-04)	Tok/s 232817 (244002)	Loss/tok 3.3150 (4.7571)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.099 (0.088)	Data 4.36e-05 (2.91e-04)	Tok/s 254597 (244002)	Loss/tok 3.5021 (4.7477)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592446549587, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446549588, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.489 (0.489)	Decoder iters 149.0 (149.0)	Tok/s 33230 (33230)
0: Running moses detokenizer
0: BLEU(score=20.107725589455555, counts=[34064, 15701, 8399, 4717], totals=[63494, 60491, 57488, 54491], precisions=[53.64916370050713, 25.95592732803227, 14.61000556637907, 8.65647538125562], bp=0.9815562740032647, sys_len=63494, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446551519, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2011, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446551519, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7466	Test BLEU: 20.11
0: Performance: Epoch: 0	Training: 1952623 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592446551519, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446551519, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446551519, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2793452204
0: TRAIN [1][0/1291]	Time 0.335 (0.335)	Data 1.99e-01 (1.99e-01)	Tok/s 104594 (104594)	Loss/tok 3.7670 (3.7670)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.066 (0.106)	Data 1.17e-04 (1.82e-02)	Tok/s 232857 (230948)	Loss/tok 3.1733 (3.4438)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.066 (0.097)	Data 1.09e-04 (9.60e-03)	Tok/s 232908 (236410)	Loss/tok 3.2881 (3.4669)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.099 (0.090)	Data 1.16e-04 (6.54e-03)	Tok/s 256605 (238283)	Loss/tok 3.4971 (3.4471)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][40/1291]	Time 0.099 (0.091)	Data 1.09e-04 (4.98e-03)	Tok/s 252395 (239290)	Loss/tok 3.4697 (3.4759)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.066 (0.089)	Data 1.09e-04 (4.02e-03)	Tok/s 232084 (239201)	Loss/tok 3.2167 (3.4660)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][60/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.38e-03)	Tok/s 230316 (239316)	Loss/tok 3.1988 (3.4745)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][70/1291]	Time 0.066 (0.087)	Data 1.10e-04 (2.92e-03)	Tok/s 235029 (238887)	Loss/tok 3.1851 (3.4708)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.100 (0.087)	Data 1.11e-04 (2.57e-03)	Tok/s 251513 (239699)	Loss/tok 3.4562 (3.4683)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.066 (0.087)	Data 1.20e-04 (2.30e-03)	Tok/s 235968 (239675)	Loss/tok 3.2847 (3.4748)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.099 (0.088)	Data 1.15e-04 (2.09e-03)	Tok/s 253551 (240225)	Loss/tok 3.4826 (3.4783)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.135 (0.089)	Data 1.17e-04 (1.91e-03)	Tok/s 256680 (240949)	Loss/tok 3.7304 (3.4834)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.067 (0.087)	Data 1.11e-04 (1.76e-03)	Tok/s 232856 (240600)	Loss/tok 3.2825 (3.4735)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.100 (0.086)	Data 1.39e-04 (1.64e-03)	Tok/s 251171 (240439)	Loss/tok 3.5960 (3.4690)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.037 (0.085)	Data 1.28e-04 (1.53e-03)	Tok/s 213795 (239675)	Loss/tok 2.8751 (3.4614)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.099 (0.085)	Data 1.18e-04 (1.43e-03)	Tok/s 255236 (239954)	Loss/tok 3.4262 (3.4575)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.066 (0.086)	Data 1.13e-04 (1.35e-03)	Tok/s 236216 (240418)	Loss/tok 3.2991 (3.4597)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.066 (0.086)	Data 1.19e-04 (1.28e-03)	Tok/s 232178 (240450)	Loss/tok 3.2177 (3.4639)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.067 (0.086)	Data 1.17e-04 (1.22e-03)	Tok/s 230279 (240559)	Loss/tok 3.2676 (3.4596)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][190/1291]	Time 0.135 (0.086)	Data 1.18e-04 (1.16e-03)	Tok/s 257597 (240734)	Loss/tok 3.6974 (3.4676)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.099 (0.086)	Data 1.21e-04 (1.11e-03)	Tok/s 250707 (240594)	Loss/tok 3.5240 (3.4640)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.174 (0.086)	Data 1.19e-04 (1.06e-03)	Tok/s 257084 (240783)	Loss/tok 3.9198 (3.4654)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.036 (0.086)	Data 1.11e-04 (1.02e-03)	Tok/s 217495 (241082)	Loss/tok 2.8303 (3.4660)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.134 (0.087)	Data 1.12e-04 (9.78e-04)	Tok/s 261207 (241380)	Loss/tok 3.6915 (3.4700)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.099 (0.087)	Data 1.08e-04 (9.42e-04)	Tok/s 253797 (241398)	Loss/tok 3.5844 (3.4720)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.135 (0.087)	Data 1.15e-04 (9.10e-04)	Tok/s 256428 (241658)	Loss/tok 3.6286 (3.4767)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.067 (0.087)	Data 1.10e-04 (8.79e-04)	Tok/s 232220 (241575)	Loss/tok 3.1719 (3.4731)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.099 (0.087)	Data 1.14e-04 (8.52e-04)	Tok/s 254139 (241825)	Loss/tok 3.5183 (3.4721)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.035 (0.088)	Data 1.20e-04 (8.25e-04)	Tok/s 223852 (242013)	Loss/tok 2.7693 (3.4749)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.035 (0.087)	Data 1.14e-04 (8.01e-04)	Tok/s 222917 (241933)	Loss/tok 2.7467 (3.4703)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.067 (0.087)	Data 1.12e-04 (7.78e-04)	Tok/s 232679 (241799)	Loss/tok 3.2947 (3.4680)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.069 (0.086)	Data 1.11e-04 (7.57e-04)	Tok/s 227777 (241664)	Loss/tok 3.3018 (3.4653)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][320/1291]	Time 0.135 (0.087)	Data 1.12e-04 (7.37e-04)	Tok/s 259818 (241774)	Loss/tok 3.6894 (3.4688)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.068 (0.086)	Data 1.12e-04 (7.18e-04)	Tok/s 227394 (241447)	Loss/tok 3.1810 (3.4658)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.036 (0.087)	Data 1.17e-04 (7.00e-04)	Tok/s 222931 (241386)	Loss/tok 2.8408 (3.4687)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.099 (0.087)	Data 1.11e-04 (6.84e-04)	Tok/s 252464 (241361)	Loss/tok 3.5299 (3.4667)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.135 (0.087)	Data 1.20e-04 (6.68e-04)	Tok/s 261006 (241459)	Loss/tok 3.6431 (3.4666)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.066 (0.087)	Data 1.16e-04 (6.53e-04)	Tok/s 235412 (241512)	Loss/tok 3.2414 (3.4657)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.099 (0.087)	Data 1.15e-04 (6.39e-04)	Tok/s 252244 (241599)	Loss/tok 3.4441 (3.4665)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.099 (0.087)	Data 1.20e-04 (6.26e-04)	Tok/s 254319 (241545)	Loss/tok 3.4849 (3.4655)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.135 (0.087)	Data 1.13e-04 (6.13e-04)	Tok/s 256948 (241677)	Loss/tok 3.7897 (3.4668)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.099 (0.087)	Data 1.12e-04 (6.01e-04)	Tok/s 254701 (241777)	Loss/tok 3.5024 (3.4655)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.135 (0.087)	Data 1.16e-04 (5.89e-04)	Tok/s 259274 (241947)	Loss/tok 3.6618 (3.4674)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.067 (0.087)	Data 1.33e-04 (5.78e-04)	Tok/s 230892 (242008)	Loss/tok 3.2080 (3.4664)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.099 (0.087)	Data 1.11e-04 (5.68e-04)	Tok/s 252030 (242001)	Loss/tok 3.4354 (3.4651)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][450/1291]	Time 0.066 (0.087)	Data 1.12e-04 (5.58e-04)	Tok/s 236633 (241968)	Loss/tok 3.1987 (3.4633)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][460/1291]	Time 0.066 (0.087)	Data 1.12e-04 (5.48e-04)	Tok/s 237547 (242004)	Loss/tok 3.2394 (3.4634)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.088)	Data 1.10e-04 (5.39e-04)	Tok/s 232832 (242034)	Loss/tok 3.2876 (3.4645)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.066 (0.088)	Data 1.14e-04 (5.30e-04)	Tok/s 234166 (242034)	Loss/tok 3.1931 (3.4636)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.036 (0.087)	Data 1.16e-04 (5.21e-04)	Tok/s 225904 (241919)	Loss/tok 2.8278 (3.4602)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.067 (0.087)	Data 1.23e-04 (5.13e-04)	Tok/s 236648 (242019)	Loss/tok 3.2664 (3.4595)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.099 (0.087)	Data 1.10e-04 (5.05e-04)	Tok/s 254102 (242052)	Loss/tok 3.4625 (3.4615)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.067 (0.088)	Data 1.27e-04 (4.98e-04)	Tok/s 232150 (242106)	Loss/tok 3.3090 (3.4620)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.066 (0.088)	Data 1.14e-04 (4.91e-04)	Tok/s 234220 (242093)	Loss/tok 3.3655 (3.4628)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][540/1291]	Time 0.067 (0.088)	Data 1.12e-04 (4.84e-04)	Tok/s 229848 (242124)	Loss/tok 3.0966 (3.4629)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.099 (0.088)	Data 1.18e-04 (4.77e-04)	Tok/s 254473 (242063)	Loss/tok 3.4642 (3.4605)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.099 (0.088)	Data 1.15e-04 (4.71e-04)	Tok/s 253206 (242036)	Loss/tok 3.4082 (3.4608)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.066 (0.088)	Data 1.15e-04 (4.64e-04)	Tok/s 230886 (242166)	Loss/tok 3.2019 (3.4617)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.066 (0.088)	Data 1.10e-04 (4.58e-04)	Tok/s 237811 (242246)	Loss/tok 3.2871 (3.4614)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.099 (0.088)	Data 1.11e-04 (4.53e-04)	Tok/s 255966 (242389)	Loss/tok 3.4054 (3.4624)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.099 (0.088)	Data 1.10e-04 (4.47e-04)	Tok/s 255929 (242418)	Loss/tok 3.3237 (3.4609)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.134 (0.088)	Data 1.13e-04 (4.41e-04)	Tok/s 258559 (242459)	Loss/tok 3.7389 (3.4615)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.099 (0.089)	Data 1.13e-04 (4.36e-04)	Tok/s 252874 (242542)	Loss/tok 3.4854 (3.4629)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.099 (0.089)	Data 1.17e-04 (4.31e-04)	Tok/s 254813 (242581)	Loss/tok 3.3729 (3.4614)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.173 (0.088)	Data 1.11e-04 (4.26e-04)	Tok/s 257897 (242508)	Loss/tok 3.8272 (3.4610)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.21e-04)	Tok/s 231750 (242480)	Loss/tok 3.1655 (3.4593)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][660/1291]	Time 0.099 (0.088)	Data 1.11e-04 (4.17e-04)	Tok/s 253476 (242533)	Loss/tok 3.4252 (3.4583)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.066 (0.088)	Data 1.13e-04 (4.12e-04)	Tok/s 229789 (242586)	Loss/tok 3.2375 (3.4597)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.134 (0.088)	Data 1.10e-04 (4.08e-04)	Tok/s 261450 (242516)	Loss/tok 3.6105 (3.4586)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.066 (0.088)	Data 1.14e-04 (4.04e-04)	Tok/s 235214 (242507)	Loss/tok 3.2102 (3.4574)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.099 (0.088)	Data 1.13e-04 (4.00e-04)	Tok/s 255454 (242501)	Loss/tok 3.3871 (3.4566)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.96e-04)	Tok/s 255330 (242520)	Loss/tok 3.4075 (3.4550)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.92e-04)	Tok/s 234650 (242450)	Loss/tok 3.2365 (3.4541)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.89e-04)	Tok/s 255295 (242500)	Loss/tok 3.3208 (3.4535)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.067 (0.088)	Data 1.08e-04 (3.85e-04)	Tok/s 228289 (242488)	Loss/tok 3.2204 (3.4522)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.067 (0.088)	Data 1.15e-04 (3.81e-04)	Tok/s 233211 (242464)	Loss/tok 3.2093 (3.4504)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.036 (0.088)	Data 1.13e-04 (3.78e-04)	Tok/s 223859 (242403)	Loss/tok 2.7082 (3.4488)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.74e-04)	Tok/s 236697 (242408)	Loss/tok 3.1811 (3.4475)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.099 (0.088)	Data 1.14e-04 (3.71e-04)	Tok/s 252344 (242403)	Loss/tok 3.4394 (3.4469)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][790/1291]	Time 0.066 (0.088)	Data 1.14e-04 (3.68e-04)	Tok/s 236630 (242462)	Loss/tok 3.2416 (3.4470)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.101 (0.088)	Data 1.11e-04 (3.65e-04)	Tok/s 251609 (242416)	Loss/tok 3.4169 (3.4457)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.069 (0.088)	Data 1.18e-04 (3.61e-04)	Tok/s 226007 (242378)	Loss/tok 3.1724 (3.4449)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][820/1291]	Time 0.099 (0.088)	Data 1.29e-04 (3.58e-04)	Tok/s 255881 (242370)	Loss/tok 3.3182 (3.4448)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.066 (0.088)	Data 1.18e-04 (3.56e-04)	Tok/s 234194 (242388)	Loss/tok 3.2236 (3.4444)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][840/1291]	Time 0.066 (0.088)	Data 1.16e-04 (3.53e-04)	Tok/s 230948 (242379)	Loss/tok 3.1880 (3.4453)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.066 (0.088)	Data 1.15e-04 (3.50e-04)	Tok/s 234225 (242337)	Loss/tok 3.1561 (3.4456)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.135 (0.088)	Data 1.12e-04 (3.47e-04)	Tok/s 256237 (242359)	Loss/tok 3.5771 (3.4453)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.035 (0.088)	Data 1.11e-04 (3.44e-04)	Tok/s 227651 (242364)	Loss/tok 2.7884 (3.4439)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.42e-04)	Tok/s 231475 (242333)	Loss/tok 3.1188 (3.4434)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.036 (0.088)	Data 1.34e-04 (3.39e-04)	Tok/s 221228 (242380)	Loss/tok 2.6547 (3.4431)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.173 (0.088)	Data 1.17e-04 (3.37e-04)	Tok/s 259402 (242443)	Loss/tok 3.7606 (3.4436)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.066 (0.088)	Data 1.11e-04 (3.34e-04)	Tok/s 232203 (242455)	Loss/tok 3.1408 (3.4437)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.066 (0.088)	Data 1.10e-04 (3.32e-04)	Tok/s 234133 (242469)	Loss/tok 3.1902 (3.4442)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.30e-04)	Tok/s 232717 (242547)	Loss/tok 3.1333 (3.4444)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.135 (0.088)	Data 1.12e-04 (3.27e-04)	Tok/s 260706 (242530)	Loss/tok 3.5946 (3.4435)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.25e-04)	Tok/s 232259 (242489)	Loss/tok 3.1839 (3.4418)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][960/1291]	Time 0.099 (0.088)	Data 1.12e-04 (3.23e-04)	Tok/s 257330 (242558)	Loss/tok 3.4371 (3.4417)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.099 (0.088)	Data 1.33e-04 (3.21e-04)	Tok/s 255055 (242547)	Loss/tok 3.4267 (3.4409)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.19e-04)	Tok/s 232213 (242578)	Loss/tok 3.1360 (3.4402)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.067 (0.088)	Data 1.23e-04 (3.17e-04)	Tok/s 234818 (242572)	Loss/tok 3.1260 (3.4391)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.099 (0.088)	Data 1.14e-04 (3.15e-04)	Tok/s 252580 (242511)	Loss/tok 3.4288 (3.4375)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.066 (0.088)	Data 1.16e-04 (3.13e-04)	Tok/s 233350 (242540)	Loss/tok 3.1181 (3.4379)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.066 (0.088)	Data 1.08e-04 (3.11e-04)	Tok/s 232034 (242497)	Loss/tok 3.1653 (3.4365)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.067 (0.088)	Data 1.18e-04 (3.09e-04)	Tok/s 229808 (242485)	Loss/tok 3.2035 (3.4359)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.172 (0.088)	Data 1.14e-04 (3.07e-04)	Tok/s 259575 (242527)	Loss/tok 3.7088 (3.4361)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1050/1291]	Time 0.067 (0.088)	Data 1.14e-04 (3.05e-04)	Tok/s 227574 (242545)	Loss/tok 3.1763 (3.4358)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.100 (0.088)	Data 1.12e-04 (3.03e-04)	Tok/s 253760 (242572)	Loss/tok 3.3295 (3.4353)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.066 (0.088)	Data 1.08e-04 (3.01e-04)	Tok/s 232139 (242587)	Loss/tok 3.1478 (3.4359)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.00e-04)	Tok/s 238122 (242622)	Loss/tok 3.1736 (3.4359)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.099 (0.088)	Data 1.15e-04 (2.98e-04)	Tok/s 253531 (242632)	Loss/tok 3.3877 (3.4358)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.175 (0.088)	Data 1.12e-04 (2.96e-04)	Tok/s 255072 (242625)	Loss/tok 3.7651 (3.4354)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.067 (0.088)	Data 1.40e-04 (2.95e-04)	Tok/s 230020 (242587)	Loss/tok 3.1999 (3.4345)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.100 (0.088)	Data 1.14e-04 (2.93e-04)	Tok/s 253356 (242556)	Loss/tok 3.3531 (3.4330)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.099 (0.088)	Data 1.13e-04 (2.91e-04)	Tok/s 256797 (242643)	Loss/tok 3.3363 (3.4344)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.174 (0.088)	Data 1.11e-04 (2.90e-04)	Tok/s 259304 (242621)	Loss/tok 3.6694 (3.4342)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.067 (0.089)	Data 1.25e-04 (2.88e-04)	Tok/s 235837 (242672)	Loss/tok 3.2873 (3.4339)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.87e-04)	Tok/s 253992 (242755)	Loss/tok 3.3497 (3.4333)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.85e-04)	Tok/s 253687 (242771)	Loss/tok 3.3306 (3.4331)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1180/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.84e-04)	Tok/s 238790 (242728)	Loss/tok 3.1704 (3.4319)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.83e-04)	Tok/s 230709 (242725)	Loss/tok 3.2167 (3.4312)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.066 (0.089)	Data 1.25e-04 (2.81e-04)	Tok/s 231738 (242755)	Loss/tok 3.1027 (3.4306)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.80e-04)	Tok/s 220334 (242747)	Loss/tok 2.7474 (3.4295)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.78e-04)	Tok/s 230973 (242764)	Loss/tok 3.1958 (3.4298)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.173 (0.089)	Data 1.08e-04 (2.77e-04)	Tok/s 256857 (242765)	Loss/tok 3.7502 (3.4298)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.099 (0.089)	Data 1.06e-04 (2.76e-04)	Tok/s 253060 (242759)	Loss/tok 3.3452 (3.4289)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.74e-04)	Tok/s 234570 (242758)	Loss/tok 3.1019 (3.4280)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.73e-04)	Tok/s 231858 (242716)	Loss/tok 3.0856 (3.4267)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.135 (0.089)	Data 1.08e-04 (2.72e-04)	Tok/s 257309 (242783)	Loss/tok 3.5863 (3.4267)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.70e-04)	Tok/s 254100 (242773)	Loss/tok 3.3799 (3.4259)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.134 (0.089)	Data 4.82e-05 (2.72e-04)	Tok/s 262037 (242800)	Loss/tok 3.5152 (3.4256)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592446666859, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592446666859, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.497 (0.497)	Decoder iters 149.0 (149.0)	Tok/s 32899 (32899)
0: Running moses detokenizer
0: BLEU(score=21.84383397396173, counts=[35934, 17243, 9474, 5408], totals=[65703, 62700, 59697, 56698], precisions=[54.69156659513264, 27.50079744816587, 15.870144228353183, 9.53825531764789], bp=1.0, sys_len=65703, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446668903, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2184, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592446668903, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4254	Test BLEU: 21.84
0: Performance: Epoch: 1	Training: 1942099 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592446668904, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592446668904, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446668904, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2744537373
0: TRAIN [2][0/1291]	Time 0.274 (0.274)	Data 1.98e-01 (1.98e-01)	Tok/s 55529 (55529)	Loss/tok 3.0964 (3.0964)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][10/1291]	Time 0.066 (0.092)	Data 1.26e-04 (1.81e-02)	Tok/s 233114 (220313)	Loss/tok 3.0752 (3.1761)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.066 (0.088)	Data 1.10e-04 (9.53e-03)	Tok/s 230977 (229597)	Loss/tok 3.1200 (3.2110)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][30/1291]	Time 0.066 (0.086)	Data 1.18e-04 (6.49e-03)	Tok/s 233669 (231896)	Loss/tok 3.0880 (3.2357)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.066 (0.086)	Data 1.23e-04 (4.94e-03)	Tok/s 232105 (234432)	Loss/tok 3.0850 (3.2590)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.099 (0.087)	Data 1.18e-04 (3.99e-03)	Tok/s 254052 (236360)	Loss/tok 3.3558 (3.2744)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.066 (0.091)	Data 1.11e-04 (3.36e-03)	Tok/s 230745 (238103)	Loss/tok 3.0393 (3.3037)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.066 (0.089)	Data 1.18e-04 (2.90e-03)	Tok/s 234583 (238221)	Loss/tok 3.0586 (3.2933)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.036 (0.088)	Data 1.13e-04 (2.56e-03)	Tok/s 229428 (238548)	Loss/tok 2.5988 (3.2812)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.174 (0.089)	Data 1.17e-04 (2.29e-03)	Tok/s 256048 (239337)	Loss/tok 3.6764 (3.2869)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.07e-03)	Tok/s 255119 (239865)	Loss/tok 3.2486 (3.2922)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.100 (0.088)	Data 1.25e-04 (1.90e-03)	Tok/s 251569 (239809)	Loss/tok 3.3405 (3.2816)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.136 (0.088)	Data 1.26e-04 (1.75e-03)	Tok/s 260340 (239764)	Loss/tok 3.4223 (3.2822)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.100 (0.088)	Data 1.30e-04 (1.63e-03)	Tok/s 249648 (240103)	Loss/tok 3.3460 (3.2784)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.100 (0.087)	Data 1.13e-04 (1.52e-03)	Tok/s 252810 (240121)	Loss/tok 3.3127 (3.2724)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.173 (0.090)	Data 1.12e-04 (1.43e-03)	Tok/s 261594 (240802)	Loss/tok 3.5669 (3.2923)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][160/1291]	Time 0.100 (0.089)	Data 1.16e-04 (1.35e-03)	Tok/s 251510 (240900)	Loss/tok 3.2748 (3.2883)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][170/1291]	Time 0.066 (0.088)	Data 1.14e-04 (1.28e-03)	Tok/s 234484 (240746)	Loss/tok 3.1542 (3.2834)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.087)	Data 1.21e-04 (1.21e-03)	Tok/s 234828 (240594)	Loss/tok 3.0853 (3.2789)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.099 (0.087)	Data 1.21e-04 (1.15e-03)	Tok/s 253076 (240830)	Loss/tok 3.2533 (3.2774)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.066 (0.088)	Data 1.20e-04 (1.10e-03)	Tok/s 234157 (241040)	Loss/tok 3.0242 (3.2795)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.099 (0.088)	Data 1.28e-04 (1.06e-03)	Tok/s 250169 (241363)	Loss/tok 3.2362 (3.2837)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.066 (0.089)	Data 1.15e-04 (1.01e-03)	Tok/s 234886 (241569)	Loss/tok 3.0193 (3.2835)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.066 (0.088)	Data 1.11e-04 (9.76e-04)	Tok/s 239071 (241552)	Loss/tok 3.1251 (3.2824)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.135 (0.089)	Data 1.38e-04 (9.40e-04)	Tok/s 259991 (241936)	Loss/tok 3.4399 (3.2850)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.099 (0.089)	Data 1.18e-04 (9.08e-04)	Tok/s 255494 (242050)	Loss/tok 3.3091 (3.2825)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.134 (0.089)	Data 1.19e-04 (8.78e-04)	Tok/s 258333 (242080)	Loss/tok 3.5492 (3.2824)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.066 (0.088)	Data 1.16e-04 (8.50e-04)	Tok/s 235494 (242085)	Loss/tok 3.0606 (3.2803)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.066 (0.088)	Data 1.31e-04 (8.24e-04)	Tok/s 232940 (241909)	Loss/tok 3.0641 (3.2795)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.174 (0.088)	Data 1.19e-04 (7.99e-04)	Tok/s 256926 (242034)	Loss/tok 3.6138 (3.2841)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][300/1291]	Time 0.067 (0.088)	Data 1.16e-04 (7.77e-04)	Tok/s 230428 (241843)	Loss/tok 3.1198 (3.2817)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.066 (0.088)	Data 1.14e-04 (7.56e-04)	Tok/s 234866 (241901)	Loss/tok 3.0634 (3.2826)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.100 (0.088)	Data 1.14e-04 (7.36e-04)	Tok/s 256686 (242140)	Loss/tok 3.2179 (3.2817)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.099 (0.088)	Data 1.13e-04 (7.17e-04)	Tok/s 251140 (242312)	Loss/tok 3.3498 (3.2819)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.135 (0.088)	Data 1.14e-04 (6.99e-04)	Tok/s 258623 (242414)	Loss/tok 3.4739 (3.2842)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.067 (0.089)	Data 1.16e-04 (6.83e-04)	Tok/s 229928 (242607)	Loss/tok 3.0225 (3.2850)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.067 (0.089)	Data 1.17e-04 (6.67e-04)	Tok/s 229916 (242610)	Loss/tok 3.0920 (3.2869)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.099 (0.088)	Data 1.37e-04 (6.52e-04)	Tok/s 251752 (242519)	Loss/tok 3.2944 (3.2853)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.134 (0.089)	Data 1.20e-04 (6.38e-04)	Tok/s 262523 (242824)	Loss/tok 3.3651 (3.2870)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.100 (0.089)	Data 1.13e-04 (6.25e-04)	Tok/s 255218 (242852)	Loss/tok 3.3369 (3.2873)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.100 (0.089)	Data 1.13e-04 (6.12e-04)	Tok/s 255414 (242783)	Loss/tok 3.3259 (3.2876)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.099 (0.089)	Data 1.31e-04 (6.00e-04)	Tok/s 254958 (242838)	Loss/tok 3.3385 (3.2876)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.067 (0.089)	Data 1.15e-04 (5.89e-04)	Tok/s 232523 (242968)	Loss/tok 3.1231 (3.2896)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][430/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.78e-04)	Tok/s 253123 (242912)	Loss/tok 3.3059 (3.2882)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.066 (0.089)	Data 1.19e-04 (5.67e-04)	Tok/s 228844 (242780)	Loss/tok 3.0456 (3.2853)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.57e-04)	Tok/s 256008 (242856)	Loss/tok 3.3691 (3.2862)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][460/1291]	Time 0.066 (0.089)	Data 1.11e-04 (5.48e-04)	Tok/s 233510 (242946)	Loss/tok 3.1069 (3.2873)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][470/1291]	Time 0.099 (0.089)	Data 1.18e-04 (5.38e-04)	Tok/s 255710 (243097)	Loss/tok 3.2851 (3.2916)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.099 (0.089)	Data 1.12e-04 (5.30e-04)	Tok/s 255989 (243192)	Loss/tok 3.2970 (3.2915)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.099 (0.090)	Data 1.20e-04 (5.21e-04)	Tok/s 255378 (243269)	Loss/tok 3.2698 (3.2964)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.066 (0.090)	Data 1.16e-04 (5.13e-04)	Tok/s 231248 (243222)	Loss/tok 3.0800 (3.2970)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.066 (0.090)	Data 1.11e-04 (5.05e-04)	Tok/s 231639 (243055)	Loss/tok 3.0568 (3.2943)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.98e-04)	Tok/s 251844 (243078)	Loss/tok 3.2653 (3.2942)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.090)	Data 1.15e-04 (4.91e-04)	Tok/s 234002 (243070)	Loss/tok 3.1298 (3.2930)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.099 (0.090)	Data 1.18e-04 (4.84e-04)	Tok/s 254560 (243146)	Loss/tok 3.2227 (3.2950)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.066 (0.090)	Data 1.15e-04 (4.77e-04)	Tok/s 234426 (243083)	Loss/tok 3.0994 (3.2935)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.066 (0.089)	Data 1.12e-04 (4.70e-04)	Tok/s 233334 (242955)	Loss/tok 3.1196 (3.2918)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.066 (0.089)	Data 1.14e-04 (4.64e-04)	Tok/s 235390 (243010)	Loss/tok 3.1623 (3.2924)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.58e-04)	Tok/s 232923 (243005)	Loss/tok 3.0661 (3.2909)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.52e-04)	Tok/s 233754 (243026)	Loss/tok 3.1550 (3.2909)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][600/1291]	Time 0.099 (0.089)	Data 1.17e-04 (4.47e-04)	Tok/s 255550 (243131)	Loss/tok 3.3215 (3.2916)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.066 (0.089)	Data 1.13e-04 (4.41e-04)	Tok/s 239507 (243039)	Loss/tok 3.1952 (3.2920)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.067 (0.089)	Data 1.20e-04 (4.36e-04)	Tok/s 232556 (243070)	Loss/tok 3.0515 (3.2924)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][630/1291]	Time 0.099 (0.089)	Data 1.16e-04 (4.31e-04)	Tok/s 254079 (243077)	Loss/tok 3.2514 (3.2912)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.066 (0.089)	Data 1.13e-04 (4.26e-04)	Tok/s 233135 (243065)	Loss/tok 3.0926 (3.2901)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.21e-04)	Tok/s 234263 (243049)	Loss/tok 3.1446 (3.2894)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.099 (0.089)	Data 1.13e-04 (4.17e-04)	Tok/s 254787 (243076)	Loss/tok 3.2292 (3.2892)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.134 (0.089)	Data 1.12e-04 (4.12e-04)	Tok/s 260357 (243050)	Loss/tok 3.5056 (3.2902)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.08e-04)	Tok/s 233753 (243143)	Loss/tok 2.9708 (3.2911)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.134 (0.090)	Data 1.20e-04 (4.04e-04)	Tok/s 261145 (243137)	Loss/tok 3.5374 (3.2916)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.066 (0.090)	Data 1.14e-04 (4.00e-04)	Tok/s 236900 (243174)	Loss/tok 3.0693 (3.2932)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.134 (0.090)	Data 1.35e-04 (3.96e-04)	Tok/s 259066 (243159)	Loss/tok 3.5734 (3.2930)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.135 (0.090)	Data 1.12e-04 (3.92e-04)	Tok/s 258940 (243244)	Loss/tok 3.4774 (3.2947)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.066 (0.090)	Data 1.14e-04 (3.88e-04)	Tok/s 230564 (243207)	Loss/tok 3.0504 (3.2936)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.174 (0.090)	Data 1.10e-04 (3.84e-04)	Tok/s 256358 (243291)	Loss/tok 3.6882 (3.2963)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][750/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.81e-04)	Tok/s 258278 (243307)	Loss/tok 3.2267 (3.2962)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.066 (0.090)	Data 1.15e-04 (3.77e-04)	Tok/s 230094 (243266)	Loss/tok 3.1142 (3.2961)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.099 (0.090)	Data 1.22e-04 (3.74e-04)	Tok/s 253213 (243347)	Loss/tok 3.3385 (3.2974)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.134 (0.090)	Data 1.13e-04 (3.71e-04)	Tok/s 260629 (243324)	Loss/tok 3.4641 (3.2976)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.67e-04)	Tok/s 253267 (243429)	Loss/tok 3.2912 (3.2974)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.174 (0.090)	Data 1.13e-04 (3.64e-04)	Tok/s 256507 (243504)	Loss/tok 3.6796 (3.2984)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.099 (0.090)	Data 1.15e-04 (3.61e-04)	Tok/s 252031 (243542)	Loss/tok 3.3489 (3.2992)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.58e-04)	Tok/s 254799 (243493)	Loss/tok 3.4146 (3.2981)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.134 (0.090)	Data 1.09e-04 (3.55e-04)	Tok/s 258610 (243417)	Loss/tok 3.5755 (3.2973)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.099 (0.090)	Data 1.16e-04 (3.52e-04)	Tok/s 253576 (243446)	Loss/tok 3.4082 (3.2974)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.066 (0.090)	Data 1.14e-04 (3.50e-04)	Tok/s 232396 (243364)	Loss/tok 3.1307 (3.2963)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.100 (0.090)	Data 1.10e-04 (3.47e-04)	Tok/s 252213 (243425)	Loss/tok 3.2276 (3.2960)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.44e-04)	Tok/s 230606 (243356)	Loss/tok 3.1654 (3.2944)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][880/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.42e-04)	Tok/s 251471 (243334)	Loss/tok 3.3363 (3.2941)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.39e-04)	Tok/s 231154 (243322)	Loss/tok 3.0583 (3.2934)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][900/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.37e-04)	Tok/s 233711 (243336)	Loss/tok 3.0968 (3.2946)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.089)	Data 1.23e-04 (3.34e-04)	Tok/s 235075 (243375)	Loss/tok 3.1033 (3.2944)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.32e-04)	Tok/s 256653 (243363)	Loss/tok 3.3234 (3.2936)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.066 (0.089)	Data 1.15e-04 (3.29e-04)	Tok/s 232063 (243330)	Loss/tok 3.0297 (3.2936)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.099 (0.089)	Data 1.16e-04 (3.27e-04)	Tok/s 256085 (243395)	Loss/tok 3.2912 (3.2936)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.25e-04)	Tok/s 230905 (243291)	Loss/tok 3.2065 (3.2922)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.136 (0.089)	Data 1.14e-04 (3.23e-04)	Tok/s 256237 (243345)	Loss/tok 3.4189 (3.2927)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.099 (0.089)	Data 1.14e-04 (3.21e-04)	Tok/s 255899 (243398)	Loss/tok 3.2392 (3.2928)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.134 (0.089)	Data 1.15e-04 (3.18e-04)	Tok/s 262193 (243415)	Loss/tok 3.4485 (3.2925)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.16e-04)	Tok/s 250939 (243365)	Loss/tok 3.2935 (3.2915)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.099 (0.089)	Data 1.13e-04 (3.14e-04)	Tok/s 252203 (243402)	Loss/tok 3.2741 (3.2914)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.12e-04)	Tok/s 234781 (243440)	Loss/tok 3.0047 (3.2922)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1020/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.10e-04)	Tok/s 251758 (243394)	Loss/tok 3.2702 (3.2913)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.09e-04)	Tok/s 255415 (243387)	Loss/tok 3.2406 (3.2905)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.07e-04)	Tok/s 232850 (243417)	Loss/tok 3.0283 (3.2908)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.05e-04)	Tok/s 251841 (243352)	Loss/tok 3.3122 (3.2898)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.03e-04)	Tok/s 232695 (243322)	Loss/tok 3.1997 (3.2893)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1070/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.01e-04)	Tok/s 232444 (243327)	Loss/tok 3.0516 (3.2894)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.066 (0.089)	Data 1.15e-04 (3.00e-04)	Tok/s 229857 (243340)	Loss/tok 2.9782 (3.2895)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.134 (0.089)	Data 1.13e-04 (2.98e-04)	Tok/s 261106 (243324)	Loss/tok 3.3704 (3.2886)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.96e-04)	Tok/s 233209 (243273)	Loss/tok 3.1109 (3.2880)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.066 (0.089)	Data 1.17e-04 (2.95e-04)	Tok/s 230493 (243352)	Loss/tok 3.0165 (3.2888)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.036 (0.089)	Data 1.13e-04 (2.93e-04)	Tok/s 219909 (243358)	Loss/tok 2.7311 (3.2884)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.91e-04)	Tok/s 257900 (243360)	Loss/tok 3.3548 (3.2880)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.90e-04)	Tok/s 254069 (243396)	Loss/tok 3.2089 (3.2879)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.88e-04)	Tok/s 234276 (243359)	Loss/tok 3.1022 (3.2872)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.87e-04)	Tok/s 234839 (243367)	Loss/tok 3.1419 (3.2871)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.066 (0.089)	Data 1.16e-04 (2.85e-04)	Tok/s 231266 (243392)	Loss/tok 3.0705 (3.2870)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.066 (0.089)	Data 1.15e-04 (2.84e-04)	Tok/s 235097 (243345)	Loss/tok 3.1769 (3.2863)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.099 (0.089)	Data 1.16e-04 (2.82e-04)	Tok/s 253398 (243365)	Loss/tok 3.2774 (3.2867)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1200/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.81e-04)	Tok/s 231084 (243340)	Loss/tok 3.1094 (3.2871)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.80e-04)	Tok/s 253016 (243318)	Loss/tok 3.3395 (3.2868)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.78e-04)	Tok/s 258268 (243318)	Loss/tok 3.4149 (3.2862)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.77e-04)	Tok/s 233175 (243275)	Loss/tok 3.1898 (3.2855)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.76e-04)	Tok/s 227631 (243235)	Loss/tok 3.1365 (3.2845)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.74e-04)	Tok/s 230317 (243264)	Loss/tok 3.0535 (3.2840)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.73e-04)	Tok/s 231361 (243224)	Loss/tok 3.0062 (3.2838)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.72e-04)	Tok/s 235197 (243233)	Loss/tok 3.0934 (3.2833)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.174 (0.089)	Data 1.19e-04 (2.71e-04)	Tok/s 256466 (243188)	Loss/tok 3.6231 (3.2829)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.066 (0.089)	Data 4.48e-05 (2.72e-04)	Tok/s 237618 (243186)	Loss/tok 2.9552 (3.2827)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592446784010, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592446784010, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.490 (0.490)	Decoder iters 149.0 (149.0)	Tok/s 34200 (34200)
0: Running moses detokenizer
0: BLEU(score=22.23754493025852, counts=[36664, 17866, 9969, 5786], totals=[67289, 64286, 61283, 58284], precisions=[54.48736048982746, 27.79143203807983, 16.267154023138556, 9.927252762336147], bp=1.0, sys_len=67289, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446786048, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2224, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592446786048, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2809	Test BLEU: 22.24
0: Performance: Epoch: 2	Training: 1946189 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592446786048, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592446786048, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446786049, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 466212830
0: TRAIN [3][0/1291]	Time 0.279 (0.279)	Data 2.08e-01 (2.08e-01)	Tok/s 57000 (57000)	Loss/tok 2.9034 (2.9034)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.066 (0.104)	Data 1.21e-04 (1.91e-02)	Tok/s 228901 (226521)	Loss/tok 2.8686 (3.1673)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.066 (0.104)	Data 1.16e-04 (1.00e-02)	Tok/s 234493 (237211)	Loss/tok 3.0112 (3.2184)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.099 (0.101)	Data 1.24e-04 (6.84e-03)	Tok/s 252572 (239375)	Loss/tok 3.2337 (3.2326)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][40/1291]	Time 0.067 (0.099)	Data 1.24e-04 (5.20e-03)	Tok/s 230754 (240496)	Loss/tok 2.9579 (3.2362)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.100 (0.100)	Data 1.19e-04 (4.21e-03)	Tok/s 251150 (242061)	Loss/tok 3.3094 (3.2378)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.137 (0.099)	Data 1.12e-04 (3.54e-03)	Tok/s 254348 (242434)	Loss/tok 3.3602 (3.2360)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][70/1291]	Time 0.099 (0.100)	Data 1.16e-04 (3.05e-03)	Tok/s 253881 (243252)	Loss/tok 3.1857 (3.2471)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.067 (0.099)	Data 1.14e-04 (2.69e-03)	Tok/s 232026 (243430)	Loss/tok 2.9520 (3.2410)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.067 (0.097)	Data 1.21e-04 (2.41e-03)	Tok/s 232010 (243650)	Loss/tok 2.9825 (3.2296)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.134 (0.098)	Data 1.16e-04 (2.18e-03)	Tok/s 262639 (244142)	Loss/tok 3.4596 (3.2328)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.099 (0.098)	Data 1.13e-04 (2.00e-03)	Tok/s 251988 (244461)	Loss/tok 3.1673 (3.2297)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.100 (0.099)	Data 1.15e-04 (1.84e-03)	Tok/s 254101 (245200)	Loss/tok 3.1370 (3.2346)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.099 (0.098)	Data 1.17e-04 (1.71e-03)	Tok/s 252008 (245188)	Loss/tok 3.2433 (3.2325)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.098 (0.099)	Data 1.15e-04 (1.60e-03)	Tok/s 255942 (245397)	Loss/tok 3.1543 (3.2346)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.099 (0.098)	Data 1.11e-04 (1.50e-03)	Tok/s 255420 (245291)	Loss/tok 3.1735 (3.2298)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.100 (0.096)	Data 1.12e-04 (1.41e-03)	Tok/s 253445 (244560)	Loss/tok 3.2035 (3.2215)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.067 (0.095)	Data 1.13e-04 (1.34e-03)	Tok/s 233894 (244396)	Loss/tok 3.0557 (3.2173)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.066 (0.094)	Data 1.13e-04 (1.27e-03)	Tok/s 232949 (243931)	Loss/tok 2.9481 (3.2108)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.099 (0.094)	Data 1.14e-04 (1.21e-03)	Tok/s 256754 (244153)	Loss/tok 3.2494 (3.2129)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][200/1291]	Time 0.066 (0.094)	Data 1.18e-04 (1.15e-03)	Tok/s 234626 (244191)	Loss/tok 3.1400 (3.2114)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.035 (0.093)	Data 1.19e-04 (1.10e-03)	Tok/s 228224 (244038)	Loss/tok 2.6211 (3.2101)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.099 (0.093)	Data 1.17e-04 (1.06e-03)	Tok/s 254540 (244039)	Loss/tok 3.2212 (3.2079)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.099 (0.093)	Data 1.14e-04 (1.02e-03)	Tok/s 255610 (243978)	Loss/tok 3.1683 (3.2117)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][240/1291]	Time 0.066 (0.093)	Data 1.20e-04 (9.82e-04)	Tok/s 236295 (244079)	Loss/tok 3.0183 (3.2122)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.099 (0.094)	Data 1.15e-04 (9.47e-04)	Tok/s 255945 (244338)	Loss/tok 3.2580 (3.2137)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.134 (0.094)	Data 1.13e-04 (9.15e-04)	Tok/s 261123 (244567)	Loss/tok 3.3028 (3.2142)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.066 (0.093)	Data 1.14e-04 (8.86e-04)	Tok/s 232484 (244201)	Loss/tok 3.0141 (3.2098)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.100 (0.093)	Data 1.16e-04 (8.58e-04)	Tok/s 251966 (244129)	Loss/tok 3.1496 (3.2064)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.173 (0.093)	Data 1.12e-04 (8.33e-04)	Tok/s 255712 (244141)	Loss/tok 3.6502 (3.2063)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.066 (0.092)	Data 1.16e-04 (8.09e-04)	Tok/s 231522 (244128)	Loss/tok 2.9941 (3.2046)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.135 (0.092)	Data 1.14e-04 (7.87e-04)	Tok/s 257001 (244206)	Loss/tok 3.3784 (3.2036)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.135 (0.093)	Data 1.28e-04 (7.66e-04)	Tok/s 259955 (244293)	Loss/tok 3.2751 (3.2038)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.173 (0.093)	Data 1.20e-04 (7.46e-04)	Tok/s 255823 (244355)	Loss/tok 3.5635 (3.2042)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.135 (0.093)	Data 1.16e-04 (7.28e-04)	Tok/s 261448 (244417)	Loss/tok 3.2148 (3.2021)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.066 (0.093)	Data 1.13e-04 (7.10e-04)	Tok/s 236192 (244399)	Loss/tok 3.0121 (3.2015)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.066 (0.092)	Data 1.14e-04 (6.94e-04)	Tok/s 231846 (244281)	Loss/tok 3.0208 (3.1997)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][370/1291]	Time 0.099 (0.092)	Data 1.16e-04 (6.78e-04)	Tok/s 252136 (244280)	Loss/tok 3.1989 (3.1991)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.100 (0.092)	Data 1.16e-04 (6.63e-04)	Tok/s 252681 (244318)	Loss/tok 3.0834 (3.1986)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.067 (0.092)	Data 1.15e-04 (6.49e-04)	Tok/s 233966 (244207)	Loss/tok 3.0184 (3.1976)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.175 (0.092)	Data 1.22e-04 (6.36e-04)	Tok/s 255337 (244068)	Loss/tok 3.4585 (3.1957)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.067 (0.092)	Data 1.12e-04 (6.23e-04)	Tok/s 232637 (243967)	Loss/tok 2.9637 (3.1931)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.135 (0.092)	Data 1.17e-04 (6.11e-04)	Tok/s 257519 (244028)	Loss/tok 3.2543 (3.1945)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.135 (0.092)	Data 1.16e-04 (6.00e-04)	Tok/s 260843 (243844)	Loss/tok 3.2227 (3.1921)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.067 (0.092)	Data 1.12e-04 (5.89e-04)	Tok/s 232280 (243835)	Loss/tok 2.9980 (3.1939)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.099 (0.092)	Data 1.12e-04 (5.78e-04)	Tok/s 257480 (243835)	Loss/tok 3.1485 (3.1921)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.135 (0.092)	Data 1.13e-04 (5.68e-04)	Tok/s 257184 (243993)	Loss/tok 3.3780 (3.1929)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.135 (0.092)	Data 1.17e-04 (5.58e-04)	Tok/s 260110 (243996)	Loss/tok 3.2937 (3.1917)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.091)	Data 1.26e-04 (5.49e-04)	Tok/s 235458 (243839)	Loss/tok 2.9631 (3.1893)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][490/1291]	Time 0.099 (0.091)	Data 1.13e-04 (5.40e-04)	Tok/s 254149 (243719)	Loss/tok 3.0980 (3.1872)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.036 (0.091)	Data 1.17e-04 (5.32e-04)	Tok/s 219641 (243746)	Loss/tok 2.5478 (3.1857)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.035 (0.091)	Data 1.15e-04 (5.24e-04)	Tok/s 219457 (243698)	Loss/tok 2.5896 (3.1868)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.036 (0.090)	Data 1.13e-04 (5.16e-04)	Tok/s 220660 (243537)	Loss/tok 2.5518 (3.1841)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.135 (0.090)	Data 1.18e-04 (5.08e-04)	Tok/s 257609 (243557)	Loss/tok 3.2890 (3.1832)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.067 (0.090)	Data 1.12e-04 (5.01e-04)	Tok/s 230141 (243391)	Loss/tok 2.9925 (3.1826)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.099 (0.090)	Data 1.13e-04 (4.94e-04)	Tok/s 253099 (243311)	Loss/tok 3.2978 (3.1810)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.099 (0.090)	Data 1.18e-04 (4.87e-04)	Tok/s 251965 (243409)	Loss/tok 3.2663 (3.1805)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.035 (0.090)	Data 1.11e-04 (4.81e-04)	Tok/s 222468 (243434)	Loss/tok 2.5215 (3.1813)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.175 (0.091)	Data 1.12e-04 (4.75e-04)	Tok/s 257541 (243588)	Loss/tok 3.4140 (3.1832)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.099 (0.091)	Data 1.35e-04 (4.69e-04)	Tok/s 255129 (243589)	Loss/tok 3.0973 (3.1820)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.175 (0.091)	Data 1.15e-04 (4.63e-04)	Tok/s 257169 (243612)	Loss/tok 3.4403 (3.1826)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.066 (0.091)	Data 1.13e-04 (4.57e-04)	Tok/s 233829 (243653)	Loss/tok 2.8744 (3.1827)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][620/1291]	Time 0.066 (0.091)	Data 1.19e-04 (4.51e-04)	Tok/s 233409 (243610)	Loss/tok 2.8931 (3.1811)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.099 (0.091)	Data 1.17e-04 (4.46e-04)	Tok/s 254678 (243636)	Loss/tok 3.0689 (3.1798)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.099 (0.091)	Data 1.15e-04 (4.41e-04)	Tok/s 255325 (243683)	Loss/tok 3.0713 (3.1807)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.134 (0.091)	Data 1.11e-04 (4.36e-04)	Tok/s 262271 (243765)	Loss/tok 3.2672 (3.1807)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.066 (0.091)	Data 1.34e-04 (4.31e-04)	Tok/s 233141 (243725)	Loss/tok 2.8520 (3.1802)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.135 (0.091)	Data 1.19e-04 (4.26e-04)	Tok/s 261235 (243763)	Loss/tok 3.2076 (3.1801)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.135 (0.091)	Data 1.13e-04 (4.22e-04)	Tok/s 257325 (243760)	Loss/tok 3.2535 (3.1792)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.066 (0.091)	Data 1.19e-04 (4.17e-04)	Tok/s 233649 (243664)	Loss/tok 2.8839 (3.1783)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.100 (0.091)	Data 1.16e-04 (4.13e-04)	Tok/s 253356 (243651)	Loss/tok 3.1476 (3.1776)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.099 (0.091)	Data 1.13e-04 (4.09e-04)	Tok/s 253969 (243684)	Loss/tok 3.1507 (3.1770)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.036 (0.091)	Data 1.15e-04 (4.05e-04)	Tok/s 222125 (243612)	Loss/tok 2.5878 (3.1768)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.099 (0.091)	Data 1.12e-04 (4.01e-04)	Tok/s 253299 (243665)	Loss/tok 3.1069 (3.1765)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.091)	Data 1.13e-04 (3.97e-04)	Tok/s 230390 (243665)	Loss/tok 2.9293 (3.1766)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][750/1291]	Time 0.066 (0.091)	Data 1.15e-04 (3.93e-04)	Tok/s 233910 (243669)	Loss/tok 2.9060 (3.1760)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.099 (0.091)	Data 1.12e-04 (3.89e-04)	Tok/s 254098 (243719)	Loss/tok 3.2095 (3.1757)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.135 (0.091)	Data 1.18e-04 (3.86e-04)	Tok/s 261522 (243682)	Loss/tok 3.2361 (3.1745)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.100 (0.091)	Data 1.18e-04 (3.82e-04)	Tok/s 254091 (243641)	Loss/tok 3.1793 (3.1734)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.066 (0.091)	Data 1.10e-04 (3.79e-04)	Tok/s 232589 (243623)	Loss/tok 3.0062 (3.1723)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.066 (0.091)	Data 1.17e-04 (3.76e-04)	Tok/s 231698 (243598)	Loss/tok 2.9337 (3.1716)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.134 (0.090)	Data 1.15e-04 (3.73e-04)	Tok/s 260892 (243550)	Loss/tok 3.2833 (3.1703)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.035 (0.091)	Data 1.14e-04 (3.69e-04)	Tok/s 226975 (243590)	Loss/tok 2.6333 (3.1712)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.66e-04)	Tok/s 253761 (243497)	Loss/tok 3.1839 (3.1695)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.63e-04)	Tok/s 233000 (243413)	Loss/tok 2.9139 (3.1687)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.066 (0.090)	Data 1.08e-04 (3.60e-04)	Tok/s 232212 (243330)	Loss/tok 2.9628 (3.1675)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.58e-04)	Tok/s 229907 (243270)	Loss/tok 2.9665 (3.1660)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.066 (0.090)	Data 1.15e-04 (3.55e-04)	Tok/s 228474 (243265)	Loss/tok 2.9329 (3.1659)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][880/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.52e-04)	Tok/s 230447 (243240)	Loss/tok 2.9050 (3.1655)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.099 (0.090)	Data 1.14e-04 (3.49e-04)	Tok/s 252439 (243211)	Loss/tok 3.1192 (3.1641)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.47e-04)	Tok/s 232208 (243178)	Loss/tok 3.0185 (3.1634)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.44e-04)	Tok/s 233685 (243142)	Loss/tok 2.9415 (3.1631)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.067 (0.090)	Data 1.18e-04 (3.42e-04)	Tok/s 234471 (243168)	Loss/tok 3.0322 (3.1625)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.39e-04)	Tok/s 234782 (243226)	Loss/tok 2.9370 (3.1624)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.37e-04)	Tok/s 229478 (243210)	Loss/tok 2.9021 (3.1615)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.036 (0.089)	Data 1.41e-04 (3.35e-04)	Tok/s 216212 (243193)	Loss/tok 2.6012 (3.1612)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.32e-04)	Tok/s 233799 (243173)	Loss/tok 3.0213 (3.1605)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.135 (0.090)	Data 1.16e-04 (3.30e-04)	Tok/s 259414 (243227)	Loss/tok 3.1814 (3.1603)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.28e-04)	Tok/s 253951 (243138)	Loss/tok 3.1467 (3.1589)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.26e-04)	Tok/s 233699 (243162)	Loss/tok 3.0033 (3.1596)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.24e-04)	Tok/s 253909 (243202)	Loss/tok 3.1403 (3.1590)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1010/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.21e-04)	Tok/s 238990 (243164)	Loss/tok 3.0059 (3.1589)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.067 (0.089)	Data 1.20e-04 (3.20e-04)	Tok/s 227625 (243086)	Loss/tok 2.9613 (3.1576)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.134 (0.089)	Data 1.16e-04 (3.18e-04)	Tok/s 260555 (243084)	Loss/tok 3.2491 (3.1575)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.16e-04)	Tok/s 232203 (243032)	Loss/tok 2.9512 (3.1570)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.14e-04)	Tok/s 255005 (243119)	Loss/tok 3.0673 (3.1578)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.176 (0.089)	Data 1.14e-04 (3.12e-04)	Tok/s 253296 (243092)	Loss/tok 3.4578 (3.1577)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.10e-04)	Tok/s 232095 (243088)	Loss/tok 2.9081 (3.1572)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.134 (0.089)	Data 1.13e-04 (3.08e-04)	Tok/s 256763 (243096)	Loss/tok 3.3266 (3.1566)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.035 (0.089)	Data 1.11e-04 (3.06e-04)	Tok/s 221773 (243073)	Loss/tok 2.4975 (3.1559)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.135 (0.089)	Data 1.11e-04 (3.05e-04)	Tok/s 257616 (243130)	Loss/tok 3.2552 (3.1559)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.098 (0.089)	Data 1.12e-04 (3.03e-04)	Tok/s 254496 (243076)	Loss/tok 3.1585 (3.1548)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.01e-04)	Tok/s 233864 (243024)	Loss/tok 2.9116 (3.1540)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1130/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.00e-04)	Tok/s 233132 (242961)	Loss/tok 2.9527 (3.1529)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.98e-04)	Tok/s 262923 (242996)	Loss/tok 3.3393 (3.1538)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.96e-04)	Tok/s 233708 (243018)	Loss/tok 2.8740 (3.1534)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.95e-04)	Tok/s 249515 (242987)	Loss/tok 3.1628 (3.1535)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.93e-04)	Tok/s 255184 (242999)	Loss/tok 3.1394 (3.1536)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.92e-04)	Tok/s 255842 (242996)	Loss/tok 3.3755 (3.1532)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.90e-04)	Tok/s 233870 (242945)	Loss/tok 2.9305 (3.1522)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.036 (0.089)	Data 1.14e-04 (2.89e-04)	Tok/s 220754 (242912)	Loss/tok 2.5372 (3.1515)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.87e-04)	Tok/s 233315 (242861)	Loss/tok 2.8958 (3.1504)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.86e-04)	Tok/s 238137 (242831)	Loss/tok 2.8977 (3.1497)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.099 (0.089)	Data 1.15e-04 (2.85e-04)	Tok/s 257376 (242884)	Loss/tok 3.2285 (3.1495)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.83e-04)	Tok/s 252715 (242922)	Loss/tok 3.1043 (3.1495)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.135 (0.089)	Data 1.14e-04 (2.82e-04)	Tok/s 259866 (242947)	Loss/tok 3.2635 (3.1499)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1260/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.81e-04)	Tok/s 230946 (242975)	Loss/tok 2.8588 (3.1500)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.067 (0.089)	Data 1.19e-04 (2.79e-04)	Tok/s 233434 (242970)	Loss/tok 2.9544 (3.1493)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.78e-04)	Tok/s 233224 (242916)	Loss/tok 2.9448 (3.1481)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.099 (0.089)	Data 4.24e-05 (2.79e-04)	Tok/s 254238 (242924)	Loss/tok 3.1598 (3.1479)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592446901312, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592446901312, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.479 (0.479)	Decoder iters 149.0 (149.0)	Tok/s 34164 (34164)
0: Running moses detokenizer
0: BLEU(score=24.081048838571306, counts=[37065, 18627, 10640, 6281], totals=[65458, 62455, 59452, 56452], precisions=[56.62409483943903, 29.82467376511088, 17.896790688286348, 11.12626656274357], bp=1.0, sys_len=65458, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446903288, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2408, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592446903288, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1491	Test BLEU: 24.08
0: Performance: Epoch: 3	Training: 1943771 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592446903289, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592446903289, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-17 07:21:49 PM
RESULT,RNN_TRANSLATOR,,500,nvidia,2020-06-17 07:13:29 PM
ENDING TIMING RUN AT 2020-06-17 07:21:49 PM
RESULT,RNN_TRANSLATOR,,500,nvidia,2020-06-17 07:13:29 PM
ENDING TIMING RUN AT 2020-06-17 07:21:50 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 07:13:29 PM
ENDING TIMING RUN AT 2020-06-17 07:21:50 PM
ENDING TIMING RUN AT 2020-06-17 07:21:50 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 07:13:29 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 07:13:29 PM
ENDING TIMING RUN AT 2020-06-17 07:21:50 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 07:13:29 PM
ENDING TIMING RUN AT 2020-06-17 07:21:50 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 07:13:29 PM
ENDING TIMING RUN AT 2020-06-17 07:21:50 PM
RESULT,RNN_TRANSLATOR,,501,nvidia,2020-06-17 07:13:29 PM
