+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592453791520, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592453791559, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592453791559, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592453791560, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592453791560, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0053
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592453797695, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/13842444/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ LR=2.875e-3
+ TEST_BATCH_SIZE=128
+ TRAIN_BATCH_SIZE=384
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=128
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ WARMUP_STEPS=200
+ TARGET=24.0
+ REMAIN_STEPS=4054
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=506
+ NUMEPOCHS=8
+ TARGET=24.0
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ NUMEPOCHS=8
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ '[' 8 -gt 1 ']'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DATASET_DIR=/data
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=2.875e-3
+ NUMEPOCHS=8
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DECAY_INTERVAL=506
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' -n 3 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
Using TCMalloc
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 09:16:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592453801885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453801891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453801916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453801980, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453802028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453802056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453802060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592453802067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1782279955
:::MLLOG {"namespace": "", "time_ms": 1592453810439, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1782279955, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 296647414
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592453824439, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592453824439, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592453824440, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592453824440, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592453824440, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592453826094, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592453826095, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592453826095, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592453826384, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592453826385, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592453826385, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592453826386, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453826387, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3654848218
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.342 (0.342)	Data 2.34e-01 (2.34e-01)	Tok/s 73084 (73084)	Loss/tok 10.6947 (10.6947)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.134 (0.107)	Data 1.11e-04 (2.14e-02)	Tok/s 261204 (226554)	Loss/tok 9.7917 (10.1004)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.066 (0.094)	Data 1.12e-04 (1.12e-02)	Tok/s 234257 (234622)	Loss/tok 9.0762 (9.7634)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.066 (0.093)	Data 1.10e-04 (7.66e-03)	Tok/s 233139 (239141)	Loss/tok 8.8157 (9.5076)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.172 (0.099)	Data 1.16e-04 (5.82e-03)	Tok/s 259728 (242364)	Loss/tok 8.6579 (9.2842)	LR 7.390e-05
0: TRAIN [0][50/1291]	Time 0.066 (0.099)	Data 1.10e-04 (4.70e-03)	Tok/s 234338 (243028)	Loss/tok 8.3324 (9.1229)	LR 9.303e-05
0: TRAIN [0][60/1291]	Time 0.134 (0.101)	Data 1.15e-04 (3.95e-03)	Tok/s 262069 (243996)	Loss/tok 8.2762 (8.9691)	LR 1.171e-04
0: TRAIN [0][70/1291]	Time 0.066 (0.099)	Data 1.14e-04 (3.41e-03)	Tok/s 234552 (244265)	Loss/tok 7.9226 (8.8599)	LR 1.474e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][80/1291]	Time 0.066 (0.097)	Data 1.09e-04 (3.00e-03)	Tok/s 235337 (244760)	Loss/tok 7.8929 (8.7663)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.066 (0.096)	Data 1.08e-04 (2.68e-03)	Tok/s 232056 (244644)	Loss/tok 7.8442 (8.6863)	LR 2.232e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][100/1291]	Time 0.098 (0.096)	Data 1.15e-04 (2.43e-03)	Tok/s 258115 (245106)	Loss/tok 8.7345 (8.6346)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.066 (0.095)	Data 1.10e-04 (2.22e-03)	Tok/s 234802 (245073)	Loss/tok 7.8136 (8.5892)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.092)	Data 1.11e-04 (2.04e-03)	Tok/s 236395 (244479)	Loss/tok 7.6424 (8.5396)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.098 (0.092)	Data 1.08e-04 (1.90e-03)	Tok/s 255801 (244357)	Loss/tok 7.8169 (8.4880)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.098 (0.092)	Data 1.11e-04 (1.77e-03)	Tok/s 255544 (244714)	Loss/tok 7.6901 (8.4349)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.134 (0.091)	Data 1.07e-04 (1.66e-03)	Tok/s 258114 (244575)	Loss/tok 7.7015 (8.3810)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.066 (0.091)	Data 1.12e-04 (1.56e-03)	Tok/s 238843 (244762)	Loss/tok 7.1721 (8.3239)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.099 (0.092)	Data 1.10e-04 (1.48e-03)	Tok/s 252265 (245198)	Loss/tok 7.2721 (8.2592)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.099 (0.093)	Data 1.14e-04 (1.40e-03)	Tok/s 258339 (245555)	Loss/tok 7.0398 (8.1877)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.098 (0.093)	Data 1.08e-04 (1.34e-03)	Tok/s 256358 (245683)	Loss/tok 6.8083 (8.1208)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.099 (0.092)	Data 1.12e-04 (1.28e-03)	Tok/s 252132 (245366)	Loss/tok 6.9147 (8.0649)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.066 (0.092)	Data 1.09e-04 (1.22e-03)	Tok/s 235544 (245213)	Loss/tok 6.5959 (8.0107)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.035 (0.090)	Data 1.05e-04 (1.17e-03)	Tok/s 215490 (244747)	Loss/tok 5.5342 (7.9592)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][230/1291]	Time 0.099 (0.090)	Data 1.09e-04 (1.12e-03)	Tok/s 255671 (244906)	Loss/tok 6.3529 (7.8926)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.067 (0.090)	Data 1.11e-04 (1.08e-03)	Tok/s 229175 (244574)	Loss/tok 6.0540 (7.8368)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.099 (0.089)	Data 1.09e-04 (1.04e-03)	Tok/s 255062 (244510)	Loss/tok 6.1483 (7.7748)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.089)	Data 1.24e-04 (1.01e-03)	Tok/s 234088 (244375)	Loss/tok 5.6015 (7.7099)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.173 (0.090)	Data 1.12e-04 (9.75e-04)	Tok/s 258127 (244569)	Loss/tok 6.1472 (7.6303)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.099 (0.089)	Data 1.10e-04 (9.44e-04)	Tok/s 254716 (244604)	Loss/tok 5.7118 (7.5625)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.134 (0.090)	Data 1.11e-04 (9.15e-04)	Tok/s 261699 (244648)	Loss/tok 5.8023 (7.4930)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.099 (0.090)	Data 1.11e-04 (8.88e-04)	Tok/s 255396 (244630)	Loss/tok 5.4917 (7.4283)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.099 (0.090)	Data 1.12e-04 (8.63e-04)	Tok/s 253400 (244873)	Loss/tok 5.3876 (7.3525)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.066 (0.090)	Data 1.08e-04 (8.40e-04)	Tok/s 231923 (244829)	Loss/tok 5.0344 (7.2885)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.066 (0.090)	Data 1.13e-04 (8.18e-04)	Tok/s 233478 (244882)	Loss/tok 4.8918 (7.2230)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.099 (0.090)	Data 3.03e-04 (7.98e-04)	Tok/s 254298 (244836)	Loss/tok 5.1045 (7.1656)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.066 (0.090)	Data 1.11e-04 (7.78e-04)	Tok/s 233432 (244708)	Loss/tok 4.6930 (7.1090)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][360/1291]	Time 0.099 (0.090)	Data 1.08e-04 (7.60e-04)	Tok/s 254474 (244720)	Loss/tok 4.9552 (7.0475)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.066 (0.089)	Data 1.09e-04 (7.42e-04)	Tok/s 234081 (244531)	Loss/tok 4.5335 (6.9981)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.066 (0.089)	Data 1.14e-04 (7.26e-04)	Tok/s 233081 (244463)	Loss/tok 4.3359 (6.9384)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.066 (0.089)	Data 1.09e-04 (7.10e-04)	Tok/s 233767 (244338)	Loss/tok 4.3780 (6.8850)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.174 (0.089)	Data 1.10e-04 (6.95e-04)	Tok/s 256512 (244158)	Loss/tok 5.0900 (6.8310)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.099 (0.089)	Data 1.10e-04 (6.81e-04)	Tok/s 255742 (244144)	Loss/tok 4.5692 (6.7775)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.174 (0.089)	Data 1.12e-04 (6.67e-04)	Tok/s 254951 (244184)	Loss/tok 5.0698 (6.7203)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.134 (0.089)	Data 1.09e-04 (6.54e-04)	Tok/s 262444 (244293)	Loss/tok 4.5860 (6.6620)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.036 (0.090)	Data 1.16e-04 (6.42e-04)	Tok/s 221621 (244396)	Loss/tok 3.5477 (6.6059)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.134 (0.089)	Data 1.12e-04 (6.30e-04)	Tok/s 261962 (244396)	Loss/tok 4.5168 (6.5574)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.067 (0.089)	Data 1.30e-04 (6.19e-04)	Tok/s 231618 (244231)	Loss/tok 4.0508 (6.5173)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.066 (0.089)	Data 1.11e-04 (6.08e-04)	Tok/s 231857 (244103)	Loss/tok 3.9831 (6.4753)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.066 (0.089)	Data 1.11e-04 (5.98e-04)	Tok/s 235888 (244113)	Loss/tok 4.1854 (6.4287)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][490/1291]	Time 0.036 (0.089)	Data 1.09e-04 (5.88e-04)	Tok/s 219559 (244120)	Loss/tok 3.2771 (6.3859)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.067 (0.089)	Data 1.11e-04 (5.78e-04)	Tok/s 231366 (244067)	Loss/tok 3.9026 (6.3469)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.69e-04)	Tok/s 252072 (244091)	Loss/tok 4.2255 (6.3026)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.089)	Data 1.10e-04 (5.61e-04)	Tok/s 232152 (244106)	Loss/tok 3.7910 (6.2598)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.067 (0.089)	Data 1.08e-04 (5.52e-04)	Tok/s 230975 (244235)	Loss/tok 3.8853 (6.2110)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.036 (0.089)	Data 1.11e-04 (5.44e-04)	Tok/s 222853 (244141)	Loss/tok 3.2874 (6.1763)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.099 (0.089)	Data 1.08e-04 (5.36e-04)	Tok/s 256123 (244206)	Loss/tok 4.2333 (6.1354)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.099 (0.090)	Data 1.13e-04 (5.28e-04)	Tok/s 253176 (244285)	Loss/tok 4.0816 (6.0946)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.100 (0.089)	Data 1.10e-04 (5.21e-04)	Tok/s 254716 (244258)	Loss/tok 3.9602 (6.0606)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.067 (0.089)	Data 1.08e-04 (5.14e-04)	Tok/s 230351 (244264)	Loss/tok 3.7828 (6.0248)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.066 (0.089)	Data 1.10e-04 (5.07e-04)	Tok/s 233378 (244232)	Loss/tok 3.8306 (5.9899)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.066 (0.090)	Data 1.09e-04 (5.01e-04)	Tok/s 232504 (244361)	Loss/tok 3.7878 (5.9493)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.135 (0.090)	Data 1.11e-04 (4.94e-04)	Tok/s 261363 (244273)	Loss/tok 4.1451 (5.9198)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][620/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.88e-04)	Tok/s 234540 (244163)	Loss/tok 3.7642 (5.8941)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.135 (0.089)	Data 1.10e-04 (4.82e-04)	Tok/s 259923 (244046)	Loss/tok 4.2768 (5.8676)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.066 (0.089)	Data 1.09e-04 (4.76e-04)	Tok/s 234856 (244005)	Loss/tok 3.6298 (5.8399)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.099 (0.089)	Data 1.10e-04 (4.71e-04)	Tok/s 251490 (244020)	Loss/tok 4.0662 (5.8113)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.100 (0.089)	Data 1.08e-04 (4.65e-04)	Tok/s 252757 (244131)	Loss/tok 3.9033 (5.7774)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.067 (0.089)	Data 1.12e-04 (4.60e-04)	Tok/s 232754 (244066)	Loss/tok 3.5919 (5.7531)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.100 (0.089)	Data 1.12e-04 (4.55e-04)	Tok/s 252976 (244082)	Loss/tok 3.9149 (5.7251)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.50e-04)	Tok/s 230360 (244036)	Loss/tok 3.7268 (5.6980)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.067 (0.089)	Data 1.15e-04 (4.45e-04)	Tok/s 234962 (244017)	Loss/tok 3.5583 (5.6718)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.099 (0.089)	Data 1.14e-04 (4.41e-04)	Tok/s 253015 (243993)	Loss/tok 3.8601 (5.6475)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.066 (0.089)	Data 1.11e-04 (4.36e-04)	Tok/s 233864 (243906)	Loss/tok 3.5645 (5.6259)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.100 (0.089)	Data 1.08e-04 (4.32e-04)	Tok/s 253064 (243978)	Loss/tok 3.8782 (5.5986)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][740/1291]	Time 0.066 (0.089)	Data 1.10e-04 (4.27e-04)	Tok/s 230329 (243983)	Loss/tok 3.6549 (5.5735)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.068 (0.089)	Data 1.10e-04 (4.23e-04)	Tok/s 226201 (243951)	Loss/tok 3.4708 (5.5514)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.174 (0.089)	Data 1.11e-04 (4.19e-04)	Tok/s 256343 (243941)	Loss/tok 4.2372 (5.5281)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][770/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.15e-04)	Tok/s 226814 (243892)	Loss/tok 3.5516 (5.5081)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.11e-04)	Tok/s 255956 (243817)	Loss/tok 3.8159 (5.4889)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.07e-04)	Tok/s 229858 (243760)	Loss/tok 3.5400 (5.4694)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.04e-04)	Tok/s 251573 (243682)	Loss/tok 3.8287 (5.4513)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.100 (0.089)	Data 1.08e-04 (4.00e-04)	Tok/s 251943 (243695)	Loss/tok 3.7177 (5.4300)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.099 (0.089)	Data 1.34e-04 (3.97e-04)	Tok/s 254870 (243779)	Loss/tok 3.8763 (5.4067)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.036 (0.089)	Data 1.09e-04 (3.93e-04)	Tok/s 228030 (243700)	Loss/tok 3.0548 (5.3895)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.90e-04)	Tok/s 252888 (243756)	Loss/tok 3.8315 (5.3686)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.87e-04)	Tok/s 231135 (243739)	Loss/tok 3.5366 (5.3501)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.136 (0.089)	Data 1.11e-04 (3.83e-04)	Tok/s 259336 (243830)	Loss/tok 3.9493 (5.3278)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.80e-04)	Tok/s 255310 (243798)	Loss/tok 3.7635 (5.3098)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.77e-04)	Tok/s 254952 (243757)	Loss/tok 3.7653 (5.2930)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][890/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.74e-04)	Tok/s 253679 (243842)	Loss/tok 3.7091 (5.2727)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.71e-04)	Tok/s 227588 (243838)	Loss/tok 3.5391 (5.2548)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.69e-04)	Tok/s 228544 (243768)	Loss/tok 3.5553 (5.2400)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.173 (0.089)	Data 1.10e-04 (3.66e-04)	Tok/s 258932 (243818)	Loss/tok 4.0643 (5.2218)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.63e-04)	Tok/s 225219 (243695)	Loss/tok 2.9596 (5.2091)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.099 (0.089)	Data 1.07e-04 (3.60e-04)	Tok/s 254523 (243699)	Loss/tok 3.6454 (5.1932)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.067 (0.089)	Data 1.07e-04 (3.58e-04)	Tok/s 227752 (243643)	Loss/tok 3.5309 (5.1784)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.55e-04)	Tok/s 231503 (243697)	Loss/tok 3.4784 (5.1605)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.136 (0.089)	Data 1.08e-04 (3.53e-04)	Tok/s 257313 (243695)	Loss/tok 3.9492 (5.1453)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.099 (0.089)	Data 1.17e-04 (3.50e-04)	Tok/s 254367 (243720)	Loss/tok 3.7702 (5.1294)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][990/1291]	Time 0.172 (0.089)	Data 1.11e-04 (3.48e-04)	Tok/s 257864 (243729)	Loss/tok 4.1190 (5.1141)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.45e-04)	Tok/s 231446 (243677)	Loss/tok 3.4266 (5.1014)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.43e-04)	Tok/s 233396 (243610)	Loss/tok 3.4233 (5.0888)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.41e-04)	Tok/s 234714 (243593)	Loss/tok 3.3980 (5.0753)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.035 (0.089)	Data 1.16e-04 (3.39e-04)	Tok/s 223495 (243512)	Loss/tok 2.9131 (5.0640)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.36e-04)	Tok/s 252561 (243481)	Loss/tok 3.7292 (5.0511)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.066 (0.089)	Data 1.10e-04 (3.34e-04)	Tok/s 230608 (243418)	Loss/tok 3.3713 (5.0391)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.136 (0.089)	Data 1.09e-04 (3.32e-04)	Tok/s 261117 (243443)	Loss/tok 3.6625 (5.0250)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.134 (0.089)	Data 1.09e-04 (3.30e-04)	Tok/s 259813 (243481)	Loss/tok 3.8441 (5.0113)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.066 (0.089)	Data 1.08e-04 (3.28e-04)	Tok/s 227994 (243477)	Loss/tok 3.3337 (4.9987)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.066 (0.089)	Data 1.14e-04 (3.26e-04)	Tok/s 235711 (243466)	Loss/tok 3.4983 (4.9869)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.24e-04)	Tok/s 255270 (243510)	Loss/tok 3.6229 (4.9738)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.134 (0.089)	Data 1.11e-04 (3.22e-04)	Tok/s 261834 (243535)	Loss/tok 3.8265 (4.9610)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1120/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.21e-04)	Tok/s 219098 (243473)	Loss/tok 2.8463 (4.9509)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.19e-04)	Tok/s 254721 (243527)	Loss/tok 3.5351 (4.9373)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.036 (0.089)	Data 1.09e-04 (3.17e-04)	Tok/s 222159 (243487)	Loss/tok 2.8689 (4.9266)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1150/1291]	Time 0.173 (0.089)	Data 1.08e-04 (3.15e-04)	Tok/s 258978 (243491)	Loss/tok 4.0917 (4.9155)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.13e-04)	Tok/s 230008 (243448)	Loss/tok 3.4294 (4.9055)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.12e-04)	Tok/s 230943 (243415)	Loss/tok 3.2677 (4.8949)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.10e-04)	Tok/s 233543 (243343)	Loss/tok 3.3066 (4.8857)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.135 (0.089)	Data 1.10e-04 (3.08e-04)	Tok/s 255611 (243389)	Loss/tok 3.8058 (4.8737)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.06e-04)	Tok/s 255073 (243394)	Loss/tok 3.5982 (4.8623)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.05e-04)	Tok/s 232947 (243384)	Loss/tok 3.3760 (4.8519)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.067 (0.089)	Data 1.05e-04 (3.03e-04)	Tok/s 227002 (243394)	Loss/tok 3.4485 (4.8410)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.035 (0.089)	Data 1.08e-04 (3.02e-04)	Tok/s 224910 (243341)	Loss/tok 2.8418 (4.8319)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.00e-04)	Tok/s 222205 (243328)	Loss/tok 2.8357 (4.8222)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.99e-04)	Tok/s 226124 (243323)	Loss/tok 3.2856 (4.8122)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.036 (0.089)	Data 1.09e-04 (2.97e-04)	Tok/s 223219 (243257)	Loss/tok 2.9295 (4.8044)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.135 (0.088)	Data 1.09e-04 (2.96e-04)	Tok/s 256103 (243233)	Loss/tok 3.8301 (4.7955)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1280/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.94e-04)	Tok/s 230597 (243234)	Loss/tok 3.3976 (4.7857)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.099 (0.089)	Data 4.17e-05 (2.95e-04)	Tok/s 252911 (243231)	Loss/tok 3.5070 (4.7758)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592453941367, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453941367, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.426 (0.426)	Decoder iters 116.0 (116.0)	Tok/s 37153 (37153)
0: Running moses detokenizer
0: BLEU(score=19.62040820361893, counts=[33680, 15419, 8182, 4502], totals=[62236, 59233, 56231, 53232], precisions=[54.116588469696, 26.03109753009302, 14.550692678415821, 8.45731890592125], bp=0.9615529892251181, sys_len=62236, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592453943239, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1962, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453943240, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7762	Test BLEU: 19.62
0: Performance: Epoch: 0	Training: 1946120 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592453943240, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453943240, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592453943240, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1199466697
0: TRAIN [1][0/1291]	Time 0.305 (0.305)	Data 2.06e-01 (2.06e-01)	Tok/s 82801 (82801)	Loss/tok 3.4641 (3.4641)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.134 (0.107)	Data 1.11e-04 (1.88e-02)	Tok/s 263541 (227932)	Loss/tok 3.6628 (3.4859)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.067 (0.095)	Data 1.14e-04 (9.90e-03)	Tok/s 238769 (233016)	Loss/tok 3.3414 (3.4902)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.100 (0.091)	Data 1.11e-04 (6.75e-03)	Tok/s 255994 (236364)	Loss/tok 3.4420 (3.4755)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.100 (0.093)	Data 1.25e-04 (5.13e-03)	Tok/s 250028 (238693)	Loss/tok 3.4217 (3.4863)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.089)	Data 1.23e-04 (4.15e-03)	Tok/s 229164 (237986)	Loss/tok 3.3364 (3.4685)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.100 (0.088)	Data 1.08e-04 (3.49e-03)	Tok/s 254254 (238199)	Loss/tok 3.5886 (3.4754)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.036 (0.088)	Data 1.09e-04 (3.01e-03)	Tok/s 222236 (238776)	Loss/tok 2.8923 (3.4753)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.65e-03)	Tok/s 234831 (239603)	Loss/tok 3.3495 (3.4869)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.100 (0.088)	Data 1.09e-04 (2.37e-03)	Tok/s 252617 (239721)	Loss/tok 3.5494 (3.4947)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.15e-03)	Tok/s 253727 (240511)	Loss/tok 3.5446 (3.4983)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.067 (0.089)	Data 1.13e-04 (1.97e-03)	Tok/s 236347 (240440)	Loss/tok 3.2054 (3.5048)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][120/1291]	Time 0.067 (0.088)	Data 1.10e-04 (1.81e-03)	Tok/s 232179 (240349)	Loss/tok 3.3999 (3.5052)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.089)	Data 1.50e-04 (1.68e-03)	Tok/s 229214 (240353)	Loss/tok 3.2097 (3.5044)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.100 (0.088)	Data 1.11e-04 (1.57e-03)	Tok/s 253105 (240648)	Loss/tok 3.4739 (3.5008)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.067 (0.089)	Data 1.07e-04 (1.47e-03)	Tok/s 228437 (240831)	Loss/tok 3.3506 (3.5003)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.067 (0.089)	Data 1.11e-04 (1.39e-03)	Tok/s 235013 (241120)	Loss/tok 3.3326 (3.4972)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.067 (0.089)	Data 1.09e-04 (1.31e-03)	Tok/s 228444 (241096)	Loss/tok 3.2791 (3.4962)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.174 (0.090)	Data 1.10e-04 (1.25e-03)	Tok/s 253575 (241318)	Loss/tok 3.8597 (3.5004)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.135 (0.089)	Data 1.12e-04 (1.19e-03)	Tok/s 257359 (241208)	Loss/tok 3.6668 (3.4958)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.135 (0.088)	Data 1.09e-04 (1.14e-03)	Tok/s 261909 (241133)	Loss/tok 3.6231 (3.4942)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.137 (0.089)	Data 1.11e-04 (1.09e-03)	Tok/s 253961 (241146)	Loss/tok 3.7276 (3.4951)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.136 (0.090)	Data 1.09e-04 (1.04e-03)	Tok/s 256395 (241627)	Loss/tok 3.7849 (3.5026)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.100 (0.090)	Data 1.13e-04 (1.00e-03)	Tok/s 253591 (241975)	Loss/tok 3.4588 (3.5040)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.135 (0.090)	Data 1.10e-04 (9.65e-04)	Tok/s 259395 (241969)	Loss/tok 3.6396 (3.5030)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][250/1291]	Time 0.067 (0.090)	Data 1.10e-04 (9.31e-04)	Tok/s 232841 (241960)	Loss/tok 3.3411 (3.5039)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.066 (0.091)	Data 1.19e-04 (9.00e-04)	Tok/s 231194 (242107)	Loss/tok 3.2049 (3.5062)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.099 (0.090)	Data 1.12e-04 (8.71e-04)	Tok/s 253061 (242107)	Loss/tok 3.5286 (3.5040)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.067 (0.091)	Data 1.14e-04 (8.44e-04)	Tok/s 231441 (242157)	Loss/tok 3.1552 (3.5045)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.066 (0.090)	Data 1.11e-04 (8.19e-04)	Tok/s 232912 (241997)	Loss/tok 3.3154 (3.5007)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.175 (0.090)	Data 1.11e-04 (7.95e-04)	Tok/s 251718 (242190)	Loss/tok 3.9980 (3.5038)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.067 (0.090)	Data 1.10e-04 (7.73e-04)	Tok/s 230817 (242176)	Loss/tok 3.2940 (3.5018)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.067 (0.091)	Data 1.30e-04 (7.53e-04)	Tok/s 232433 (242313)	Loss/tok 3.2292 (3.5052)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.135 (0.091)	Data 1.10e-04 (7.33e-04)	Tok/s 262443 (242273)	Loss/tok 3.5590 (3.5022)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.067 (0.091)	Data 1.10e-04 (7.15e-04)	Tok/s 227455 (242249)	Loss/tok 3.2265 (3.5024)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.067 (0.090)	Data 1.05e-04 (6.98e-04)	Tok/s 230731 (242187)	Loss/tok 3.3198 (3.4991)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.099 (0.090)	Data 1.13e-04 (6.82e-04)	Tok/s 252483 (242109)	Loss/tok 3.4827 (3.4982)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.067 (0.091)	Data 1.12e-04 (6.66e-04)	Tok/s 230863 (242331)	Loss/tok 3.2066 (3.4995)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][380/1291]	Time 0.066 (0.091)	Data 1.09e-04 (6.52e-04)	Tok/s 237764 (242235)	Loss/tok 3.2742 (3.4990)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.135 (0.091)	Data 1.12e-04 (6.38e-04)	Tok/s 259372 (242332)	Loss/tok 3.5847 (3.5003)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][400/1291]	Time 0.066 (0.091)	Data 1.16e-04 (6.25e-04)	Tok/s 236474 (242264)	Loss/tok 3.1814 (3.5001)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.067 (0.091)	Data 1.12e-04 (6.12e-04)	Tok/s 232596 (242212)	Loss/tok 3.2303 (3.5001)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.067 (0.091)	Data 1.10e-04 (6.00e-04)	Tok/s 228537 (242265)	Loss/tok 3.2616 (3.4980)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.175 (0.090)	Data 1.33e-04 (5.89e-04)	Tok/s 255012 (242127)	Loss/tok 3.8551 (3.4965)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.100 (0.090)	Data 1.09e-04 (5.78e-04)	Tok/s 254611 (241968)	Loss/tok 3.4873 (3.4932)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.100 (0.090)	Data 1.11e-04 (5.68e-04)	Tok/s 253753 (242073)	Loss/tok 3.4805 (3.4925)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.100 (0.090)	Data 1.12e-04 (5.58e-04)	Tok/s 252816 (242145)	Loss/tok 3.4461 (3.4931)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.067 (0.090)	Data 1.08e-04 (5.48e-04)	Tok/s 232969 (242096)	Loss/tok 3.2638 (3.4935)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.067 (0.090)	Data 1.07e-04 (5.39e-04)	Tok/s 230381 (242058)	Loss/tok 3.2192 (3.4919)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.067 (0.090)	Data 1.07e-04 (5.30e-04)	Tok/s 232815 (241993)	Loss/tok 3.2624 (3.4891)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.100 (0.090)	Data 1.15e-04 (5.22e-04)	Tok/s 252276 (242097)	Loss/tok 3.3283 (3.4886)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.174 (0.090)	Data 1.09e-04 (5.14e-04)	Tok/s 254529 (242160)	Loss/tok 3.7838 (3.4897)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.067 (0.091)	Data 1.10e-04 (5.06e-04)	Tok/s 229257 (242270)	Loss/tok 3.1706 (3.4927)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][530/1291]	Time 0.135 (0.091)	Data 1.11e-04 (4.99e-04)	Tok/s 260436 (242403)	Loss/tok 3.6737 (3.4921)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.067 (0.091)	Data 1.11e-04 (4.92e-04)	Tok/s 230663 (242420)	Loss/tok 3.2150 (3.4917)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.036 (0.090)	Data 1.09e-04 (4.85e-04)	Tok/s 222315 (242355)	Loss/tok 2.8238 (3.4899)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.068 (0.090)	Data 1.09e-04 (4.78e-04)	Tok/s 227828 (242285)	Loss/tok 3.1617 (3.4873)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.72e-04)	Tok/s 236652 (242367)	Loss/tok 3.1890 (3.4869)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][580/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.66e-04)	Tok/s 230374 (242369)	Loss/tok 3.1464 (3.4874)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.136 (0.091)	Data 1.17e-04 (4.60e-04)	Tok/s 256595 (242388)	Loss/tok 3.6220 (3.4868)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.174 (0.091)	Data 1.12e-04 (4.54e-04)	Tok/s 259739 (242506)	Loss/tok 3.6894 (3.4882)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.036 (0.091)	Data 1.10e-04 (4.48e-04)	Tok/s 217783 (242409)	Loss/tok 2.9108 (3.4865)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.067 (0.091)	Data 1.10e-04 (4.43e-04)	Tok/s 229252 (242359)	Loss/tok 3.2175 (3.4849)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.036 (0.090)	Data 1.07e-04 (4.38e-04)	Tok/s 220489 (242249)	Loss/tok 2.7122 (3.4831)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.33e-04)	Tok/s 226504 (242216)	Loss/tok 3.2887 (3.4819)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.099 (0.090)	Data 1.10e-04 (4.28e-04)	Tok/s 255540 (242186)	Loss/tok 3.3831 (3.4808)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.100 (0.090)	Data 1.26e-04 (4.23e-04)	Tok/s 251475 (242191)	Loss/tok 3.4731 (3.4801)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.067 (0.090)	Data 1.08e-04 (4.18e-04)	Tok/s 235141 (242228)	Loss/tok 3.1267 (3.4798)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.100 (0.090)	Data 1.12e-04 (4.14e-04)	Tok/s 253613 (242192)	Loss/tok 3.3591 (3.4782)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.09e-04)	Tok/s 232703 (242105)	Loss/tok 3.2826 (3.4767)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][700/1291]	Time 0.036 (0.090)	Data 1.08e-04 (4.05e-04)	Tok/s 222754 (242074)	Loss/tok 2.6848 (3.4764)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.100 (0.090)	Data 1.11e-04 (4.01e-04)	Tok/s 248246 (242158)	Loss/tok 3.5054 (3.4753)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.067 (0.090)	Data 1.06e-04 (3.97e-04)	Tok/s 229381 (242120)	Loss/tok 3.2222 (3.4734)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.099 (0.090)	Data 1.08e-04 (3.93e-04)	Tok/s 253245 (242096)	Loss/tok 3.4518 (3.4729)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][740/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.89e-04)	Tok/s 252573 (242106)	Loss/tok 3.3166 (3.4723)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.86e-04)	Tok/s 229471 (242097)	Loss/tok 3.2063 (3.4714)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.066 (0.090)	Data 1.09e-04 (3.82e-04)	Tok/s 232937 (242175)	Loss/tok 3.2391 (3.4715)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.099 (0.090)	Data 1.11e-04 (3.79e-04)	Tok/s 256717 (242285)	Loss/tok 3.4393 (3.4714)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.068 (0.090)	Data 1.08e-04 (3.75e-04)	Tok/s 229657 (242235)	Loss/tok 3.1797 (3.4693)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.036 (0.090)	Data 1.08e-04 (3.72e-04)	Tok/s 221531 (242153)	Loss/tok 2.7865 (3.4680)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.036 (0.089)	Data 1.09e-04 (3.68e-04)	Tok/s 221226 (242069)	Loss/tok 2.7147 (3.4658)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.65e-04)	Tok/s 229518 (242218)	Loss/tok 3.1713 (3.4674)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.066 (0.090)	Data 1.12e-04 (3.62e-04)	Tok/s 232681 (242225)	Loss/tok 3.2540 (3.4668)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.135 (0.090)	Data 1.16e-04 (3.59e-04)	Tok/s 260338 (242237)	Loss/tok 3.6142 (3.4657)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.036 (0.090)	Data 1.09e-04 (3.56e-04)	Tok/s 221656 (242218)	Loss/tok 2.7969 (3.4642)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.53e-04)	Tok/s 232373 (242205)	Loss/tok 3.1654 (3.4637)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.099 (0.090)	Data 1.07e-04 (3.50e-04)	Tok/s 259054 (242288)	Loss/tok 3.3705 (3.4639)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][870/1291]	Time 0.036 (0.090)	Data 1.10e-04 (3.48e-04)	Tok/s 222729 (242202)	Loss/tok 2.7175 (3.4619)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][880/1291]	Time 0.100 (0.090)	Data 1.11e-04 (3.45e-04)	Tok/s 255657 (242256)	Loss/tok 3.3757 (3.4628)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.43e-04)	Tok/s 234493 (242233)	Loss/tok 3.2280 (3.4611)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.40e-04)	Tok/s 229528 (242241)	Loss/tok 3.1378 (3.4606)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.067 (0.090)	Data 1.06e-04 (3.37e-04)	Tok/s 226876 (242198)	Loss/tok 3.1727 (3.4597)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.099 (0.090)	Data 1.10e-04 (3.35e-04)	Tok/s 253530 (242197)	Loss/tok 3.4744 (3.4585)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.067 (0.090)	Data 1.07e-04 (3.33e-04)	Tok/s 234063 (242215)	Loss/tok 3.2399 (3.4585)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.100 (0.090)	Data 1.14e-04 (3.30e-04)	Tok/s 249212 (242197)	Loss/tok 3.4770 (3.4582)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.090)	Data 1.21e-04 (3.28e-04)	Tok/s 230498 (242204)	Loss/tok 3.1798 (3.4572)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.135 (0.090)	Data 1.09e-04 (3.26e-04)	Tok/s 259672 (242302)	Loss/tok 3.4742 (3.4572)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.136 (0.090)	Data 1.07e-04 (3.23e-04)	Tok/s 257710 (242286)	Loss/tok 3.6559 (3.4569)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.21e-04)	Tok/s 233778 (242238)	Loss/tok 3.2292 (3.4555)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.19e-04)	Tok/s 233657 (242279)	Loss/tok 3.1660 (3.4544)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.17e-04)	Tok/s 233920 (242249)	Loss/tok 3.1623 (3.4532)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1010/1291]	Time 0.066 (0.089)	Data 1.08e-04 (3.15e-04)	Tok/s 234422 (242157)	Loss/tok 3.1926 (3.4515)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.067 (0.090)	Data 1.09e-04 (3.13e-04)	Tok/s 231832 (242150)	Loss/tok 3.2176 (3.4510)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.11e-04)	Tok/s 229829 (242113)	Loss/tok 3.1193 (3.4496)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.09e-04)	Tok/s 248708 (242136)	Loss/tok 3.4361 (3.4496)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 257954 (242134)	Loss/tok 3.6362 (3.4486)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.05e-04)	Tok/s 222114 (242133)	Loss/tok 2.6611 (3.4475)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.04e-04)	Tok/s 231648 (242186)	Loss/tok 3.1495 (3.4468)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.135 (0.089)	Data 1.25e-04 (3.02e-04)	Tok/s 260028 (242223)	Loss/tok 3.5626 (3.4467)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.00e-04)	Tok/s 220858 (242197)	Loss/tok 2.7047 (3.4458)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.98e-04)	Tok/s 234782 (242168)	Loss/tok 3.1056 (3.4449)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.97e-04)	Tok/s 227139 (242153)	Loss/tok 3.1198 (3.4438)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.95e-04)	Tok/s 251069 (242201)	Loss/tok 3.3751 (3.4434)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1130/1291]	Time 0.135 (0.089)	Data 1.13e-04 (2.93e-04)	Tok/s 257885 (242284)	Loss/tok 3.6058 (3.4444)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.92e-04)	Tok/s 254853 (242279)	Loss/tok 3.4064 (3.4431)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.067 (0.089)	Data 1.27e-04 (2.90e-04)	Tok/s 231683 (242282)	Loss/tok 3.1915 (3.4431)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.100 (0.089)	Data 1.09e-04 (2.89e-04)	Tok/s 252325 (242302)	Loss/tok 3.3216 (3.4420)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.87e-04)	Tok/s 228202 (242300)	Loss/tok 3.1960 (3.4409)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1180/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.86e-04)	Tok/s 252166 (242296)	Loss/tok 3.4122 (3.4406)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.036 (0.089)	Data 1.11e-04 (2.84e-04)	Tok/s 214956 (242231)	Loss/tok 2.7008 (3.4391)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.067 (0.089)	Data 1.28e-04 (2.83e-04)	Tok/s 228740 (242208)	Loss/tok 3.1996 (3.4381)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.81e-04)	Tok/s 229622 (242182)	Loss/tok 3.1436 (3.4373)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.100 (0.089)	Data 1.29e-04 (2.80e-04)	Tok/s 253184 (242167)	Loss/tok 3.3821 (3.4365)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.099 (0.089)	Data 1.33e-04 (2.79e-04)	Tok/s 251283 (242220)	Loss/tok 3.3586 (3.4361)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.77e-04)	Tok/s 232951 (242156)	Loss/tok 3.1451 (3.4347)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.76e-04)	Tok/s 229938 (242152)	Loss/tok 3.0814 (3.4342)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.067 (0.089)	Data 1.07e-04 (2.75e-04)	Tok/s 229851 (242115)	Loss/tok 3.1716 (3.4328)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.73e-04)	Tok/s 232823 (242105)	Loss/tok 3.1591 (3.4319)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.72e-04)	Tok/s 251448 (242092)	Loss/tok 3.3181 (3.4316)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.135 (0.089)	Data 5.77e-05 (2.73e-04)	Tok/s 264586 (242152)	Loss/tok 3.4814 (3.4310)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592454058861, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454058861, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.422 (0.422)	Decoder iters 114.0 (114.0)	Tok/s 38591 (38591)
0: Running moses detokenizer
0: BLEU(score=21.74840368060185, counts=[35665, 17052, 9388, 5377], totals=[65458, 62455, 59454, 56456], precisions=[54.485318830395066, 27.30285805780162, 15.790358932956572, 9.5242312597421], bp=1.0, sys_len=65458, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454060763, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2175, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454060764, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4287	Test BLEU: 21.75
0: Performance: Epoch: 1	Training: 1936979 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592454060764, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592454060764, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592454060764, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3104265191
0: TRAIN [2][0/1291]	Time 0.284 (0.284)	Data 2.04e-01 (2.04e-01)	Tok/s 54500 (54500)	Loss/tok 3.1702 (3.1702)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.036 (0.099)	Data 1.07e-04 (1.87e-02)	Tok/s 222129 (224175)	Loss/tok 2.6178 (3.2019)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][20/1291]	Time 0.175 (0.102)	Data 1.04e-04 (9.82e-03)	Tok/s 256907 (235663)	Loss/tok 3.6667 (3.3192)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][30/1291]	Time 0.135 (0.102)	Data 1.19e-04 (6.69e-03)	Tok/s 258792 (240603)	Loss/tok 3.4138 (3.3229)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.035 (0.096)	Data 1.01e-04 (5.09e-03)	Tok/s 224936 (239137)	Loss/tok 2.5656 (3.3165)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.067 (0.094)	Data 1.11e-04 (4.11e-03)	Tok/s 228657 (240087)	Loss/tok 3.0395 (3.3088)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.099 (0.095)	Data 1.15e-04 (3.45e-03)	Tok/s 253229 (241654)	Loss/tok 3.3031 (3.3166)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.099 (0.095)	Data 9.89e-05 (2.98e-03)	Tok/s 257831 (241792)	Loss/tok 3.2971 (3.3133)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.036 (0.094)	Data 1.05e-04 (2.63e-03)	Tok/s 222975 (241925)	Loss/tok 2.6460 (3.3117)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.096)	Data 1.04e-04 (2.35e-03)	Tok/s 232877 (242710)	Loss/tok 3.0991 (3.3246)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.066 (0.095)	Data 1.02e-04 (2.13e-03)	Tok/s 238095 (242603)	Loss/tok 3.0373 (3.3186)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.066 (0.093)	Data 1.00e-04 (1.95e-03)	Tok/s 237468 (242275)	Loss/tok 3.1301 (3.3069)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.135 (0.092)	Data 1.00e-04 (1.79e-03)	Tok/s 260140 (242064)	Loss/tok 3.3863 (3.3012)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.100 (0.092)	Data 1.06e-04 (1.66e-03)	Tok/s 251721 (242729)	Loss/tok 3.2526 (3.3009)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.099 (0.093)	Data 1.14e-04 (1.55e-03)	Tok/s 256915 (243359)	Loss/tok 3.2583 (3.3030)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][150/1291]	Time 0.066 (0.093)	Data 9.97e-05 (1.46e-03)	Tok/s 233024 (243542)	Loss/tok 3.0722 (3.3020)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.036 (0.093)	Data 1.05e-04 (1.37e-03)	Tok/s 219180 (243439)	Loss/tok 2.5968 (3.3005)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.100 (0.093)	Data 1.02e-04 (1.30e-03)	Tok/s 255459 (243349)	Loss/tok 3.2590 (3.3025)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.036 (0.092)	Data 1.09e-04 (1.23e-03)	Tok/s 226064 (243158)	Loss/tok 2.6687 (3.2972)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][190/1291]	Time 0.100 (0.092)	Data 1.08e-04 (1.17e-03)	Tok/s 252728 (243517)	Loss/tok 3.3366 (3.3017)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.135 (0.093)	Data 1.08e-04 (1.12e-03)	Tok/s 256951 (243679)	Loss/tok 3.5608 (3.3074)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.134 (0.093)	Data 1.13e-04 (1.07e-03)	Tok/s 262215 (243709)	Loss/tok 3.4047 (3.3083)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.100 (0.093)	Data 1.09e-04 (1.03e-03)	Tok/s 251081 (243601)	Loss/tok 3.3656 (3.3090)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.066 (0.092)	Data 1.09e-04 (9.90e-04)	Tok/s 236014 (243369)	Loss/tok 2.9807 (3.3027)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.067 (0.091)	Data 1.11e-04 (9.54e-04)	Tok/s 229105 (243052)	Loss/tok 3.1011 (3.2973)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.099 (0.091)	Data 1.07e-04 (9.20e-04)	Tok/s 257212 (243338)	Loss/tok 3.2298 (3.2978)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.099 (0.091)	Data 1.09e-04 (8.89e-04)	Tok/s 254123 (243240)	Loss/tok 3.3440 (3.2941)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.066 (0.091)	Data 1.11e-04 (8.60e-04)	Tok/s 233361 (243241)	Loss/tok 3.1034 (3.2942)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.067 (0.091)	Data 1.06e-04 (8.34e-04)	Tok/s 233274 (243223)	Loss/tok 3.0656 (3.2911)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.099 (0.090)	Data 1.11e-04 (8.09e-04)	Tok/s 257657 (243033)	Loss/tok 3.3756 (3.2889)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.067 (0.090)	Data 1.09e-04 (7.85e-04)	Tok/s 233151 (242906)	Loss/tok 3.0899 (3.2854)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][310/1291]	Time 0.067 (0.090)	Data 1.11e-04 (7.64e-04)	Tok/s 231895 (243036)	Loss/tok 3.0353 (3.2894)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][320/1291]	Time 0.067 (0.090)	Data 1.06e-04 (7.43e-04)	Tok/s 230696 (243091)	Loss/tok 3.0437 (3.2915)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.067 (0.090)	Data 1.13e-04 (7.24e-04)	Tok/s 233256 (243033)	Loss/tok 3.1020 (3.2904)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.067 (0.090)	Data 1.08e-04 (7.06e-04)	Tok/s 229059 (243021)	Loss/tok 3.0955 (3.2934)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.067 (0.090)	Data 1.10e-04 (6.89e-04)	Tok/s 229181 (242947)	Loss/tok 3.1492 (3.2913)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.100 (0.090)	Data 1.08e-04 (6.73e-04)	Tok/s 252477 (242874)	Loss/tok 3.2730 (3.2898)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.067 (0.090)	Data 1.12e-04 (6.58e-04)	Tok/s 234263 (242726)	Loss/tok 3.1101 (3.2881)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.174 (0.090)	Data 1.09e-04 (6.44e-04)	Tok/s 255831 (242787)	Loss/tok 3.6787 (3.2922)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.067 (0.090)	Data 1.07e-04 (6.30e-04)	Tok/s 228877 (242729)	Loss/tok 3.0361 (3.2910)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.067 (0.090)	Data 1.08e-04 (6.17e-04)	Tok/s 228980 (242627)	Loss/tok 3.0941 (3.2911)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.067 (0.089)	Data 1.10e-04 (6.05e-04)	Tok/s 233141 (242478)	Loss/tok 3.1017 (3.2880)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.067 (0.089)	Data 1.14e-04 (5.94e-04)	Tok/s 231212 (242394)	Loss/tok 3.1058 (3.2871)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.066 (0.089)	Data 1.10e-04 (5.83e-04)	Tok/s 234304 (242440)	Loss/tok 3.1554 (3.2886)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][440/1291]	Time 0.136 (0.090)	Data 1.08e-04 (5.73e-04)	Tok/s 256745 (242554)	Loss/tok 3.5468 (3.2900)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][450/1291]	Time 0.099 (0.090)	Data 1.08e-04 (5.62e-04)	Tok/s 251950 (242569)	Loss/tok 3.3472 (3.2903)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.036 (0.089)	Data 1.10e-04 (5.52e-04)	Tok/s 220091 (242484)	Loss/tok 2.6363 (3.2888)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.100 (0.090)	Data 1.13e-04 (5.43e-04)	Tok/s 255475 (242462)	Loss/tok 3.3184 (3.2917)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][480/1291]	Time 0.135 (0.090)	Data 1.11e-04 (5.34e-04)	Tok/s 258933 (242485)	Loss/tok 3.4952 (3.2959)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.068 (0.090)	Data 1.17e-04 (5.25e-04)	Tok/s 231178 (242476)	Loss/tok 3.0491 (3.2953)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.036 (0.089)	Data 1.10e-04 (5.17e-04)	Tok/s 223618 (242341)	Loss/tok 2.6710 (3.2936)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.100 (0.089)	Data 1.21e-04 (5.10e-04)	Tok/s 251006 (242356)	Loss/tok 3.2298 (3.2935)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.135 (0.089)	Data 1.08e-04 (5.02e-04)	Tok/s 255609 (242363)	Loss/tok 3.4660 (3.2930)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.067 (0.089)	Data 1.07e-04 (4.94e-04)	Tok/s 227208 (242348)	Loss/tok 3.1209 (3.2919)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.099 (0.089)	Data 1.09e-04 (4.87e-04)	Tok/s 257197 (242333)	Loss/tok 3.3192 (3.2914)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.137 (0.090)	Data 1.09e-04 (4.80e-04)	Tok/s 255726 (242483)	Loss/tok 3.4707 (3.2942)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.099 (0.090)	Data 1.08e-04 (4.74e-04)	Tok/s 257258 (242510)	Loss/tok 3.2947 (3.2938)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.100 (0.090)	Data 1.08e-04 (4.67e-04)	Tok/s 248681 (242675)	Loss/tok 3.3689 (3.2965)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.036 (0.090)	Data 1.08e-04 (4.61e-04)	Tok/s 224386 (242624)	Loss/tok 2.6649 (3.2948)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.068 (0.090)	Data 1.07e-04 (4.55e-04)	Tok/s 226563 (242553)	Loss/tok 3.1486 (3.2942)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][600/1291]	Time 0.135 (0.090)	Data 1.08e-04 (4.50e-04)	Tok/s 260014 (242581)	Loss/tok 3.5418 (3.2949)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.44e-04)	Tok/s 250048 (242682)	Loss/tok 3.1784 (3.2942)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.100 (0.090)	Data 1.08e-04 (4.39e-04)	Tok/s 252153 (242687)	Loss/tok 3.2862 (3.2939)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][630/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.33e-04)	Tok/s 233575 (242653)	Loss/tok 3.0809 (3.2972)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.036 (0.090)	Data 1.09e-04 (4.28e-04)	Tok/s 221556 (242718)	Loss/tok 2.6998 (3.2979)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.23e-04)	Tok/s 236589 (242733)	Loss/tok 3.1588 (3.2977)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.036 (0.090)	Data 1.10e-04 (4.19e-04)	Tok/s 222814 (242722)	Loss/tok 2.6896 (3.2975)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.100 (0.090)	Data 1.09e-04 (4.14e-04)	Tok/s 250638 (242619)	Loss/tok 3.2737 (3.2961)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.036 (0.090)	Data 1.08e-04 (4.10e-04)	Tok/s 220419 (242574)	Loss/tok 2.7015 (3.2948)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.135 (0.090)	Data 1.08e-04 (4.05e-04)	Tok/s 258534 (242550)	Loss/tok 3.4704 (3.2940)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.066 (0.089)	Data 1.07e-04 (4.01e-04)	Tok/s 232240 (242470)	Loss/tok 3.0121 (3.2930)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.97e-04)	Tok/s 231783 (242496)	Loss/tok 3.0973 (3.2937)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.93e-04)	Tok/s 231652 (242480)	Loss/tok 3.1164 (3.2927)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.89e-04)	Tok/s 229174 (242349)	Loss/tok 3.1264 (3.2922)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.85e-04)	Tok/s 253274 (242344)	Loss/tok 3.2811 (3.2920)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.82e-04)	Tok/s 232953 (242332)	Loss/tok 3.1337 (3.2919)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][760/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.78e-04)	Tok/s 252988 (242296)	Loss/tok 3.3590 (3.2910)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.100 (0.089)	Data 1.07e-04 (3.75e-04)	Tok/s 251868 (242309)	Loss/tok 3.2972 (3.2920)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.71e-04)	Tok/s 234139 (242288)	Loss/tok 2.9968 (3.2918)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.68e-04)	Tok/s 256721 (242312)	Loss/tok 3.4397 (3.2926)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.65e-04)	Tok/s 258401 (242318)	Loss/tok 3.5249 (3.2928)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.101 (0.089)	Data 1.13e-04 (3.62e-04)	Tok/s 248818 (242294)	Loss/tok 3.2186 (3.2919)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.036 (0.089)	Data 1.05e-04 (3.59e-04)	Tok/s 227945 (242324)	Loss/tok 2.6599 (3.2919)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.100 (0.089)	Data 1.07e-04 (3.56e-04)	Tok/s 253093 (242343)	Loss/tok 3.3277 (3.2921)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.53e-04)	Tok/s 231174 (242380)	Loss/tok 3.0504 (3.2929)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.50e-04)	Tok/s 253657 (242362)	Loss/tok 3.3587 (3.2915)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.136 (0.089)	Data 1.08e-04 (3.47e-04)	Tok/s 254650 (242352)	Loss/tok 3.4974 (3.2913)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.44e-04)	Tok/s 229517 (242342)	Loss/tok 3.0764 (3.2909)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.42e-04)	Tok/s 250398 (242294)	Loss/tok 3.2202 (3.2900)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][890/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.39e-04)	Tok/s 228098 (242292)	Loss/tok 2.9087 (3.2890)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.36e-04)	Tok/s 226227 (242267)	Loss/tok 3.0750 (3.2886)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.34e-04)	Tok/s 233299 (242220)	Loss/tok 3.0558 (3.2881)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.32e-04)	Tok/s 233887 (242273)	Loss/tok 3.0355 (3.2893)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.135 (0.089)	Data 1.09e-04 (3.29e-04)	Tok/s 260651 (242279)	Loss/tok 3.4418 (3.2894)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][940/1291]	Time 0.036 (0.089)	Data 1.05e-04 (3.27e-04)	Tok/s 225595 (242225)	Loss/tok 2.6443 (3.2893)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.100 (0.089)	Data 1.07e-04 (3.25e-04)	Tok/s 252665 (242289)	Loss/tok 3.3284 (3.2897)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.22e-04)	Tok/s 235172 (242272)	Loss/tok 2.9823 (3.2899)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.136 (0.089)	Data 1.08e-04 (3.20e-04)	Tok/s 259056 (242292)	Loss/tok 3.4391 (3.2902)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.173 (0.089)	Data 1.11e-04 (3.18e-04)	Tok/s 259917 (242389)	Loss/tok 3.6149 (3.2924)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.16e-04)	Tok/s 229192 (242365)	Loss/tok 3.1917 (3.2917)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.14e-04)	Tok/s 253930 (242322)	Loss/tok 3.2590 (3.2910)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.12e-04)	Tok/s 255599 (242339)	Loss/tok 3.1738 (3.2902)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.135 (0.089)	Data 1.09e-04 (3.10e-04)	Tok/s 260911 (242366)	Loss/tok 3.3980 (3.2905)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.135 (0.089)	Data 1.08e-04 (3.08e-04)	Tok/s 257499 (242383)	Loss/tok 3.4441 (3.2900)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.06e-04)	Tok/s 229430 (242377)	Loss/tok 3.0109 (3.2890)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.04e-04)	Tok/s 253566 (242405)	Loss/tok 3.3359 (3.2892)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.067 (0.089)	Data 1.07e-04 (3.02e-04)	Tok/s 231523 (242401)	Loss/tok 3.0492 (3.2887)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1070/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.01e-04)	Tok/s 252874 (242353)	Loss/tok 3.3533 (3.2880)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.99e-04)	Tok/s 222630 (242333)	Loss/tok 2.6659 (3.2878)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.97e-04)	Tok/s 251980 (242305)	Loss/tok 3.3597 (3.2877)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.135 (0.089)	Data 1.09e-04 (2.95e-04)	Tok/s 260275 (242303)	Loss/tok 3.4308 (3.2873)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.94e-04)	Tok/s 232751 (242283)	Loss/tok 3.0800 (3.2875)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.136 (0.089)	Data 1.09e-04 (2.92e-04)	Tok/s 259601 (242256)	Loss/tok 3.4856 (3.2870)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.90e-04)	Tok/s 250745 (242320)	Loss/tok 3.2327 (3.2882)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.89e-04)	Tok/s 252778 (242308)	Loss/tok 3.3152 (3.2877)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.067 (0.089)	Data 1.09e-04 (2.87e-04)	Tok/s 234471 (242263)	Loss/tok 3.0745 (3.2867)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.86e-04)	Tok/s 229360 (242220)	Loss/tok 3.0789 (3.2856)	LR 2.875e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1170/1291]	Time 0.100 (0.089)	Data 1.20e-04 (2.84e-04)	Tok/s 249071 (242203)	Loss/tok 3.3621 (3.2858)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.83e-04)	Tok/s 255794 (242206)	Loss/tok 3.2216 (3.2852)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.068 (0.089)	Data 1.09e-04 (2.81e-04)	Tok/s 231283 (242208)	Loss/tok 2.9968 (3.2854)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.036 (0.089)	Data 1.08e-04 (2.80e-04)	Tok/s 220508 (242203)	Loss/tok 2.7001 (3.2857)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.136 (0.089)	Data 1.10e-04 (2.79e-04)	Tok/s 258480 (242196)	Loss/tok 3.4584 (3.2862)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.77e-04)	Tok/s 253309 (242193)	Loss/tok 3.3163 (3.2856)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.76e-04)	Tok/s 251459 (242217)	Loss/tok 3.3360 (3.2858)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.067 (0.089)	Data 1.17e-04 (2.75e-04)	Tok/s 229127 (242219)	Loss/tok 3.1068 (3.2854)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.067 (0.089)	Data 1.06e-04 (2.73e-04)	Tok/s 233128 (242177)	Loss/tok 3.0272 (3.2844)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.72e-04)	Tok/s 256266 (242180)	Loss/tok 3.3052 (3.2842)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.067 (0.089)	Data 1.30e-04 (2.71e-04)	Tok/s 232757 (242197)	Loss/tok 3.0313 (3.2842)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.134 (0.089)	Data 1.19e-04 (2.69e-04)	Tok/s 261050 (242192)	Loss/tok 3.4186 (3.2840)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.136 (0.089)	Data 4.29e-05 (2.71e-04)	Tok/s 253474 (242222)	Loss/tok 3.4438 (3.2844)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592454176370, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454176371, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.368 (0.368)	Decoder iters 96.0 (96.0)	Tok/s 43742 (43742)
0: Running moses detokenizer
0: BLEU(score=22.874957913733077, counts=[35980, 17620, 9840, 5704], totals=[64124, 61121, 58119, 55121], precisions=[56.11003680369284, 28.828062368089526, 16.93077995147886, 10.348143175922063], bp=0.9914286241321908, sys_len=64124, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454178184, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22870000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454178184, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2840	Test BLEU: 22.87
0: Performance: Epoch: 2	Training: 1937460 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592454178185, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592454178185, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592454178185, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1420198369
0: TRAIN [3][0/1291]	Time 0.314 (0.314)	Data 2.14e-01 (2.14e-01)	Tok/s 80011 (80011)	Loss/tok 3.0908 (3.0908)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][10/1291]	Time 0.099 (0.117)	Data 1.13e-04 (1.96e-02)	Tok/s 253992 (231915)	Loss/tok 3.1134 (3.2049)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.067 (0.108)	Data 1.07e-04 (1.03e-02)	Tok/s 228023 (237400)	Loss/tok 2.9515 (3.2229)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.067 (0.100)	Data 1.12e-04 (7.02e-03)	Tok/s 228811 (238983)	Loss/tok 2.9123 (3.1959)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][40/1291]	Time 0.067 (0.105)	Data 1.19e-04 (5.34e-03)	Tok/s 230984 (241745)	Loss/tok 3.0212 (3.2449)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.136 (0.102)	Data 1.14e-04 (4.31e-03)	Tok/s 261031 (241571)	Loss/tok 3.3994 (3.2390)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][60/1291]	Time 0.100 (0.100)	Data 1.07e-04 (3.63e-03)	Tok/s 255644 (241616)	Loss/tok 3.1754 (3.2449)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.067 (0.099)	Data 1.14e-04 (3.13e-03)	Tok/s 230268 (241566)	Loss/tok 3.0310 (3.2495)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.100 (0.099)	Data 1.07e-04 (2.76e-03)	Tok/s 253285 (241710)	Loss/tok 3.2018 (3.2524)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.067 (0.096)	Data 1.08e-04 (2.47e-03)	Tok/s 234690 (241091)	Loss/tok 3.0237 (3.2437)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.067 (0.096)	Data 1.09e-04 (2.23e-03)	Tok/s 230156 (241504)	Loss/tok 3.0521 (3.2433)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.036 (0.095)	Data 1.10e-04 (2.04e-03)	Tok/s 222661 (241097)	Loss/tok 2.6509 (3.2310)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.099 (0.095)	Data 1.10e-04 (1.88e-03)	Tok/s 252641 (241561)	Loss/tok 3.2232 (3.2319)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.036 (0.093)	Data 1.09e-04 (1.75e-03)	Tok/s 219120 (240672)	Loss/tok 2.5745 (3.2198)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.099 (0.093)	Data 1.08e-04 (1.63e-03)	Tok/s 248914 (241089)	Loss/tok 3.1860 (3.2229)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.067 (0.093)	Data 1.08e-04 (1.53e-03)	Tok/s 231461 (241206)	Loss/tok 3.1327 (3.2205)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.067 (0.091)	Data 1.09e-04 (1.44e-03)	Tok/s 231765 (240626)	Loss/tok 3.0486 (3.2141)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.067 (0.092)	Data 1.06e-04 (1.37e-03)	Tok/s 229219 (240918)	Loss/tok 3.0319 (3.2132)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.067 (0.092)	Data 1.07e-04 (1.30e-03)	Tok/s 234275 (240919)	Loss/tok 2.9888 (3.2154)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][190/1291]	Time 0.101 (0.093)	Data 1.09e-04 (1.23e-03)	Tok/s 250333 (241027)	Loss/tok 3.2181 (3.2234)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.101 (0.092)	Data 1.26e-04 (1.18e-03)	Tok/s 245744 (240694)	Loss/tok 3.1795 (3.2183)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.068 (0.092)	Data 1.07e-04 (1.13e-03)	Tok/s 227995 (240684)	Loss/tok 2.9823 (3.2171)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.068 (0.092)	Data 1.08e-04 (1.08e-03)	Tok/s 226654 (240368)	Loss/tok 2.9613 (3.2133)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.069 (0.092)	Data 1.07e-04 (1.04e-03)	Tok/s 220931 (240123)	Loss/tok 2.9604 (3.2101)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.066 (0.091)	Data 1.09e-04 (1.00e-03)	Tok/s 233330 (239930)	Loss/tok 3.0509 (3.2094)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.100 (0.092)	Data 1.04e-04 (9.65e-04)	Tok/s 249863 (240309)	Loss/tok 3.1650 (3.2092)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.136 (0.092)	Data 1.11e-04 (9.32e-04)	Tok/s 258576 (240587)	Loss/tok 3.3760 (3.2114)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.100 (0.093)	Data 1.07e-04 (9.02e-04)	Tok/s 250730 (240905)	Loss/tok 3.2152 (3.2172)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.036 (0.093)	Data 1.10e-04 (8.74e-04)	Tok/s 218570 (240740)	Loss/tok 2.5643 (3.2138)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.137 (0.093)	Data 1.10e-04 (8.48e-04)	Tok/s 254046 (241045)	Loss/tok 3.3505 (3.2175)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.099 (0.093)	Data 1.14e-04 (8.23e-04)	Tok/s 254734 (241070)	Loss/tok 3.2117 (3.2145)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.067 (0.092)	Data 1.08e-04 (8.00e-04)	Tok/s 227717 (240912)	Loss/tok 2.9586 (3.2105)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][320/1291]	Time 0.175 (0.092)	Data 1.33e-04 (7.79e-04)	Tok/s 255720 (240875)	Loss/tok 3.4309 (3.2106)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.036 (0.092)	Data 1.23e-04 (7.59e-04)	Tok/s 216617 (240879)	Loss/tok 2.5663 (3.2091)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][340/1291]	Time 0.175 (0.092)	Data 1.09e-04 (7.40e-04)	Tok/s 255775 (240725)	Loss/tok 3.4539 (3.2066)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.135 (0.091)	Data 1.09e-04 (7.22e-04)	Tok/s 258886 (240643)	Loss/tok 3.3372 (3.2039)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.100 (0.091)	Data 1.11e-04 (7.05e-04)	Tok/s 251878 (240677)	Loss/tok 3.1698 (3.2019)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.067 (0.091)	Data 1.19e-04 (6.89e-04)	Tok/s 235044 (240805)	Loss/tok 2.9602 (3.2010)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.067 (0.091)	Data 1.08e-04 (6.74e-04)	Tok/s 229558 (240890)	Loss/tok 2.9244 (3.1992)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.067 (0.091)	Data 1.05e-04 (6.59e-04)	Tok/s 232804 (240889)	Loss/tok 2.9607 (3.1976)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.067 (0.091)	Data 1.10e-04 (6.45e-04)	Tok/s 229884 (240991)	Loss/tok 2.9548 (3.1976)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.067 (0.091)	Data 1.09e-04 (6.32e-04)	Tok/s 228387 (240922)	Loss/tok 2.9007 (3.1972)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.099 (0.091)	Data 1.08e-04 (6.20e-04)	Tok/s 257066 (241037)	Loss/tok 3.0861 (3.1966)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.100 (0.092)	Data 1.09e-04 (6.08e-04)	Tok/s 251850 (241244)	Loss/tok 3.1149 (3.1959)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.100 (0.091)	Data 1.12e-04 (5.97e-04)	Tok/s 250698 (241235)	Loss/tok 3.0893 (3.1946)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.135 (0.092)	Data 1.09e-04 (5.86e-04)	Tok/s 261937 (241413)	Loss/tok 3.2924 (3.1949)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][460/1291]	Time 0.066 (0.092)	Data 1.08e-04 (5.76e-04)	Tok/s 231057 (241381)	Loss/tok 3.0118 (3.1925)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.067 (0.092)	Data 1.08e-04 (5.66e-04)	Tok/s 229616 (241412)	Loss/tok 3.0110 (3.1931)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.135 (0.092)	Data 1.23e-04 (5.57e-04)	Tok/s 261838 (241435)	Loss/tok 3.2567 (3.1925)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.100 (0.092)	Data 1.28e-04 (5.48e-04)	Tok/s 247872 (241547)	Loss/tok 3.2910 (3.1920)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.067 (0.092)	Data 1.12e-04 (5.39e-04)	Tok/s 231555 (241534)	Loss/tok 2.9247 (3.1921)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.092)	Data 1.06e-04 (5.31e-04)	Tok/s 237936 (241599)	Loss/tok 3.0043 (3.1921)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.035 (0.092)	Data 1.09e-04 (5.23e-04)	Tok/s 218513 (241549)	Loss/tok 2.5293 (3.1915)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.100 (0.092)	Data 1.09e-04 (5.15e-04)	Tok/s 250735 (241566)	Loss/tok 3.1354 (3.1896)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.135 (0.092)	Data 1.09e-04 (5.07e-04)	Tok/s 260608 (241514)	Loss/tok 3.1776 (3.1891)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.100 (0.092)	Data 1.10e-04 (5.00e-04)	Tok/s 254388 (241522)	Loss/tok 3.1284 (3.1891)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.066 (0.092)	Data 1.09e-04 (4.93e-04)	Tok/s 232738 (241580)	Loss/tok 2.9766 (3.1904)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.134 (0.091)	Data 1.07e-04 (4.86e-04)	Tok/s 260495 (241536)	Loss/tok 3.3388 (3.1887)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.136 (0.091)	Data 1.11e-04 (4.80e-04)	Tok/s 257680 (241518)	Loss/tok 3.3785 (3.1881)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][590/1291]	Time 0.068 (0.091)	Data 1.08e-04 (4.74e-04)	Tok/s 226895 (241501)	Loss/tok 3.0021 (3.1869)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.135 (0.092)	Data 1.12e-04 (4.68e-04)	Tok/s 262415 (241699)	Loss/tok 3.2373 (3.1882)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.067 (0.092)	Data 1.06e-04 (4.62e-04)	Tok/s 233811 (241698)	Loss/tok 2.9704 (3.1887)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.135 (0.092)	Data 1.10e-04 (4.56e-04)	Tok/s 260624 (241776)	Loss/tok 3.2596 (3.1882)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.135 (0.092)	Data 1.07e-04 (4.50e-04)	Tok/s 257904 (241797)	Loss/tok 3.4041 (3.1876)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.100 (0.091)	Data 1.10e-04 (4.45e-04)	Tok/s 254194 (241739)	Loss/tok 3.2075 (3.1860)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.099 (0.091)	Data 1.06e-04 (4.40e-04)	Tok/s 252520 (241633)	Loss/tok 3.1196 (3.1839)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.067 (0.091)	Data 1.09e-04 (4.35e-04)	Tok/s 235406 (241653)	Loss/tok 2.9984 (3.1821)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.099 (0.091)	Data 1.09e-04 (4.30e-04)	Tok/s 256813 (241635)	Loss/tok 3.2239 (3.1821)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.067 (0.091)	Data 1.08e-04 (4.25e-04)	Tok/s 229433 (241637)	Loss/tok 2.9994 (3.1815)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.100 (0.091)	Data 1.12e-04 (4.21e-04)	Tok/s 252071 (241595)	Loss/tok 3.1723 (3.1800)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.067 (0.090)	Data 1.18e-04 (4.16e-04)	Tok/s 228920 (241581)	Loss/tok 2.9240 (3.1795)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.067 (0.090)	Data 1.08e-04 (4.12e-04)	Tok/s 229887 (241589)	Loss/tok 3.0193 (3.1788)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][720/1291]	Time 0.135 (0.090)	Data 1.09e-04 (4.08e-04)	Tok/s 259200 (241626)	Loss/tok 3.2341 (3.1780)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.136 (0.091)	Data 1.09e-04 (4.04e-04)	Tok/s 254972 (241710)	Loss/tok 3.3660 (3.1784)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.00e-04)	Tok/s 227953 (241679)	Loss/tok 2.8904 (3.1773)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.100 (0.090)	Data 1.09e-04 (3.96e-04)	Tok/s 252819 (241613)	Loss/tok 3.1146 (3.1757)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.036 (0.090)	Data 1.08e-04 (3.92e-04)	Tok/s 225578 (241547)	Loss/tok 2.5983 (3.1739)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.134 (0.090)	Data 1.07e-04 (3.89e-04)	Tok/s 257953 (241565)	Loss/tok 3.3429 (3.1729)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.067 (0.090)	Data 1.08e-04 (3.85e-04)	Tok/s 233740 (241550)	Loss/tok 2.9379 (3.1722)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.100 (0.089)	Data 1.10e-04 (3.82e-04)	Tok/s 250078 (241455)	Loss/tok 3.1352 (3.1703)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.135 (0.090)	Data 1.10e-04 (3.78e-04)	Tok/s 260345 (241525)	Loss/tok 3.3243 (3.1703)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.75e-04)	Tok/s 252937 (241519)	Loss/tok 3.0674 (3.1693)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.72e-04)	Tok/s 230138 (241521)	Loss/tok 3.0151 (3.1686)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.100 (0.089)	Data 1.08e-04 (3.69e-04)	Tok/s 251293 (241531)	Loss/tok 3.1895 (3.1676)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.066 (0.089)	Data 1.27e-04 (3.66e-04)	Tok/s 233971 (241455)	Loss/tok 2.9574 (3.1660)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][850/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.63e-04)	Tok/s 231769 (241485)	Loss/tok 2.9327 (3.1658)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.100 (0.089)	Data 1.19e-04 (3.60e-04)	Tok/s 252601 (241533)	Loss/tok 3.0602 (3.1652)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.174 (0.089)	Data 1.08e-04 (3.57e-04)	Tok/s 256995 (241577)	Loss/tok 3.4023 (3.1654)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.135 (0.089)	Data 1.11e-04 (3.54e-04)	Tok/s 257096 (241617)	Loss/tok 3.2662 (3.1653)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.036 (0.089)	Data 1.07e-04 (3.51e-04)	Tok/s 221590 (241636)	Loss/tok 2.6302 (3.1649)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.48e-04)	Tok/s 230638 (241658)	Loss/tok 2.9643 (3.1648)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.036 (0.089)	Data 1.31e-04 (3.46e-04)	Tok/s 219747 (241631)	Loss/tok 2.5295 (3.1637)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.43e-04)	Tok/s 235237 (241615)	Loss/tok 2.9437 (3.1631)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.41e-04)	Tok/s 251265 (241612)	Loss/tok 3.0986 (3.1625)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.100 (0.089)	Data 1.08e-04 (3.38e-04)	Tok/s 256435 (241717)	Loss/tok 3.1574 (3.1621)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.36e-04)	Tok/s 230541 (241694)	Loss/tok 3.0105 (3.1617)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.066 (0.089)	Data 1.22e-04 (3.34e-04)	Tok/s 233049 (241717)	Loss/tok 2.9599 (3.1624)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.137 (0.089)	Data 1.06e-04 (3.31e-04)	Tok/s 255371 (241755)	Loss/tok 3.1647 (3.1621)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][980/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.29e-04)	Tok/s 232412 (241699)	Loss/tok 3.0081 (3.1609)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.27e-04)	Tok/s 233066 (241634)	Loss/tok 2.9403 (3.1598)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.25e-04)	Tok/s 236276 (241666)	Loss/tok 2.9813 (3.1593)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.067 (0.089)	Data 1.16e-04 (3.23e-04)	Tok/s 233476 (241649)	Loss/tok 2.9877 (3.1588)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.21e-04)	Tok/s 254102 (241612)	Loss/tok 3.1203 (3.1582)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.036 (0.089)	Data 1.09e-04 (3.18e-04)	Tok/s 221068 (241612)	Loss/tok 2.5388 (3.1588)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.16e-04)	Tok/s 229360 (241603)	Loss/tok 2.9156 (3.1589)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.15e-04)	Tok/s 255742 (241589)	Loss/tok 3.2419 (3.1588)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.13e-04)	Tok/s 231226 (241586)	Loss/tok 2.9418 (3.1585)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.11e-04)	Tok/s 229237 (241603)	Loss/tok 2.9538 (3.1579)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.100 (0.089)	Data 1.08e-04 (3.09e-04)	Tok/s 254074 (241596)	Loss/tok 3.0556 (3.1570)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.07e-04)	Tok/s 234025 (241596)	Loss/tok 2.9434 (3.1564)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1100/1291]	Time 0.067 (0.089)	Data 1.07e-04 (3.05e-04)	Tok/s 227106 (241576)	Loss/tok 2.8865 (3.1556)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.03e-04)	Tok/s 253685 (241609)	Loss/tok 3.1646 (3.1552)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.134 (0.089)	Data 1.08e-04 (3.02e-04)	Tok/s 261461 (241691)	Loss/tok 3.3392 (3.1553)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.00e-04)	Tok/s 233335 (241678)	Loss/tok 2.9741 (3.1544)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.98e-04)	Tok/s 251353 (241674)	Loss/tok 3.1217 (3.1541)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.97e-04)	Tok/s 233781 (241691)	Loss/tok 3.0023 (3.1544)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.95e-04)	Tok/s 251094 (241731)	Loss/tok 3.1750 (3.1545)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.100 (0.089)	Data 1.08e-04 (2.94e-04)	Tok/s 252333 (241744)	Loss/tok 3.1372 (3.1540)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.100 (0.089)	Data 1.06e-04 (2.92e-04)	Tok/s 252172 (241767)	Loss/tok 3.1119 (3.1536)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.175 (0.089)	Data 1.06e-04 (2.90e-04)	Tok/s 254265 (241791)	Loss/tok 3.4764 (3.1540)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.135 (0.089)	Data 1.12e-04 (2.89e-04)	Tok/s 257032 (241873)	Loss/tok 3.2789 (3.1544)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.036 (0.089)	Data 1.10e-04 (2.87e-04)	Tok/s 221107 (241815)	Loss/tok 2.5179 (3.1532)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.173 (0.089)	Data 1.06e-04 (2.86e-04)	Tok/s 255676 (241795)	Loss/tok 3.5944 (3.1532)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1230/1291]	Time 0.036 (0.089)	Data 1.08e-04 (2.85e-04)	Tok/s 221105 (241717)	Loss/tok 2.5209 (3.1519)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.83e-04)	Tok/s 234453 (241752)	Loss/tok 2.9423 (3.1516)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.100 (0.089)	Data 1.06e-04 (2.82e-04)	Tok/s 251523 (241755)	Loss/tok 3.1235 (3.1518)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.80e-04)	Tok/s 230823 (241757)	Loss/tok 2.9297 (3.1516)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.176 (0.089)	Data 1.09e-04 (2.79e-04)	Tok/s 253959 (241787)	Loss/tok 3.4791 (3.1520)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.100 (0.089)	Data 1.09e-04 (2.78e-04)	Tok/s 254957 (241829)	Loss/tok 3.1006 (3.1518)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.100 (0.089)	Data 4.32e-05 (2.79e-04)	Tok/s 252258 (241815)	Loss/tok 3.0944 (3.1513)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592454293957, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592454293957, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.409 (0.409)	Decoder iters 105.0 (105.0)	Tok/s 39922 (39922)
0: Running moses detokenizer
0: BLEU(score=24.12066028960049, counts=[37045, 18565, 10530, 6264], totals=[65101, 62098, 59095, 56096], precisions=[56.90388780510284, 29.89629295629489, 17.818766393095864, 11.16657159155733], bp=1.0, sys_len=65101, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592454295820, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2412, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592454295820, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1502	Test BLEU: 24.12
0: Performance: Epoch: 3	Training: 1934523 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592454295820, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592454295820, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-17 09:25:01 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:39 PM
ENDING TIMING RUN AT 2020-06-17 09:25:01 PM
RESULT,RNN_TRANSLATOR,,502,nvidia,2020-06-17 09:16:39 PM
ENDING TIMING RUN AT 2020-06-17 09:25:02 PM
RESULT,RNN_TRANSLATOR,,503,nvidia,2020-06-17 09:16:39 PM
ENDING TIMING RUN AT 2020-06-17 09:25:02 PM
ENDING TIMING RUN AT 2020-06-17 09:25:02 PM
ENDING TIMING RUN AT 2020-06-17 09:25:02 PM
RESULT,RNN_TRANSLATOR,,503,nvidia,2020-06-17 09:16:39 PM
RESULT,RNN_TRANSLATOR,,503,nvidia,2020-06-17 09:16:39 PM
RESULT,RNN_TRANSLATOR,,503,nvidia,2020-06-17 09:16:39 PM
ENDING TIMING RUN AT 2020-06-17 09:25:02 PM
RESULT,RNN_TRANSLATOR,,503,nvidia,2020-06-17 09:16:39 PM
ENDING TIMING RUN AT 2020-06-17 09:25:03 PM
RESULT,RNN_TRANSLATOR,,504,nvidia,2020-06-17 09:16:39 PM
