+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592446398177, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592446398216, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592446398216, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592446398216, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592446398216, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0190
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592446404164, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/13842439/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=384
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=128
+ LR=2.875e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=384
+ REMAIN_STEPS=4054
+ TEST_BATCH_SIZE=128
+ DECAY_INTERVAL=506
+ WARMUP_STEPS=200
+ TARGET=24.0
+ REMAIN_STEPS=4054
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=506
+ NUMEPOCHS=8
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-17 07:13:26 PM
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=384
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ '[' -n 3 ']'
+ MATH=fp16
+ '[' 8 -gt 1 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ '[' -n 6 ']'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 384 --test-batch-size 128 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592446408360, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408515, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408590, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408599, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408632, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408646, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592446408647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=384, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 148521887
:::MLLOG {"namespace": "", "time_ms": 1592446416715, "event_type": "POINT_IN_TIME", "key": "seed", "value": 148521887, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 651016246
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592446430872, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592446430873, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592446430873, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592446430873, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592446430873, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592446432510, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592446432511, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592446432511, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592446432861, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592446432862, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592446432862, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592446432862, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446432863, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 667714900
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.277 (0.277)	Data 2.32e-01 (2.32e-01)	Tok/s 27963 (27963)	Loss/tok 10.5774 (10.5774)	LR 2.942e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/1291]	Time 0.098 (0.099)	Data 1.03e-04 (2.18e-02)	Tok/s 260798 (218291)	Loss/tok 9.7090 (10.0275)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.098 (0.095)	Data 1.12e-04 (1.15e-02)	Tok/s 256649 (233824)	Loss/tok 9.2138 (9.6921)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.098 (0.097)	Data 1.15e-04 (7.81e-03)	Tok/s 257952 (238175)	Loss/tok 8.9219 (9.4660)	LR 5.870e-05
0: TRAIN [0][40/1291]	Time 0.098 (0.097)	Data 1.11e-04 (5.93e-03)	Tok/s 256473 (241891)	Loss/tok 8.6434 (9.2825)	LR 7.390e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.097 (0.094)	Data 1.09e-04 (4.79e-03)	Tok/s 256729 (242605)	Loss/tok 8.4026 (9.1639)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.066 (0.090)	Data 1.15e-04 (4.02e-03)	Tok/s 236917 (242066)	Loss/tok 8.3939 (9.0500)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.066 (0.088)	Data 1.10e-04 (3.47e-03)	Tok/s 237837 (242150)	Loss/tok 7.9694 (8.9486)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.098 (0.088)	Data 1.11e-04 (3.06e-03)	Tok/s 259665 (242732)	Loss/tok 8.1135 (8.8361)	LR 1.773e-04
0: TRAIN [0][90/1291]	Time 0.098 (0.086)	Data 1.14e-04 (2.73e-03)	Tok/s 256585 (242004)	Loss/tok 8.0413 (8.7522)	LR 2.232e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][100/1291]	Time 0.066 (0.085)	Data 1.11e-04 (2.47e-03)	Tok/s 232532 (241990)	Loss/tok 7.8538 (8.6895)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.098 (0.084)	Data 1.09e-04 (2.26e-03)	Tok/s 260345 (242405)	Loss/tok 8.2065 (8.6274)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.066 (0.085)	Data 1.13e-04 (2.08e-03)	Tok/s 235852 (242971)	Loss/tok 7.7110 (8.5605)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.098 (0.085)	Data 1.10e-04 (1.93e-03)	Tok/s 256565 (243235)	Loss/tok 7.8672 (8.5096)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.134 (0.085)	Data 1.06e-04 (1.80e-03)	Tok/s 259601 (243435)	Loss/tok 7.9391 (8.4572)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.098 (0.086)	Data 1.08e-04 (1.69e-03)	Tok/s 258132 (244025)	Loss/tok 7.6338 (8.4052)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.171 (0.086)	Data 1.03e-04 (1.59e-03)	Tok/s 261977 (243731)	Loss/tok 7.7993 (8.3552)	LR 1.093e-03
0: TRAIN [0][170/1291]	Time 0.066 (0.086)	Data 1.04e-04 (1.51e-03)	Tok/s 233195 (243837)	Loss/tok 7.1672 (8.3003)	LR 1.376e-03
0: TRAIN [0][180/1291]	Time 0.066 (0.087)	Data 1.09e-04 (1.43e-03)	Tok/s 232240 (244054)	Loss/tok 6.9554 (8.2373)	LR 1.732e-03
0: TRAIN [0][190/1291]	Time 0.098 (0.086)	Data 1.02e-04 (1.36e-03)	Tok/s 254964 (243904)	Loss/tok 7.1449 (8.1806)	LR 2.181e-03
0: TRAIN [0][200/1291]	Time 0.036 (0.086)	Data 1.09e-04 (1.30e-03)	Tok/s 225706 (243994)	Loss/tok 5.9133 (8.1200)	LR 2.746e-03
0: TRAIN [0][210/1291]	Time 0.134 (0.087)	Data 1.05e-04 (1.24e-03)	Tok/s 260760 (244465)	Loss/tok 6.9410 (8.0468)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.098 (0.088)	Data 1.08e-04 (1.19e-03)	Tok/s 256296 (244851)	Loss/tok 6.5787 (7.9747)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][230/1291]	Time 0.098 (0.088)	Data 1.07e-04 (1.14e-03)	Tok/s 257760 (244753)	Loss/tok 6.3795 (7.9077)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.066 (0.089)	Data 1.04e-04 (1.10e-03)	Tok/s 235027 (244856)	Loss/tok 6.0634 (7.8321)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.098 (0.088)	Data 1.04e-04 (1.06e-03)	Tok/s 254533 (244775)	Loss/tok 6.0040 (7.7667)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.066 (0.088)	Data 1.05e-04 (1.02e-03)	Tok/s 233811 (244638)	Loss/tok 5.7712 (7.7030)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.066 (0.088)	Data 1.05e-04 (9.89e-04)	Tok/s 232217 (244694)	Loss/tok 5.5449 (7.6349)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.134 (0.089)	Data 1.08e-04 (9.58e-04)	Tok/s 257800 (244751)	Loss/tok 5.8599 (7.5616)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.066 (0.089)	Data 1.07e-04 (9.29e-04)	Tok/s 235955 (244809)	Loss/tok 5.3479 (7.4939)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.066 (0.088)	Data 1.05e-04 (9.02e-04)	Tok/s 234275 (244540)	Loss/tok 5.2509 (7.4402)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.066 (0.088)	Data 1.03e-04 (8.77e-04)	Tok/s 236291 (244466)	Loss/tok 5.1309 (7.3802)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.066 (0.088)	Data 1.07e-04 (8.53e-04)	Tok/s 232764 (244486)	Loss/tok 4.9755 (7.3148)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.066 (0.088)	Data 1.03e-04 (8.30e-04)	Tok/s 236739 (244689)	Loss/tok 4.8411 (7.2420)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.172 (0.089)	Data 1.08e-04 (8.09e-04)	Tok/s 259914 (244857)	Loss/tok 5.3803 (7.1703)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.099 (0.089)	Data 1.03e-04 (7.89e-04)	Tok/s 257014 (245026)	Loss/tok 5.0139 (7.1018)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][360/1291]	Time 0.099 (0.088)	Data 1.07e-04 (7.70e-04)	Tok/s 256007 (244736)	Loss/tok 4.8895 (7.0532)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.066 (0.088)	Data 1.05e-04 (7.52e-04)	Tok/s 236752 (244737)	Loss/tok 4.4434 (6.9899)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.066 (0.088)	Data 1.08e-04 (7.35e-04)	Tok/s 234260 (244694)	Loss/tok 4.3429 (6.9312)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.066 (0.088)	Data 1.22e-04 (7.19e-04)	Tok/s 234511 (244671)	Loss/tok 4.1711 (6.8718)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.066 (0.088)	Data 1.05e-04 (7.04e-04)	Tok/s 234842 (244589)	Loss/tok 4.3013 (6.8143)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.066 (0.088)	Data 1.07e-04 (6.89e-04)	Tok/s 238535 (244611)	Loss/tok 4.1683 (6.7592)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.066 (0.088)	Data 1.08e-04 (6.76e-04)	Tok/s 232260 (244508)	Loss/tok 4.1088 (6.7072)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.035 (0.089)	Data 1.15e-04 (6.62e-04)	Tok/s 223792 (244637)	Loss/tok 3.5652 (6.6443)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.134 (0.089)	Data 1.07e-04 (6.50e-04)	Tok/s 258182 (244617)	Loss/tok 4.5934 (6.5950)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.035 (0.089)	Data 1.06e-04 (6.38e-04)	Tok/s 226256 (244609)	Loss/tok 3.4047 (6.5440)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.066 (0.089)	Data 1.05e-04 (6.26e-04)	Tok/s 227604 (244630)	Loss/tok 3.9647 (6.4930)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.066 (0.089)	Data 1.08e-04 (6.15e-04)	Tok/s 235074 (244513)	Loss/tok 3.9278 (6.4512)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.135 (0.089)	Data 1.08e-04 (6.05e-04)	Tok/s 257910 (244681)	Loss/tok 4.4095 (6.3982)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][490/1291]	Time 0.099 (0.089)	Data 1.04e-04 (5.95e-04)	Tok/s 256241 (244700)	Loss/tok 4.2824 (6.3529)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.066 (0.089)	Data 1.09e-04 (5.85e-04)	Tok/s 232962 (244770)	Loss/tok 3.8540 (6.3048)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.099 (0.089)	Data 1.09e-04 (5.75e-04)	Tok/s 257176 (244709)	Loss/tok 4.2111 (6.2647)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.066 (0.089)	Data 1.07e-04 (5.66e-04)	Tok/s 233877 (244647)	Loss/tok 3.8162 (6.2261)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][530/1291]	Time 0.172 (0.089)	Data 1.08e-04 (5.58e-04)	Tok/s 262514 (244737)	Loss/tok 4.5027 (6.1819)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.134 (0.089)	Data 1.06e-04 (5.50e-04)	Tok/s 263286 (244797)	Loss/tok 4.2706 (6.1415)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.099 (0.089)	Data 1.05e-04 (5.42e-04)	Tok/s 255461 (244715)	Loss/tok 4.0540 (6.1064)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.099 (0.089)	Data 1.04e-04 (5.34e-04)	Tok/s 254373 (244734)	Loss/tok 4.1439 (6.0682)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.066 (0.090)	Data 1.04e-04 (5.26e-04)	Tok/s 233419 (244788)	Loss/tok 3.7512 (6.0297)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.099 (0.090)	Data 1.06e-04 (5.19e-04)	Tok/s 255490 (244882)	Loss/tok 4.0108 (5.9943)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.066 (0.089)	Data 1.06e-04 (5.12e-04)	Tok/s 230724 (244728)	Loss/tok 3.7427 (5.9678)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.066 (0.089)	Data 1.08e-04 (5.05e-04)	Tok/s 233674 (244774)	Loss/tok 3.6442 (5.9337)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.099 (0.089)	Data 1.06e-04 (4.99e-04)	Tok/s 252938 (244823)	Loss/tok 3.9191 (5.9003)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.099 (0.089)	Data 1.06e-04 (4.93e-04)	Tok/s 253309 (244754)	Loss/tok 3.9571 (5.8719)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.066 (0.089)	Data 1.05e-04 (4.87e-04)	Tok/s 235337 (244758)	Loss/tok 3.5789 (5.8411)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.099 (0.089)	Data 1.06e-04 (4.81e-04)	Tok/s 254579 (244758)	Loss/tok 3.8736 (5.8113)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1291]	Time 0.135 (0.089)	Data 1.09e-04 (4.75e-04)	Tok/s 261958 (244693)	Loss/tok 4.1854 (5.7846)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.135 (0.089)	Data 1.06e-04 (4.70e-04)	Tok/s 257596 (244562)	Loss/tok 4.1719 (5.7605)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.066 (0.089)	Data 1.05e-04 (4.64e-04)	Tok/s 233741 (244549)	Loss/tok 3.7023 (5.7339)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.099 (0.088)	Data 1.02e-04 (4.59e-04)	Tok/s 257642 (244492)	Loss/tok 3.8425 (5.7094)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.134 (0.088)	Data 1.09e-04 (4.54e-04)	Tok/s 261685 (244451)	Loss/tok 3.9854 (5.6838)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.099 (0.088)	Data 1.10e-04 (4.49e-04)	Tok/s 255722 (244443)	Loss/tok 3.9975 (5.6584)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.067 (0.088)	Data 1.05e-04 (4.44e-04)	Tok/s 231411 (244368)	Loss/tok 3.5723 (5.6355)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.036 (0.088)	Data 1.10e-04 (4.39e-04)	Tok/s 226675 (244382)	Loss/tok 3.1271 (5.6109)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.036 (0.088)	Data 1.10e-04 (4.35e-04)	Tok/s 223108 (244420)	Loss/tok 2.9587 (5.5836)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.066 (0.088)	Data 1.08e-04 (4.30e-04)	Tok/s 232852 (244457)	Loss/tok 3.4571 (5.5587)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.135 (0.088)	Data 1.05e-04 (4.26e-04)	Tok/s 257174 (244405)	Loss/tok 4.0907 (5.5381)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.066 (0.088)	Data 1.05e-04 (4.22e-04)	Tok/s 234009 (244311)	Loss/tok 3.5303 (5.5195)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.099 (0.088)	Data 1.06e-04 (4.18e-04)	Tok/s 252129 (244292)	Loss/tok 3.7646 (5.4987)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1291]	Time 0.067 (0.088)	Data 1.05e-04 (4.14e-04)	Tok/s 229799 (244214)	Loss/tok 3.5389 (5.4795)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.035 (0.088)	Data 1.08e-04 (4.10e-04)	Tok/s 224041 (244209)	Loss/tok 3.0047 (5.4587)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.066 (0.088)	Data 1.06e-04 (4.06e-04)	Tok/s 235373 (244228)	Loss/tok 3.5289 (5.4369)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.134 (0.088)	Data 1.16e-04 (4.02e-04)	Tok/s 256284 (244204)	Loss/tok 4.1395 (5.4162)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.099 (0.088)	Data 1.04e-04 (3.99e-04)	Tok/s 255884 (244230)	Loss/tok 3.7686 (5.3964)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.099 (0.088)	Data 1.05e-04 (3.95e-04)	Tok/s 253483 (244151)	Loss/tok 3.7934 (5.3786)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.099 (0.087)	Data 1.10e-04 (3.92e-04)	Tok/s 254981 (244129)	Loss/tok 3.7081 (5.3602)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][850/1291]	Time 0.098 (0.088)	Data 1.04e-04 (3.89e-04)	Tok/s 258146 (244193)	Loss/tok 3.7297 (5.3387)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.035 (0.088)	Data 1.07e-04 (3.85e-04)	Tok/s 224145 (244170)	Loss/tok 2.9292 (5.3213)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.066 (0.087)	Data 1.06e-04 (3.82e-04)	Tok/s 236696 (244139)	Loss/tok 3.3620 (5.3044)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.099 (0.087)	Data 1.05e-04 (3.79e-04)	Tok/s 254798 (244176)	Loss/tok 3.7282 (5.2854)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.066 (0.088)	Data 1.07e-04 (3.76e-04)	Tok/s 232730 (244201)	Loss/tok 3.6070 (5.2677)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.134 (0.087)	Data 1.08e-04 (3.73e-04)	Tok/s 265959 (244224)	Loss/tok 3.9885 (5.2504)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.134 (0.088)	Data 1.05e-04 (3.70e-04)	Tok/s 260750 (244201)	Loss/tok 4.0906 (5.2341)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.066 (0.088)	Data 1.24e-04 (3.67e-04)	Tok/s 232885 (244231)	Loss/tok 3.5081 (5.2157)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.036 (0.088)	Data 1.07e-04 (3.65e-04)	Tok/s 220579 (244302)	Loss/tok 2.9809 (5.1976)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.099 (0.088)	Data 1.07e-04 (3.62e-04)	Tok/s 256561 (244268)	Loss/tok 3.6147 (5.1828)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.066 (0.088)	Data 1.06e-04 (3.59e-04)	Tok/s 235010 (244239)	Loss/tok 3.4247 (5.1680)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.066 (0.088)	Data 1.07e-04 (3.56e-04)	Tok/s 232784 (244145)	Loss/tok 3.4081 (5.1553)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][970/1291]	Time 0.134 (0.088)	Data 1.07e-04 (3.54e-04)	Tok/s 261674 (244157)	Loss/tok 3.8207 (5.1392)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.099 (0.087)	Data 1.04e-04 (3.51e-04)	Tok/s 253920 (244069)	Loss/tok 3.8090 (5.1263)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.036 (0.088)	Data 1.08e-04 (3.49e-04)	Tok/s 217115 (244053)	Loss/tok 3.0412 (5.1118)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.066 (0.087)	Data 1.04e-04 (3.47e-04)	Tok/s 231086 (244005)	Loss/tok 3.3822 (5.0989)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.066 (0.087)	Data 1.04e-04 (3.44e-04)	Tok/s 232380 (243992)	Loss/tok 3.3357 (5.0850)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.42e-04)	Tok/s 252408 (244056)	Loss/tok 3.6167 (5.0695)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.173 (0.087)	Data 1.05e-04 (3.40e-04)	Tok/s 260566 (244047)	Loss/tok 4.1562 (5.0562)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.066 (0.088)	Data 1.09e-04 (3.37e-04)	Tok/s 233666 (244077)	Loss/tok 3.3235 (5.0416)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.066 (0.088)	Data 1.07e-04 (3.35e-04)	Tok/s 234013 (244125)	Loss/tok 3.3329 (5.0269)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.099 (0.088)	Data 1.08e-04 (3.33e-04)	Tok/s 256164 (244194)	Loss/tok 3.6588 (5.0123)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.099 (0.088)	Data 1.08e-04 (3.31e-04)	Tok/s 258714 (244201)	Loss/tok 3.5479 (4.9992)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.099 (0.088)	Data 1.06e-04 (3.29e-04)	Tok/s 254637 (244230)	Loss/tok 3.6662 (4.9860)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.035 (0.088)	Data 1.10e-04 (3.27e-04)	Tok/s 220327 (244296)	Loss/tok 2.8791 (4.9712)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1100/1291]	Time 0.134 (0.088)	Data 1.08e-04 (3.25e-04)	Tok/s 261000 (244305)	Loss/tok 3.8591 (4.9588)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1110/1291]	Time 0.098 (0.088)	Data 1.07e-04 (3.23e-04)	Tok/s 257447 (244253)	Loss/tok 3.6111 (4.9482)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.066 (0.088)	Data 1.08e-04 (3.21e-04)	Tok/s 233897 (244284)	Loss/tok 3.4036 (4.9352)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.066 (0.088)	Data 1.05e-04 (3.19e-04)	Tok/s 237924 (244329)	Loss/tok 3.3408 (4.9229)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.099 (0.088)	Data 1.06e-04 (3.17e-04)	Tok/s 256726 (244348)	Loss/tok 3.6099 (4.9111)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.16e-04)	Tok/s 257211 (244396)	Loss/tok 3.6102 (4.8983)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.14e-04)	Tok/s 260694 (244366)	Loss/tok 3.5500 (4.8877)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.035 (0.088)	Data 1.09e-04 (3.12e-04)	Tok/s 220622 (244355)	Loss/tok 2.9479 (4.8771)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.066 (0.088)	Data 1.06e-04 (3.10e-04)	Tok/s 232828 (244310)	Loss/tok 3.4005 (4.8674)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.036 (0.088)	Data 1.06e-04 (3.09e-04)	Tok/s 223345 (244270)	Loss/tok 2.9266 (4.8572)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.134 (0.088)	Data 1.10e-04 (3.07e-04)	Tok/s 261901 (244247)	Loss/tok 3.7704 (4.8470)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.066 (0.088)	Data 1.05e-04 (3.05e-04)	Tok/s 236460 (244276)	Loss/tok 3.4181 (4.8361)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.173 (0.088)	Data 1.09e-04 (3.04e-04)	Tok/s 256422 (244251)	Loss/tok 3.9634 (4.8261)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.134 (0.088)	Data 1.09e-04 (3.02e-04)	Tok/s 260982 (244283)	Loss/tok 3.7324 (4.8151)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1240/1291]	Time 0.067 (0.088)	Data 1.08e-04 (3.00e-04)	Tok/s 231196 (244302)	Loss/tok 3.3395 (4.8042)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.066 (0.089)	Data 1.06e-04 (2.99e-04)	Tok/s 234653 (244323)	Loss/tok 3.3443 (4.7938)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.066 (0.088)	Data 1.08e-04 (2.97e-04)	Tok/s 232007 (244271)	Loss/tok 3.3544 (4.7849)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.036 (0.088)	Data 1.09e-04 (2.96e-04)	Tok/s 227161 (244234)	Loss/tok 2.8627 (4.7761)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.066 (0.088)	Data 1.05e-04 (2.94e-04)	Tok/s 234369 (244148)	Loss/tok 3.3305 (4.7684)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.066 (0.088)	Data 4.29e-05 (2.96e-04)	Tok/s 232366 (244165)	Loss/tok 3.3104 (4.7584)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592446547387, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446547387, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.491 (0.491)	Decoder iters 149.0 (149.0)	Tok/s 32952 (32952)
0: Running moses detokenizer
0: BLEU(score=19.88611839654437, counts=[34386, 15718, 8291, 4546], totals=[64636, 61633, 58630, 55633], precisions=[53.199455411844795, 25.50257167426541, 14.141224629029507, 8.171409055776248], bp=0.9993813412719036, sys_len=64636, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446549321, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1989, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446549321, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7586	Test BLEU: 19.89
0: Performance: Epoch: 0	Training: 1953631 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592446549321, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446549321, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446549321, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 912758371
0: TRAIN [1][0/1291]	Time 0.303 (0.303)	Data 2.03e-01 (2.03e-01)	Tok/s 82509 (82509)	Loss/tok 3.5684 (3.5684)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.134 (0.101)	Data 1.16e-04 (1.85e-02)	Tok/s 260796 (226768)	Loss/tok 3.7806 (3.4517)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][20/1291]	Time 0.173 (0.101)	Data 1.16e-04 (9.76e-03)	Tok/s 256304 (235586)	Loss/tok 4.0180 (3.5396)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.136 (0.103)	Data 1.15e-04 (6.65e-03)	Tok/s 259647 (241117)	Loss/tok 3.6988 (3.5556)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.066 (0.104)	Data 1.13e-04 (5.06e-03)	Tok/s 231053 (243308)	Loss/tok 3.3372 (3.5769)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.067 (0.098)	Data 1.22e-04 (4.09e-03)	Tok/s 235718 (242067)	Loss/tok 3.3172 (3.5497)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.134 (0.099)	Data 1.11e-04 (3.44e-03)	Tok/s 258977 (243637)	Loss/tok 3.6811 (3.5616)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.035 (0.096)	Data 1.25e-04 (2.97e-03)	Tok/s 227679 (242550)	Loss/tok 2.7573 (3.5444)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.066 (0.093)	Data 1.08e-04 (2.62e-03)	Tok/s 232996 (242158)	Loss/tok 3.2413 (3.5267)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.067 (0.093)	Data 1.21e-04 (2.34e-03)	Tok/s 234550 (242473)	Loss/tok 3.2484 (3.5264)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.066 (0.094)	Data 1.36e-04 (2.12e-03)	Tok/s 236552 (242968)	Loss/tok 3.2275 (3.5291)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.176 (0.094)	Data 1.12e-04 (1.94e-03)	Tok/s 255712 (243513)	Loss/tok 3.8056 (3.5266)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.036 (0.092)	Data 1.20e-04 (1.79e-03)	Tok/s 219745 (242903)	Loss/tok 2.8485 (3.5169)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.067 (0.093)	Data 1.12e-04 (1.66e-03)	Tok/s 231836 (243313)	Loss/tok 3.3098 (3.5289)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][140/1291]	Time 0.036 (0.092)	Data 1.14e-04 (1.55e-03)	Tok/s 222621 (242787)	Loss/tok 2.7706 (3.5199)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][150/1291]	Time 0.134 (0.092)	Data 1.14e-04 (1.46e-03)	Tok/s 262965 (242777)	Loss/tok 3.5625 (3.5143)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.067 (0.091)	Data 1.16e-04 (1.37e-03)	Tok/s 231056 (242778)	Loss/tok 3.2511 (3.5101)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.099 (0.091)	Data 1.15e-04 (1.30e-03)	Tok/s 258575 (242948)	Loss/tok 3.4614 (3.5086)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.099 (0.092)	Data 1.12e-04 (1.23e-03)	Tok/s 256651 (243279)	Loss/tok 3.5001 (3.5079)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.066 (0.090)	Data 1.11e-04 (1.18e-03)	Tok/s 232169 (242756)	Loss/tok 3.1744 (3.5029)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.067 (0.091)	Data 1.12e-04 (1.12e-03)	Tok/s 234899 (243002)	Loss/tok 3.3352 (3.5096)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.067 (0.090)	Data 1.13e-04 (1.08e-03)	Tok/s 227653 (242720)	Loss/tok 3.2385 (3.5028)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.066 (0.090)	Data 1.12e-04 (1.03e-03)	Tok/s 226691 (242724)	Loss/tok 3.1861 (3.5010)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.067 (0.090)	Data 1.18e-04 (9.93e-04)	Tok/s 230175 (242772)	Loss/tok 3.2835 (3.4999)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.134 (0.091)	Data 1.13e-04 (9.56e-04)	Tok/s 262599 (242978)	Loss/tok 3.7377 (3.5057)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.067 (0.090)	Data 1.11e-04 (9.23e-04)	Tok/s 232385 (242853)	Loss/tok 3.2291 (3.5028)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.100 (0.090)	Data 1.29e-04 (8.92e-04)	Tok/s 256638 (242932)	Loss/tok 3.3505 (3.5006)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.066 (0.090)	Data 1.10e-04 (8.63e-04)	Tok/s 234651 (242885)	Loss/tok 3.2244 (3.5021)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][280/1291]	Time 0.066 (0.089)	Data 1.18e-04 (8.36e-04)	Tok/s 227446 (242699)	Loss/tok 3.3004 (3.4978)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.135 (0.090)	Data 1.12e-04 (8.11e-04)	Tok/s 259331 (242797)	Loss/tok 3.6959 (3.5001)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.067 (0.090)	Data 1.12e-04 (7.88e-04)	Tok/s 230461 (242704)	Loss/tok 3.1686 (3.4974)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.173 (0.090)	Data 1.13e-04 (7.67e-04)	Tok/s 260845 (242864)	Loss/tok 3.8093 (3.5003)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.066 (0.090)	Data 1.13e-04 (7.46e-04)	Tok/s 229770 (242892)	Loss/tok 3.2843 (3.4994)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.099 (0.090)	Data 1.16e-04 (7.27e-04)	Tok/s 253912 (242870)	Loss/tok 3.4618 (3.4992)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.067 (0.089)	Data 1.11e-04 (7.09e-04)	Tok/s 234054 (242706)	Loss/tok 3.2541 (3.4945)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.036 (0.089)	Data 1.15e-04 (6.92e-04)	Tok/s 223207 (242588)	Loss/tok 2.7447 (3.4917)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.035 (0.088)	Data 1.13e-04 (6.76e-04)	Tok/s 223627 (242483)	Loss/tok 2.6877 (3.4889)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.066 (0.089)	Data 1.14e-04 (6.61e-04)	Tok/s 232180 (242416)	Loss/tok 3.2582 (3.4896)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.066 (0.088)	Data 1.13e-04 (6.47e-04)	Tok/s 229331 (242439)	Loss/tok 3.1394 (3.4878)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.136 (0.089)	Data 1.11e-04 (6.33e-04)	Tok/s 255586 (242556)	Loss/tok 3.7466 (3.4917)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.173 (0.089)	Data 1.13e-04 (6.20e-04)	Tok/s 256652 (242704)	Loss/tok 3.7740 (3.4923)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][410/1291]	Time 0.136 (0.089)	Data 1.16e-04 (6.08e-04)	Tok/s 253944 (242656)	Loss/tok 3.7643 (3.4903)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.099 (0.089)	Data 1.12e-04 (5.96e-04)	Tok/s 253811 (242653)	Loss/tok 3.4913 (3.4881)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.135 (0.089)	Data 1.38e-04 (5.85e-04)	Tok/s 260695 (242701)	Loss/tok 3.5746 (3.4866)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.066 (0.089)	Data 1.10e-04 (5.74e-04)	Tok/s 232003 (242723)	Loss/tok 3.2774 (3.4864)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][450/1291]	Time 0.099 (0.089)	Data 1.13e-04 (5.64e-04)	Tok/s 251948 (242713)	Loss/tok 3.5022 (3.4874)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.067 (0.089)	Data 1.18e-04 (5.54e-04)	Tok/s 238056 (242681)	Loss/tok 3.1757 (3.4870)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.099 (0.089)	Data 1.11e-04 (5.45e-04)	Tok/s 252107 (242749)	Loss/tok 3.3760 (3.4858)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.099 (0.089)	Data 1.17e-04 (5.37e-04)	Tok/s 254552 (242804)	Loss/tok 3.3637 (3.4850)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.067 (0.089)	Data 1.14e-04 (5.29e-04)	Tok/s 233989 (242841)	Loss/tok 3.2955 (3.4826)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.100 (0.089)	Data 1.14e-04 (5.20e-04)	Tok/s 252472 (242896)	Loss/tok 3.4422 (3.4804)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.067 (0.089)	Data 1.10e-04 (5.12e-04)	Tok/s 232908 (242961)	Loss/tok 3.2076 (3.4808)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.099 (0.089)	Data 1.14e-04 (5.05e-04)	Tok/s 255335 (243022)	Loss/tok 3.4960 (3.4815)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.035 (0.089)	Data 1.13e-04 (4.97e-04)	Tok/s 225704 (243041)	Loss/tok 2.6892 (3.4798)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][540/1291]	Time 0.099 (0.089)	Data 1.12e-04 (4.90e-04)	Tok/s 255329 (243039)	Loss/tok 3.4105 (3.4800)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.099 (0.089)	Data 1.18e-04 (4.83e-04)	Tok/s 257014 (243106)	Loss/tok 3.4370 (3.4812)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.036 (0.089)	Data 1.14e-04 (4.77e-04)	Tok/s 225537 (243046)	Loss/tok 2.8106 (3.4799)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.135 (0.089)	Data 1.14e-04 (4.71e-04)	Tok/s 262191 (243105)	Loss/tok 3.5304 (3.4820)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.035 (0.089)	Data 1.12e-04 (4.65e-04)	Tok/s 224142 (243044)	Loss/tok 2.7798 (3.4802)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.099 (0.089)	Data 1.12e-04 (4.59e-04)	Tok/s 256006 (243148)	Loss/tok 3.3914 (3.4805)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.53e-04)	Tok/s 229127 (243108)	Loss/tok 3.2143 (3.4797)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.066 (0.089)	Data 1.13e-04 (4.47e-04)	Tok/s 233128 (243122)	Loss/tok 3.3403 (3.4777)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.066 (0.089)	Data 1.16e-04 (4.42e-04)	Tok/s 233683 (243133)	Loss/tok 3.1080 (3.4759)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.099 (0.089)	Data 1.31e-04 (4.37e-04)	Tok/s 254341 (243109)	Loss/tok 3.5391 (3.4747)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.067 (0.089)	Data 1.31e-04 (4.32e-04)	Tok/s 231873 (243068)	Loss/tok 3.1995 (3.4728)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.099 (0.089)	Data 1.16e-04 (4.27e-04)	Tok/s 254102 (242984)	Loss/tok 3.3798 (3.4714)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.066 (0.089)	Data 1.20e-04 (4.22e-04)	Tok/s 236146 (243077)	Loss/tok 3.2126 (3.4717)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][670/1291]	Time 0.099 (0.089)	Data 1.11e-04 (4.18e-04)	Tok/s 256141 (243040)	Loss/tok 3.4328 (3.4704)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.13e-04)	Tok/s 232398 (243034)	Loss/tok 3.1973 (3.4685)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.035 (0.089)	Data 1.11e-04 (4.09e-04)	Tok/s 228313 (243085)	Loss/tok 2.7967 (3.4705)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.067 (0.089)	Data 1.15e-04 (4.05e-04)	Tok/s 237794 (243108)	Loss/tok 3.2633 (3.4700)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.174 (0.089)	Data 1.11e-04 (4.01e-04)	Tok/s 257005 (243116)	Loss/tok 3.7935 (3.4699)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.97e-04)	Tok/s 258516 (243187)	Loss/tok 3.6054 (3.4696)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.93e-04)	Tok/s 227662 (243105)	Loss/tok 3.1259 (3.4677)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.89e-04)	Tok/s 234365 (243054)	Loss/tok 3.2183 (3.4660)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.85e-04)	Tok/s 236045 (243061)	Loss/tok 3.1929 (3.4647)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.82e-04)	Tok/s 233133 (243013)	Loss/tok 3.1647 (3.4631)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.174 (0.089)	Data 1.09e-04 (3.78e-04)	Tok/s 254754 (242957)	Loss/tok 3.8014 (3.4623)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.135 (0.089)	Data 1.13e-04 (3.75e-04)	Tok/s 259932 (242962)	Loss/tok 3.5726 (3.4626)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.71e-04)	Tok/s 232166 (243031)	Loss/tok 3.1565 (3.4625)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][800/1291]	Time 0.174 (0.089)	Data 1.16e-04 (3.68e-04)	Tok/s 258274 (243026)	Loss/tok 3.8503 (3.4625)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.65e-04)	Tok/s 238346 (243018)	Loss/tok 3.2128 (3.4617)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.066 (0.089)	Data 1.24e-04 (3.62e-04)	Tok/s 229899 (242969)	Loss/tok 3.1526 (3.4601)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.59e-04)	Tok/s 233892 (242944)	Loss/tok 3.2457 (3.4591)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.173 (0.089)	Data 1.14e-04 (3.56e-04)	Tok/s 259685 (242938)	Loss/tok 3.7013 (3.4582)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.135 (0.089)	Data 1.11e-04 (3.53e-04)	Tok/s 255034 (242916)	Loss/tok 3.6154 (3.4575)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.067 (0.089)	Data 1.27e-04 (3.51e-04)	Tok/s 233860 (242927)	Loss/tok 3.1699 (3.4572)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.100 (0.089)	Data 1.51e-04 (3.48e-04)	Tok/s 254666 (242947)	Loss/tok 3.4798 (3.4564)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][880/1291]	Time 0.066 (0.089)	Data 1.17e-04 (3.45e-04)	Tok/s 233992 (242913)	Loss/tok 3.1777 (3.4549)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.43e-04)	Tok/s 234481 (242934)	Loss/tok 3.0922 (3.4536)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.40e-04)	Tok/s 231967 (242892)	Loss/tok 3.2227 (3.4533)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.38e-04)	Tok/s 232825 (242918)	Loss/tok 3.1936 (3.4533)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.35e-04)	Tok/s 254768 (242978)	Loss/tok 3.3837 (3.4530)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.099 (0.089)	Data 1.15e-04 (3.33e-04)	Tok/s 252489 (242953)	Loss/tok 3.4679 (3.4525)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.30e-04)	Tok/s 234916 (242907)	Loss/tok 3.1534 (3.4512)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.28e-04)	Tok/s 232746 (242896)	Loss/tok 3.2290 (3.4510)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.073 (0.088)	Data 1.14e-04 (3.26e-04)	Tok/s 209516 (242844)	Loss/tok 3.1326 (3.4496)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.066 (0.088)	Data 1.13e-04 (3.24e-04)	Tok/s 235582 (242808)	Loss/tok 3.1541 (3.4485)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.036 (0.088)	Data 1.12e-04 (3.22e-04)	Tok/s 218332 (242814)	Loss/tok 2.7464 (3.4480)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.099 (0.088)	Data 1.13e-04 (3.20e-04)	Tok/s 255280 (242869)	Loss/tok 3.3840 (3.4482)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.135 (0.089)	Data 1.17e-04 (3.18e-04)	Tok/s 258181 (242972)	Loss/tok 3.5713 (3.4486)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1010/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.16e-04)	Tok/s 230111 (243060)	Loss/tok 3.1555 (3.4488)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.066 (0.089)	Data 1.31e-04 (3.14e-04)	Tok/s 229372 (243086)	Loss/tok 3.1666 (3.4483)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.099 (0.089)	Data 2.72e-04 (3.12e-04)	Tok/s 254576 (243100)	Loss/tok 3.4240 (3.4478)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.134 (0.089)	Data 1.15e-04 (3.10e-04)	Tok/s 261632 (243107)	Loss/tok 3.5594 (3.4478)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.08e-04)	Tok/s 234354 (243052)	Loss/tok 3.0807 (3.4463)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.06e-04)	Tok/s 254991 (243006)	Loss/tok 3.4051 (3.4448)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.04e-04)	Tok/s 254620 (242993)	Loss/tok 3.4587 (3.4442)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.173 (0.089)	Data 1.12e-04 (3.03e-04)	Tok/s 259745 (243023)	Loss/tok 3.6848 (3.4436)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.036 (0.089)	Data 1.13e-04 (3.01e-04)	Tok/s 226365 (243032)	Loss/tok 2.6824 (3.4442)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.99e-04)	Tok/s 255160 (243081)	Loss/tok 3.4158 (3.4440)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.173 (0.089)	Data 1.14e-04 (2.98e-04)	Tok/s 258355 (243150)	Loss/tok 3.6937 (3.4447)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.066 (0.089)	Data 1.11e-04 (2.96e-04)	Tok/s 238182 (243057)	Loss/tok 3.1762 (3.4432)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1130/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.94e-04)	Tok/s 236128 (243076)	Loss/tok 3.1200 (3.4426)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.066 (0.089)	Data 1.29e-04 (2.93e-04)	Tok/s 233890 (243066)	Loss/tok 3.1652 (3.4418)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.066 (0.089)	Data 1.09e-04 (2.91e-04)	Tok/s 233260 (243029)	Loss/tok 3.1731 (3.4413)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.173 (0.089)	Data 1.11e-04 (2.90e-04)	Tok/s 261850 (243068)	Loss/tok 3.6712 (3.4409)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1170/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.88e-04)	Tok/s 232275 (243040)	Loss/tok 3.1781 (3.4396)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.87e-04)	Tok/s 235411 (243039)	Loss/tok 3.1184 (3.4396)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.86e-04)	Tok/s 252492 (243037)	Loss/tok 3.3705 (3.4388)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.100 (0.089)	Data 1.10e-04 (2.84e-04)	Tok/s 252389 (242988)	Loss/tok 3.3555 (3.4377)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.83e-04)	Tok/s 222315 (243016)	Loss/tok 2.7671 (3.4371)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.81e-04)	Tok/s 230292 (243007)	Loss/tok 3.1233 (3.4362)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.036 (0.089)	Data 1.12e-04 (2.80e-04)	Tok/s 224648 (243034)	Loss/tok 2.5913 (3.4362)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.79e-04)	Tok/s 231853 (243058)	Loss/tok 3.1284 (3.4355)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.77e-04)	Tok/s 232726 (243060)	Loss/tok 3.1797 (3.4349)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.76e-04)	Tok/s 232317 (243046)	Loss/tok 3.1707 (3.4339)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.135 (0.089)	Data 1.17e-04 (2.75e-04)	Tok/s 261521 (243074)	Loss/tok 3.5561 (3.4333)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.067 (0.089)	Data 1.13e-04 (2.73e-04)	Tok/s 231975 (243054)	Loss/tok 3.0740 (3.4318)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.066 (0.089)	Data 4.05e-05 (2.75e-04)	Tok/s 232673 (243003)	Loss/tok 3.1123 (3.4306)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592446664493, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592446664494, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.471 (0.471)	Decoder iters 139.0 (139.0)	Tok/s 35227 (35227)
0: Running moses detokenizer
0: BLEU(score=21.820847884922053, counts=[36225, 17378, 9564, 5473], totals=[66339, 63336, 60333, 57334], precisions=[54.605887939221276, 27.437792092964507, 15.852021281885536, 9.545819234660062], bp=1.0, sys_len=66339, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446666438, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2182, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592446666438, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4270	Test BLEU: 21.82
0: Performance: Epoch: 1	Training: 1944564 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592446666439, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592446666439, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446666439, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1450007178
0: TRAIN [2][0/1291]	Time 0.274 (0.274)	Data 2.02e-01 (2.02e-01)	Tok/s 56678 (56678)	Loss/tok 3.0468 (3.0468)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][10/1291]	Time 0.066 (0.095)	Data 1.21e-04 (1.84e-02)	Tok/s 235108 (221539)	Loss/tok 2.9441 (3.2170)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][20/1291]	Time 0.099 (0.094)	Data 1.12e-04 (9.72e-03)	Tok/s 257067 (230448)	Loss/tok 3.2571 (3.2745)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.036 (0.089)	Data 1.15e-04 (6.62e-03)	Tok/s 221875 (233586)	Loss/tok 2.7041 (3.2523)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.066 (0.094)	Data 1.21e-04 (5.03e-03)	Tok/s 235756 (237386)	Loss/tok 3.0858 (3.2990)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.067 (0.093)	Data 1.28e-04 (4.07e-03)	Tok/s 230475 (238314)	Loss/tok 3.0727 (3.2962)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.099 (0.091)	Data 1.13e-04 (3.42e-03)	Tok/s 255875 (238896)	Loss/tok 3.3063 (3.2798)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.067 (0.092)	Data 1.14e-04 (2.96e-03)	Tok/s 230558 (239711)	Loss/tok 3.1148 (3.2917)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.067 (0.091)	Data 1.10e-04 (2.61e-03)	Tok/s 232730 (239760)	Loss/tok 3.0917 (3.2844)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.067 (0.091)	Data 1.12e-04 (2.33e-03)	Tok/s 229245 (240122)	Loss/tok 3.1109 (3.2880)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.134 (0.092)	Data 1.10e-04 (2.11e-03)	Tok/s 262001 (240711)	Loss/tok 3.4015 (3.2938)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.067 (0.092)	Data 1.12e-04 (1.93e-03)	Tok/s 227682 (240878)	Loss/tok 3.0669 (3.2981)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.066 (0.091)	Data 1.52e-04 (1.78e-03)	Tok/s 234887 (240582)	Loss/tok 3.0768 (3.2936)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.067 (0.090)	Data 1.09e-04 (1.66e-03)	Tok/s 230770 (240530)	Loss/tok 3.0621 (3.2961)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.066 (0.090)	Data 1.31e-04 (1.55e-03)	Tok/s 233023 (240550)	Loss/tok 3.0713 (3.2915)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][150/1291]	Time 0.035 (0.090)	Data 1.37e-04 (1.45e-03)	Tok/s 221743 (240830)	Loss/tok 2.7095 (3.2970)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.067 (0.090)	Data 1.09e-04 (1.37e-03)	Tok/s 235063 (240901)	Loss/tok 3.0765 (3.2935)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.099 (0.090)	Data 1.41e-04 (1.30e-03)	Tok/s 256081 (241430)	Loss/tok 3.2872 (3.2954)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.067 (0.091)	Data 1.10e-04 (1.23e-03)	Tok/s 234047 (241626)	Loss/tok 3.0490 (3.2997)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.099 (0.091)	Data 1.13e-04 (1.17e-03)	Tok/s 253887 (241970)	Loss/tok 3.3049 (3.3022)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.067 (0.091)	Data 1.08e-04 (1.12e-03)	Tok/s 232720 (241903)	Loss/tok 3.0620 (3.3004)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.066 (0.091)	Data 1.13e-04 (1.07e-03)	Tok/s 231273 (241865)	Loss/tok 3.0024 (3.2979)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.067 (0.090)	Data 1.11e-04 (1.03e-03)	Tok/s 230214 (241646)	Loss/tok 3.0643 (3.2922)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.036 (0.090)	Data 1.08e-04 (9.88e-04)	Tok/s 218503 (241792)	Loss/tok 2.6661 (3.2926)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.135 (0.090)	Data 1.15e-04 (9.52e-04)	Tok/s 257988 (242007)	Loss/tok 3.5561 (3.2936)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.135 (0.090)	Data 1.13e-04 (9.19e-04)	Tok/s 263372 (241862)	Loss/tok 3.4729 (3.2922)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.135 (0.089)	Data 1.12e-04 (8.88e-04)	Tok/s 258500 (241825)	Loss/tok 3.4512 (3.2905)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.036 (0.089)	Data 1.14e-04 (8.59e-04)	Tok/s 222868 (241918)	Loss/tok 2.6580 (3.2908)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][280/1291]	Time 0.173 (0.089)	Data 1.16e-04 (8.33e-04)	Tok/s 258311 (241872)	Loss/tok 3.6869 (3.2921)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.067 (0.090)	Data 1.13e-04 (8.08e-04)	Tok/s 232410 (242033)	Loss/tok 3.0971 (3.2948)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.137 (0.090)	Data 1.11e-04 (7.85e-04)	Tok/s 257339 (242187)	Loss/tok 3.4311 (3.2979)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.067 (0.090)	Data 1.14e-04 (7.64e-04)	Tok/s 233667 (242208)	Loss/tok 3.0178 (3.2956)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.099 (0.090)	Data 1.14e-04 (7.43e-04)	Tok/s 256337 (242422)	Loss/tok 3.2816 (3.2981)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.099 (0.091)	Data 1.11e-04 (7.24e-04)	Tok/s 254933 (242529)	Loss/tok 3.2607 (3.3013)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.099 (0.091)	Data 1.12e-04 (7.06e-04)	Tok/s 253450 (242692)	Loss/tok 3.3293 (3.3036)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.066 (0.091)	Data 1.12e-04 (6.89e-04)	Tok/s 236167 (242784)	Loss/tok 3.1248 (3.3034)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.067 (0.091)	Data 1.25e-04 (6.73e-04)	Tok/s 231490 (242694)	Loss/tok 3.1389 (3.3031)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.066 (0.091)	Data 1.11e-04 (6.58e-04)	Tok/s 235799 (242761)	Loss/tok 3.1661 (3.3039)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.068 (0.091)	Data 1.25e-04 (6.45e-04)	Tok/s 227039 (242759)	Loss/tok 3.0946 (3.3024)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.066 (0.091)	Data 1.15e-04 (6.32e-04)	Tok/s 235164 (242895)	Loss/tok 3.0407 (3.3042)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.100 (0.091)	Data 1.16e-04 (6.19e-04)	Tok/s 251510 (242899)	Loss/tok 3.2772 (3.3049)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][410/1291]	Time 0.035 (0.091)	Data 1.13e-04 (6.07e-04)	Tok/s 222178 (243068)	Loss/tok 2.7419 (3.3065)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][420/1291]	Time 0.067 (0.091)	Data 1.09e-04 (5.95e-04)	Tok/s 236798 (243164)	Loss/tok 3.0935 (3.3066)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.067 (0.091)	Data 1.15e-04 (5.84e-04)	Tok/s 231407 (243051)	Loss/tok 3.1162 (3.3051)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.066 (0.091)	Data 1.11e-04 (5.73e-04)	Tok/s 235764 (243105)	Loss/tok 3.0379 (3.3032)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.035 (0.090)	Data 1.12e-04 (5.63e-04)	Tok/s 224548 (242928)	Loss/tok 2.6276 (3.3005)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.067 (0.091)	Data 1.24e-04 (5.53e-04)	Tok/s 231734 (243081)	Loss/tok 3.0953 (3.3006)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.135 (0.091)	Data 1.14e-04 (5.44e-04)	Tok/s 259297 (243211)	Loss/tok 3.3469 (3.3007)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.067 (0.091)	Data 1.09e-04 (5.35e-04)	Tok/s 230251 (243254)	Loss/tok 3.1339 (3.3030)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.099 (0.091)	Data 1.14e-04 (5.26e-04)	Tok/s 255286 (243310)	Loss/tok 3.3794 (3.3026)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.100 (0.091)	Data 1.12e-04 (5.18e-04)	Tok/s 251586 (243338)	Loss/tok 3.3222 (3.3030)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.099 (0.091)	Data 1.11e-04 (5.10e-04)	Tok/s 254333 (243310)	Loss/tok 3.2863 (3.3021)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.099 (0.091)	Data 1.08e-04 (5.03e-04)	Tok/s 253373 (243269)	Loss/tok 3.2964 (3.3021)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.066 (0.091)	Data 1.12e-04 (4.95e-04)	Tok/s 227854 (243212)	Loss/tok 3.0765 (3.3005)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][540/1291]	Time 0.135 (0.090)	Data 1.24e-04 (4.88e-04)	Tok/s 259678 (243145)	Loss/tok 3.4668 (3.2995)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][550/1291]	Time 0.174 (0.090)	Data 1.09e-04 (4.81e-04)	Tok/s 256383 (243107)	Loss/tok 3.6532 (3.3001)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.036 (0.090)	Data 1.13e-04 (4.75e-04)	Tok/s 217793 (243110)	Loss/tok 2.6329 (3.2997)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.68e-04)	Tok/s 231803 (243049)	Loss/tok 3.1030 (3.3001)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.067 (0.090)	Data 1.10e-04 (4.62e-04)	Tok/s 231692 (242985)	Loss/tok 3.0245 (3.2987)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.067 (0.090)	Data 1.13e-04 (4.56e-04)	Tok/s 229302 (243027)	Loss/tok 3.0825 (3.2982)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.099 (0.090)	Data 1.12e-04 (4.51e-04)	Tok/s 253865 (243089)	Loss/tok 3.2431 (3.2976)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.067 (0.090)	Data 1.14e-04 (4.45e-04)	Tok/s 233814 (243070)	Loss/tok 3.0797 (3.2972)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.066 (0.090)	Data 1.12e-04 (4.40e-04)	Tok/s 237252 (243141)	Loss/tok 3.0536 (3.2973)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.35e-04)	Tok/s 231856 (243090)	Loss/tok 3.1208 (3.2959)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.30e-04)	Tok/s 254033 (243115)	Loss/tok 3.2004 (3.2968)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.036 (0.090)	Data 1.13e-04 (4.25e-04)	Tok/s 221850 (243147)	Loss/tok 2.6443 (3.2956)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.036 (0.090)	Data 1.09e-04 (4.20e-04)	Tok/s 214514 (243042)	Loss/tok 2.5983 (3.2936)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][670/1291]	Time 0.066 (0.090)	Data 1.13e-04 (4.15e-04)	Tok/s 231605 (243045)	Loss/tok 3.0745 (3.2937)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.100 (0.090)	Data 1.12e-04 (4.11e-04)	Tok/s 252955 (243004)	Loss/tok 3.3564 (3.2935)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.099 (0.089)	Data 1.15e-04 (4.07e-04)	Tok/s 253383 (242955)	Loss/tok 3.3118 (3.2921)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.099 (0.089)	Data 1.12e-04 (4.02e-04)	Tok/s 256116 (242965)	Loss/tok 3.2984 (3.2918)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.099 (0.089)	Data 1.11e-04 (3.98e-04)	Tok/s 253727 (242960)	Loss/tok 3.2877 (3.2917)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.174 (0.089)	Data 1.11e-04 (3.94e-04)	Tok/s 253974 (242985)	Loss/tok 3.6034 (3.2924)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.134 (0.089)	Data 1.12e-04 (3.91e-04)	Tok/s 258888 (242986)	Loss/tok 3.4063 (3.2916)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.067 (0.089)	Data 5.76e-04 (3.87e-04)	Tok/s 232555 (242893)	Loss/tok 3.0535 (3.2910)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.84e-04)	Tok/s 220387 (242920)	Loss/tok 2.6376 (3.2914)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.80e-04)	Tok/s 256606 (242818)	Loss/tok 3.1908 (3.2893)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.067 (0.089)	Data 1.11e-04 (3.77e-04)	Tok/s 235657 (242789)	Loss/tok 3.0674 (3.2887)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.73e-04)	Tok/s 251540 (242877)	Loss/tok 3.2826 (3.2890)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.136 (0.089)	Data 1.15e-04 (3.70e-04)	Tok/s 259390 (242941)	Loss/tok 3.4372 (3.2895)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][800/1291]	Time 0.066 (0.089)	Data 1.11e-04 (3.67e-04)	Tok/s 235811 (242960)	Loss/tok 3.0259 (3.2894)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.036 (0.089)	Data 1.08e-04 (3.64e-04)	Tok/s 225106 (242897)	Loss/tok 2.6810 (3.2881)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.61e-04)	Tok/s 250404 (242937)	Loss/tok 3.3566 (3.2885)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.58e-04)	Tok/s 232627 (242856)	Loss/tok 3.0767 (3.2875)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.100 (0.089)	Data 1.09e-04 (3.55e-04)	Tok/s 252125 (242862)	Loss/tok 3.3622 (3.2870)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.100 (0.089)	Data 1.11e-04 (3.52e-04)	Tok/s 253290 (242838)	Loss/tok 3.3173 (3.2859)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.135 (0.089)	Data 1.16e-04 (3.49e-04)	Tok/s 260516 (242897)	Loss/tok 3.3917 (3.2864)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.46e-04)	Tok/s 230318 (242855)	Loss/tok 3.0531 (3.2853)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.44e-04)	Tok/s 231034 (242832)	Loss/tok 3.0977 (3.2843)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.41e-04)	Tok/s 232814 (242821)	Loss/tok 3.0306 (3.2843)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.100 (0.088)	Data 1.14e-04 (3.39e-04)	Tok/s 254579 (242798)	Loss/tok 3.1378 (3.2830)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.067 (0.089)	Data 1.13e-04 (3.36e-04)	Tok/s 236256 (242831)	Loss/tok 2.9972 (3.2837)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.174 (0.089)	Data 1.11e-04 (3.34e-04)	Tok/s 256402 (242892)	Loss/tok 3.6324 (3.2855)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][930/1291]	Time 0.099 (0.089)	Data 1.12e-04 (3.31e-04)	Tok/s 251982 (242865)	Loss/tok 3.3450 (3.2849)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.135 (0.089)	Data 1.11e-04 (3.29e-04)	Tok/s 259361 (242803)	Loss/tok 3.4829 (3.2842)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.066 (0.089)	Data 1.12e-04 (3.27e-04)	Tok/s 238911 (242825)	Loss/tok 2.9874 (3.2831)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.066 (0.088)	Data 1.08e-04 (3.25e-04)	Tok/s 232061 (242753)	Loss/tok 3.0603 (3.2819)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.22e-04)	Tok/s 255141 (242683)	Loss/tok 3.2769 (3.2806)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.20e-04)	Tok/s 218108 (242626)	Loss/tok 2.6907 (3.2797)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.135 (0.088)	Data 1.13e-04 (3.18e-04)	Tok/s 257733 (242597)	Loss/tok 3.3729 (3.2788)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.099 (0.088)	Data 1.11e-04 (3.16e-04)	Tok/s 255391 (242628)	Loss/tok 3.2196 (3.2786)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.174 (0.088)	Data 1.13e-04 (3.14e-04)	Tok/s 258261 (242718)	Loss/tok 3.5865 (3.2808)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.067 (0.088)	Data 1.11e-04 (3.12e-04)	Tok/s 233711 (242687)	Loss/tok 3.0150 (3.2802)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.174 (0.088)	Data 1.08e-04 (3.10e-04)	Tok/s 261574 (242734)	Loss/tok 3.5385 (3.2800)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.173 (0.088)	Data 1.08e-04 (3.08e-04)	Tok/s 256575 (242742)	Loss/tok 3.5866 (3.2802)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1050/1291]	Time 0.066 (0.088)	Data 1.18e-04 (3.06e-04)	Tok/s 232628 (242704)	Loss/tok 3.0446 (3.2808)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.099 (0.088)	Data 1.17e-04 (3.05e-04)	Tok/s 251317 (242711)	Loss/tok 3.3536 (3.2820)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.066 (0.088)	Data 1.12e-04 (3.03e-04)	Tok/s 237979 (242731)	Loss/tok 3.1313 (3.2824)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.01e-04)	Tok/s 224535 (242726)	Loss/tok 2.6652 (3.2823)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.067 (0.088)	Data 1.12e-04 (3.00e-04)	Tok/s 232712 (242753)	Loss/tok 3.0757 (3.2832)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.98e-04)	Tok/s 231769 (242809)	Loss/tok 3.0439 (3.2834)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.96e-04)	Tok/s 229553 (242804)	Loss/tok 2.9772 (3.2836)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.95e-04)	Tok/s 255429 (242792)	Loss/tok 3.1909 (3.2827)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.93e-04)	Tok/s 256745 (242850)	Loss/tok 3.1808 (3.2825)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.134 (0.089)	Data 1.30e-04 (2.91e-04)	Tok/s 260381 (242864)	Loss/tok 3.4606 (3.2833)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.067 (0.089)	Data 1.11e-04 (2.90e-04)	Tok/s 230433 (242848)	Loss/tok 3.0763 (3.2828)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.88e-04)	Tok/s 234005 (242826)	Loss/tok 3.0389 (3.2830)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.135 (0.089)	Data 1.15e-04 (2.87e-04)	Tok/s 260931 (242854)	Loss/tok 3.3993 (3.2836)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1180/1291]	Time 0.066 (0.089)	Data 1.07e-04 (2.85e-04)	Tok/s 234361 (242840)	Loss/tok 3.1064 (3.2833)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.099 (0.088)	Data 1.14e-04 (2.84e-04)	Tok/s 256030 (242787)	Loss/tok 3.2768 (3.2825)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1200/1291]	Time 0.100 (0.089)	Data 1.12e-04 (2.82e-04)	Tok/s 257623 (242818)	Loss/tok 3.2566 (3.2833)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.81e-04)	Tok/s 252549 (242808)	Loss/tok 3.3588 (3.2837)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.100 (0.089)	Data 1.15e-04 (2.80e-04)	Tok/s 252066 (242788)	Loss/tok 3.3230 (3.2832)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.134 (0.089)	Data 1.11e-04 (2.78e-04)	Tok/s 260652 (242846)	Loss/tok 3.4877 (3.2845)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.77e-04)	Tok/s 255479 (242850)	Loss/tok 3.2835 (3.2837)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.067 (0.089)	Data 1.10e-04 (2.76e-04)	Tok/s 231848 (242887)	Loss/tok 3.0839 (3.2835)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.067 (0.089)	Data 1.12e-04 (2.74e-04)	Tok/s 230296 (242864)	Loss/tok 3.0359 (3.2837)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.135 (0.089)	Data 1.13e-04 (2.73e-04)	Tok/s 257100 (242862)	Loss/tok 3.3868 (3.2832)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.72e-04)	Tok/s 251380 (242941)	Loss/tok 3.3344 (3.2836)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.066 (0.089)	Data 4.41e-05 (2.73e-04)	Tok/s 234534 (242898)	Loss/tok 3.1326 (3.2828)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1592446781707, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592446781707, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.384 (0.384)	Decoder iters 100.0 (100.0)	Tok/s 42066 (42066)
0: Running moses detokenizer
0: BLEU(score=22.83927463634808, counts=[36026, 17632, 9820, 5669], totals=[64131, 61128, 58125, 55128], precisions=[56.175640485880464, 28.844392095275488, 16.89462365591398, 10.283340589174285], bp=0.9915377776997566, sys_len=64131, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446783535, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2284, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592446783535, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2825	Test BLEU: 22.84
0: Performance: Epoch: 2	Training: 1942998 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592446783536, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592446783536, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446783536, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 812160412
0: TRAIN [3][0/1291]	Time 0.278 (0.278)	Data 2.08e-01 (2.08e-01)	Tok/s 55977 (55977)	Loss/tok 3.0366 (3.0366)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.099 (0.101)	Data 1.21e-04 (1.91e-02)	Tok/s 251200 (225198)	Loss/tok 3.1446 (3.1405)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.099 (0.083)	Data 1.19e-04 (1.00e-02)	Tok/s 258261 (229164)	Loss/tok 3.1616 (3.0839)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.135 (0.085)	Data 1.27e-04 (6.84e-03)	Tok/s 262452 (233812)	Loss/tok 3.2988 (3.1236)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][40/1291]	Time 0.099 (0.087)	Data 1.19e-04 (5.20e-03)	Tok/s 252999 (236538)	Loss/tok 3.1961 (3.1424)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.100 (0.088)	Data 1.16e-04 (4.21e-03)	Tok/s 247839 (237283)	Loss/tok 3.2422 (3.1502)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.102 (0.089)	Data 1.22e-04 (3.54e-03)	Tok/s 246732 (238465)	Loss/tok 3.1833 (3.1642)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.067 (0.089)	Data 1.26e-04 (3.06e-03)	Tok/s 230765 (238955)	Loss/tok 3.0223 (3.1654)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.099 (0.087)	Data 1.31e-04 (2.70e-03)	Tok/s 253015 (238930)	Loss/tok 3.2826 (3.1567)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.100 (0.088)	Data 1.31e-04 (2.41e-03)	Tok/s 250861 (239578)	Loss/tok 3.2082 (3.1708)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.067 (0.089)	Data 1.23e-04 (2.19e-03)	Tok/s 232764 (240588)	Loss/tok 2.9914 (3.1766)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.066 (0.090)	Data 1.13e-04 (2.00e-03)	Tok/s 229476 (240618)	Loss/tok 3.0784 (3.1839)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.135 (0.090)	Data 1.13e-04 (1.85e-03)	Tok/s 255690 (240941)	Loss/tok 3.3700 (3.1891)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.099 (0.090)	Data 1.21e-04 (1.71e-03)	Tok/s 254157 (241174)	Loss/tok 3.1580 (3.1891)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.099 (0.091)	Data 1.25e-04 (1.60e-03)	Tok/s 256333 (241799)	Loss/tok 3.1629 (3.1917)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.066 (0.090)	Data 1.11e-04 (1.50e-03)	Tok/s 236955 (241554)	Loss/tok 3.0127 (3.1925)	LR 2.875e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][160/1291]	Time 0.067 (0.090)	Data 1.14e-04 (1.42e-03)	Tok/s 234050 (241452)	Loss/tok 3.0171 (3.1878)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.067 (0.089)	Data 1.23e-04 (1.34e-03)	Tok/s 232592 (241221)	Loss/tok 2.9393 (3.1872)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][180/1291]	Time 0.036 (0.089)	Data 1.24e-04 (1.27e-03)	Tok/s 220754 (241232)	Loss/tok 2.5742 (3.1899)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.099 (0.089)	Data 1.14e-04 (1.21e-03)	Tok/s 249305 (241397)	Loss/tok 3.2506 (3.1891)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.067 (0.089)	Data 1.16e-04 (1.16e-03)	Tok/s 235165 (241573)	Loss/tok 3.0001 (3.1908)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.100 (0.089)	Data 1.16e-04 (1.11e-03)	Tok/s 251609 (241576)	Loss/tok 3.1980 (3.1875)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.175 (0.089)	Data 1.21e-04 (1.06e-03)	Tok/s 254601 (241525)	Loss/tok 3.5413 (3.1923)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.100 (0.089)	Data 1.17e-04 (1.02e-03)	Tok/s 252693 (241217)	Loss/tok 3.2302 (3.1888)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.066 (0.088)	Data 1.12e-04 (9.86e-04)	Tok/s 233185 (241200)	Loss/tok 3.0755 (3.1869)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.067 (0.088)	Data 1.20e-04 (9.51e-04)	Tok/s 232042 (241191)	Loss/tok 2.9792 (3.1857)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.067 (0.088)	Data 1.17e-04 (9.19e-04)	Tok/s 230485 (241258)	Loss/tok 2.9649 (3.1846)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.100 (0.088)	Data 1.10e-04 (8.90e-04)	Tok/s 253597 (241269)	Loss/tok 3.1177 (3.1851)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.135 (0.088)	Data 1.18e-04 (8.62e-04)	Tok/s 256760 (241444)	Loss/tok 3.3439 (3.1829)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.100 (0.088)	Data 1.14e-04 (8.36e-04)	Tok/s 254162 (241607)	Loss/tok 3.1687 (3.1831)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][300/1291]	Time 0.099 (0.089)	Data 1.18e-04 (8.13e-04)	Tok/s 252300 (241707)	Loss/tok 3.1359 (3.1863)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.135 (0.089)	Data 1.15e-04 (7.90e-04)	Tok/s 260292 (242069)	Loss/tok 3.4448 (3.1891)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.066 (0.090)	Data 1.19e-04 (7.69e-04)	Tok/s 232230 (242164)	Loss/tok 2.9670 (3.1910)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.174 (0.090)	Data 1.22e-04 (7.49e-04)	Tok/s 257262 (242201)	Loss/tok 3.4972 (3.1923)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.100 (0.090)	Data 1.14e-04 (7.31e-04)	Tok/s 248333 (242285)	Loss/tok 3.2455 (3.1898)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.067 (0.089)	Data 1.12e-04 (7.13e-04)	Tok/s 229487 (242089)	Loss/tok 2.9683 (3.1864)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.099 (0.090)	Data 1.18e-04 (6.97e-04)	Tok/s 252210 (242212)	Loss/tok 3.1926 (3.1872)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.067 (0.090)	Data 1.18e-04 (6.81e-04)	Tok/s 234328 (242201)	Loss/tok 3.0189 (3.1875)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.100 (0.090)	Data 1.16e-04 (6.66e-04)	Tok/s 250845 (242335)	Loss/tok 3.1257 (3.1887)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.066 (0.090)	Data 1.13e-04 (6.52e-04)	Tok/s 236555 (242353)	Loss/tok 2.9507 (3.1897)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.067 (0.090)	Data 1.22e-04 (6.39e-04)	Tok/s 232440 (242319)	Loss/tok 2.9953 (3.1885)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.100 (0.090)	Data 1.15e-04 (6.26e-04)	Tok/s 251938 (242309)	Loss/tok 3.0325 (3.1864)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.100 (0.090)	Data 1.19e-04 (6.14e-04)	Tok/s 252894 (242266)	Loss/tok 3.1792 (3.1834)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][430/1291]	Time 0.174 (0.090)	Data 1.20e-04 (6.02e-04)	Tok/s 256459 (242309)	Loss/tok 3.4367 (3.1839)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.067 (0.090)	Data 1.15e-04 (5.91e-04)	Tok/s 231478 (242310)	Loss/tok 2.9576 (3.1858)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.036 (0.090)	Data 1.13e-04 (5.81e-04)	Tok/s 225970 (242334)	Loss/tok 2.5874 (3.1853)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.099 (0.091)	Data 1.10e-04 (5.71e-04)	Tok/s 254439 (242506)	Loss/tok 3.1741 (3.1875)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.036 (0.090)	Data 1.24e-04 (5.61e-04)	Tok/s 220537 (242483)	Loss/tok 2.6353 (3.1859)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.067 (0.090)	Data 1.17e-04 (5.52e-04)	Tok/s 230416 (242368)	Loss/tok 2.8934 (3.1849)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.099 (0.090)	Data 1.17e-04 (5.43e-04)	Tok/s 256753 (242525)	Loss/tok 3.2258 (3.1867)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.067 (0.090)	Data 1.10e-04 (5.34e-04)	Tok/s 233887 (242454)	Loss/tok 3.0420 (3.1853)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.067 (0.090)	Data 1.13e-04 (5.26e-04)	Tok/s 233092 (242491)	Loss/tok 2.9451 (3.1842)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.136 (0.090)	Data 1.13e-04 (5.18e-04)	Tok/s 257161 (242481)	Loss/tok 3.2363 (3.1824)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.099 (0.090)	Data 1.17e-04 (5.11e-04)	Tok/s 254997 (242546)	Loss/tok 3.1828 (3.1814)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.100 (0.090)	Data 1.14e-04 (5.03e-04)	Tok/s 252572 (242511)	Loss/tok 3.1763 (3.1803)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.174 (0.090)	Data 1.15e-04 (4.96e-04)	Tok/s 257241 (242542)	Loss/tok 3.4777 (3.1812)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1291]	Time 0.067 (0.090)	Data 1.16e-04 (4.89e-04)	Tok/s 231402 (242541)	Loss/tok 3.0572 (3.1812)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.067 (0.090)	Data 1.11e-04 (4.83e-04)	Tok/s 229184 (242420)	Loss/tok 2.9909 (3.1793)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.100 (0.090)	Data 1.13e-04 (4.76e-04)	Tok/s 254689 (242412)	Loss/tok 3.1045 (3.1776)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.067 (0.089)	Data 1.08e-04 (4.70e-04)	Tok/s 230852 (242289)	Loss/tok 2.8233 (3.1766)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.099 (0.089)	Data 1.13e-04 (4.64e-04)	Tok/s 254851 (242180)	Loss/tok 3.2333 (3.1750)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.066 (0.089)	Data 1.15e-04 (4.59e-04)	Tok/s 237105 (242150)	Loss/tok 2.9514 (3.1734)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.067 (0.089)	Data 1.13e-04 (4.53e-04)	Tok/s 230869 (242140)	Loss/tok 2.9596 (3.1718)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.099 (0.089)	Data 1.15e-04 (4.48e-04)	Tok/s 253454 (242140)	Loss/tok 3.1748 (3.1707)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.100 (0.089)	Data 1.15e-04 (4.43e-04)	Tok/s 254702 (242161)	Loss/tok 3.1567 (3.1703)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.135 (0.089)	Data 1.20e-04 (4.38e-04)	Tok/s 257552 (242300)	Loss/tok 3.3028 (3.1710)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.067 (0.089)	Data 1.15e-04 (4.33e-04)	Tok/s 234591 (242400)	Loss/tok 2.9630 (3.1728)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.035 (0.089)	Data 1.13e-04 (4.28e-04)	Tok/s 222971 (242276)	Loss/tok 2.5521 (3.1716)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.100 (0.089)	Data 1.12e-04 (4.23e-04)	Tok/s 251897 (242279)	Loss/tok 3.1614 (3.1708)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][690/1291]	Time 0.036 (0.089)	Data 1.16e-04 (4.19e-04)	Tok/s 220719 (242260)	Loss/tok 2.5861 (3.1698)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.15e-04)	Tok/s 255148 (242241)	Loss/tok 3.0468 (3.1685)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.066 (0.089)	Data 1.15e-04 (4.10e-04)	Tok/s 233732 (242267)	Loss/tok 3.0741 (3.1690)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.067 (0.089)	Data 1.14e-04 (4.06e-04)	Tok/s 236957 (242217)	Loss/tok 2.9011 (3.1680)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.066 (0.089)	Data 1.17e-04 (4.02e-04)	Tok/s 231022 (242257)	Loss/tok 2.8485 (3.1686)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.036 (0.089)	Data 1.18e-04 (3.99e-04)	Tok/s 222974 (242278)	Loss/tok 2.5925 (3.1686)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.100 (0.089)	Data 1.22e-04 (3.95e-04)	Tok/s 250494 (242357)	Loss/tok 3.1819 (3.1690)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.100 (0.089)	Data 1.12e-04 (3.91e-04)	Tok/s 251188 (242385)	Loss/tok 3.0309 (3.1685)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.87e-04)	Tok/s 234032 (242478)	Loss/tok 2.9570 (3.1691)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.175 (0.089)	Data 1.13e-04 (3.84e-04)	Tok/s 255392 (242477)	Loss/tok 3.5041 (3.1699)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.067 (0.089)	Data 1.15e-04 (3.81e-04)	Tok/s 229501 (242421)	Loss/tok 2.9254 (3.1680)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.174 (0.089)	Data 1.15e-04 (3.77e-04)	Tok/s 253721 (242495)	Loss/tok 3.5300 (3.1688)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.036 (0.089)	Data 1.11e-04 (3.74e-04)	Tok/s 222921 (242548)	Loss/tok 2.6326 (3.1695)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][820/1291]	Time 0.099 (0.090)	Data 1.15e-04 (3.71e-04)	Tok/s 257399 (242599)	Loss/tok 3.0862 (3.1693)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.067 (0.090)	Data 1.17e-04 (3.68e-04)	Tok/s 234884 (242659)	Loss/tok 2.8328 (3.1697)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.066 (0.090)	Data 1.13e-04 (3.65e-04)	Tok/s 233130 (242638)	Loss/tok 3.0017 (3.1706)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.067 (0.090)	Data 1.12e-04 (3.62e-04)	Tok/s 232128 (242671)	Loss/tok 3.0431 (3.1700)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.067 (0.090)	Data 1.10e-04 (3.59e-04)	Tok/s 229382 (242636)	Loss/tok 2.9780 (3.1688)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.100 (0.090)	Data 1.14e-04 (3.56e-04)	Tok/s 253616 (242670)	Loss/tok 3.1010 (3.1688)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.100 (0.090)	Data 1.13e-04 (3.53e-04)	Tok/s 250022 (242686)	Loss/tok 3.1046 (3.1684)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.067 (0.090)	Data 1.16e-04 (3.51e-04)	Tok/s 230332 (242682)	Loss/tok 2.8976 (3.1691)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.48e-04)	Tok/s 233267 (242716)	Loss/tok 2.9259 (3.1690)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.100 (0.090)	Data 1.12e-04 (3.45e-04)	Tok/s 251381 (242731)	Loss/tok 3.2185 (3.1694)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.066 (0.090)	Data 1.10e-04 (3.43e-04)	Tok/s 233072 (242766)	Loss/tok 2.9283 (3.1688)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.067 (0.090)	Data 1.13e-04 (3.40e-04)	Tok/s 234802 (242714)	Loss/tok 2.8708 (3.1684)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][940/1291]	Time 0.100 (0.090)	Data 1.09e-04 (3.38e-04)	Tok/s 255441 (242703)	Loss/tok 3.0189 (3.1675)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.36e-04)	Tok/s 232527 (242716)	Loss/tok 2.8680 (3.1673)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.036 (0.090)	Data 1.10e-04 (3.34e-04)	Tok/s 217828 (242658)	Loss/tok 2.4742 (3.1661)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.136 (0.090)	Data 1.13e-04 (3.31e-04)	Tok/s 262200 (242702)	Loss/tok 3.3271 (3.1659)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.067 (0.090)	Data 1.15e-04 (3.29e-04)	Tok/s 226562 (242668)	Loss/tok 2.9314 (3.1658)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.135 (0.090)	Data 1.18e-04 (3.27e-04)	Tok/s 260589 (242713)	Loss/tok 3.2144 (3.1652)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.036 (0.090)	Data 1.13e-04 (3.25e-04)	Tok/s 225531 (242625)	Loss/tok 2.6343 (3.1635)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.135 (0.090)	Data 1.13e-04 (3.23e-04)	Tok/s 259187 (242714)	Loss/tok 3.2694 (3.1642)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.067 (0.090)	Data 1.14e-04 (3.21e-04)	Tok/s 232108 (242695)	Loss/tok 2.9386 (3.1639)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.135 (0.090)	Data 1.14e-04 (3.19e-04)	Tok/s 259218 (242675)	Loss/tok 3.2338 (3.1631)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.035 (0.089)	Data 1.09e-04 (3.17e-04)	Tok/s 229945 (242614)	Loss/tok 2.5156 (3.1617)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.135 (0.089)	Data 1.17e-04 (3.15e-04)	Tok/s 262672 (242609)	Loss/tok 3.1332 (3.1604)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.100 (0.089)	Data 1.15e-04 (3.13e-04)	Tok/s 251514 (242528)	Loss/tok 3.1004 (3.1591)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1070/1291]	Time 0.067 (0.089)	Data 1.12e-04 (3.11e-04)	Tok/s 230150 (242523)	Loss/tok 2.8762 (3.1585)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.135 (0.089)	Data 1.17e-04 (3.09e-04)	Tok/s 259074 (242550)	Loss/tok 3.3791 (3.1584)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.07e-04)	Tok/s 255434 (242557)	Loss/tok 3.0747 (3.1591)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.067 (0.089)	Data 1.17e-04 (3.06e-04)	Tok/s 230185 (242567)	Loss/tok 2.9497 (3.1582)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.067 (0.089)	Data 1.31e-04 (3.04e-04)	Tok/s 233325 (242594)	Loss/tok 2.9286 (3.1575)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.067 (0.089)	Data 1.14e-04 (3.02e-04)	Tok/s 232020 (242534)	Loss/tok 2.9208 (3.1563)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.100 (0.089)	Data 1.13e-04 (3.01e-04)	Tok/s 253836 (242563)	Loss/tok 2.9918 (3.1568)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.99e-04)	Tok/s 234187 (242576)	Loss/tok 2.8035 (3.1564)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.066 (0.089)	Data 1.13e-04 (2.97e-04)	Tok/s 228933 (242577)	Loss/tok 2.9661 (3.1555)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.100 (0.089)	Data 1.13e-04 (2.96e-04)	Tok/s 250951 (242590)	Loss/tok 3.1749 (3.1548)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.067 (0.089)	Data 1.14e-04 (2.94e-04)	Tok/s 231249 (242584)	Loss/tok 2.9365 (3.1539)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.174 (0.089)	Data 1.13e-04 (2.93e-04)	Tok/s 256039 (242608)	Loss/tok 3.4290 (3.1540)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.91e-04)	Tok/s 231917 (242650)	Loss/tok 2.9502 (3.1544)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1291]	Time 0.036 (0.089)	Data 1.13e-04 (2.90e-04)	Tok/s 221259 (242643)	Loss/tok 2.5377 (3.1540)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.88e-04)	Tok/s 236464 (242645)	Loss/tok 2.9957 (3.1542)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.87e-04)	Tok/s 254270 (242671)	Loss/tok 3.0440 (3.1542)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.136 (0.089)	Data 1.15e-04 (2.85e-04)	Tok/s 259502 (242668)	Loss/tok 3.2941 (3.1536)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.099 (0.089)	Data 1.21e-04 (2.84e-04)	Tok/s 255285 (242646)	Loss/tok 3.1795 (3.1534)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.067 (0.089)	Data 1.16e-04 (2.83e-04)	Tok/s 232587 (242629)	Loss/tok 2.8867 (3.1529)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.100 (0.089)	Data 1.16e-04 (2.81e-04)	Tok/s 253399 (242611)	Loss/tok 3.0722 (3.1519)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.80e-04)	Tok/s 233766 (242547)	Loss/tok 2.9478 (3.1511)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.067 (0.089)	Data 1.15e-04 (2.79e-04)	Tok/s 234269 (242507)	Loss/tok 2.9149 (3.1505)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.174 (0.089)	Data 6.37e-05 (2.80e-04)	Tok/s 259305 (242511)	Loss/tok 3.3765 (3.1506)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1592446899041, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592446899041, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.387 (0.387)	Decoder iters 102.0 (102.0)	Tok/s 42905 (42905)
0: Running moses detokenizer
0: BLEU(score=23.961601480575382, counts=[37173, 18554, 10589, 6275], totals=[65657, 62654, 59651, 56653], precisions=[56.61696391854638, 29.613432502314296, 17.751588405894285, 11.076200730764478], bp=1.0, sys_len=65657, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592446900890, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2396, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592446900891, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1493	Test BLEU: 23.96
0: Performance: Epoch: 3	Training: 1939620 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592446900891, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592446900891, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592446900891, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 744873040
0: TRAIN [4][0/1291]	Time 0.296 (0.296)	Data 1.98e-01 (1.98e-01)	Tok/s 51334 (51334)	Loss/tok 2.8487 (2.8487)	LR 3.594e-04
0: TRAIN [4][10/1291]	Time 0.067 (0.115)	Data 1.13e-04 (1.81e-02)	Tok/s 229378 (229813)	Loss/tok 2.9514 (3.0843)	LR 3.594e-04
0: TRAIN [4][20/1291]	Time 0.066 (0.098)	Data 1.13e-04 (9.55e-03)	Tok/s 233324 (234894)	Loss/tok 2.7642 (3.0197)	LR 3.594e-04
0: TRAIN [4][30/1291]	Time 0.173 (0.101)	Data 1.15e-04 (6.51e-03)	Tok/s 259712 (239628)	Loss/tok 3.2985 (3.0552)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][40/1291]	Time 0.067 (0.097)	Data 1.11e-04 (4.96e-03)	Tok/s 231041 (240116)	Loss/tok 2.8334 (3.0329)	LR 3.594e-04
0: TRAIN [4][50/1291]	Time 0.099 (0.094)	Data 1.14e-04 (4.01e-03)	Tok/s 254806 (240047)	Loss/tok 3.0484 (3.0343)	LR 3.594e-04
0: TRAIN [4][60/1291]	Time 0.102 (0.091)	Data 1.10e-04 (3.37e-03)	Tok/s 247065 (239791)	Loss/tok 2.9740 (3.0240)	LR 3.594e-04
0: TRAIN [4][70/1291]	Time 0.174 (0.093)	Data 1.15e-04 (2.91e-03)	Tok/s 257231 (240903)	Loss/tok 3.3591 (3.0436)	LR 3.594e-04
0: TRAIN [4][80/1291]	Time 0.066 (0.092)	Data 1.10e-04 (2.56e-03)	Tok/s 227337 (241109)	Loss/tok 2.9231 (3.0355)	LR 3.594e-04
0: TRAIN [4][90/1291]	Time 0.135 (0.091)	Data 1.10e-04 (2.30e-03)	Tok/s 258776 (240994)	Loss/tok 3.2655 (3.0335)	LR 3.594e-04
0: TRAIN [4][100/1291]	Time 0.099 (0.091)	Data 1.07e-04 (2.08e-03)	Tok/s 253020 (241711)	Loss/tok 3.0379 (3.0372)	LR 3.594e-04
0: TRAIN [4][110/1291]	Time 0.135 (0.092)	Data 1.09e-04 (1.90e-03)	Tok/s 261050 (242155)	Loss/tok 3.1520 (3.0400)	LR 3.594e-04
0: TRAIN [4][120/1291]	Time 0.136 (0.092)	Data 1.12e-04 (1.75e-03)	Tok/s 255552 (242610)	Loss/tok 3.2558 (3.0463)	LR 3.594e-04
0: TRAIN [4][130/1291]	Time 0.099 (0.093)	Data 1.20e-04 (1.63e-03)	Tok/s 254580 (243055)	Loss/tok 3.0870 (3.0523)	LR 3.594e-04
0: TRAIN [4][140/1291]	Time 0.035 (0.093)	Data 1.11e-04 (1.52e-03)	Tok/s 224847 (243031)	Loss/tok 2.4348 (3.0528)	LR 3.594e-04
0: TRAIN [4][150/1291]	Time 0.099 (0.093)	Data 1.11e-04 (1.43e-03)	Tok/s 250668 (243213)	Loss/tok 3.0650 (3.0583)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][160/1291]	Time 0.135 (0.093)	Data 1.18e-04 (1.35e-03)	Tok/s 259265 (243302)	Loss/tok 3.0987 (3.0575)	LR 3.594e-04
0: TRAIN [4][170/1291]	Time 0.135 (0.093)	Data 1.09e-04 (1.27e-03)	Tok/s 259317 (243353)	Loss/tok 3.2945 (3.0541)	LR 3.594e-04
0: TRAIN [4][180/1291]	Time 0.066 (0.094)	Data 1.08e-04 (1.21e-03)	Tok/s 234228 (243493)	Loss/tok 2.8706 (3.0582)	LR 3.594e-04
0: TRAIN [4][190/1291]	Time 0.173 (0.094)	Data 1.11e-04 (1.15e-03)	Tok/s 258209 (243776)	Loss/tok 3.3904 (3.0634)	LR 3.594e-04
0: TRAIN [4][200/1291]	Time 0.099 (0.095)	Data 1.10e-04 (1.10e-03)	Tok/s 253383 (243886)	Loss/tok 3.0322 (3.0663)	LR 3.594e-04
0: TRAIN [4][210/1291]	Time 0.067 (0.094)	Data 1.06e-04 (1.05e-03)	Tok/s 234134 (243886)	Loss/tok 2.8917 (3.0650)	LR 3.594e-04
0: TRAIN [4][220/1291]	Time 0.099 (0.094)	Data 1.12e-04 (1.01e-03)	Tok/s 251000 (243814)	Loss/tok 3.0877 (3.0638)	LR 3.594e-04
0: TRAIN [4][230/1291]	Time 0.135 (0.094)	Data 1.08e-04 (9.71e-04)	Tok/s 258352 (243697)	Loss/tok 3.3122 (3.0622)	LR 3.594e-04
0: TRAIN [4][240/1291]	Time 0.134 (0.094)	Data 1.10e-04 (9.35e-04)	Tok/s 259423 (243940)	Loss/tok 3.1613 (3.0653)	LR 3.594e-04
0: TRAIN [4][250/1291]	Time 0.135 (0.094)	Data 1.09e-04 (9.03e-04)	Tok/s 262565 (243720)	Loss/tok 3.1621 (3.0629)	LR 3.594e-04
0: TRAIN [4][260/1291]	Time 0.101 (0.093)	Data 1.11e-04 (8.72e-04)	Tok/s 251543 (243463)	Loss/tok 3.0620 (3.0604)	LR 3.594e-04
0: TRAIN [4][270/1291]	Time 0.101 (0.092)	Data 1.08e-04 (8.44e-04)	Tok/s 249614 (243051)	Loss/tok 3.0437 (3.0562)	LR 3.594e-04
0: TRAIN [4][280/1291]	Time 0.067 (0.092)	Data 1.11e-04 (8.20e-04)	Tok/s 231107 (242914)	Loss/tok 2.8454 (3.0539)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][290/1291]	Time 0.037 (0.092)	Data 1.19e-04 (7.95e-04)	Tok/s 214707 (242800)	Loss/tok 2.5340 (3.0524)	LR 3.594e-04
0: TRAIN [4][300/1291]	Time 0.066 (0.091)	Data 1.09e-04 (7.72e-04)	Tok/s 232935 (242693)	Loss/tok 2.8587 (3.0511)	LR 3.594e-04
0: TRAIN [4][310/1291]	Time 0.035 (0.091)	Data 1.05e-04 (7.51e-04)	Tok/s 222427 (242575)	Loss/tok 2.5463 (3.0475)	LR 3.594e-04
0: TRAIN [4][320/1291]	Time 0.067 (0.090)	Data 1.08e-04 (7.31e-04)	Tok/s 230595 (242427)	Loss/tok 2.8553 (3.0462)	LR 3.594e-04
0: TRAIN [4][330/1291]	Time 0.099 (0.090)	Data 1.08e-04 (7.12e-04)	Tok/s 254719 (242420)	Loss/tok 2.9748 (3.0443)	LR 3.594e-04
0: TRAIN [4][340/1291]	Time 0.067 (0.090)	Data 1.09e-04 (6.94e-04)	Tok/s 233424 (242290)	Loss/tok 2.8648 (3.0424)	LR 3.594e-04
0: TRAIN [4][350/1291]	Time 0.067 (0.089)	Data 1.08e-04 (6.78e-04)	Tok/s 233514 (242244)	Loss/tok 2.9026 (3.0421)	LR 3.594e-04
0: TRAIN [4][360/1291]	Time 0.100 (0.089)	Data 1.18e-04 (6.62e-04)	Tok/s 250426 (242249)	Loss/tok 3.0686 (3.0429)	LR 3.594e-04
0: TRAIN [4][370/1291]	Time 0.099 (0.089)	Data 1.05e-04 (6.47e-04)	Tok/s 257237 (242167)	Loss/tok 3.0333 (3.0427)	LR 3.594e-04
0: TRAIN [4][380/1291]	Time 0.099 (0.089)	Data 1.08e-04 (6.33e-04)	Tok/s 253798 (242221)	Loss/tok 3.0383 (3.0425)	LR 3.594e-04
0: TRAIN [4][390/1291]	Time 0.173 (0.090)	Data 1.09e-04 (6.20e-04)	Tok/s 257065 (242409)	Loss/tok 3.3692 (3.0459)	LR 3.594e-04
0: TRAIN [4][400/1291]	Time 0.099 (0.090)	Data 1.25e-04 (6.07e-04)	Tok/s 254696 (242595)	Loss/tok 3.0123 (3.0510)	LR 3.594e-04
0: TRAIN [4][410/1291]	Time 0.135 (0.090)	Data 1.11e-04 (5.95e-04)	Tok/s 255392 (242574)	Loss/tok 3.3231 (3.0516)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][420/1291]	Time 0.036 (0.090)	Data 1.06e-04 (5.83e-04)	Tok/s 218679 (242365)	Loss/tok 2.5492 (3.0490)	LR 3.594e-04
0: TRAIN [4][430/1291]	Time 0.100 (0.090)	Data 1.10e-04 (5.72e-04)	Tok/s 252968 (242482)	Loss/tok 3.0150 (3.0505)	LR 1.797e-04
0: TRAIN [4][440/1291]	Time 0.035 (0.090)	Data 1.10e-04 (5.62e-04)	Tok/s 227429 (242419)	Loss/tok 2.5822 (3.0481)	LR 1.797e-04
0: TRAIN [4][450/1291]	Time 0.099 (0.090)	Data 1.15e-04 (5.52e-04)	Tok/s 255134 (242388)	Loss/tok 3.0369 (3.0493)	LR 1.797e-04
0: TRAIN [4][460/1291]	Time 0.036 (0.090)	Data 1.27e-04 (5.42e-04)	Tok/s 224458 (242419)	Loss/tok 2.5626 (3.0502)	LR 1.797e-04
0: TRAIN [4][470/1291]	Time 0.135 (0.090)	Data 1.11e-04 (5.33e-04)	Tok/s 258104 (242436)	Loss/tok 3.2032 (3.0510)	LR 1.797e-04
0: TRAIN [4][480/1291]	Time 0.100 (0.090)	Data 1.13e-04 (5.25e-04)	Tok/s 256683 (242605)	Loss/tok 3.0643 (3.0519)	LR 1.797e-04
0: TRAIN [4][490/1291]	Time 0.036 (0.090)	Data 1.10e-04 (5.16e-04)	Tok/s 224076 (242559)	Loss/tok 2.5553 (3.0505)	LR 1.797e-04
0: TRAIN [4][500/1291]	Time 0.066 (0.090)	Data 1.10e-04 (5.08e-04)	Tok/s 237044 (242487)	Loss/tok 2.8749 (3.0485)	LR 1.797e-04
0: TRAIN [4][510/1291]	Time 0.035 (0.090)	Data 1.09e-04 (5.00e-04)	Tok/s 226958 (242435)	Loss/tok 2.5350 (3.0485)	LR 1.797e-04
0: TRAIN [4][520/1291]	Time 0.067 (0.090)	Data 1.09e-04 (4.93e-04)	Tok/s 233172 (242512)	Loss/tok 2.8458 (3.0492)	LR 1.797e-04
0: TRAIN [4][530/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.86e-04)	Tok/s 251360 (242486)	Loss/tok 2.9337 (3.0472)	LR 1.797e-04
0: TRAIN [4][540/1291]	Time 0.100 (0.089)	Data 1.10e-04 (4.79e-04)	Tok/s 254903 (242494)	Loss/tok 3.0118 (3.0466)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][550/1291]	Time 0.067 (0.089)	Data 1.08e-04 (4.72e-04)	Tok/s 230046 (242406)	Loss/tok 2.9033 (3.0468)	LR 1.797e-04
0: TRAIN [4][560/1291]	Time 0.067 (0.089)	Data 1.11e-04 (4.65e-04)	Tok/s 227979 (242353)	Loss/tok 2.7940 (3.0455)	LR 1.797e-04
0: TRAIN [4][570/1291]	Time 0.100 (0.089)	Data 1.13e-04 (4.59e-04)	Tok/s 253865 (242446)	Loss/tok 2.9250 (3.0462)	LR 1.797e-04
0: TRAIN [4][580/1291]	Time 0.175 (0.089)	Data 1.27e-04 (4.53e-04)	Tok/s 255353 (242378)	Loss/tok 3.3065 (3.0458)	LR 1.797e-04
0: TRAIN [4][590/1291]	Time 0.099 (0.089)	Data 1.11e-04 (4.48e-04)	Tok/s 251670 (242458)	Loss/tok 3.1216 (3.0463)	LR 1.797e-04
0: TRAIN [4][600/1291]	Time 0.067 (0.089)	Data 1.20e-04 (4.42e-04)	Tok/s 234731 (242489)	Loss/tok 2.8317 (3.0454)	LR 1.797e-04
0: TRAIN [4][610/1291]	Time 0.067 (0.089)	Data 1.10e-04 (4.36e-04)	Tok/s 230831 (242490)	Loss/tok 2.8067 (3.0452)	LR 1.797e-04
0: TRAIN [4][620/1291]	Time 0.067 (0.089)	Data 1.09e-04 (4.31e-04)	Tok/s 231827 (242502)	Loss/tok 2.7713 (3.0440)	LR 1.797e-04
0: TRAIN [4][630/1291]	Time 0.134 (0.089)	Data 1.12e-04 (4.26e-04)	Tok/s 261638 (242491)	Loss/tok 3.2542 (3.0446)	LR 1.797e-04
0: TRAIN [4][640/1291]	Time 0.135 (0.089)	Data 1.07e-04 (4.21e-04)	Tok/s 261170 (242468)	Loss/tok 3.1436 (3.0442)	LR 1.797e-04
0: TRAIN [4][650/1291]	Time 0.036 (0.089)	Data 1.16e-04 (4.16e-04)	Tok/s 220891 (242436)	Loss/tok 2.4838 (3.0442)	LR 1.797e-04
0: TRAIN [4][660/1291]	Time 0.134 (0.089)	Data 1.08e-04 (4.12e-04)	Tok/s 259979 (242497)	Loss/tok 3.2004 (3.0456)	LR 1.797e-04
0: TRAIN [4][670/1291]	Time 0.099 (0.089)	Data 1.19e-04 (4.07e-04)	Tok/s 257729 (242525)	Loss/tok 3.0390 (3.0449)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][680/1291]	Time 0.066 (0.089)	Data 1.09e-04 (4.03e-04)	Tok/s 239871 (242553)	Loss/tok 2.8566 (3.0446)	LR 1.797e-04
0: TRAIN [4][690/1291]	Time 0.067 (0.090)	Data 1.11e-04 (3.99e-04)	Tok/s 233716 (242636)	Loss/tok 2.9051 (3.0454)	LR 1.797e-04
0: TRAIN [4][700/1291]	Time 0.067 (0.090)	Data 1.07e-04 (3.95e-04)	Tok/s 234153 (242665)	Loss/tok 2.8997 (3.0450)	LR 1.797e-04
0: TRAIN [4][710/1291]	Time 0.067 (0.089)	Data 1.06e-04 (3.91e-04)	Tok/s 237309 (242673)	Loss/tok 2.8326 (3.0447)	LR 1.797e-04
0: TRAIN [4][720/1291]	Time 0.066 (0.089)	Data 1.23e-04 (3.87e-04)	Tok/s 235308 (242704)	Loss/tok 2.8367 (3.0439)	LR 1.797e-04
0: TRAIN [4][730/1291]	Time 0.066 (0.089)	Data 1.06e-04 (3.84e-04)	Tok/s 234788 (242608)	Loss/tok 2.8218 (3.0426)	LR 1.797e-04
0: TRAIN [4][740/1291]	Time 0.099 (0.089)	Data 1.08e-04 (3.80e-04)	Tok/s 254041 (242584)	Loss/tok 3.0369 (3.0420)	LR 1.797e-04
0: TRAIN [4][750/1291]	Time 0.174 (0.089)	Data 1.10e-04 (3.76e-04)	Tok/s 255672 (242608)	Loss/tok 3.3733 (3.0429)	LR 1.797e-04
0: TRAIN [4][760/1291]	Time 0.066 (0.089)	Data 1.09e-04 (3.73e-04)	Tok/s 230245 (242611)	Loss/tok 2.8383 (3.0429)	LR 1.797e-04
0: TRAIN [4][770/1291]	Time 0.067 (0.089)	Data 1.10e-04 (3.69e-04)	Tok/s 230947 (242587)	Loss/tok 2.8941 (3.0422)	LR 1.797e-04
0: TRAIN [4][780/1291]	Time 0.066 (0.089)	Data 1.21e-04 (3.66e-04)	Tok/s 233628 (242614)	Loss/tok 2.7706 (3.0421)	LR 1.797e-04
0: TRAIN [4][790/1291]	Time 0.067 (0.089)	Data 1.05e-04 (3.63e-04)	Tok/s 233274 (242581)	Loss/tok 2.8747 (3.0411)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][800/1291]	Time 0.067 (0.089)	Data 1.09e-04 (3.60e-04)	Tok/s 232663 (242498)	Loss/tok 2.9038 (3.0403)	LR 1.797e-04
0: TRAIN [4][810/1291]	Time 0.066 (0.089)	Data 1.08e-04 (3.57e-04)	Tok/s 232536 (242477)	Loss/tok 2.8512 (3.0394)	LR 1.797e-04
0: TRAIN [4][820/1291]	Time 0.036 (0.088)	Data 1.14e-04 (3.54e-04)	Tok/s 220030 (242482)	Loss/tok 2.4831 (3.0388)	LR 1.797e-04
0: TRAIN [4][830/1291]	Time 0.067 (0.089)	Data 1.08e-04 (3.51e-04)	Tok/s 226806 (242550)	Loss/tok 2.7908 (3.0391)	LR 1.797e-04
0: TRAIN [4][840/1291]	Time 0.066 (0.089)	Data 1.16e-04 (3.48e-04)	Tok/s 229770 (242617)	Loss/tok 2.9180 (3.0396)	LR 1.797e-04
0: TRAIN [4][850/1291]	Time 0.066 (0.089)	Data 1.15e-04 (3.45e-04)	Tok/s 233499 (242588)	Loss/tok 2.7327 (3.0389)	LR 1.797e-04
0: TRAIN [4][860/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.42e-04)	Tok/s 251678 (242597)	Loss/tok 3.0145 (3.0397)	LR 1.797e-04
0: TRAIN [4][870/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.40e-04)	Tok/s 252906 (242564)	Loss/tok 3.1270 (3.0392)	LR 1.797e-04
0: TRAIN [4][880/1291]	Time 0.066 (0.089)	Data 1.06e-04 (3.37e-04)	Tok/s 236008 (242542)	Loss/tok 2.8596 (3.0391)	LR 1.797e-04
0: TRAIN [4][890/1291]	Time 0.099 (0.089)	Data 1.10e-04 (3.35e-04)	Tok/s 255129 (242580)	Loss/tok 3.0661 (3.0398)	LR 1.797e-04
0: TRAIN [4][900/1291]	Time 0.100 (0.088)	Data 1.05e-04 (3.32e-04)	Tok/s 251246 (242505)	Loss/tok 3.0512 (3.0389)	LR 1.797e-04
0: TRAIN [4][910/1291]	Time 0.100 (0.088)	Data 1.07e-04 (3.30e-04)	Tok/s 254586 (242514)	Loss/tok 3.1371 (3.0386)	LR 1.797e-04
0: TRAIN [4][920/1291]	Time 0.134 (0.088)	Data 1.27e-04 (3.27e-04)	Tok/s 262213 (242546)	Loss/tok 3.2204 (3.0385)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][930/1291]	Time 0.066 (0.089)	Data 1.13e-04 (3.25e-04)	Tok/s 234191 (242609)	Loss/tok 2.7720 (3.0389)	LR 1.797e-04
0: TRAIN [4][940/1291]	Time 0.035 (0.088)	Data 1.10e-04 (3.23e-04)	Tok/s 225204 (242620)	Loss/tok 2.5569 (3.0392)	LR 1.797e-04
0: TRAIN [4][950/1291]	Time 0.067 (0.088)	Data 1.10e-04 (3.20e-04)	Tok/s 235004 (242563)	Loss/tok 2.8317 (3.0383)	LR 1.797e-04
0: TRAIN [4][960/1291]	Time 0.036 (0.088)	Data 1.05e-04 (3.18e-04)	Tok/s 221428 (242552)	Loss/tok 2.5611 (3.0385)	LR 1.797e-04
0: TRAIN [4][970/1291]	Time 0.135 (0.088)	Data 1.09e-04 (3.16e-04)	Tok/s 263193 (242564)	Loss/tok 3.1245 (3.0395)	LR 1.797e-04
0: TRAIN [4][980/1291]	Time 0.099 (0.088)	Data 1.09e-04 (3.14e-04)	Tok/s 255768 (242607)	Loss/tok 3.0003 (3.0401)	LR 1.797e-04
0: TRAIN [4][990/1291]	Time 0.067 (0.088)	Data 1.11e-04 (3.12e-04)	Tok/s 231882 (242582)	Loss/tok 2.8513 (3.0399)	LR 1.797e-04
0: TRAIN [4][1000/1291]	Time 0.099 (0.089)	Data 1.09e-04 (3.10e-04)	Tok/s 255549 (242653)	Loss/tok 2.9824 (3.0403)	LR 1.797e-04
0: TRAIN [4][1010/1291]	Time 0.035 (0.088)	Data 1.07e-04 (3.08e-04)	Tok/s 223287 (242656)	Loss/tok 2.5148 (3.0401)	LR 1.797e-04
0: TRAIN [4][1020/1291]	Time 0.100 (0.088)	Data 1.07e-04 (3.06e-04)	Tok/s 253732 (242639)	Loss/tok 3.0675 (3.0398)	LR 1.797e-04
0: TRAIN [4][1030/1291]	Time 0.066 (0.088)	Data 1.19e-04 (3.04e-04)	Tok/s 231722 (242620)	Loss/tok 2.8687 (3.0399)	LR 1.797e-04
0: TRAIN [4][1040/1291]	Time 0.173 (0.089)	Data 1.11e-04 (3.02e-04)	Tok/s 254450 (242661)	Loss/tok 3.4118 (3.0417)	LR 1.797e-04
0: TRAIN [4][1050/1291]	Time 0.174 (0.089)	Data 1.44e-04 (3.00e-04)	Tok/s 255228 (242665)	Loss/tok 3.3064 (3.0422)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1060/1291]	Time 0.100 (0.089)	Data 1.09e-04 (2.99e-04)	Tok/s 255192 (242778)	Loss/tok 3.0942 (3.0437)	LR 1.797e-04
0: TRAIN [4][1070/1291]	Time 0.099 (0.089)	Data 1.11e-04 (2.97e-04)	Tok/s 254938 (242813)	Loss/tok 3.0819 (3.0446)	LR 1.797e-04
0: TRAIN [4][1080/1291]	Time 0.066 (0.089)	Data 1.10e-04 (2.95e-04)	Tok/s 230838 (242772)	Loss/tok 2.8566 (3.0446)	LR 1.797e-04
0: TRAIN [4][1090/1291]	Time 0.099 (0.089)	Data 1.13e-04 (2.93e-04)	Tok/s 254928 (242753)	Loss/tok 3.0403 (3.0437)	LR 1.797e-04
0: TRAIN [4][1100/1291]	Time 0.067 (0.089)	Data 1.08e-04 (2.92e-04)	Tok/s 234010 (242726)	Loss/tok 2.8597 (3.0431)	LR 1.797e-04
0: TRAIN [4][1110/1291]	Time 0.099 (0.089)	Data 1.10e-04 (2.90e-04)	Tok/s 254227 (242760)	Loss/tok 2.9913 (3.0441)	LR 1.797e-04
0: TRAIN [4][1120/1291]	Time 0.099 (0.089)	Data 1.12e-04 (2.88e-04)	Tok/s 251719 (242812)	Loss/tok 3.0313 (3.0451)	LR 1.797e-04
0: TRAIN [4][1130/1291]	Time 0.099 (0.089)	Data 1.09e-04 (2.87e-04)	Tok/s 253086 (242851)	Loss/tok 3.2283 (3.0455)	LR 1.797e-04
0: TRAIN [4][1140/1291]	Time 0.135 (0.089)	Data 1.11e-04 (2.85e-04)	Tok/s 261417 (242872)	Loss/tok 3.1621 (3.0459)	LR 1.797e-04
0: TRAIN [4][1150/1291]	Time 0.035 (0.089)	Data 1.07e-04 (2.84e-04)	Tok/s 220891 (242816)	Loss/tok 2.5061 (3.0453)	LR 1.797e-04
0: TRAIN [4][1160/1291]	Time 0.099 (0.089)	Data 1.34e-04 (2.82e-04)	Tok/s 252508 (242736)	Loss/tok 3.0288 (3.0443)	LR 1.797e-04
0: TRAIN [4][1170/1291]	Time 0.100 (0.089)	Data 1.11e-04 (2.81e-04)	Tok/s 254337 (242730)	Loss/tok 3.0095 (3.0439)	LR 1.797e-04
0: TRAIN [4][1180/1291]	Time 0.066 (0.089)	Data 1.12e-04 (2.80e-04)	Tok/s 229815 (242725)	Loss/tok 2.9644 (3.0433)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1190/1291]	Time 0.135 (0.089)	Data 1.10e-04 (2.78e-04)	Tok/s 262519 (242788)	Loss/tok 3.2037 (3.0448)	LR 1.797e-04
0: TRAIN [4][1200/1291]	Time 0.099 (0.089)	Data 1.07e-04 (2.77e-04)	Tok/s 256146 (242728)	Loss/tok 2.9739 (3.0439)	LR 1.797e-04
0: TRAIN [4][1210/1291]	Time 0.135 (0.089)	Data 1.09e-04 (2.75e-04)	Tok/s 258402 (242734)	Loss/tok 3.1729 (3.0443)	LR 1.797e-04
0: TRAIN [4][1220/1291]	Time 0.036 (0.089)	Data 1.08e-04 (2.74e-04)	Tok/s 223649 (242729)	Loss/tok 2.5032 (3.0439)	LR 1.797e-04
0: TRAIN [4][1230/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.73e-04)	Tok/s 253844 (242713)	Loss/tok 3.0279 (3.0436)	LR 1.797e-04
0: TRAIN [4][1240/1291]	Time 0.099 (0.089)	Data 1.06e-04 (2.71e-04)	Tok/s 255160 (242722)	Loss/tok 3.1408 (3.0443)	LR 1.797e-04
0: TRAIN [4][1250/1291]	Time 0.099 (0.089)	Data 1.08e-04 (2.70e-04)	Tok/s 252787 (242714)	Loss/tok 3.0013 (3.0450)	LR 1.797e-04
0: TRAIN [4][1260/1291]	Time 0.100 (0.089)	Data 1.21e-04 (2.69e-04)	Tok/s 255998 (242781)	Loss/tok 3.0338 (3.0464)	LR 1.797e-04
0: TRAIN [4][1270/1291]	Time 0.066 (0.089)	Data 1.14e-04 (2.68e-04)	Tok/s 232735 (242749)	Loss/tok 2.9055 (3.0455)	LR 1.797e-04
0: TRAIN [4][1280/1291]	Time 0.035 (0.089)	Data 1.11e-04 (2.66e-04)	Tok/s 222376 (242725)	Loss/tok 2.4793 (3.0456)	LR 1.797e-04
0: TRAIN [4][1290/1291]	Time 0.066 (0.089)	Data 4.51e-05 (2.68e-04)	Tok/s 234315 (242723)	Loss/tok 2.8718 (3.0452)	LR 1.797e-04
:::MLLOG {"namespace": "", "time_ms": 1592447016271, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592447016271, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.386 (0.386)	Decoder iters 101.0 (101.0)	Tok/s 42862 (42862)
0: Running moses detokenizer
0: BLEU(score=24.25880485597231, counts=[37213, 18696, 10731, 6382], totals=[65499, 62496, 59493, 56495], precisions=[56.8146078566085, 29.91551459293395, 18.037416166607837, 11.296574918134349], bp=1.0, sys_len=65499, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592447018113, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2426, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592447018113, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.0439	Test BLEU: 24.26
0: Performance: Epoch: 4	Training: 1941760 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592447018113, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592447018113, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:44 PM
RESULT,RNN_TRANSLATOR,,618,nvidia,2020-06-17 07:13:26 PM
ENDING TIMING RUN AT 2020-06-17 07:23:45 PM
RESULT,RNN_TRANSLATOR,,619,nvidia,2020-06-17 07:13:26 PM
