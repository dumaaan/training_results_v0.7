+ echo 'Beginning trial 3 of 5'
Beginning trial 3 of 5
+ srun --ntasks=16 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593022178070, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593022178101, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593022178101, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593022178101, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593022178101, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
Clearing cache on circe-n044
Clearing cache on circe-n040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n047
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n045
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n051
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=16 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593022185187, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185252, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185256, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185274, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185283, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185286, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185286, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185299, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185323, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185323, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185343, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185377, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185378, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022185397, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=256 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14177838/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ LR=4.0e-3
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 3 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' 256 -gt 16 ']'
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ LR=4.0e-3
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ '[' -n 2 ']'
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ '[' -n 4 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=4.0e-3
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' -n 6 ']'
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' -n 3 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ declare -a CMD
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 10 ']'
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 12 ']'
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=12
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DIST_OPTS=
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 0 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
running benchmark
+ '[' -n 6 ']'
running benchmark
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' -n 13 ']'
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 6 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ echo 'running benchmark'
running benchmark
+ '[' -n 12 ']'
+ '[' -n 4 ']'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 14 ']'
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=2259
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=283
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 2 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 4 ']'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=283
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 14 ']'
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ DIST_OPTS=
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ declare -a CMD
+ NUMEPOCHS=12
+ '[' -n 3 ']'
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ DIST_OPTS=
+ LR=4.0e-3
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ '[' -n 7 ']'
+ '[' -n 9 ']'
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TARGET=24.0
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=16
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 5 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 13 ']'
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ LR=4.0e-3
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ NUMEPOCHS=12
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=12
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 4 ']'
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ NUMEPOCHS=12
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' -n 15 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=12
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ '[' -n 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 13 ']'
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ echo 'running benchmark'
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 12 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 15 ']'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=4.0e-3
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DIST_OPTS=
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ '[' -n 1 ']'
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 2 ']'
+ DIST_OPTS=
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' -n 3 ']'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 4 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 16 ']'
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 10 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=12
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ NUMEPOCHS=12
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=12
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 5 ']'
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS=
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
running benchmark
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' -n 8 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ REMAIN_STEPS=2259
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
running benchmark
+ '[' -n 10 ']'
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ LR=4.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ LR=4.0e-3
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=4.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ LR=4.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DIST_OPTS=
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 6 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 16 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ LR=4.0e-3
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 5 ']'
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:09:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1593022190124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190249, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190258, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190264, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190330, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190345, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190363, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190379, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190385, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190394, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190395, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190401, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190404, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190417, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190432, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190438, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190451, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190454, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190466, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190469, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190498, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190539, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190544, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190564, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190574, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190746, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190939, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190952, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190951, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190963, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190979, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190979, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190980, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190982, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190981, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190981, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022190997, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191007, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191007, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191007, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191013, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191016, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191015, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191027, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191037, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191050, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191071, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191085, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191118, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191122, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191130, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191157, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191197, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191203, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191245, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022191392, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=16067130, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=283, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=12, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.004, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=2259, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=16, test_loader_workers=0, train_batch_size=32, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1013212258
:::MLLOG {"namespace": "", "time_ms": 1593022213807, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1013212258, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 319864462
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.004}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.004
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593022217291, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593022217292, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.004, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593022217292, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593022217292, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593022217292, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593022220623, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593022221820, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593022221821, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593022222057, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8192, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593022222058, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3964928, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593022222059, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 2259, 'decay_interval': 283, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2259
0: Scheduler decay interval: 283
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593022222060, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593022222061, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593022222061, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 283, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593022222061, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593022222061, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593022222061, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 2259, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593022222061, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593022222062, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022222062, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 3532454112
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/484]	Time 0.394 (0.394)	Data 2.88e-01 (2.88e-01)	Tok/s 5340 (5340)	Loss/tok 10.6957 (10.6957)	LR 4.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/484]	Time 0.034 (0.088)	Data 1.02e-04 (2.63e-02)	Tok/s 60996 (40260)	Loss/tok 9.4254 (10.1035)	LR 4.809e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][20/484]	Time 0.029 (0.061)	Data 7.61e-05 (1.38e-02)	Tok/s 45867 (46460)	Loss/tok 8.9807 (9.7238)	LR 5.916e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][30/484]	Time 0.029 (0.053)	Data 8.75e-05 (9.37e-03)	Tok/s 45242 (48071)	Loss/tok 8.8193 (9.4915)	LR 7.279e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [0][40/484]	Time 0.040 (0.052)	Data 8.92e-05 (7.10e-03)	Tok/s 75106 (48928)	Loss/tok 8.8325 (9.3117)	LR 8.751e-05
0: TRAIN [0][50/484]	Time 0.029 (0.048)	Data 8.06e-05 (5.73e-03)	Tok/s 45285 (49666)	Loss/tok 8.2185 (9.1650)	LR 1.102e-04
0: TRAIN [0][60/484]	Time 0.040 (0.045)	Data 7.92e-05 (4.80e-03)	Tok/s 73546 (50722)	Loss/tok 8.1145 (9.0066)	LR 1.387e-04
0: TRAIN [0][70/484]	Time 0.034 (0.044)	Data 7.49e-05 (4.14e-03)	Tok/s 61226 (50906)	Loss/tok 8.2589 (8.8791)	LR 1.746e-04
0: TRAIN [0][80/484]	Time 0.040 (0.042)	Data 8.54e-05 (3.64e-03)	Tok/s 72166 (51160)	Loss/tok 8.1632 (8.7754)	LR 2.198e-04
0: TRAIN [0][90/484]	Time 0.024 (0.041)	Data 7.41e-05 (3.25e-03)	Tok/s 27883 (51424)	Loss/tok 7.2082 (8.6898)	LR 2.767e-04
0: TRAIN [0][100/484]	Time 0.040 (0.040)	Data 7.34e-05 (2.93e-03)	Tok/s 73958 (51907)	Loss/tok 8.0697 (8.6088)	LR 3.484e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][110/484]	Time 0.035 (0.039)	Data 7.65e-05 (2.68e-03)	Tok/s 61863 (52165)	Loss/tok 8.2127 (8.6010)	LR 4.189e-04
0: TRAIN [0][120/484]	Time 0.040 (0.039)	Data 1.19e-04 (2.47e-03)	Tok/s 73434 (52473)	Loss/tok 8.0618 (8.5415)	LR 5.273e-04
0: TRAIN [0][130/484]	Time 0.029 (0.038)	Data 7.87e-05 (2.29e-03)	Tok/s 47317 (52195)	Loss/tok 7.6889 (8.4893)	LR 6.638e-04
0: TRAIN [0][140/484]	Time 0.029 (0.038)	Data 7.92e-05 (2.13e-03)	Tok/s 43594 (52242)	Loss/tok 7.2373 (8.4300)	LR 8.357e-04
0: TRAIN [0][150/484]	Time 0.034 (0.037)	Data 8.06e-05 (1.99e-03)	Tok/s 60883 (52598)	Loss/tok 7.3771 (8.3665)	LR 1.052e-03
0: TRAIN [0][160/484]	Time 0.034 (0.037)	Data 8.87e-05 (1.88e-03)	Tok/s 60717 (53041)	Loss/tok 7.0892 (8.3012)	LR 1.325e-03
0: TRAIN [0][170/484]	Time 0.029 (0.037)	Data 9.42e-05 (1.77e-03)	Tok/s 44543 (52774)	Loss/tok 6.8324 (8.2358)	LR 1.667e-03
0: TRAIN [0][180/484]	Time 0.029 (0.037)	Data 9.47e-05 (1.68e-03)	Tok/s 45144 (52858)	Loss/tok 6.4217 (8.1629)	LR 2.099e-03
0: TRAIN [0][190/484]	Time 0.047 (0.037)	Data 7.77e-05 (1.59e-03)	Tok/s 78060 (53090)	Loss/tok 6.8768 (8.0883)	LR 2.643e-03
0: TRAIN [0][200/484]	Time 0.029 (0.036)	Data 7.70e-05 (1.52e-03)	Tok/s 46432 (52760)	Loss/tok 6.6387 (8.0301)	LR 3.327e-03
0: TRAIN [0][210/484]	Time 0.034 (0.036)	Data 7.63e-05 (1.45e-03)	Tok/s 60249 (52733)	Loss/tok 6.2267 (7.9629)	LR 4.000e-03
0: TRAIN [0][220/484]	Time 0.029 (0.036)	Data 7.56e-05 (1.39e-03)	Tok/s 44557 (52443)	Loss/tok 6.1110 (7.9027)	LR 4.000e-03
0: TRAIN [0][230/484]	Time 0.034 (0.036)	Data 8.08e-05 (1.33e-03)	Tok/s 61158 (52524)	Loss/tok 6.0356 (7.8248)	LR 4.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [0][240/484]	Time 0.029 (0.035)	Data 7.80e-05 (1.28e-03)	Tok/s 45405 (52213)	Loss/tok 5.6464 (7.7596)	LR 4.000e-03
0: TRAIN [0][250/484]	Time 0.034 (0.035)	Data 1.07e-04 (1.23e-03)	Tok/s 60980 (52513)	Loss/tok 5.7491 (7.6650)	LR 4.000e-03
0: TRAIN [0][260/484]	Time 0.034 (0.035)	Data 7.70e-05 (1.19e-03)	Tok/s 60783 (52657)	Loss/tok 5.7411 (7.5811)	LR 4.000e-03
0: TRAIN [0][270/484]	Time 0.034 (0.035)	Data 7.56e-05 (1.15e-03)	Tok/s 61078 (52719)	Loss/tok 5.3798 (7.4983)	LR 4.000e-03
0: TRAIN [0][280/484]	Time 0.029 (0.035)	Data 7.89e-05 (1.11e-03)	Tok/s 44390 (52499)	Loss/tok 5.0723 (7.4308)	LR 4.000e-03
0: TRAIN [0][290/484]	Time 0.034 (0.035)	Data 8.01e-05 (1.07e-03)	Tok/s 62314 (52543)	Loss/tok 5.3318 (7.3493)	LR 4.000e-03
0: TRAIN [0][300/484]	Time 0.034 (0.035)	Data 9.25e-05 (1.04e-03)	Tok/s 65506 (52680)	Loss/tok 4.9978 (7.2658)	LR 4.000e-03
0: TRAIN [0][310/484]	Time 0.034 (0.035)	Data 8.13e-05 (1.01e-03)	Tok/s 63084 (52688)	Loss/tok 4.9889 (7.1908)	LR 4.000e-03
0: TRAIN [0][320/484]	Time 0.040 (0.035)	Data 7.65e-05 (9.82e-04)	Tok/s 74139 (52978)	Loss/tok 4.8091 (7.1024)	LR 4.000e-03
0: TRAIN [0][330/484]	Time 0.028 (0.035)	Data 9.44e-05 (9.55e-04)	Tok/s 43506 (53145)	Loss/tok 4.1160 (7.0125)	LR 4.000e-03
0: TRAIN [0][340/484]	Time 0.029 (0.034)	Data 9.06e-05 (9.29e-04)	Tok/s 43451 (53047)	Loss/tok 4.0922 (6.9453)	LR 4.000e-03
0: TRAIN [0][350/484]	Time 0.029 (0.034)	Data 1.24e-04 (9.06e-04)	Tok/s 47312 (53092)	Loss/tok 3.8289 (6.8676)	LR 4.000e-03
0: TRAIN [0][360/484]	Time 0.047 (0.035)	Data 7.87e-05 (8.83e-04)	Tok/s 80723 (53320)	Loss/tok 4.5350 (6.7833)	LR 4.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [0][370/484]	Time 0.034 (0.034)	Data 8.58e-05 (8.61e-04)	Tok/s 60494 (53299)	Loss/tok 4.2723 (6.7187)	LR 4.000e-03
0: TRAIN [0][380/484]	Time 0.028 (0.034)	Data 1.07e-04 (8.41e-04)	Tok/s 45497 (53224)	Loss/tok 3.9823 (6.6612)	LR 4.000e-03
0: TRAIN [0][390/484]	Time 0.029 (0.034)	Data 1.23e-04 (8.22e-04)	Tok/s 44907 (53272)	Loss/tok 3.6598 (6.5975)	LR 4.000e-03
0: TRAIN [0][400/484]	Time 0.035 (0.034)	Data 8.15e-05 (8.04e-04)	Tok/s 60271 (53290)	Loss/tok 3.8093 (6.5343)	LR 4.000e-03
0: TRAIN [0][410/484]	Time 0.034 (0.034)	Data 1.06e-04 (7.87e-04)	Tok/s 61317 (53242)	Loss/tok 3.9593 (6.4794)	LR 4.000e-03
0: TRAIN [0][420/484]	Time 0.034 (0.034)	Data 1.05e-04 (7.70e-04)	Tok/s 62306 (53420)	Loss/tok 4.1029 (6.4121)	LR 4.000e-03
0: TRAIN [0][430/484]	Time 0.035 (0.034)	Data 1.08e-04 (7.55e-04)	Tok/s 60530 (53411)	Loss/tok 4.0543 (6.3585)	LR 4.000e-03
0: TRAIN [0][440/484]	Time 0.040 (0.034)	Data 7.77e-05 (7.40e-04)	Tok/s 74839 (53453)	Loss/tok 4.1704 (6.3012)	LR 4.000e-03
0: TRAIN [0][450/484]	Time 0.040 (0.034)	Data 1.03e-04 (7.25e-04)	Tok/s 73211 (53468)	Loss/tok 4.1988 (6.2510)	LR 4.000e-03
0: TRAIN [0][460/484]	Time 0.028 (0.034)	Data 1.07e-04 (7.12e-04)	Tok/s 45145 (53407)	Loss/tok 3.6870 (6.2073)	LR 4.000e-03
0: TRAIN [0][470/484]	Time 0.029 (0.034)	Data 7.99e-05 (6.99e-04)	Tok/s 43498 (53429)	Loss/tok 3.4045 (6.1604)	LR 4.000e-03
0: TRAIN [0][480/484]	Time 0.029 (0.034)	Data 9.73e-05 (6.86e-04)	Tok/s 42681 (53614)	Loss/tok 3.4039 (6.1050)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022238593, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022238593, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.334 (0.334)	Decoder iters 149.0 (149.0)	Tok/s 7353 (7353)
0: Running moses detokenizer
0: BLEU(score=15.887926472242176, counts=[31255, 13023, 6414, 3275], totals=[65120, 62117, 59114, 56116], precisions=[47.99600737100737, 20.96527520646522, 10.850221605710999, 5.83612516929218], bp=1.0, sys_len=65120, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022239142, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1589, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022239142, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 6.0854	Test BLEU: 15.89
0: Performance: Epoch: 0	Training: 13698543 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593022239142, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022239142, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022239143, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 3754340669
0: TRAIN [1][0/484]	Time 0.389 (0.389)	Data 3.43e-01 (3.43e-01)	Tok/s 3487 (3487)	Loss/tok 3.8060 (3.8060)	LR 4.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [1][10/484]	Time 0.023 (0.067)	Data 7.87e-05 (3.13e-02)	Tok/s 28535 (52519)	Loss/tok 3.1568 (3.9130)	LR 4.000e-03
0: TRAIN [1][20/484]	Time 0.029 (0.051)	Data 7.72e-05 (1.64e-02)	Tok/s 46416 (55447)	Loss/tok 3.5398 (3.9643)	LR 4.000e-03
0: TRAIN [1][30/484]	Time 0.029 (0.046)	Data 7.99e-05 (1.11e-02)	Tok/s 44067 (57109)	Loss/tok 3.5226 (3.9191)	LR 4.000e-03
0: TRAIN [1][40/484]	Time 0.029 (0.042)	Data 7.84e-05 (8.44e-03)	Tok/s 43580 (54482)	Loss/tok 3.2515 (3.8369)	LR 4.000e-03
0: TRAIN [1][50/484]	Time 0.047 (0.041)	Data 7.94e-05 (6.80e-03)	Tok/s 78935 (56527)	Loss/tok 4.4313 (3.8681)	LR 4.000e-03
0: TRAIN [1][60/484]	Time 0.023 (0.040)	Data 7.84e-05 (5.70e-03)	Tok/s 29457 (56027)	Loss/tok 3.1037 (3.8300)	LR 4.000e-03
0: TRAIN [1][70/484]	Time 0.029 (0.038)	Data 7.80e-05 (4.91e-03)	Tok/s 45527 (54871)	Loss/tok 3.5475 (3.8051)	LR 4.000e-03
0: TRAIN [1][80/484]	Time 0.040 (0.037)	Data 7.72e-05 (4.31e-03)	Tok/s 73684 (54715)	Loss/tok 4.0281 (3.7984)	LR 4.000e-03
0: TRAIN [1][90/484]	Time 0.029 (0.037)	Data 8.08e-05 (3.85e-03)	Tok/s 46470 (54662)	Loss/tok 3.5647 (3.7962)	LR 4.000e-03
0: TRAIN [1][100/484]	Time 0.034 (0.037)	Data 8.11e-05 (3.48e-03)	Tok/s 64196 (55181)	Loss/tok 3.3578 (3.7960)	LR 4.000e-03
0: TRAIN [1][110/484]	Time 0.034 (0.036)	Data 8.06e-05 (3.17e-03)	Tok/s 59846 (54977)	Loss/tok 3.8048 (3.7929)	LR 4.000e-03
0: TRAIN [1][120/484]	Time 0.029 (0.036)	Data 7.96e-05 (2.91e-03)	Tok/s 46294 (54443)	Loss/tok 3.6277 (3.7770)	LR 4.000e-03
0: TRAIN [1][130/484]	Time 0.040 (0.036)	Data 7.82e-05 (2.70e-03)	Tok/s 74569 (54722)	Loss/tok 3.6265 (3.7684)	LR 4.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [1][140/484]	Time 0.040 (0.036)	Data 7.96e-05 (2.51e-03)	Tok/s 74085 (55437)	Loss/tok 3.6275 (3.7600)	LR 4.000e-03
0: TRAIN [1][150/484]	Time 0.029 (0.035)	Data 8.32e-05 (2.35e-03)	Tok/s 44373 (55419)	Loss/tok 3.2376 (3.7532)	LR 4.000e-03
0: TRAIN [1][160/484]	Time 0.034 (0.035)	Data 8.06e-05 (2.21e-03)	Tok/s 60588 (55763)	Loss/tok 3.7334 (3.7537)	LR 4.000e-03
0: TRAIN [1][170/484]	Time 0.023 (0.035)	Data 7.87e-05 (2.09e-03)	Tok/s 28836 (55531)	Loss/tok 2.9784 (3.7422)	LR 4.000e-03
0: TRAIN [1][180/484]	Time 0.029 (0.035)	Data 7.72e-05 (1.98e-03)	Tok/s 43822 (54903)	Loss/tok 3.3107 (3.7299)	LR 4.000e-03
0: TRAIN [1][190/484]	Time 0.029 (0.035)	Data 7.82e-05 (1.88e-03)	Tok/s 45086 (54674)	Loss/tok 3.3700 (3.7277)	LR 4.000e-03
0: TRAIN [1][200/484]	Time 0.023 (0.034)	Data 7.70e-05 (1.79e-03)	Tok/s 27323 (54378)	Loss/tok 3.0291 (3.7169)	LR 4.000e-03
0: TRAIN [1][210/484]	Time 0.034 (0.034)	Data 8.25e-05 (1.71e-03)	Tok/s 61551 (54502)	Loss/tok 3.7847 (3.7148)	LR 4.000e-03
0: TRAIN [1][220/484]	Time 0.029 (0.034)	Data 7.61e-05 (1.63e-03)	Tok/s 44361 (54242)	Loss/tok 3.4476 (3.7046)	LR 4.000e-03
0: TRAIN [1][230/484]	Time 0.023 (0.034)	Data 7.96e-05 (1.57e-03)	Tok/s 28428 (54258)	Loss/tok 3.0600 (3.7062)	LR 4.000e-03
0: TRAIN [1][240/484]	Time 0.029 (0.034)	Data 7.87e-05 (1.50e-03)	Tok/s 45957 (54360)	Loss/tok 3.4977 (3.7013)	LR 4.000e-03
0: TRAIN [1][250/484]	Time 0.029 (0.034)	Data 9.20e-05 (1.45e-03)	Tok/s 48604 (54228)	Loss/tok 3.1329 (3.6949)	LR 4.000e-03
0: TRAIN [1][260/484]	Time 0.040 (0.034)	Data 7.94e-05 (1.40e-03)	Tok/s 72470 (54273)	Loss/tok 3.6008 (3.6950)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [1][270/484]	Time 0.029 (0.034)	Data 8.34e-05 (1.35e-03)	Tok/s 47460 (54484)	Loss/tok 3.4648 (3.6867)	LR 4.000e-03
0: TRAIN [1][280/484]	Time 0.034 (0.034)	Data 8.23e-05 (1.30e-03)	Tok/s 62027 (54744)	Loss/tok 3.4654 (3.6834)	LR 4.000e-03
0: TRAIN [1][290/484]	Time 0.029 (0.034)	Data 9.30e-05 (1.26e-03)	Tok/s 43625 (54676)	Loss/tok 3.2821 (3.6766)	LR 4.000e-03
0: TRAIN [1][300/484]	Time 0.029 (0.034)	Data 7.65e-05 (1.22e-03)	Tok/s 47889 (54623)	Loss/tok 3.3303 (3.6745)	LR 4.000e-03
0: TRAIN [1][310/484]	Time 0.029 (0.034)	Data 8.39e-05 (1.18e-03)	Tok/s 46162 (54545)	Loss/tok 3.3069 (3.6735)	LR 4.000e-03
0: TRAIN [1][320/484]	Time 0.029 (0.034)	Data 9.47e-05 (1.15e-03)	Tok/s 45501 (54433)	Loss/tok 3.3509 (3.6717)	LR 4.000e-03
0: TRAIN [1][330/484]	Time 0.029 (0.034)	Data 7.96e-05 (1.12e-03)	Tok/s 42178 (54299)	Loss/tok 3.3869 (3.6668)	LR 4.000e-03
0: TRAIN [1][340/484]	Time 0.029 (0.034)	Data 7.89e-05 (1.09e-03)	Tok/s 43016 (54443)	Loss/tok 3.1630 (3.6604)	LR 4.000e-03
0: TRAIN [1][350/484]	Time 0.029 (0.034)	Data 7.94e-05 (1.06e-03)	Tok/s 46277 (54393)	Loss/tok 3.4030 (3.6584)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [1][360/484]	Time 0.029 (0.033)	Data 7.41e-05 (1.03e-03)	Tok/s 47758 (54261)	Loss/tok 3.5331 (3.6546)	LR 4.000e-03
0: TRAIN [1][370/484]	Time 0.029 (0.033)	Data 8.46e-05 (1.01e-03)	Tok/s 43716 (54165)	Loss/tok 2.9498 (3.6508)	LR 4.000e-03
0: TRAIN [1][380/484]	Time 0.029 (0.033)	Data 7.75e-05 (9.82e-04)	Tok/s 45634 (54013)	Loss/tok 3.0338 (3.6434)	LR 4.000e-03
0: TRAIN [1][390/484]	Time 0.029 (0.033)	Data 7.80e-05 (9.59e-04)	Tok/s 45673 (53992)	Loss/tok 3.4105 (3.6384)	LR 4.000e-03
0: TRAIN [1][400/484]	Time 0.029 (0.033)	Data 7.92e-05 (9.37e-04)	Tok/s 43574 (54154)	Loss/tok 3.3483 (3.6403)	LR 4.000e-03
0: TRAIN [1][410/484]	Time 0.029 (0.033)	Data 7.53e-05 (9.16e-04)	Tok/s 44148 (54012)	Loss/tok 3.0973 (3.6364)	LR 4.000e-03
0: TRAIN [1][420/484]	Time 0.034 (0.033)	Data 7.94e-05 (8.97e-04)	Tok/s 60390 (54062)	Loss/tok 3.9632 (3.6322)	LR 4.000e-03
0: TRAIN [1][430/484]	Time 0.040 (0.033)	Data 7.84e-05 (8.78e-04)	Tok/s 72137 (54214)	Loss/tok 4.0674 (3.6318)	LR 4.000e-03
0: TRAIN [1][440/484]	Time 0.034 (0.033)	Data 9.44e-05 (8.60e-04)	Tok/s 61180 (54174)	Loss/tok 3.3732 (3.6275)	LR 4.000e-03
0: TRAIN [1][450/484]	Time 0.035 (0.033)	Data 1.21e-04 (8.43e-04)	Tok/s 60553 (54045)	Loss/tok 3.4823 (3.6253)	LR 4.000e-03
0: TRAIN [1][460/484]	Time 0.034 (0.033)	Data 7.80e-05 (8.27e-04)	Tok/s 62808 (54104)	Loss/tok 3.5685 (3.6201)	LR 4.000e-03
0: TRAIN [1][470/484]	Time 0.034 (0.033)	Data 1.12e-04 (8.12e-04)	Tok/s 61106 (54030)	Loss/tok 3.4633 (3.6170)	LR 4.000e-03
0: TRAIN [1][480/484]	Time 0.023 (0.033)	Data 8.20e-05 (7.97e-04)	Tok/s 27256 (53987)	Loss/tok 2.8925 (3.6170)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022255215, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593022255215, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.420 (0.420)	Decoder iters 149.0 (149.0)	Tok/s 6069 (6069)
0: Running moses detokenizer
0: BLEU(score=20.290122317583304, counts=[34272, 15867, 8507, 4769], totals=[64479, 61476, 58474, 55476], precisions=[53.15218908481831, 25.810072223306655, 14.54834627355748, 8.596510202610137], bp=0.9969494042508488, sys_len=64479, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022255804, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2029, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593022255805, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.6242	Test BLEU: 20.29
0: Performance: Epoch: 1	Training: 13808329 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593022255805, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593022255805, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022255805, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 4095948496
0: TRAIN [2][0/484]	Time 0.389 (0.389)	Data 3.48e-01 (3.48e-01)	Tok/s 5456 (5456)	Loss/tok 3.5656 (3.5656)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [2][10/484]	Time 0.047 (0.068)	Data 8.03e-05 (3.17e-02)	Tok/s 79084 (56117)	Loss/tok 3.6521 (3.4539)	LR 4.000e-03
0: TRAIN [2][20/484]	Time 0.035 (0.051)	Data 7.56e-05 (1.67e-02)	Tok/s 61407 (56060)	Loss/tok 3.3161 (3.4376)	LR 4.000e-03
0: TRAIN [2][30/484]	Time 0.040 (0.046)	Data 8.06e-05 (1.13e-02)	Tok/s 73500 (56600)	Loss/tok 3.4795 (3.4536)	LR 4.000e-03
0: TRAIN [2][40/484]	Time 0.034 (0.043)	Data 8.01e-05 (8.58e-03)	Tok/s 60111 (56300)	Loss/tok 3.6053 (3.4566)	LR 4.000e-03
0: TRAIN [2][50/484]	Time 0.040 (0.041)	Data 8.44e-05 (6.91e-03)	Tok/s 74036 (56523)	Loss/tok 3.6530 (3.4582)	LR 4.000e-03
0: TRAIN [2][60/484]	Time 0.047 (0.040)	Data 8.25e-05 (5.80e-03)	Tok/s 79029 (56252)	Loss/tok 3.4070 (3.4243)	LR 4.000e-03
0: TRAIN [2][70/484]	Time 0.029 (0.039)	Data 1.08e-04 (4.99e-03)	Tok/s 47715 (55966)	Loss/tok 3.3794 (3.4367)	LR 4.000e-03
0: TRAIN [2][80/484]	Time 0.023 (0.038)	Data 1.05e-04 (4.39e-03)	Tok/s 31196 (55693)	Loss/tok 2.9527 (3.4409)	LR 4.000e-03
0: TRAIN [2][90/484]	Time 0.029 (0.037)	Data 1.09e-04 (3.92e-03)	Tok/s 43508 (55297)	Loss/tok 3.1849 (3.4391)	LR 4.000e-03
0: TRAIN [2][100/484]	Time 0.040 (0.037)	Data 1.04e-04 (3.54e-03)	Tok/s 74272 (55017)	Loss/tok 3.0988 (3.4255)	LR 4.000e-03
0: TRAIN [2][110/484]	Time 0.034 (0.036)	Data 1.07e-04 (3.23e-03)	Tok/s 61096 (54632)	Loss/tok 3.1761 (3.4086)	LR 4.000e-03
0: TRAIN [2][120/484]	Time 0.028 (0.036)	Data 9.87e-05 (2.97e-03)	Tok/s 42970 (53930)	Loss/tok 2.9631 (3.3939)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [2][130/484]	Time 0.029 (0.035)	Data 7.68e-05 (2.75e-03)	Tok/s 47026 (53969)	Loss/tok 2.9863 (3.3991)	LR 4.000e-03
0: TRAIN [2][140/484]	Time 0.029 (0.035)	Data 8.20e-05 (2.56e-03)	Tok/s 46712 (53840)	Loss/tok 3.1124 (3.3939)	LR 4.000e-03
0: TRAIN [2][150/484]	Time 0.034 (0.035)	Data 7.61e-05 (2.40e-03)	Tok/s 62406 (53943)	Loss/tok 3.5274 (3.4019)	LR 4.000e-03
0: TRAIN [2][160/484]	Time 0.047 (0.035)	Data 7.68e-05 (2.25e-03)	Tok/s 77925 (53790)	Loss/tok 3.7788 (3.3993)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][170/484]	Time 0.047 (0.035)	Data 7.77e-05 (2.13e-03)	Tok/s 81035 (53920)	Loss/tok 3.7847 (3.3980)	LR 4.000e-03
0: TRAIN [2][180/484]	Time 0.034 (0.034)	Data 7.61e-05 (2.01e-03)	Tok/s 61035 (53817)	Loss/tok 3.6804 (3.3980)	LR 4.000e-03
0: TRAIN [2][190/484]	Time 0.024 (0.034)	Data 7.51e-05 (1.91e-03)	Tok/s 28257 (53298)	Loss/tok 2.8104 (3.3878)	LR 4.000e-03
0: TRAIN [2][200/484]	Time 0.040 (0.034)	Data 7.72e-05 (1.82e-03)	Tok/s 72140 (53368)	Loss/tok 3.2912 (3.3824)	LR 4.000e-03
0: TRAIN [2][210/484]	Time 0.034 (0.034)	Data 7.94e-05 (1.74e-03)	Tok/s 59240 (53150)	Loss/tok 3.6351 (3.3800)	LR 4.000e-03
0: TRAIN [2][220/484]	Time 0.029 (0.034)	Data 7.61e-05 (1.66e-03)	Tok/s 44827 (52846)	Loss/tok 2.9862 (3.3738)	LR 4.000e-03
0: TRAIN [2][230/484]	Time 0.029 (0.034)	Data 7.68e-05 (1.60e-03)	Tok/s 44638 (52961)	Loss/tok 3.3064 (3.3769)	LR 4.000e-03
0: TRAIN [2][240/484]	Time 0.029 (0.034)	Data 7.92e-05 (1.53e-03)	Tok/s 48525 (53073)	Loss/tok 2.7726 (3.3805)	LR 4.000e-03
0: TRAIN [2][250/484]	Time 0.034 (0.034)	Data 8.03e-05 (1.48e-03)	Tok/s 60420 (53543)	Loss/tok 3.2271 (3.3899)	LR 4.000e-03
0: TRAIN [2][260/484]	Time 0.023 (0.034)	Data 7.96e-05 (1.42e-03)	Tok/s 28098 (53350)	Loss/tok 2.6492 (3.3873)	LR 4.000e-03
0: TRAIN [2][270/484]	Time 0.029 (0.034)	Data 8.03e-05 (1.37e-03)	Tok/s 44835 (53286)	Loss/tok 3.4499 (3.3854)	LR 4.000e-03
0: TRAIN [2][280/484]	Time 0.029 (0.033)	Data 7.65e-05 (1.33e-03)	Tok/s 43885 (53283)	Loss/tok 3.1010 (3.3848)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [2][290/484]	Time 0.029 (0.033)	Data 7.75e-05 (1.29e-03)	Tok/s 41601 (53135)	Loss/tok 3.1111 (3.3795)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][300/484]	Time 0.047 (0.033)	Data 1.10e-04 (1.25e-03)	Tok/s 80030 (53190)	Loss/tok 3.6409 (3.3857)	LR 4.000e-03
0: TRAIN [2][310/484]	Time 0.040 (0.033)	Data 8.23e-05 (1.21e-03)	Tok/s 72877 (53333)	Loss/tok 3.7670 (3.3873)	LR 4.000e-03
0: TRAIN [2][320/484]	Time 0.040 (0.033)	Data 8.18e-05 (1.17e-03)	Tok/s 72993 (53349)	Loss/tok 3.6830 (3.3876)	LR 4.000e-03
0: TRAIN [2][330/484]	Time 0.029 (0.033)	Data 7.87e-05 (1.14e-03)	Tok/s 45778 (53074)	Loss/tok 2.9668 (3.3828)	LR 4.000e-03
0: TRAIN [2][340/484]	Time 0.029 (0.033)	Data 7.89e-05 (1.11e-03)	Tok/s 46775 (53047)	Loss/tok 3.3339 (3.3776)	LR 4.000e-03
0: TRAIN [2][350/484]	Time 0.040 (0.033)	Data 1.08e-04 (1.08e-03)	Tok/s 70269 (53277)	Loss/tok 3.5990 (3.3812)	LR 4.000e-03
0: TRAIN [2][360/484]	Time 0.040 (0.033)	Data 9.42e-05 (1.05e-03)	Tok/s 73051 (53508)	Loss/tok 3.5405 (3.3826)	LR 4.000e-03
0: TRAIN [2][370/484]	Time 0.040 (0.033)	Data 8.68e-05 (1.03e-03)	Tok/s 73737 (53748)	Loss/tok 3.5600 (3.3855)	LR 4.000e-03
0: TRAIN [2][380/484]	Time 0.040 (0.033)	Data 9.42e-05 (1.00e-03)	Tok/s 75306 (53822)	Loss/tok 3.6183 (3.3892)	LR 4.000e-03
0: TRAIN [2][390/484]	Time 0.029 (0.033)	Data 1.09e-04 (9.81e-04)	Tok/s 43951 (53879)	Loss/tok 3.3719 (3.3856)	LR 4.000e-03
0: TRAIN [2][400/484]	Time 0.029 (0.033)	Data 8.30e-05 (9.60e-04)	Tok/s 43131 (53737)	Loss/tok 3.2218 (3.3821)	LR 4.000e-03
0: TRAIN [2][410/484]	Time 0.034 (0.033)	Data 1.09e-04 (9.39e-04)	Tok/s 60239 (53671)	Loss/tok 3.3491 (3.3818)	LR 4.000e-03
0: TRAIN [2][420/484]	Time 0.034 (0.033)	Data 1.13e-04 (9.19e-04)	Tok/s 61562 (53726)	Loss/tok 3.4788 (3.3810)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [2][430/484]	Time 0.029 (0.033)	Data 1.01e-04 (9.00e-04)	Tok/s 43417 (53897)	Loss/tok 3.3573 (3.3830)	LR 4.000e-03
0: TRAIN [2][440/484]	Time 0.039 (0.033)	Data 1.08e-04 (8.82e-04)	Tok/s 75436 (53978)	Loss/tok 3.5413 (3.3854)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][450/484]	Time 0.029 (0.033)	Data 8.37e-05 (8.65e-04)	Tok/s 43819 (53905)	Loss/tok 3.2876 (3.3864)	LR 4.000e-03
0: TRAIN [2][460/484]	Time 0.029 (0.033)	Data 8.63e-05 (8.48e-04)	Tok/s 45541 (53957)	Loss/tok 3.0269 (3.3845)	LR 4.000e-03
0: TRAIN [2][470/484]	Time 0.035 (0.033)	Data 1.06e-04 (8.32e-04)	Tok/s 62717 (53982)	Loss/tok 3.4224 (3.3814)	LR 4.000e-03
0: TRAIN [2][480/484]	Time 0.023 (0.033)	Data 1.07e-04 (8.17e-04)	Tok/s 27133 (53986)	Loss/tok 2.9246 (3.3813)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022271886, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593022271886, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.387 (0.387)	Decoder iters 140.0 (140.0)	Tok/s 6546 (6546)
0: Running moses detokenizer
0: BLEU(score=21.757010547777217, counts=[34947, 16754, 9269, 5337], totals=[62711, 59708, 56705, 53707], precisions=[55.72706542711805, 28.05989147182957, 16.34600123445904, 9.937252127283221], bp=0.9691516132697687, sys_len=62711, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022272429, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21760000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593022272430, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.3796	Test BLEU: 21.76
0: Performance: Epoch: 2	Training: 13790675 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593022272430, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593022272430, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022272431, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 318208662
0: TRAIN [3][0/484]	Time 0.385 (0.385)	Data 3.41e-01 (3.41e-01)	Tok/s 3308 (3308)	Loss/tok 3.2575 (3.2575)	LR 4.000e-03
0: TRAIN [3][10/484]	Time 0.047 (0.066)	Data 1.18e-04 (3.11e-02)	Tok/s 77942 (52516)	Loss/tok 3.7887 (3.3602)	LR 4.000e-03
0: TRAIN [3][20/484]	Time 0.034 (0.052)	Data 1.02e-04 (1.63e-02)	Tok/s 59329 (56540)	Loss/tok 3.0527 (3.4537)	LR 4.000e-03
0: TRAIN [3][30/484]	Time 0.034 (0.045)	Data 1.16e-04 (1.11e-02)	Tok/s 60394 (54092)	Loss/tok 3.3094 (3.4267)	LR 4.000e-03
0: TRAIN [3][40/484]	Time 0.034 (0.042)	Data 1.03e-04 (8.41e-03)	Tok/s 62255 (55362)	Loss/tok 3.4874 (3.4061)	LR 4.000e-03
0: TRAIN [3][50/484]	Time 0.029 (0.040)	Data 1.02e-04 (6.78e-03)	Tok/s 44007 (53887)	Loss/tok 3.0337 (3.3745)	LR 4.000e-03
0: TRAIN [3][60/484]	Time 0.040 (0.039)	Data 7.82e-05 (5.69e-03)	Tok/s 73686 (54406)	Loss/tok 3.4871 (3.3553)	LR 4.000e-03
0: TRAIN [3][70/484]	Time 0.029 (0.038)	Data 1.00e-04 (4.90e-03)	Tok/s 42805 (54173)	Loss/tok 3.0400 (3.3285)	LR 4.000e-03
0: TRAIN [3][80/484]	Time 0.028 (0.037)	Data 1.06e-04 (4.31e-03)	Tok/s 45424 (53619)	Loss/tok 3.0539 (3.3148)	LR 4.000e-03
0: TRAIN [3][90/484]	Time 0.028 (0.036)	Data 1.01e-04 (3.84e-03)	Tok/s 47207 (53682)	Loss/tok 3.2422 (3.3070)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [3][100/484]	Time 0.028 (0.036)	Data 1.08e-04 (3.47e-03)	Tok/s 43157 (53667)	Loss/tok 3.0047 (3.2936)	LR 4.000e-03
0: TRAIN [3][110/484]	Time 0.029 (0.035)	Data 7.75e-05 (3.17e-03)	Tok/s 44859 (53423)	Loss/tok 3.1506 (3.2824)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [3][120/484]	Time 0.026 (0.035)	Data 8.30e-05 (2.91e-03)	Tok/s 49920 (54071)	Loss/tok 3.1757 (3.2998)	LR 4.000e-03
0: TRAIN [3][130/484]	Time 0.034 (0.035)	Data 8.08e-05 (2.70e-03)	Tok/s 59859 (54030)	Loss/tok 3.1666 (3.3016)	LR 4.000e-03
0: TRAIN [3][140/484]	Time 0.034 (0.035)	Data 8.30e-05 (2.51e-03)	Tok/s 62834 (54523)	Loss/tok 2.8653 (3.2966)	LR 4.000e-03
0: TRAIN [3][150/484]	Time 0.034 (0.035)	Data 8.01e-05 (2.35e-03)	Tok/s 63351 (54426)	Loss/tok 3.1689 (3.2934)	LR 4.000e-03
0: TRAIN [3][160/484]	Time 0.040 (0.035)	Data 7.92e-05 (2.21e-03)	Tok/s 76454 (54518)	Loss/tok 3.1872 (3.2879)	LR 4.000e-03
0: TRAIN [3][170/484]	Time 0.023 (0.035)	Data 8.06e-05 (2.09e-03)	Tok/s 26787 (54436)	Loss/tok 2.6654 (3.2840)	LR 4.000e-03
0: TRAIN [3][180/484]	Time 0.029 (0.035)	Data 8.03e-05 (1.97e-03)	Tok/s 45513 (54694)	Loss/tok 3.0024 (3.2822)	LR 4.000e-03
0: TRAIN [3][190/484]	Time 0.029 (0.034)	Data 7.84e-05 (1.88e-03)	Tok/s 45190 (54295)	Loss/tok 3.1397 (3.2790)	LR 4.000e-03
0: TRAIN [3][200/484]	Time 0.023 (0.034)	Data 9.80e-05 (1.79e-03)	Tok/s 27278 (53977)	Loss/tok 2.4484 (3.2777)	LR 4.000e-03
0: TRAIN [3][210/484]	Time 0.029 (0.034)	Data 9.70e-05 (1.71e-03)	Tok/s 47272 (54166)	Loss/tok 3.1543 (3.2796)	LR 4.000e-03
0: TRAIN [3][220/484]	Time 0.040 (0.034)	Data 8.08e-05 (1.63e-03)	Tok/s 72983 (54070)	Loss/tok 3.3392 (3.2719)	LR 4.000e-03
0: TRAIN [3][230/484]	Time 0.029 (0.034)	Data 7.92e-05 (1.57e-03)	Tok/s 44504 (53753)	Loss/tok 3.1183 (3.2699)	LR 4.000e-03
0: TRAIN [3][240/484]	Time 0.040 (0.034)	Data 7.89e-05 (1.50e-03)	Tok/s 74330 (53627)	Loss/tok 3.3233 (3.2695)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [3][250/484]	Time 0.034 (0.034)	Data 7.80e-05 (1.45e-03)	Tok/s 61853 (53626)	Loss/tok 3.5644 (3.2632)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [3][260/484]	Time 0.029 (0.034)	Data 8.42e-05 (1.39e-03)	Tok/s 45092 (53834)	Loss/tok 2.9370 (3.2689)	LR 4.000e-03
0: TRAIN [3][270/484]	Time 0.040 (0.034)	Data 8.03e-05 (1.35e-03)	Tok/s 74700 (53667)	Loss/tok 3.0647 (3.2669)	LR 4.000e-03
0: TRAIN [3][280/484]	Time 0.034 (0.033)	Data 8.06e-05 (1.30e-03)	Tok/s 60647 (53681)	Loss/tok 3.4355 (3.2668)	LR 4.000e-03
0: TRAIN [3][290/484]	Time 0.040 (0.033)	Data 8.15e-05 (1.26e-03)	Tok/s 73407 (53688)	Loss/tok 3.4544 (3.2702)	LR 4.000e-03
0: TRAIN [3][300/484]	Time 0.029 (0.033)	Data 7.65e-05 (1.22e-03)	Tok/s 43940 (53661)	Loss/tok 2.8839 (3.2727)	LR 4.000e-03
0: TRAIN [3][310/484]	Time 0.029 (0.033)	Data 7.99e-05 (1.18e-03)	Tok/s 46120 (53641)	Loss/tok 3.0167 (3.2695)	LR 4.000e-03
0: TRAIN [3][320/484]	Time 0.034 (0.033)	Data 7.89e-05 (1.15e-03)	Tok/s 62156 (53526)	Loss/tok 3.2056 (3.2635)	LR 4.000e-03
0: TRAIN [3][330/484]	Time 0.034 (0.033)	Data 8.18e-05 (1.12e-03)	Tok/s 63025 (53641)	Loss/tok 3.4603 (3.2649)	LR 4.000e-03
0: TRAIN [3][340/484]	Time 0.035 (0.033)	Data 8.15e-05 (1.09e-03)	Tok/s 60964 (53703)	Loss/tok 3.4348 (3.2643)	LR 4.000e-03
0: TRAIN [3][350/484]	Time 0.040 (0.033)	Data 8.20e-05 (1.06e-03)	Tok/s 72189 (53705)	Loss/tok 3.8483 (3.2685)	LR 4.000e-03
0: TRAIN [3][360/484]	Time 0.034 (0.033)	Data 8.34e-05 (1.03e-03)	Tok/s 58575 (53667)	Loss/tok 3.8888 (3.2702)	LR 4.000e-03
0: TRAIN [3][370/484]	Time 0.023 (0.033)	Data 7.61e-05 (1.01e-03)	Tok/s 29565 (53549)	Loss/tok 2.8357 (3.2693)	LR 4.000e-03
0: TRAIN [3][380/484]	Time 0.034 (0.033)	Data 8.01e-05 (9.81e-04)	Tok/s 62149 (53679)	Loss/tok 3.6672 (3.2737)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [3][390/484]	Time 0.028 (0.033)	Data 8.13e-05 (9.58e-04)	Tok/s 43865 (53665)	Loss/tok 3.2701 (3.2727)	LR 4.000e-03
0: TRAIN [3][400/484]	Time 0.040 (0.033)	Data 7.51e-05 (9.36e-04)	Tok/s 74142 (53568)	Loss/tok 3.5292 (3.2727)	LR 4.000e-03
0: TRAIN [3][410/484]	Time 0.029 (0.033)	Data 8.96e-05 (9.15e-04)	Tok/s 44674 (53578)	Loss/tok 3.1626 (3.2718)	LR 4.000e-03
0: TRAIN [3][420/484]	Time 0.029 (0.033)	Data 8.58e-05 (8.96e-04)	Tok/s 42718 (53555)	Loss/tok 3.2257 (3.2702)	LR 4.000e-03
0: TRAIN [3][430/484]	Time 0.029 (0.033)	Data 8.58e-05 (8.77e-04)	Tok/s 45919 (53695)	Loss/tok 3.1016 (3.2731)	LR 4.000e-03
0: TRAIN [3][440/484]	Time 0.047 (0.033)	Data 8.37e-05 (8.59e-04)	Tok/s 78333 (53846)	Loss/tok 3.5657 (3.2764)	LR 4.000e-03
0: TRAIN [3][450/484]	Time 0.023 (0.033)	Data 8.27e-05 (8.42e-04)	Tok/s 27649 (53817)	Loss/tok 2.7250 (3.2732)	LR 4.000e-03
0: TRAIN [3][460/484]	Time 0.034 (0.033)	Data 7.84e-05 (8.26e-04)	Tok/s 58607 (53882)	Loss/tok 3.2795 (3.2742)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [3][470/484]	Time 0.040 (0.033)	Data 7.63e-05 (8.10e-04)	Tok/s 75868 (53826)	Loss/tok 3.2219 (3.2732)	LR 4.000e-03
0: TRAIN [3][480/484]	Time 0.029 (0.033)	Data 7.80e-05 (7.95e-04)	Tok/s 44088 (53893)	Loss/tok 3.0053 (3.2721)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022288512, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022288513, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.340 (0.340)	Decoder iters 149.0 (149.0)	Tok/s 7615 (7615)
0: Running moses detokenizer
0: BLEU(score=22.504605810900312, counts=[35314, 17296, 9692, 5609], totals=[63152, 60149, 57146, 54148], precisions=[55.91905244489486, 28.75525777652164, 16.960067196304202, 10.358646672083918], bp=0.9761566022223926, sys_len=63152, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022289002, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.225, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022289002, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.2751	Test BLEU: 22.50
0: Performance: Epoch: 3	Training: 13799050 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593022289002, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022289002, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022289003, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 1470556587
0: TRAIN [4][0/484]	Time 0.378 (0.378)	Data 2.85e-01 (2.85e-01)	Tok/s 3337 (3337)	Loss/tok 2.9333 (2.9333)	LR 4.000e-03
0: TRAIN [4][10/484]	Time 0.029 (0.065)	Data 8.08e-05 (2.60e-02)	Tok/s 41953 (51422)	Loss/tok 3.0203 (3.2793)	LR 4.000e-03
0: TRAIN [4][20/484]	Time 0.029 (0.048)	Data 7.77e-05 (1.36e-02)	Tok/s 46287 (48849)	Loss/tok 3.0375 (3.1590)	LR 4.000e-03
0: TRAIN [4][30/484]	Time 0.029 (0.043)	Data 7.87e-05 (9.26e-03)	Tok/s 44749 (51376)	Loss/tok 3.6064 (3.2092)	LR 4.000e-03
0: TRAIN [4][40/484]	Time 0.029 (0.041)	Data 9.32e-05 (7.02e-03)	Tok/s 46730 (52845)	Loss/tok 3.0837 (3.2214)	LR 4.000e-03
0: TRAIN [4][50/484]	Time 0.023 (0.039)	Data 8.03e-05 (5.66e-03)	Tok/s 28073 (52720)	Loss/tok 2.6047 (3.2078)	LR 4.000e-03
0: TRAIN [4][60/484]	Time 0.034 (0.038)	Data 7.80e-05 (4.75e-03)	Tok/s 60747 (53460)	Loss/tok 3.0112 (3.1896)	LR 4.000e-03
0: TRAIN [4][70/484]	Time 0.029 (0.038)	Data 7.68e-05 (4.09e-03)	Tok/s 44109 (53894)	Loss/tok 2.7770 (3.1937)	LR 4.000e-03
0: TRAIN [4][80/484]	Time 0.029 (0.037)	Data 8.03e-05 (3.60e-03)	Tok/s 43333 (54292)	Loss/tok 2.9278 (3.1756)	LR 4.000e-03
0: TRAIN [4][90/484]	Time 0.023 (0.037)	Data 8.73e-05 (3.21e-03)	Tok/s 27813 (54003)	Loss/tok 2.5247 (3.1946)	LR 4.000e-03
0: TRAIN [4][100/484]	Time 0.034 (0.036)	Data 9.97e-05 (2.90e-03)	Tok/s 61554 (53581)	Loss/tok 3.0373 (3.1838)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][110/484]	Time 0.023 (0.036)	Data 8.20e-05 (2.65e-03)	Tok/s 27312 (53475)	Loss/tok 2.2918 (3.1792)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [4][120/484]	Time 0.029 (0.035)	Data 7.77e-05 (2.44e-03)	Tok/s 44106 (53485)	Loss/tok 2.8966 (3.1740)	LR 4.000e-03
0: TRAIN [4][130/484]	Time 0.029 (0.035)	Data 7.65e-05 (2.26e-03)	Tok/s 46443 (53546)	Loss/tok 3.1089 (3.1802)	LR 4.000e-03
0: TRAIN [4][140/484]	Time 0.029 (0.035)	Data 7.68e-05 (2.10e-03)	Tok/s 41626 (53374)	Loss/tok 2.9521 (3.1780)	LR 4.000e-03
0: TRAIN [4][150/484]	Time 0.029 (0.034)	Data 7.49e-05 (1.97e-03)	Tok/s 45051 (53064)	Loss/tok 2.6721 (3.1658)	LR 4.000e-03
0: TRAIN [4][160/484]	Time 0.029 (0.034)	Data 7.63e-05 (1.85e-03)	Tok/s 45994 (52569)	Loss/tok 2.9958 (3.1623)	LR 4.000e-03
0: TRAIN [4][170/484]	Time 0.023 (0.034)	Data 7.75e-05 (1.75e-03)	Tok/s 28361 (52314)	Loss/tok 2.6683 (3.1628)	LR 4.000e-03
0: TRAIN [4][180/484]	Time 0.029 (0.034)	Data 1.17e-04 (1.65e-03)	Tok/s 45902 (51930)	Loss/tok 2.8925 (3.1593)	LR 4.000e-03
0: TRAIN [4][190/484]	Time 0.029 (0.034)	Data 7.80e-05 (1.57e-03)	Tok/s 44775 (52531)	Loss/tok 2.7972 (3.1736)	LR 4.000e-03
0: TRAIN [4][200/484]	Time 0.029 (0.034)	Data 7.82e-05 (1.50e-03)	Tok/s 45608 (52661)	Loss/tok 2.9892 (3.1720)	LR 4.000e-03
0: TRAIN [4][210/484]	Time 0.047 (0.034)	Data 7.51e-05 (1.43e-03)	Tok/s 79039 (52994)	Loss/tok 3.7661 (3.1781)	LR 4.000e-03
0: TRAIN [4][220/484]	Time 0.034 (0.034)	Data 7.77e-05 (1.37e-03)	Tok/s 60426 (53044)	Loss/tok 3.1374 (3.1771)	LR 4.000e-03
0: TRAIN [4][230/484]	Time 0.034 (0.034)	Data 7.99e-05 (1.31e-03)	Tok/s 60777 (52949)	Loss/tok 3.2615 (3.1771)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][240/484]	Time 0.029 (0.033)	Data 7.56e-05 (1.26e-03)	Tok/s 43564 (52693)	Loss/tok 2.8703 (3.1773)	LR 4.000e-03
0: TRAIN [4][250/484]	Time 0.029 (0.033)	Data 7.58e-05 (1.22e-03)	Tok/s 44522 (52792)	Loss/tok 3.2242 (3.1763)	LR 4.000e-03
0: TRAIN [4][260/484]	Time 0.029 (0.033)	Data 8.08e-05 (1.17e-03)	Tok/s 46049 (52572)	Loss/tok 2.9893 (3.1735)	LR 4.000e-03
0: TRAIN [4][270/484]	Time 0.029 (0.033)	Data 7.94e-05 (1.13e-03)	Tok/s 45869 (52669)	Loss/tok 3.2161 (3.1742)	LR 4.000e-03
0: TRAIN [4][280/484]	Time 0.029 (0.033)	Data 7.46e-05 (1.09e-03)	Tok/s 45733 (52755)	Loss/tok 2.8768 (3.1732)	LR 4.000e-03
0: TRAIN [4][290/484]	Time 0.047 (0.033)	Data 7.77e-05 (1.06e-03)	Tok/s 78764 (52661)	Loss/tok 3.9940 (3.1777)	LR 4.000e-03
0: TRAIN [4][300/484]	Time 0.029 (0.033)	Data 7.84e-05 (1.03e-03)	Tok/s 42112 (52844)	Loss/tok 2.9820 (3.1814)	LR 4.000e-03
0: TRAIN [4][310/484]	Time 0.034 (0.033)	Data 8.15e-05 (9.96e-04)	Tok/s 59088 (52968)	Loss/tok 3.3345 (3.1851)	LR 4.000e-03
0: TRAIN [4][320/484]	Time 0.029 (0.033)	Data 7.77e-05 (9.68e-04)	Tok/s 44109 (53060)	Loss/tok 3.1213 (3.1859)	LR 4.000e-03
0: TRAIN [4][330/484]	Time 0.040 (0.033)	Data 7.87e-05 (9.41e-04)	Tok/s 74171 (53188)	Loss/tok 3.3464 (3.1876)	LR 4.000e-03
0: TRAIN [4][340/484]	Time 0.029 (0.033)	Data 7.75e-05 (9.16e-04)	Tok/s 45083 (53203)	Loss/tok 2.7991 (3.1916)	LR 2.000e-03
0: TRAIN [4][350/484]	Time 0.040 (0.033)	Data 7.61e-05 (8.92e-04)	Tok/s 74141 (53463)	Loss/tok 3.5626 (3.1967)	LR 2.000e-03
0: TRAIN [4][360/484]	Time 0.029 (0.033)	Data 7.41e-05 (8.69e-04)	Tok/s 44301 (53370)	Loss/tok 3.2188 (3.1963)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [4][370/484]	Time 0.047 (0.033)	Data 7.56e-05 (8.48e-04)	Tok/s 77063 (53547)	Loss/tok 3.6117 (3.1975)	LR 2.000e-03
0: TRAIN [4][380/484]	Time 0.029 (0.033)	Data 7.96e-05 (8.28e-04)	Tok/s 43049 (53549)	Loss/tok 2.8313 (3.1969)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [4][390/484]	Time 0.034 (0.033)	Data 7.65e-05 (8.09e-04)	Tok/s 61812 (53724)	Loss/tok 2.9946 (3.1979)	LR 2.000e-03
0: TRAIN [4][400/484]	Time 0.034 (0.033)	Data 8.08e-05 (7.91e-04)	Tok/s 60828 (53641)	Loss/tok 3.2436 (3.1972)	LR 2.000e-03
0: TRAIN [4][410/484]	Time 0.034 (0.033)	Data 9.61e-05 (7.73e-04)	Tok/s 60957 (53611)	Loss/tok 3.4787 (3.1984)	LR 2.000e-03
0: TRAIN [4][420/484]	Time 0.047 (0.033)	Data 7.84e-05 (7.57e-04)	Tok/s 81635 (53608)	Loss/tok 3.0973 (3.1985)	LR 2.000e-03
0: TRAIN [4][430/484]	Time 0.047 (0.033)	Data 7.53e-05 (7.41e-04)	Tok/s 81195 (53625)	Loss/tok 3.6275 (3.1984)	LR 2.000e-03
0: TRAIN [4][440/484]	Time 0.040 (0.033)	Data 7.96e-05 (7.26e-04)	Tok/s 73091 (53713)	Loss/tok 3.4427 (3.1986)	LR 2.000e-03
0: TRAIN [4][450/484]	Time 0.029 (0.033)	Data 7.96e-05 (7.12e-04)	Tok/s 48270 (53885)	Loss/tok 3.1042 (3.2004)	LR 2.000e-03
0: TRAIN [4][460/484]	Time 0.023 (0.033)	Data 7.99e-05 (6.98e-04)	Tok/s 27332 (53795)	Loss/tok 2.5892 (3.1996)	LR 2.000e-03
0: TRAIN [4][470/484]	Time 0.047 (0.033)	Data 8.15e-05 (6.85e-04)	Tok/s 79863 (53798)	Loss/tok 3.3824 (3.1982)	LR 2.000e-03
0: TRAIN [4][480/484]	Time 0.034 (0.033)	Data 7.61e-05 (6.72e-04)	Tok/s 62394 (53814)	Loss/tok 3.0601 (3.1955)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022305089, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593022305089, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.276 (0.276)	Decoder iters 124.0 (124.0)	Tok/s 9560 (9560)
0: Running moses detokenizer
0: BLEU(score=23.538822593078955, counts=[36674, 18268, 10349, 6071], totals=[65447, 62444, 59441, 56442], precisions=[56.03618194875243, 29.255012491192108, 17.410541545397958, 10.756174479997165], bp=1.0, sys_len=65447, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022305567, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2354, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593022305568, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.1970	Test BLEU: 23.54
0: Performance: Epoch: 4	Training: 13789007 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1593022305568, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593022305568, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022305568, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 3674382794
0: TRAIN [5][0/484]	Time 0.373 (0.373)	Data 2.68e-01 (2.68e-01)	Tok/s 1733 (1733)	Loss/tok 2.4103 (2.4103)	LR 2.000e-03
0: TRAIN [5][10/484]	Time 0.035 (0.067)	Data 9.16e-05 (2.44e-02)	Tok/s 59181 (58574)	Loss/tok 3.2684 (3.1628)	LR 2.000e-03
0: TRAIN [5][20/484]	Time 0.034 (0.052)	Data 7.87e-05 (1.28e-02)	Tok/s 58892 (59290)	Loss/tok 3.0513 (3.1864)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [5][30/484]	Time 0.029 (0.046)	Data 8.34e-05 (8.73e-03)	Tok/s 45542 (58374)	Loss/tok 2.7277 (3.1626)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [5][40/484]	Time 0.029 (0.043)	Data 8.03e-05 (6.62e-03)	Tok/s 42117 (56979)	Loss/tok 2.9148 (3.1617)	LR 2.000e-03
0: TRAIN [5][50/484]	Time 0.040 (0.041)	Data 8.23e-05 (5.34e-03)	Tok/s 73055 (57244)	Loss/tok 2.9810 (3.1407)	LR 2.000e-03
0: TRAIN [5][60/484]	Time 0.035 (0.040)	Data 8.13e-05 (4.48e-03)	Tok/s 58510 (56806)	Loss/tok 3.2915 (3.1396)	LR 2.000e-03
0: TRAIN [5][70/484]	Time 0.029 (0.038)	Data 8.13e-05 (3.86e-03)	Tok/s 44210 (55945)	Loss/tok 3.0129 (3.1411)	LR 2.000e-03
0: TRAIN [5][80/484]	Time 0.023 (0.038)	Data 7.84e-05 (3.39e-03)	Tok/s 28462 (55442)	Loss/tok 2.5859 (3.1367)	LR 2.000e-03
0: TRAIN [5][90/484]	Time 0.034 (0.037)	Data 1.19e-04 (3.03e-03)	Tok/s 60665 (55806)	Loss/tok 3.0737 (3.1496)	LR 2.000e-03
0: TRAIN [5][100/484]	Time 0.040 (0.037)	Data 8.06e-05 (2.74e-03)	Tok/s 71789 (55756)	Loss/tok 3.3553 (3.1466)	LR 2.000e-03
0: TRAIN [5][110/484]	Time 0.040 (0.036)	Data 7.96e-05 (2.50e-03)	Tok/s 72353 (55430)	Loss/tok 3.3373 (3.1346)	LR 2.000e-03
0: TRAIN [5][120/484]	Time 0.029 (0.036)	Data 8.03e-05 (2.30e-03)	Tok/s 44690 (55404)	Loss/tok 2.7952 (3.1232)	LR 2.000e-03
0: TRAIN [5][130/484]	Time 0.029 (0.036)	Data 8.63e-05 (2.13e-03)	Tok/s 45280 (54996)	Loss/tok 2.6787 (3.1166)	LR 2.000e-03
0: TRAIN [5][140/484]	Time 0.029 (0.035)	Data 8.32e-05 (1.99e-03)	Tok/s 43884 (54601)	Loss/tok 3.0146 (3.1030)	LR 1.000e-03
0: TRAIN [5][150/484]	Time 0.029 (0.035)	Data 7.99e-05 (1.86e-03)	Tok/s 44914 (53960)	Loss/tok 2.8713 (3.0972)	LR 1.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [5][160/484]	Time 0.035 (0.035)	Data 8.34e-05 (1.75e-03)	Tok/s 58752 (53840)	Loss/tok 3.0084 (3.0900)	LR 1.000e-03
0: TRAIN [5][170/484]	Time 0.029 (0.035)	Data 8.92e-05 (1.65e-03)	Tok/s 43606 (53842)	Loss/tok 3.1857 (3.0856)	LR 1.000e-03
0: TRAIN [5][180/484]	Time 0.035 (0.034)	Data 7.96e-05 (1.57e-03)	Tok/s 61707 (53659)	Loss/tok 3.0274 (3.0781)	LR 1.000e-03
0: TRAIN [5][190/484]	Time 0.035 (0.034)	Data 8.56e-05 (1.49e-03)	Tok/s 61527 (53876)	Loss/tok 2.8525 (3.0841)	LR 1.000e-03
0: TRAIN [5][200/484]	Time 0.034 (0.034)	Data 1.06e-04 (1.42e-03)	Tok/s 60968 (53982)	Loss/tok 2.8469 (3.0830)	LR 1.000e-03
0: TRAIN [5][210/484]	Time 0.034 (0.034)	Data 1.08e-04 (1.36e-03)	Tok/s 60067 (54013)	Loss/tok 3.2822 (3.0926)	LR 1.000e-03
0: TRAIN [5][220/484]	Time 0.023 (0.034)	Data 1.07e-04 (1.30e-03)	Tok/s 28414 (53917)	Loss/tok 2.1899 (3.0935)	LR 1.000e-03
0: TRAIN [5][230/484]	Time 0.023 (0.034)	Data 1.09e-04 (1.25e-03)	Tok/s 29537 (53890)	Loss/tok 2.4671 (3.0941)	LR 1.000e-03
0: TRAIN [5][240/484]	Time 0.028 (0.034)	Data 1.04e-04 (1.20e-03)	Tok/s 46097 (53957)	Loss/tok 2.7361 (3.0929)	LR 1.000e-03
0: TRAIN [5][250/484]	Time 0.035 (0.034)	Data 1.06e-04 (1.16e-03)	Tok/s 62363 (54065)	Loss/tok 3.0264 (3.0952)	LR 1.000e-03
0: TRAIN [5][260/484]	Time 0.029 (0.034)	Data 1.11e-04 (1.12e-03)	Tok/s 45857 (54285)	Loss/tok 2.7835 (3.1005)	LR 1.000e-03
0: TRAIN [5][270/484]	Time 0.040 (0.034)	Data 8.11e-05 (1.08e-03)	Tok/s 72712 (54176)	Loss/tok 3.1945 (3.0981)	LR 1.000e-03
0: TRAIN [5][280/484]	Time 0.029 (0.034)	Data 1.04e-04 (1.05e-03)	Tok/s 46195 (53859)	Loss/tok 2.5930 (3.0923)	LR 1.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [5][290/484]	Time 0.034 (0.034)	Data 1.05e-04 (1.01e-03)	Tok/s 60539 (53910)	Loss/tok 3.0173 (3.0920)	LR 1.000e-03
0: TRAIN [5][300/484]	Time 0.034 (0.034)	Data 1.09e-04 (9.84e-04)	Tok/s 60412 (53876)	Loss/tok 3.0649 (3.0883)	LR 1.000e-03
0: TRAIN [5][310/484]	Time 0.029 (0.034)	Data 1.07e-04 (9.56e-04)	Tok/s 44770 (53878)	Loss/tok 3.0489 (3.0882)	LR 1.000e-03
0: TRAIN [5][320/484]	Time 0.040 (0.034)	Data 8.56e-05 (9.29e-04)	Tok/s 74663 (53958)	Loss/tok 2.9533 (3.0864)	LR 1.000e-03
0: TRAIN [5][330/484]	Time 0.029 (0.034)	Data 8.42e-05 (9.04e-04)	Tok/s 45269 (54161)	Loss/tok 2.9943 (3.0868)	LR 1.000e-03
0: TRAIN [5][340/484]	Time 0.023 (0.033)	Data 1.08e-04 (8.81e-04)	Tok/s 27661 (53924)	Loss/tok 2.4179 (3.0859)	LR 1.000e-03
0: TRAIN [5][350/484]	Time 0.029 (0.033)	Data 1.09e-04 (8.59e-04)	Tok/s 45533 (53790)	Loss/tok 2.7944 (3.0822)	LR 1.000e-03
0: TRAIN [5][360/484]	Time 0.028 (0.033)	Data 8.27e-05 (8.38e-04)	Tok/s 42526 (53865)	Loss/tok 2.7482 (3.0826)	LR 1.000e-03
0: TRAIN [5][370/484]	Time 0.034 (0.033)	Data 1.10e-04 (8.18e-04)	Tok/s 63285 (54027)	Loss/tok 3.1288 (3.0851)	LR 1.000e-03
0: TRAIN [5][380/484]	Time 0.029 (0.033)	Data 8.20e-05 (7.99e-04)	Tok/s 42353 (54033)	Loss/tok 3.1975 (3.0873)	LR 1.000e-03
0: TRAIN [5][390/484]	Time 0.028 (0.033)	Data 1.12e-04 (7.82e-04)	Tok/s 47422 (54113)	Loss/tok 2.8875 (3.0866)	LR 1.000e-03
0: TRAIN [5][400/484]	Time 0.029 (0.033)	Data 1.03e-04 (7.65e-04)	Tok/s 42013 (54229)	Loss/tok 3.2355 (3.0901)	LR 1.000e-03
0: TRAIN [5][410/484]	Time 0.029 (0.033)	Data 2.40e-04 (7.49e-04)	Tok/s 44709 (54351)	Loss/tok 2.7254 (3.0898)	LR 1.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [5][420/484]	Time 0.029 (0.033)	Data 8.20e-05 (7.34e-04)	Tok/s 45067 (54241)	Loss/tok 2.9809 (3.0865)	LR 1.000e-03
0: TRAIN [5][430/484]	Time 0.034 (0.033)	Data 7.89e-05 (7.19e-04)	Tok/s 61096 (54174)	Loss/tok 2.9535 (3.0864)	LR 5.000e-04
0: TRAIN [5][440/484]	Time 0.034 (0.033)	Data 1.15e-04 (7.05e-04)	Tok/s 58045 (54125)	Loss/tok 3.2610 (3.0857)	LR 5.000e-04
0: TRAIN [5][450/484]	Time 0.034 (0.033)	Data 8.27e-05 (6.91e-04)	Tok/s 60941 (53893)	Loss/tok 3.0016 (3.0821)	LR 5.000e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [5][460/484]	Time 0.029 (0.033)	Data 8.15e-05 (6.78e-04)	Tok/s 44509 (53897)	Loss/tok 2.8366 (3.0817)	LR 5.000e-04
0: TRAIN [5][470/484]	Time 0.029 (0.033)	Data 8.15e-05 (6.66e-04)	Tok/s 47868 (53933)	Loss/tok 2.8881 (3.0826)	LR 5.000e-04
0: TRAIN [5][480/484]	Time 0.029 (0.033)	Data 9.27e-05 (6.54e-04)	Tok/s 43694 (54007)	Loss/tok 3.0026 (3.0811)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593022321641, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593022321641, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.242 (0.242)	Decoder iters 107.0 (107.0)	Tok/s 10466 (10466)
0: Running moses detokenizer
0: BLEU(score=24.108586586014777, counts=[36919, 18502, 10569, 6247], totals=[65043, 62040, 59037, 56039], precisions=[56.760912012053566, 29.822695035460992, 17.90233243559124, 11.147593640143471], bp=1.0, sys_len=65043, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022322108, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24109999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593022322108, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.0811	Test BLEU: 24.11
0: Performance: Epoch: 5	Training: 13805541 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1593022322109, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593022322109, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:08 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:09 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:09:48 AM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
slurmstepd: error: _is_a_lwp: open() /proc/73251/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:11 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
ENDING TIMING RUN AT 2020-06-24 11:12:12 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:09:48 AM
