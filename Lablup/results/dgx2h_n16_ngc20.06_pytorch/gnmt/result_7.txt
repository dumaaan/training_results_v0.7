+ echo 'Beginning trial 2 of 5'
Beginning trial 2 of 5
+ srun --ntasks=16 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593022016729, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593022016761, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593022016762, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593022016762, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593022016762, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n051
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n045
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n047
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n040
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=16 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593022023794, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023807, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023810, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023825, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023837, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023858, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023885, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023893, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023896, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023915, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023920, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023928, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023942, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022023942, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022024016, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022024027, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=256 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14177838/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 1 ']'
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=4.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 8 ']'
+ LR=4.0e-3
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -n 11 ']'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ NUMEPOCHS=12
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
running benchmark
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=4.0e-3
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
+ LR=4.0e-3
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 0 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 9 ']'
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ declare -a CMD
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TEST_BATCH_SIZE=16
+ '[' -n 4 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 3 ']'
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=4.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ NUMEPOCHS=12
+ DIST_OPTS=
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' -n 12 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ TEST_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=12
+ REMAIN_STEPS=2259
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 256 -gt 16 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 4 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DIST_OPTS=
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ '[' -n 11 ']'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 0 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=12
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ declare -a CMD
+ DECAY_INTERVAL=283
+ '[' -n 9 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 4 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 10 ']'
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ LR=4.0e-3
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ REMAIN_STEPS=2259
+ '[' 256 -gt 16 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 7 ']'
running benchmark
+ TRAIN_BATCH_SIZE=32
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
running benchmark
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ '[' -n 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TARGET=24.0
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' -n 14 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 16 ']'
running benchmark
+ '[' -n 5 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ '[' 256 -gt 16 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 256 -gt 16 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ NUMEPOCHS=12
+ DIST_OPTS=
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ '[' -n 13 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 11 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-24 11:07:07 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 16067130    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 16 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_16x16x32 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 16067130 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1593022028696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028947, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028959, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022028980, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029024, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029122, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029147, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029265, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029267, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029267, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029270, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029270, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029285, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029288, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029301, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029317, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029330, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029365, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029418, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029418, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029449, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029471, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029475, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029483, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029495, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029497, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029501, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029502, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029506, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029510, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029520, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029534, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029535, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029537, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029537, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029539, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029547, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029548, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029555, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029565, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029581, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029587, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029591, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029591, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029593, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029602, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029615, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029615, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029615, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029620, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029621, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029621, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029625, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029626, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029643, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029643, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029651, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029651, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029663, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029799, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593022029924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=16067130, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=283, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=12, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.004, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=2259, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=16, test_loader_workers=0, train_batch_size=32, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 281833356
:::MLLOG {"namespace": "", "time_ms": 1593022052326, "event_type": "POINT_IN_TIME", "key": "seed", "value": 281833356, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3675308080
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.004}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.004
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593022055784, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593022055784, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.004, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593022055784, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593022055784, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593022055784, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593022059183, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593022060358, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593022060359, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593022060615, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8192, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593022060616, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3964928, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593022060616, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 2259, 'decay_interval': 283, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2259
0: Scheduler decay interval: 283
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593022060617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593022060617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593022060617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 283, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593022060617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593022060618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593022060618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 2259, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593022060618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593022060618, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022060618, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 4253063728
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/484]	Time 0.406 (0.406)	Data 2.77e-01 (2.77e-01)	Tok/s 7064 (7064)	Loss/tok 10.7412 (10.7412)	LR 4.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/484]	Time 0.029 (0.098)	Data 7.22e-05 (2.53e-02)	Tok/s 44716 (34083)	Loss/tok 9.5503 (10.2312)	LR 4.809e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][20/484]	Time 0.029 (0.070)	Data 7.72e-05 (1.33e-02)	Tok/s 42655 (40826)	Loss/tok 8.9390 (9.7690)	LR 5.916e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][30/484]	Time 0.052 (0.060)	Data 7.53e-05 (9.03e-03)	Tok/s 69737 (44830)	Loss/tok 8.9744 (9.4834)	LR 7.279e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: TRAIN [0][40/484]	Time 0.047 (0.053)	Data 7.39e-05 (6.85e-03)	Tok/s 77709 (46529)	Loss/tok 10.2874 (9.4779)	LR 8.552e-05
0: TRAIN [0][50/484]	Time 0.029 (0.048)	Data 7.44e-05 (5.52e-03)	Tok/s 45935 (46899)	Loss/tok 8.2182 (9.3201)	LR 1.077e-04
0: TRAIN [0][60/484]	Time 0.029 (0.045)	Data 7.51e-05 (4.63e-03)	Tok/s 43925 (47648)	Loss/tok 8.1755 (9.1618)	LR 1.355e-04
0: TRAIN [0][70/484]	Time 0.024 (0.043)	Data 8.15e-05 (3.99e-03)	Tok/s 27649 (47749)	Loss/tok 7.3495 (9.0269)	LR 1.706e-04
0: TRAIN [0][80/484]	Time 0.029 (0.042)	Data 9.87e-05 (3.51e-03)	Tok/s 43178 (48018)	Loss/tok 8.0296 (8.9027)	LR 2.148e-04
0: TRAIN [0][90/484]	Time 0.040 (0.041)	Data 8.32e-05 (3.13e-03)	Tok/s 74269 (49388)	Loss/tok 8.1642 (8.7800)	LR 2.704e-04
0: TRAIN [0][100/484]	Time 0.034 (0.040)	Data 8.37e-05 (2.83e-03)	Tok/s 61761 (49994)	Loss/tok 7.9144 (8.6873)	LR 3.405e-04
0: TRAIN [0][110/484]	Time 0.034 (0.039)	Data 7.70e-05 (2.58e-03)	Tok/s 61702 (50532)	Loss/tok 7.9593 (8.6068)	LR 4.286e-04
0: TRAIN [0][120/484]	Time 0.029 (0.039)	Data 1.01e-04 (2.38e-03)	Tok/s 46084 (50763)	Loss/tok 7.5989 (8.5337)	LR 5.396e-04
0: TRAIN [0][130/484]	Time 0.040 (0.038)	Data 8.87e-05 (2.20e-03)	Tok/s 73335 (51470)	Loss/tok 7.8456 (8.4631)	LR 6.793e-04
0: TRAIN [0][140/484]	Time 0.024 (0.038)	Data 8.11e-05 (2.05e-03)	Tok/s 28039 (51831)	Loss/tok 6.7596 (8.3899)	LR 8.552e-04
0: TRAIN [0][150/484]	Time 0.040 (0.038)	Data 8.75e-05 (1.92e-03)	Tok/s 75213 (52061)	Loss/tok 7.7091 (8.3279)	LR 1.077e-03
0: TRAIN [0][160/484]	Time 0.029 (0.037)	Data 1.22e-04 (1.81e-03)	Tok/s 43349 (52250)	Loss/tok 7.2544 (8.2715)	LR 1.355e-03
0: Upscaling, new scale: 8.0
0: TRAIN [0][170/484]	Time 0.029 (0.037)	Data 7.63e-05 (1.71e-03)	Tok/s 43808 (51753)	Loss/tok 7.0220 (8.2187)	LR 1.706e-03
0: TRAIN [0][180/484]	Time 0.034 (0.037)	Data 8.61e-05 (1.62e-03)	Tok/s 61370 (51880)	Loss/tok 6.7850 (8.1492)	LR 2.148e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][190/484]	Time 0.034 (0.036)	Data 7.89e-05 (1.54e-03)	Tok/s 61556 (52022)	Loss/tok 7.0851 (8.0865)	LR 2.583e-03
0: TRAIN [0][200/484]	Time 0.024 (0.036)	Data 7.92e-05 (1.47e-03)	Tok/s 28639 (51827)	Loss/tok 5.4704 (8.0308)	LR 3.251e-03
0: TRAIN [0][210/484]	Time 0.040 (0.036)	Data 1.03e-04 (1.40e-03)	Tok/s 71287 (51861)	Loss/tok 6.8651 (7.9668)	LR 4.000e-03
0: TRAIN [0][220/484]	Time 0.040 (0.036)	Data 1.03e-04 (1.34e-03)	Tok/s 74657 (51820)	Loss/tok 6.4478 (7.8984)	LR 4.000e-03
0: TRAIN [0][230/484]	Time 0.028 (0.035)	Data 1.01e-04 (1.29e-03)	Tok/s 47365 (51831)	Loss/tok 5.8909 (7.8296)	LR 4.000e-03
0: TRAIN [0][240/484]	Time 0.034 (0.035)	Data 7.82e-05 (1.24e-03)	Tok/s 61585 (51832)	Loss/tok 6.2255 (7.7541)	LR 4.000e-03
0: TRAIN [0][250/484]	Time 0.029 (0.035)	Data 1.01e-04 (1.19e-03)	Tok/s 41369 (51969)	Loss/tok 5.4523 (7.6743)	LR 4.000e-03
0: TRAIN [0][260/484]	Time 0.023 (0.035)	Data 7.68e-05 (1.15e-03)	Tok/s 30223 (51961)	Loss/tok 4.7956 (7.5975)	LR 4.000e-03
0: TRAIN [0][270/484]	Time 0.034 (0.035)	Data 8.63e-05 (1.11e-03)	Tok/s 58770 (52041)	Loss/tok 5.3148 (7.5180)	LR 4.000e-03
0: TRAIN [0][280/484]	Time 0.029 (0.035)	Data 7.96e-05 (1.07e-03)	Tok/s 45838 (52219)	Loss/tok 5.5041 (7.4333)	LR 4.000e-03
0: TRAIN [0][290/484]	Time 0.023 (0.035)	Data 7.56e-05 (1.04e-03)	Tok/s 28641 (52281)	Loss/tok 4.3973 (7.3528)	LR 4.000e-03
0: TRAIN [0][300/484]	Time 0.034 (0.035)	Data 1.22e-04 (1.01e-03)	Tok/s 60574 (52069)	Loss/tok 5.2824 (7.2913)	LR 4.000e-03
0: TRAIN [0][310/484]	Time 0.034 (0.035)	Data 7.87e-05 (9.79e-04)	Tok/s 62061 (52102)	Loss/tok 4.7160 (7.2178)	LR 4.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [0][320/484]	Time 0.034 (0.034)	Data 1.05e-04 (9.51e-04)	Tok/s 59585 (52182)	Loss/tok 5.0573 (7.1416)	LR 4.000e-03
0: TRAIN [0][330/484]	Time 0.034 (0.034)	Data 1.25e-04 (9.25e-04)	Tok/s 62996 (52530)	Loss/tok 4.8397 (7.0498)	LR 4.000e-03
0: TRAIN [0][340/484]	Time 0.023 (0.034)	Data 7.84e-05 (9.00e-04)	Tok/s 27620 (52453)	Loss/tok 3.4912 (6.9806)	LR 4.000e-03
0: TRAIN [0][350/484]	Time 0.029 (0.034)	Data 1.08e-04 (8.77e-04)	Tok/s 43839 (52649)	Loss/tok 4.1867 (6.8981)	LR 4.000e-03
0: TRAIN [0][360/484]	Time 0.029 (0.034)	Data 8.11e-05 (8.55e-04)	Tok/s 47341 (52789)	Loss/tok 4.0064 (6.8269)	LR 4.000e-03
0: TRAIN [0][370/484]	Time 0.029 (0.034)	Data 8.06e-05 (8.34e-04)	Tok/s 46161 (52863)	Loss/tok 3.8506 (6.7581)	LR 4.000e-03
0: TRAIN [0][380/484]	Time 0.029 (0.034)	Data 1.07e-04 (8.15e-04)	Tok/s 43403 (52906)	Loss/tok 4.2483 (6.6916)	LR 4.000e-03
0: TRAIN [0][390/484]	Time 0.034 (0.034)	Data 1.02e-04 (7.96e-04)	Tok/s 62760 (52911)	Loss/tok 4.1258 (6.6303)	LR 4.000e-03
0: TRAIN [0][400/484]	Time 0.034 (0.034)	Data 7.94e-05 (7.79e-04)	Tok/s 60437 (53147)	Loss/tok 4.4151 (6.5565)	LR 4.000e-03
0: TRAIN [0][410/484]	Time 0.029 (0.034)	Data 8.03e-05 (7.62e-04)	Tok/s 44242 (53265)	Loss/tok 3.8882 (6.4879)	LR 4.000e-03
0: TRAIN [0][420/484]	Time 0.029 (0.034)	Data 1.08e-04 (7.46e-04)	Tok/s 43073 (53337)	Loss/tok 4.1654 (6.4281)	LR 4.000e-03
0: TRAIN [0][430/484]	Time 0.041 (0.034)	Data 7.72e-05 (7.30e-04)	Tok/s 69699 (53330)	Loss/tok 4.2266 (6.3724)	LR 4.000e-03
0: TRAIN [0][440/484]	Time 0.034 (0.034)	Data 8.11e-05 (7.15e-04)	Tok/s 61936 (53501)	Loss/tok 4.1438 (6.3093)	LR 4.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [0][450/484]	Time 0.029 (0.034)	Data 7.56e-05 (7.02e-04)	Tok/s 43794 (53581)	Loss/tok 3.5001 (6.2523)	LR 4.000e-03
0: TRAIN [0][460/484]	Time 0.034 (0.034)	Data 7.63e-05 (6.88e-04)	Tok/s 60964 (53599)	Loss/tok 4.0281 (6.2024)	LR 4.000e-03
0: TRAIN [0][470/484]	Time 0.029 (0.034)	Data 1.12e-04 (6.75e-04)	Tok/s 46039 (53711)	Loss/tok 3.3049 (6.1500)	LR 4.000e-03
0: TRAIN [0][480/484]	Time 0.029 (0.034)	Data 8.63e-05 (6.63e-04)	Tok/s 45993 (53678)	Loss/tok 3.6366 (6.1050)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022077168, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022077169, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.407 (0.407)	Decoder iters 149.0 (149.0)	Tok/s 6803 (6803)
0: Running moses detokenizer
0: BLEU(score=13.835563288639564, counts=[32019, 13199, 6472, 3254], totals=[74786, 71783, 68781, 65782], precisions=[42.814163078651085, 18.38736191020158, 9.409575318765357, 4.946641938524216], bp=1.0, sys_len=74786, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022077751, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1384, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022077751, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 6.0999	Test BLEU: 13.84
0: Performance: Epoch: 0	Training: 13695950 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593022077751, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022077751, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022077752, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 920583969
0: TRAIN [1][0/484]	Time 0.382 (0.382)	Data 3.55e-01 (3.55e-01)	Tok/s 3140 (3140)	Loss/tok 3.4658 (3.4658)	LR 4.000e-03
0: TRAIN [1][10/484]	Time 0.047 (0.067)	Data 7.99e-05 (3.23e-02)	Tok/s 77567 (55363)	Loss/tok 4.2814 (3.9638)	LR 4.000e-03
0: TRAIN [1][20/484]	Time 0.029 (0.050)	Data 2.64e-04 (1.70e-02)	Tok/s 43873 (51192)	Loss/tok 3.7491 (3.8689)	LR 4.000e-03
0: TRAIN [1][30/484]	Time 0.029 (0.043)	Data 8.06e-05 (1.15e-02)	Tok/s 44547 (50297)	Loss/tok 3.1916 (3.7935)	LR 4.000e-03
0: TRAIN [1][40/484]	Time 0.029 (0.040)	Data 7.68e-05 (8.74e-03)	Tok/s 45127 (50965)	Loss/tok 3.5247 (3.7616)	LR 4.000e-03
0: TRAIN [1][50/484]	Time 0.029 (0.039)	Data 7.80e-05 (7.04e-03)	Tok/s 44609 (52095)	Loss/tok 3.5084 (3.7555)	LR 4.000e-03
0: TRAIN [1][60/484]	Time 0.034 (0.038)	Data 7.99e-05 (5.90e-03)	Tok/s 63001 (52943)	Loss/tok 3.5275 (3.7640)	LR 4.000e-03
0: TRAIN [1][70/484]	Time 0.029 (0.038)	Data 8.01e-05 (5.08e-03)	Tok/s 45272 (53916)	Loss/tok 3.4598 (3.7913)	LR 4.000e-03
0: TRAIN [1][80/484]	Time 0.034 (0.037)	Data 7.65e-05 (4.46e-03)	Tok/s 60095 (53696)	Loss/tok 4.0924 (3.7858)	LR 4.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [1][90/484]	Time 0.029 (0.037)	Data 7.87e-05 (3.98e-03)	Tok/s 45901 (54719)	Loss/tok 3.7111 (3.7834)	LR 4.000e-03
0: TRAIN [1][100/484]	Time 0.023 (0.036)	Data 7.89e-05 (3.60e-03)	Tok/s 27335 (54503)	Loss/tok 3.1525 (3.7756)	LR 4.000e-03
0: TRAIN [1][110/484]	Time 0.034 (0.036)	Data 8.23e-05 (3.28e-03)	Tok/s 59758 (54891)	Loss/tok 4.0476 (3.7923)	LR 4.000e-03
0: TRAIN [1][120/484]	Time 0.029 (0.036)	Data 7.94e-05 (3.01e-03)	Tok/s 42548 (54055)	Loss/tok 3.6839 (3.7890)	LR 4.000e-03
0: TRAIN [1][130/484]	Time 0.034 (0.035)	Data 7.72e-05 (2.79e-03)	Tok/s 62487 (53961)	Loss/tok 4.1568 (3.7903)	LR 4.000e-03
0: TRAIN [1][140/484]	Time 0.023 (0.035)	Data 7.94e-05 (2.60e-03)	Tok/s 27833 (54051)	Loss/tok 3.2541 (3.7907)	LR 4.000e-03
0: TRAIN [1][150/484]	Time 0.029 (0.035)	Data 7.84e-05 (2.43e-03)	Tok/s 46458 (53787)	Loss/tok 3.6222 (3.7690)	LR 4.000e-03
0: TRAIN [1][160/484]	Time 0.040 (0.035)	Data 7.84e-05 (2.29e-03)	Tok/s 74368 (53571)	Loss/tok 3.5342 (3.7518)	LR 4.000e-03
0: TRAIN [1][170/484]	Time 0.034 (0.034)	Data 8.06e-05 (2.16e-03)	Tok/s 60763 (53819)	Loss/tok 3.4990 (3.7424)	LR 4.000e-03
0: TRAIN [1][180/484]	Time 0.029 (0.034)	Data 7.92e-05 (2.04e-03)	Tok/s 44811 (53872)	Loss/tok 3.4894 (3.7345)	LR 4.000e-03
0: TRAIN [1][190/484]	Time 0.029 (0.034)	Data 7.92e-05 (1.94e-03)	Tok/s 44492 (54250)	Loss/tok 3.4592 (3.7374)	LR 4.000e-03
0: TRAIN [1][200/484]	Time 0.023 (0.034)	Data 9.04e-05 (1.85e-03)	Tok/s 28347 (54474)	Loss/tok 2.8748 (3.7440)	LR 4.000e-03
0: TRAIN [1][210/484]	Time 0.029 (0.034)	Data 1.26e-04 (1.76e-03)	Tok/s 47002 (54249)	Loss/tok 3.6851 (3.7373)	LR 4.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [1][220/484]	Time 0.034 (0.034)	Data 7.94e-05 (1.69e-03)	Tok/s 60481 (54154)	Loss/tok 3.7328 (3.7300)	LR 4.000e-03
0: TRAIN [1][230/484]	Time 0.029 (0.034)	Data 7.63e-05 (1.62e-03)	Tok/s 44730 (54298)	Loss/tok 3.3270 (3.7244)	LR 4.000e-03
0: TRAIN [1][240/484]	Time 0.029 (0.034)	Data 7.58e-05 (1.55e-03)	Tok/s 45876 (54220)	Loss/tok 3.7052 (3.7185)	LR 4.000e-03
0: TRAIN [1][250/484]	Time 0.029 (0.034)	Data 8.70e-05 (1.49e-03)	Tok/s 46304 (54039)	Loss/tok 3.6113 (3.7168)	LR 4.000e-03
0: TRAIN [1][260/484]	Time 0.029 (0.034)	Data 8.92e-05 (1.44e-03)	Tok/s 47370 (54129)	Loss/tok 3.8751 (3.7208)	LR 4.000e-03
0: TRAIN [1][270/484]	Time 0.029 (0.034)	Data 7.89e-05 (1.39e-03)	Tok/s 41513 (53873)	Loss/tok 3.6133 (3.7107)	LR 4.000e-03
0: TRAIN [1][280/484]	Time 0.029 (0.034)	Data 8.01e-05 (1.34e-03)	Tok/s 45109 (53763)	Loss/tok 3.3696 (3.7023)	LR 4.000e-03
0: TRAIN [1][290/484]	Time 0.034 (0.034)	Data 8.08e-05 (1.30e-03)	Tok/s 60683 (53997)	Loss/tok 3.2916 (3.6992)	LR 4.000e-03
0: TRAIN [1][300/484]	Time 0.029 (0.034)	Data 9.18e-05 (1.26e-03)	Tok/s 46207 (54006)	Loss/tok 3.3271 (3.6940)	LR 4.000e-03
0: TRAIN [1][310/484]	Time 0.047 (0.034)	Data 1.20e-04 (1.22e-03)	Tok/s 78485 (53982)	Loss/tok 4.4117 (3.6941)	LR 4.000e-03
0: TRAIN [1][320/484]	Time 0.034 (0.034)	Data 7.68e-05 (1.19e-03)	Tok/s 63009 (54071)	Loss/tok 3.4395 (3.6874)	LR 4.000e-03
0: TRAIN [1][330/484]	Time 0.034 (0.034)	Data 7.68e-05 (1.15e-03)	Tok/s 64272 (54070)	Loss/tok 3.3867 (3.6837)	LR 4.000e-03
0: TRAIN [1][340/484]	Time 0.023 (0.034)	Data 9.04e-05 (1.12e-03)	Tok/s 28941 (54083)	Loss/tok 2.9191 (3.6787)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [1][350/484]	Time 0.029 (0.034)	Data 7.94e-05 (1.09e-03)	Tok/s 43577 (54220)	Loss/tok 3.2648 (3.6749)	LR 4.000e-03
0: TRAIN [1][360/484]	Time 0.029 (0.033)	Data 7.99e-05 (1.07e-03)	Tok/s 44938 (53987)	Loss/tok 3.4699 (3.6686)	LR 4.000e-03
0: TRAIN [1][370/484]	Time 0.040 (0.033)	Data 8.20e-05 (1.04e-03)	Tok/s 73085 (54162)	Loss/tok 3.7395 (3.6691)	LR 4.000e-03
0: TRAIN [1][380/484]	Time 0.040 (0.034)	Data 9.23e-05 (1.01e-03)	Tok/s 72902 (54349)	Loss/tok 3.7598 (3.6673)	LR 4.000e-03
0: TRAIN [1][390/484]	Time 0.034 (0.033)	Data 7.75e-05 (9.91e-04)	Tok/s 61849 (54288)	Loss/tok 3.3564 (3.6600)	LR 4.000e-03
0: TRAIN [1][400/484]	Time 0.034 (0.033)	Data 7.94e-05 (9.68e-04)	Tok/s 62319 (54385)	Loss/tok 3.2535 (3.6562)	LR 4.000e-03
0: TRAIN [1][410/484]	Time 0.029 (0.033)	Data 7.94e-05 (9.47e-04)	Tok/s 43351 (54341)	Loss/tok 3.2303 (3.6535)	LR 4.000e-03
0: TRAIN [1][420/484]	Time 0.034 (0.033)	Data 7.94e-05 (9.26e-04)	Tok/s 59356 (54232)	Loss/tok 3.4856 (3.6482)	LR 4.000e-03
0: TRAIN [1][430/484]	Time 0.029 (0.033)	Data 7.53e-05 (9.06e-04)	Tok/s 44164 (54207)	Loss/tok 3.3023 (3.6446)	LR 4.000e-03
0: TRAIN [1][440/484]	Time 0.029 (0.033)	Data 7.75e-05 (8.88e-04)	Tok/s 45142 (54189)	Loss/tok 3.2494 (3.6393)	LR 4.000e-03
0: TRAIN [1][450/484]	Time 0.040 (0.033)	Data 7.77e-05 (8.70e-04)	Tok/s 69947 (54079)	Loss/tok 3.5674 (3.6350)	LR 4.000e-03
0: TRAIN [1][460/484]	Time 0.029 (0.033)	Data 7.65e-05 (8.53e-04)	Tok/s 45611 (53971)	Loss/tok 3.5829 (3.6316)	LR 4.000e-03
0: TRAIN [1][470/484]	Time 0.034 (0.033)	Data 1.06e-04 (8.36e-04)	Tok/s 59047 (54065)	Loss/tok 3.7128 (3.6293)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [1][480/484]	Time 0.034 (0.033)	Data 8.32e-05 (8.21e-04)	Tok/s 61352 (54055)	Loss/tok 3.4970 (3.6240)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022093831, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593022093831, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.340 (0.340)	Decoder iters 149.0 (149.0)	Tok/s 7829 (7829)
0: Running moses detokenizer
0: BLEU(score=20.626731830767373, counts=[34498, 16062, 8692, 4892], totals=[64476, 61473, 58470, 55472], precisions=[53.50518022209815, 26.128544238934165, 14.865743116127929, 8.8188635708105], bp=0.9969028766123267, sys_len=64476, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022094389, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.20629999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593022094389, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.6311	Test BLEU: 20.63
0: Performance: Epoch: 1	Training: 13799151 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593022094389, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593022094389, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022094389, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 2586509931
0: TRAIN [2][0/484]	Time 0.376 (0.376)	Data 2.73e-01 (2.73e-01)	Tok/s 3352 (3352)	Loss/tok 3.1287 (3.1287)	LR 4.000e-03
0: TRAIN [2][10/484]	Time 0.040 (0.064)	Data 7.99e-05 (2.49e-02)	Tok/s 72126 (51673)	Loss/tok 3.6260 (3.3168)	LR 4.000e-03
0: TRAIN [2][20/484]	Time 0.023 (0.048)	Data 8.94e-05 (1.31e-02)	Tok/s 28676 (51150)	Loss/tok 2.4171 (3.2827)	LR 4.000e-03
0: TRAIN [2][30/484]	Time 0.029 (0.043)	Data 7.84e-05 (8.89e-03)	Tok/s 46986 (51245)	Loss/tok 3.3043 (3.2828)	LR 4.000e-03
0: TRAIN [2][40/484]	Time 0.029 (0.040)	Data 7.63e-05 (6.74e-03)	Tok/s 43981 (51540)	Loss/tok 2.9591 (3.2980)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][50/484]	Time 0.040 (0.038)	Data 7.70e-05 (5.44e-03)	Tok/s 70079 (50810)	Loss/tok 3.4966 (3.3222)	LR 4.000e-03
0: TRAIN [2][60/484]	Time 0.029 (0.038)	Data 7.65e-05 (4.56e-03)	Tok/s 44897 (52391)	Loss/tok 3.1608 (3.3509)	LR 4.000e-03
0: TRAIN [2][70/484]	Time 0.034 (0.037)	Data 7.87e-05 (3.93e-03)	Tok/s 62432 (52120)	Loss/tok 3.1493 (3.3389)	LR 4.000e-03
0: TRAIN [2][80/484]	Time 0.034 (0.036)	Data 7.99e-05 (3.45e-03)	Tok/s 61536 (51602)	Loss/tok 3.2135 (3.3185)	LR 4.000e-03
0: TRAIN [2][90/484]	Time 0.034 (0.036)	Data 7.56e-05 (3.08e-03)	Tok/s 59059 (51854)	Loss/tok 3.5170 (3.3281)	LR 4.000e-03
0: TRAIN [2][100/484]	Time 0.029 (0.035)	Data 9.01e-05 (2.78e-03)	Tok/s 44532 (51391)	Loss/tok 3.2583 (3.3289)	LR 4.000e-03
0: TRAIN [2][110/484]	Time 0.029 (0.034)	Data 7.63e-05 (2.54e-03)	Tok/s 45694 (50768)	Loss/tok 3.4023 (3.3306)	LR 4.000e-03
0: TRAIN [2][120/484]	Time 0.047 (0.035)	Data 7.84e-05 (2.34e-03)	Tok/s 82285 (51486)	Loss/tok 3.4092 (3.3612)	LR 4.000e-03
0: TRAIN [2][130/484]	Time 0.034 (0.035)	Data 7.61e-05 (2.17e-03)	Tok/s 60693 (52366)	Loss/tok 3.0957 (3.3717)	LR 4.000e-03
0: TRAIN [2][140/484]	Time 0.029 (0.035)	Data 7.72e-05 (2.02e-03)	Tok/s 43569 (52512)	Loss/tok 2.9808 (3.3798)	LR 4.000e-03
0: TRAIN [2][150/484]	Time 0.047 (0.035)	Data 7.80e-05 (1.89e-03)	Tok/s 80880 (53125)	Loss/tok 3.3577 (3.3950)	LR 4.000e-03
0: TRAIN [2][160/484]	Time 0.029 (0.035)	Data 7.82e-05 (1.78e-03)	Tok/s 44661 (53346)	Loss/tok 3.1153 (3.3892)	LR 4.000e-03
0: TRAIN [2][170/484]	Time 0.029 (0.035)	Data 7.77e-05 (1.68e-03)	Tok/s 44288 (53597)	Loss/tok 3.1431 (3.3888)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [2][180/484]	Time 0.029 (0.034)	Data 7.75e-05 (1.59e-03)	Tok/s 45160 (53440)	Loss/tok 3.2166 (3.3897)	LR 4.000e-03
0: TRAIN [2][190/484]	Time 0.040 (0.034)	Data 7.70e-05 (1.51e-03)	Tok/s 73976 (53563)	Loss/tok 3.3034 (3.3830)	LR 4.000e-03
0: TRAIN [2][200/484]	Time 0.040 (0.034)	Data 7.39e-05 (1.44e-03)	Tok/s 75964 (53981)	Loss/tok 3.2537 (3.3803)	LR 4.000e-03
0: TRAIN [2][210/484]	Time 0.047 (0.034)	Data 7.61e-05 (1.38e-03)	Tok/s 81185 (54132)	Loss/tok 3.8409 (3.3828)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][220/484]	Time 0.026 (0.034)	Data 8.39e-05 (1.32e-03)	Tok/s 47802 (54160)	Loss/tok 3.3959 (3.3857)	LR 4.000e-03
0: TRAIN [2][230/484]	Time 0.029 (0.034)	Data 7.58e-05 (1.26e-03)	Tok/s 44934 (54111)	Loss/tok 2.7612 (3.3837)	LR 4.000e-03
0: TRAIN [2][240/484]	Time 0.034 (0.034)	Data 7.56e-05 (1.21e-03)	Tok/s 60468 (54167)	Loss/tok 3.4653 (3.3862)	LR 4.000e-03
0: TRAIN [2][250/484]	Time 0.023 (0.034)	Data 7.72e-05 (1.17e-03)	Tok/s 27560 (54326)	Loss/tok 2.6725 (3.3830)	LR 4.000e-03
0: TRAIN [2][260/484]	Time 0.034 (0.034)	Data 7.75e-05 (1.13e-03)	Tok/s 61605 (54317)	Loss/tok 3.4160 (3.3799)	LR 4.000e-03
0: TRAIN [2][270/484]	Time 0.034 (0.034)	Data 7.39e-05 (1.09e-03)	Tok/s 62612 (54183)	Loss/tok 3.2514 (3.3766)	LR 4.000e-03
0: TRAIN [2][280/484]	Time 0.029 (0.034)	Data 7.94e-05 (1.05e-03)	Tok/s 43596 (54363)	Loss/tok 3.4265 (3.3770)	LR 4.000e-03
0: TRAIN [2][290/484]	Time 0.047 (0.034)	Data 7.72e-05 (1.02e-03)	Tok/s 75458 (54379)	Loss/tok 4.2721 (3.3827)	LR 4.000e-03
0: TRAIN [2][300/484]	Time 0.029 (0.034)	Data 7.70e-05 (9.88e-04)	Tok/s 45883 (54277)	Loss/tok 3.3040 (3.3799)	LR 4.000e-03
0: TRAIN [2][310/484]	Time 0.040 (0.034)	Data 7.61e-05 (9.59e-04)	Tok/s 69897 (54249)	Loss/tok 3.6199 (3.3824)	LR 4.000e-03
0: TRAIN [2][320/484]	Time 0.034 (0.034)	Data 7.75e-05 (9.32e-04)	Tok/s 62697 (54118)	Loss/tok 3.4215 (3.3785)	LR 4.000e-03
0: TRAIN [2][330/484]	Time 0.034 (0.034)	Data 7.82e-05 (9.06e-04)	Tok/s 63512 (54337)	Loss/tok 3.0602 (3.3848)	LR 4.000e-03
0: TRAIN [2][340/484]	Time 0.024 (0.034)	Data 7.63e-05 (8.82e-04)	Tok/s 27167 (54071)	Loss/tok 2.4622 (3.3788)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [2][350/484]	Time 0.023 (0.033)	Data 7.70e-05 (8.59e-04)	Tok/s 27544 (54150)	Loss/tok 2.9700 (3.3823)	LR 4.000e-03
0: TRAIN [2][360/484]	Time 0.029 (0.033)	Data 7.92e-05 (8.37e-04)	Tok/s 44910 (54095)	Loss/tok 2.7236 (3.3811)	LR 4.000e-03
0: TRAIN [2][370/484]	Time 0.034 (0.033)	Data 7.63e-05 (8.17e-04)	Tok/s 61608 (53866)	Loss/tok 3.4962 (3.3789)	LR 4.000e-03
0: TRAIN [2][380/484]	Time 0.023 (0.033)	Data 1.74e-04 (7.98e-04)	Tok/s 28135 (53989)	Loss/tok 2.8749 (3.3782)	LR 4.000e-03
0: TRAIN [2][390/484]	Time 0.040 (0.033)	Data 7.87e-05 (7.80e-04)	Tok/s 70777 (54065)	Loss/tok 3.5558 (3.3801)	LR 4.000e-03
0: TRAIN [2][400/484]	Time 0.029 (0.033)	Data 8.75e-05 (7.62e-04)	Tok/s 44930 (54006)	Loss/tok 3.0185 (3.3791)	LR 4.000e-03
0: TRAIN [2][410/484]	Time 0.034 (0.033)	Data 7.92e-05 (7.46e-04)	Tok/s 60730 (53822)	Loss/tok 3.3895 (3.3755)	LR 4.000e-03
0: TRAIN [2][420/484]	Time 0.034 (0.033)	Data 7.99e-05 (7.30e-04)	Tok/s 61232 (53853)	Loss/tok 3.4258 (3.3756)	LR 4.000e-03
0: TRAIN [2][430/484]	Time 0.029 (0.033)	Data 8.56e-05 (7.15e-04)	Tok/s 47129 (53786)	Loss/tok 3.0202 (3.3736)	LR 4.000e-03
0: TRAIN [2][440/484]	Time 0.040 (0.033)	Data 8.18e-05 (7.01e-04)	Tok/s 71717 (53753)	Loss/tok 3.9351 (3.3759)	LR 4.000e-03
0: TRAIN [2][450/484]	Time 0.029 (0.033)	Data 8.85e-05 (6.87e-04)	Tok/s 45286 (53632)	Loss/tok 3.4003 (3.3744)	LR 4.000e-03
0: TRAIN [2][460/484]	Time 0.029 (0.033)	Data 7.96e-05 (6.74e-04)	Tok/s 43996 (53726)	Loss/tok 3.2110 (3.3754)	LR 4.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [2][470/484]	Time 0.029 (0.033)	Data 8.11e-05 (6.61e-04)	Tok/s 42668 (53879)	Loss/tok 3.2684 (3.3774)	LR 4.000e-03
0: TRAIN [2][480/484]	Time 0.034 (0.033)	Data 8.39e-05 (6.49e-04)	Tok/s 61764 (53864)	Loss/tok 2.9923 (3.3775)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022110460, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593022110460, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.283 (0.283)	Decoder iters 122.0 (122.0)	Tok/s 9104 (9104)
0: Running moses detokenizer
0: BLEU(score=22.048693285822136, counts=[35962, 17362, 9591, 5499], totals=[65692, 62689, 59686, 56687], precisions=[54.743347744017534, 27.695448962337892, 16.06909493013437, 9.700636830313828], bp=1.0, sys_len=65692, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022110886, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2205, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593022110886, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.3812	Test BLEU: 22.05
0: Performance: Epoch: 2	Training: 13799967 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593022110886, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593022110887, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022110887, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 1127276217
0: TRAIN [3][0/484]	Time 0.377 (0.377)	Data 2.75e-01 (2.75e-01)	Tok/s 3489 (3489)	Loss/tok 3.0910 (3.0910)	LR 4.000e-03
0: TRAIN [3][10/484]	Time 0.029 (0.063)	Data 7.65e-05 (2.50e-02)	Tok/s 43250 (48841)	Loss/tok 2.7486 (3.2116)	LR 4.000e-03
0: TRAIN [3][20/484]	Time 0.034 (0.048)	Data 8.23e-05 (1.31e-02)	Tok/s 60370 (50306)	Loss/tok 3.4052 (3.2223)	LR 4.000e-03
0: TRAIN [3][30/484]	Time 0.047 (0.044)	Data 7.84e-05 (8.93e-03)	Tok/s 82005 (52757)	Loss/tok 3.7487 (3.2904)	LR 4.000e-03
0: TRAIN [3][40/484]	Time 0.034 (0.041)	Data 7.92e-05 (6.77e-03)	Tok/s 60336 (54105)	Loss/tok 3.4143 (3.2919)	LR 4.000e-03
0: TRAIN [3][50/484]	Time 0.024 (0.040)	Data 7.75e-05 (5.46e-03)	Tok/s 25888 (53506)	Loss/tok 2.4015 (3.2831)	LR 4.000e-03
0: TRAIN [3][60/484]	Time 0.023 (0.038)	Data 7.58e-05 (4.58e-03)	Tok/s 28566 (52382)	Loss/tok 2.4870 (3.2611)	LR 4.000e-03
0: TRAIN [3][70/484]	Time 0.023 (0.037)	Data 7.82e-05 (3.94e-03)	Tok/s 28885 (52666)	Loss/tok 2.6368 (3.2432)	LR 4.000e-03
0: TRAIN [3][80/484]	Time 0.029 (0.037)	Data 7.89e-05 (3.47e-03)	Tok/s 44878 (52982)	Loss/tok 2.9190 (3.2294)	LR 4.000e-03
0: TRAIN [3][90/484]	Time 0.029 (0.036)	Data 7.87e-05 (3.10e-03)	Tok/s 49140 (53001)	Loss/tok 2.6565 (3.2250)	LR 4.000e-03
0: TRAIN [3][100/484]	Time 0.024 (0.036)	Data 8.39e-05 (2.80e-03)	Tok/s 27181 (53564)	Loss/tok 2.6911 (3.2249)	LR 4.000e-03
0: TRAIN [3][110/484]	Time 0.040 (0.035)	Data 7.68e-05 (2.55e-03)	Tok/s 72489 (53207)	Loss/tok 3.5864 (3.2241)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [3][120/484]	Time 0.031 (0.035)	Data 8.73e-05 (2.35e-03)	Tok/s 67430 (52624)	Loss/tok 3.3135 (3.2293)	LR 4.000e-03
0: TRAIN [3][130/484]	Time 0.034 (0.035)	Data 7.75e-05 (2.18e-03)	Tok/s 58570 (52852)	Loss/tok 3.2006 (3.2342)	LR 4.000e-03
0: TRAIN [3][140/484]	Time 0.029 (0.034)	Data 7.75e-05 (2.03e-03)	Tok/s 44842 (52934)	Loss/tok 3.3362 (3.2350)	LR 4.000e-03
0: TRAIN [3][150/484]	Time 0.034 (0.034)	Data 7.70e-05 (1.90e-03)	Tok/s 59082 (53175)	Loss/tok 3.4988 (3.2368)	LR 4.000e-03
0: TRAIN [3][160/484]	Time 0.034 (0.034)	Data 7.70e-05 (1.78e-03)	Tok/s 60971 (52928)	Loss/tok 3.7612 (3.2347)	LR 4.000e-03
0: TRAIN [3][170/484]	Time 0.040 (0.034)	Data 7.51e-05 (1.68e-03)	Tok/s 72923 (53180)	Loss/tok 3.4639 (3.2346)	LR 4.000e-03
0: TRAIN [3][180/484]	Time 0.040 (0.034)	Data 7.94e-05 (1.60e-03)	Tok/s 74017 (53349)	Loss/tok 3.2626 (3.2409)	LR 4.000e-03
0: TRAIN [3][190/484]	Time 0.029 (0.034)	Data 7.68e-05 (1.52e-03)	Tok/s 42536 (53294)	Loss/tok 3.2730 (3.2469)	LR 4.000e-03
0: TRAIN [3][200/484]	Time 0.034 (0.034)	Data 7.65e-05 (1.45e-03)	Tok/s 60144 (53347)	Loss/tok 3.4189 (3.2463)	LR 4.000e-03
0: TRAIN [3][210/484]	Time 0.040 (0.034)	Data 7.53e-05 (1.38e-03)	Tok/s 74484 (53595)	Loss/tok 3.2039 (3.2495)	LR 4.000e-03
0: TRAIN [3][220/484]	Time 0.034 (0.034)	Data 7.70e-05 (1.32e-03)	Tok/s 59281 (53779)	Loss/tok 3.6077 (3.2583)	LR 4.000e-03
0: TRAIN [3][230/484]	Time 0.034 (0.034)	Data 7.58e-05 (1.27e-03)	Tok/s 64476 (53785)	Loss/tok 3.2228 (3.2614)	LR 4.000e-03
0: TRAIN [3][240/484]	Time 0.034 (0.034)	Data 7.70e-05 (1.22e-03)	Tok/s 62169 (54065)	Loss/tok 3.2590 (3.2609)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [3][250/484]	Time 0.034 (0.034)	Data 7.94e-05 (1.17e-03)	Tok/s 62397 (53899)	Loss/tok 3.3178 (3.2616)	LR 4.000e-03
0: TRAIN [3][260/484]	Time 0.029 (0.034)	Data 7.49e-05 (1.13e-03)	Tok/s 44955 (53736)	Loss/tok 2.9379 (3.2609)	LR 4.000e-03
0: TRAIN [3][270/484]	Time 0.024 (0.034)	Data 8.13e-05 (1.09e-03)	Tok/s 26774 (53676)	Loss/tok 2.6071 (3.2638)	LR 4.000e-03
0: TRAIN [3][280/484]	Time 0.029 (0.033)	Data 7.58e-05 (1.06e-03)	Tok/s 43135 (53482)	Loss/tok 3.0644 (3.2664)	LR 4.000e-03
0: TRAIN [3][290/484]	Time 0.034 (0.033)	Data 7.18e-05 (1.02e-03)	Tok/s 60231 (53506)	Loss/tok 3.2694 (3.2642)	LR 4.000e-03
0: TRAIN [3][300/484]	Time 0.029 (0.033)	Data 7.27e-05 (9.91e-04)	Tok/s 42494 (53570)	Loss/tok 3.0033 (3.2658)	LR 4.000e-03
0: TRAIN [3][310/484]	Time 0.040 (0.033)	Data 7.61e-05 (9.61e-04)	Tok/s 72328 (53812)	Loss/tok 3.6641 (3.2642)	LR 4.000e-03
0: TRAIN [3][320/484]	Time 0.040 (0.033)	Data 7.94e-05 (9.34e-04)	Tok/s 67491 (53639)	Loss/tok 3.9237 (3.2635)	LR 4.000e-03
0: TRAIN [3][330/484]	Time 0.029 (0.033)	Data 7.63e-05 (9.08e-04)	Tok/s 42467 (53493)	Loss/tok 2.8163 (3.2591)	LR 4.000e-03
0: TRAIN [3][340/484]	Time 0.034 (0.033)	Data 7.58e-05 (8.84e-04)	Tok/s 60305 (53491)	Loss/tok 3.2975 (3.2623)	LR 4.000e-03
0: TRAIN [3][350/484]	Time 0.029 (0.033)	Data 7.63e-05 (8.61e-04)	Tok/s 47723 (53330)	Loss/tok 2.7611 (3.2585)	LR 4.000e-03
0: TRAIN [3][360/484]	Time 0.034 (0.033)	Data 7.70e-05 (8.39e-04)	Tok/s 62559 (53450)	Loss/tok 2.9587 (3.2591)	LR 4.000e-03
0: TRAIN [3][370/484]	Time 0.023 (0.033)	Data 7.77e-05 (8.19e-04)	Tok/s 30617 (53508)	Loss/tok 2.7364 (3.2583)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [3][380/484]	Time 0.047 (0.033)	Data 7.65e-05 (8.00e-04)	Tok/s 79209 (53604)	Loss/tok 3.6464 (3.2607)	LR 4.000e-03
0: TRAIN [3][390/484]	Time 0.034 (0.033)	Data 7.37e-05 (7.81e-04)	Tok/s 60721 (53670)	Loss/tok 3.3348 (3.2604)	LR 4.000e-03
0: TRAIN [3][400/484]	Time 0.047 (0.033)	Data 7.39e-05 (7.64e-04)	Tok/s 81336 (53667)	Loss/tok 3.6144 (3.2624)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [3][410/484]	Time 0.034 (0.033)	Data 1.02e-04 (7.47e-04)	Tok/s 62178 (53857)	Loss/tok 3.2016 (3.2670)	LR 4.000e-03
0: TRAIN [3][420/484]	Time 0.029 (0.033)	Data 7.61e-05 (7.31e-04)	Tok/s 46147 (53752)	Loss/tok 3.3117 (3.2665)	LR 4.000e-03
0: TRAIN [3][430/484]	Time 0.029 (0.033)	Data 7.92e-05 (7.16e-04)	Tok/s 46602 (53924)	Loss/tok 2.8791 (3.2726)	LR 4.000e-03
0: TRAIN [3][440/484]	Time 0.024 (0.033)	Data 7.82e-05 (7.02e-04)	Tok/s 30060 (53972)	Loss/tok 2.5659 (3.2778)	LR 4.000e-03
0: TRAIN [3][450/484]	Time 0.023 (0.033)	Data 7.87e-05 (6.88e-04)	Tok/s 28020 (53872)	Loss/tok 2.6824 (3.2764)	LR 4.000e-03
0: TRAIN [3][460/484]	Time 0.029 (0.033)	Data 7.49e-05 (6.75e-04)	Tok/s 44736 (53785)	Loss/tok 3.2471 (3.2755)	LR 4.000e-03
0: TRAIN [3][470/484]	Time 0.029 (0.033)	Data 7.99e-05 (6.62e-04)	Tok/s 45279 (53765)	Loss/tok 2.8540 (3.2754)	LR 4.000e-03
0: TRAIN [3][480/484]	Time 0.040 (0.033)	Data 7.61e-05 (6.50e-04)	Tok/s 74327 (53863)	Loss/tok 3.6380 (3.2763)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022126971, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022126972, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.335 (0.335)	Decoder iters 144.0 (144.0)	Tok/s 7866 (7866)
0: Running moses detokenizer
0: BLEU(score=22.377802350372765, counts=[36054, 17547, 9760, 5673], totals=[65730, 62727, 59724, 56726], precisions=[54.851665905979004, 27.973599885216892, 16.341839126649255, 10.000705144025668], bp=1.0, sys_len=65730, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022127468, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2238, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022127468, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.2833	Test BLEU: 22.38
0: Performance: Epoch: 3	Training: 13787700 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593022127469, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593022127469, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022127469, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 2662952307
0: TRAIN [4][0/484]	Time 0.374 (0.374)	Data 2.67e-01 (2.67e-01)	Tok/s 3391 (3391)	Loss/tok 3.0009 (3.0009)	LR 4.000e-03
0: TRAIN [4][10/484]	Time 0.029 (0.061)	Data 8.01e-05 (2.44e-02)	Tok/s 42149 (43668)	Loss/tok 3.0041 (3.0983)	LR 4.000e-03
0: TRAIN [4][20/484]	Time 0.035 (0.048)	Data 7.82e-05 (1.28e-02)	Tok/s 59465 (50423)	Loss/tok 3.3821 (3.1672)	LR 4.000e-03
0: TRAIN [4][30/484]	Time 0.023 (0.042)	Data 7.49e-05 (8.70e-03)	Tok/s 30315 (49309)	Loss/tok 2.5854 (3.1752)	LR 4.000e-03
0: TRAIN [4][40/484]	Time 0.034 (0.040)	Data 7.58e-05 (6.60e-03)	Tok/s 59642 (50103)	Loss/tok 3.2289 (3.1751)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][50/484]	Time 0.030 (0.037)	Data 7.89e-05 (5.32e-03)	Tok/s 43660 (48375)	Loss/tok 2.7313 (3.1327)	LR 4.000e-03
0: TRAIN [4][60/484]	Time 0.040 (0.037)	Data 1.01e-04 (4.46e-03)	Tok/s 74472 (50875)	Loss/tok 3.4019 (3.1539)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [4][70/484]	Time 0.029 (0.037)	Data 7.94e-05 (3.85e-03)	Tok/s 46862 (52055)	Loss/tok 2.9388 (3.1909)	LR 4.000e-03
0: TRAIN [4][80/484]	Time 0.035 (0.036)	Data 7.96e-05 (3.38e-03)	Tok/s 58899 (52483)	Loss/tok 3.2267 (3.2031)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [4][90/484]	Time 0.029 (0.036)	Data 7.80e-05 (3.02e-03)	Tok/s 42585 (53575)	Loss/tok 2.9687 (3.2162)	LR 4.000e-03
0: TRAIN [4][100/484]	Time 0.029 (0.036)	Data 9.18e-05 (2.73e-03)	Tok/s 42707 (54517)	Loss/tok 3.3574 (3.2362)	LR 4.000e-03
0: TRAIN [4][110/484]	Time 0.029 (0.036)	Data 7.77e-05 (2.49e-03)	Tok/s 44054 (54728)	Loss/tok 3.2174 (3.2431)	LR 4.000e-03
0: TRAIN [4][120/484]	Time 0.029 (0.036)	Data 7.70e-05 (2.29e-03)	Tok/s 45799 (54949)	Loss/tok 3.0815 (3.2422)	LR 4.000e-03
0: TRAIN [4][130/484]	Time 0.023 (0.036)	Data 7.68e-05 (2.12e-03)	Tok/s 29475 (55253)	Loss/tok 2.5497 (3.2400)	LR 4.000e-03
0: TRAIN [4][140/484]	Time 0.034 (0.036)	Data 7.61e-05 (1.98e-03)	Tok/s 58024 (55538)	Loss/tok 3.0873 (3.2502)	LR 4.000e-03
0: TRAIN [4][150/484]	Time 0.024 (0.036)	Data 7.82e-05 (1.85e-03)	Tok/s 27482 (55408)	Loss/tok 2.5446 (3.2580)	LR 4.000e-03
0: TRAIN [4][160/484]	Time 0.029 (0.035)	Data 7.53e-05 (1.74e-03)	Tok/s 44798 (55439)	Loss/tok 3.1610 (3.2573)	LR 4.000e-03
0: TRAIN [4][170/484]	Time 0.029 (0.035)	Data 1.64e-04 (1.65e-03)	Tok/s 44536 (55350)	Loss/tok 2.7090 (3.2516)	LR 4.000e-03
0: TRAIN [4][180/484]	Time 0.029 (0.035)	Data 8.11e-05 (1.56e-03)	Tok/s 47211 (55699)	Loss/tok 3.0918 (3.2630)	LR 4.000e-03
0: TRAIN [4][190/484]	Time 0.024 (0.035)	Data 7.96e-05 (1.48e-03)	Tok/s 27318 (55090)	Loss/tok 2.5889 (3.2553)	LR 4.000e-03
0: TRAIN [4][200/484]	Time 0.023 (0.035)	Data 7.75e-05 (1.41e-03)	Tok/s 28765 (54910)	Loss/tok 2.6912 (3.2525)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [4][210/484]	Time 0.035 (0.034)	Data 7.53e-05 (1.35e-03)	Tok/s 60476 (54667)	Loss/tok 3.1170 (3.2443)	LR 4.000e-03
0: TRAIN [4][220/484]	Time 0.040 (0.035)	Data 1.07e-04 (1.29e-03)	Tok/s 73204 (55013)	Loss/tok 3.7579 (3.2457)	LR 4.000e-03
0: TRAIN [4][230/484]	Time 0.034 (0.034)	Data 7.53e-05 (1.24e-03)	Tok/s 62557 (55005)	Loss/tok 2.9075 (3.2425)	LR 4.000e-03
0: TRAIN [4][240/484]	Time 0.040 (0.034)	Data 9.92e-05 (1.19e-03)	Tok/s 75389 (55110)	Loss/tok 3.3291 (3.2404)	LR 4.000e-03
0: TRAIN [4][250/484]	Time 0.034 (0.034)	Data 7.68e-05 (1.15e-03)	Tok/s 63137 (54971)	Loss/tok 3.1329 (3.2393)	LR 4.000e-03
0: TRAIN [4][260/484]	Time 0.029 (0.034)	Data 7.87e-05 (1.11e-03)	Tok/s 46544 (54634)	Loss/tok 2.8266 (3.2353)	LR 4.000e-03
0: TRAIN [4][270/484]	Time 0.029 (0.034)	Data 9.97e-05 (1.07e-03)	Tok/s 46419 (54455)	Loss/tok 2.9820 (3.2326)	LR 4.000e-03
0: TRAIN [4][280/484]	Time 0.040 (0.034)	Data 1.02e-04 (1.04e-03)	Tok/s 75094 (54151)	Loss/tok 3.4639 (3.2290)	LR 4.000e-03
0: TRAIN [4][290/484]	Time 0.034 (0.034)	Data 7.75e-05 (1.00e-03)	Tok/s 60761 (54014)	Loss/tok 2.9868 (3.2269)	LR 4.000e-03
0: TRAIN [4][300/484]	Time 0.029 (0.034)	Data 1.16e-04 (9.73e-04)	Tok/s 45953 (53754)	Loss/tok 3.1246 (3.2244)	LR 4.000e-03
0: TRAIN [4][310/484]	Time 0.040 (0.034)	Data 1.20e-04 (9.45e-04)	Tok/s 73364 (53984)	Loss/tok 3.4839 (3.2229)	LR 4.000e-03
0: TRAIN [4][320/484]	Time 0.040 (0.034)	Data 1.02e-04 (9.19e-04)	Tok/s 75024 (53998)	Loss/tok 3.2978 (3.2210)	LR 4.000e-03
0: TRAIN [4][330/484]	Time 0.030 (0.033)	Data 8.08e-05 (8.94e-04)	Tok/s 41871 (53941)	Loss/tok 2.9973 (3.2181)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][340/484]	Time 0.040 (0.033)	Data 1.98e-04 (8.71e-04)	Tok/s 74525 (53859)	Loss/tok 3.4199 (3.2203)	LR 2.000e-03
0: TRAIN [4][350/484]	Time 0.029 (0.033)	Data 7.96e-05 (8.49e-04)	Tok/s 45473 (53971)	Loss/tok 2.8947 (3.2209)	LR 2.000e-03
0: TRAIN [4][360/484]	Time 0.040 (0.033)	Data 1.11e-04 (8.28e-04)	Tok/s 70124 (54073)	Loss/tok 3.5025 (3.2210)	LR 2.000e-03
0: TRAIN [4][370/484]	Time 0.034 (0.033)	Data 7.39e-05 (8.08e-04)	Tok/s 59639 (54093)	Loss/tok 3.1399 (3.2188)	LR 2.000e-03
0: TRAIN [4][380/484]	Time 0.040 (0.033)	Data 8.03e-05 (7.89e-04)	Tok/s 73206 (54097)	Loss/tok 3.2300 (3.2166)	LR 2.000e-03
0: TRAIN [4][390/484]	Time 0.040 (0.033)	Data 8.25e-05 (7.71e-04)	Tok/s 71593 (54048)	Loss/tok 3.4270 (3.2151)	LR 2.000e-03
0: TRAIN [4][400/484]	Time 0.034 (0.033)	Data 7.82e-05 (7.53e-04)	Tok/s 61147 (54038)	Loss/tok 2.9752 (3.2108)	LR 2.000e-03
0: TRAIN [4][410/484]	Time 0.034 (0.033)	Data 7.94e-05 (7.37e-04)	Tok/s 61470 (53983)	Loss/tok 3.2750 (3.2086)	LR 2.000e-03
0: TRAIN [4][420/484]	Time 0.024 (0.033)	Data 8.30e-05 (7.21e-04)	Tok/s 27500 (53928)	Loss/tok 2.4758 (3.2071)	LR 2.000e-03
0: TRAIN [4][430/484]	Time 0.040 (0.033)	Data 8.01e-05 (7.07e-04)	Tok/s 72220 (54035)	Loss/tok 3.4136 (3.2098)	LR 2.000e-03
0: TRAIN [4][440/484]	Time 0.029 (0.033)	Data 7.92e-05 (6.92e-04)	Tok/s 45704 (54129)	Loss/tok 2.8612 (3.2095)	LR 2.000e-03
0: TRAIN [4][450/484]	Time 0.024 (0.033)	Data 9.63e-05 (6.79e-04)	Tok/s 27909 (54062)	Loss/tok 2.6666 (3.2091)	LR 2.000e-03
0: TRAIN [4][460/484]	Time 0.029 (0.033)	Data 8.11e-05 (6.66e-04)	Tok/s 44190 (53997)	Loss/tok 2.6979 (3.2084)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [4][470/484]	Time 0.023 (0.033)	Data 1.05e-04 (6.54e-04)	Tok/s 27932 (53906)	Loss/tok 2.7546 (3.2078)	LR 2.000e-03
0: TRAIN [4][480/484]	Time 0.028 (0.033)	Data 1.06e-04 (6.43e-04)	Tok/s 45092 (53903)	Loss/tok 3.0916 (3.2067)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593022143552, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593022143552, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.232 (0.232)	Decoder iters 98.0 (98.0)	Tok/s 11132 (11132)
0: Running moses detokenizer
0: BLEU(score=23.541326201198324, counts=[36969, 18380, 10395, 6094], totals=[65780, 62777, 59775, 56777], precisions=[56.20097294010338, 29.278238845437023, 17.390213299874528, 10.733219437448263], bp=1.0, sys_len=65780, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022144039, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2354, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593022144039, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.2004	Test BLEU: 23.54
0: Performance: Epoch: 4	Training: 13789371 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1593022144039, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593022144039, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593022144039, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 4277088118
0: TRAIN [5][0/484]	Time 0.365 (0.365)	Data 3.37e-01 (3.37e-01)	Tok/s 3648 (3648)	Loss/tok 2.9906 (2.9906)	LR 2.000e-03
0: TRAIN [5][10/484]	Time 0.034 (0.063)	Data 7.92e-05 (3.07e-02)	Tok/s 59788 (48304)	Loss/tok 3.1980 (3.1173)	LR 2.000e-03
0: TRAIN [5][20/484]	Time 0.029 (0.048)	Data 7.89e-05 (1.61e-02)	Tok/s 48054 (49058)	Loss/tok 2.8731 (3.0389)	LR 2.000e-03
0: TRAIN [5][30/484]	Time 0.040 (0.043)	Data 7.63e-05 (1.10e-02)	Tok/s 74465 (50358)	Loss/tok 3.0631 (3.0632)	LR 2.000e-03
0: TRAIN [5][40/484]	Time 0.029 (0.041)	Data 7.61e-05 (8.31e-03)	Tok/s 45749 (53834)	Loss/tok 2.7041 (3.1034)	LR 2.000e-03
0: TRAIN [5][50/484]	Time 0.040 (0.040)	Data 7.63e-05 (6.70e-03)	Tok/s 73905 (54527)	Loss/tok 3.0661 (3.0868)	LR 2.000e-03
0: TRAIN [5][60/484]	Time 0.029 (0.038)	Data 7.70e-05 (5.61e-03)	Tok/s 45539 (53760)	Loss/tok 2.5914 (3.0556)	LR 2.000e-03
0: TRAIN [5][70/484]	Time 0.023 (0.037)	Data 8.11e-05 (4.83e-03)	Tok/s 25810 (53728)	Loss/tok 2.5057 (3.0747)	LR 2.000e-03
0: TRAIN [5][80/484]	Time 0.028 (0.037)	Data 7.58e-05 (4.25e-03)	Tok/s 45517 (54009)	Loss/tok 2.7871 (3.0727)	LR 2.000e-03
0: TRAIN [5][90/484]	Time 0.034 (0.036)	Data 7.99e-05 (3.79e-03)	Tok/s 62429 (53804)	Loss/tok 3.2044 (3.0746)	LR 2.000e-03
0: TRAIN [5][100/484]	Time 0.034 (0.036)	Data 7.68e-05 (3.42e-03)	Tok/s 61246 (53236)	Loss/tok 3.0138 (3.0642)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [5][110/484]	Time 0.035 (0.035)	Data 7.41e-05 (3.12e-03)	Tok/s 57753 (52987)	Loss/tok 3.0867 (3.0662)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [5][120/484]	Time 0.029 (0.035)	Data 7.87e-05 (2.87e-03)	Tok/s 45125 (52969)	Loss/tok 2.7961 (3.0748)	LR 2.000e-03
0: TRAIN [5][130/484]	Time 0.040 (0.035)	Data 7.77e-05 (2.66e-03)	Tok/s 74539 (53298)	Loss/tok 3.3009 (3.0727)	LR 2.000e-03
0: TRAIN [5][140/484]	Time 0.034 (0.035)	Data 1.07e-04 (2.48e-03)	Tok/s 62613 (53528)	Loss/tok 3.2123 (3.0766)	LR 1.000e-03
0: TRAIN [5][150/484]	Time 0.034 (0.035)	Data 1.13e-04 (2.32e-03)	Tok/s 62642 (53625)	Loss/tok 2.9601 (3.0772)	LR 1.000e-03
0: TRAIN [5][160/484]	Time 0.034 (0.035)	Data 1.08e-04 (2.18e-03)	Tok/s 61886 (54337)	Loss/tok 2.9173 (3.0786)	LR 1.000e-03
0: TRAIN [5][170/484]	Time 0.048 (0.035)	Data 9.51e-05 (2.06e-03)	Tok/s 79421 (54553)	Loss/tok 3.5840 (3.0896)	LR 1.000e-03
0: TRAIN [5][180/484]	Time 0.029 (0.035)	Data 1.09e-04 (1.95e-03)	Tok/s 47033 (54905)	Loss/tok 2.8044 (3.0956)	LR 1.000e-03
0: TRAIN [5][190/484]	Time 0.029 (0.035)	Data 1.18e-04 (1.85e-03)	Tok/s 44067 (54818)	Loss/tok 2.9425 (3.0997)	LR 1.000e-03
0: TRAIN [5][200/484]	Time 0.029 (0.035)	Data 8.01e-05 (1.77e-03)	Tok/s 42488 (54868)	Loss/tok 3.0347 (3.0978)	LR 1.000e-03
0: TRAIN [5][210/484]	Time 0.034 (0.034)	Data 9.82e-05 (1.69e-03)	Tok/s 60741 (54791)	Loss/tok 3.1643 (3.1042)	LR 1.000e-03
0: TRAIN [5][220/484]	Time 0.028 (0.034)	Data 1.04e-04 (1.62e-03)	Tok/s 47118 (54889)	Loss/tok 3.0410 (3.0989)	LR 1.000e-03
0: TRAIN [5][230/484]	Time 0.047 (0.034)	Data 1.03e-04 (1.55e-03)	Tok/s 76184 (54875)	Loss/tok 3.7132 (3.1037)	LR 1.000e-03
0: TRAIN [5][240/484]	Time 0.034 (0.034)	Data 1.08e-04 (1.49e-03)	Tok/s 62292 (54779)	Loss/tok 3.3195 (3.1020)	LR 1.000e-03
0: Upscaling, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [5][250/484]	Time 0.023 (0.034)	Data 1.21e-04 (1.44e-03)	Tok/s 28311 (54604)	Loss/tok 2.4431 (3.0987)	LR 1.000e-03
0: TRAIN [5][260/484]	Time 0.029 (0.034)	Data 7.65e-05 (1.38e-03)	Tok/s 45526 (54540)	Loss/tok 3.1816 (3.0988)	LR 1.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [5][270/484]	Time 0.026 (0.034)	Data 1.12e-04 (1.34e-03)	Tok/s 52684 (54556)	Loss/tok 2.8944 (3.0998)	LR 1.000e-03
0: TRAIN [5][280/484]	Time 0.029 (0.034)	Data 1.08e-04 (1.29e-03)	Tok/s 44447 (54461)	Loss/tok 3.3211 (3.1053)	LR 1.000e-03
0: TRAIN [5][290/484]	Time 0.024 (0.034)	Data 8.11e-05 (1.25e-03)	Tok/s 28620 (54255)	Loss/tok 2.5004 (3.1049)	LR 1.000e-03
0: TRAIN [5][300/484]	Time 0.029 (0.034)	Data 7.63e-05 (1.21e-03)	Tok/s 43359 (54040)	Loss/tok 3.0076 (3.1015)	LR 1.000e-03
0: TRAIN [5][310/484]	Time 0.029 (0.034)	Data 7.80e-05 (1.18e-03)	Tok/s 44862 (54026)	Loss/tok 3.1132 (3.1019)	LR 1.000e-03
0: TRAIN [5][320/484]	Time 0.034 (0.034)	Data 1.18e-04 (1.14e-03)	Tok/s 63443 (54075)	Loss/tok 2.9890 (3.1001)	LR 1.000e-03
0: TRAIN [5][330/484]	Time 0.028 (0.034)	Data 1.11e-04 (1.11e-03)	Tok/s 45010 (54170)	Loss/tok 2.7192 (3.1016)	LR 1.000e-03
0: TRAIN [5][340/484]	Time 0.029 (0.034)	Data 7.72e-05 (1.08e-03)	Tok/s 43754 (54200)	Loss/tok 2.9835 (3.1038)	LR 1.000e-03
0: TRAIN [5][350/484]	Time 0.024 (0.033)	Data 7.82e-05 (1.05e-03)	Tok/s 26735 (54144)	Loss/tok 2.8706 (3.1032)	LR 1.000e-03
0: TRAIN [5][360/484]	Time 0.028 (0.033)	Data 1.03e-04 (1.03e-03)	Tok/s 47566 (54067)	Loss/tok 2.8813 (3.0985)	LR 1.000e-03
0: TRAIN [5][370/484]	Time 0.034 (0.033)	Data 7.96e-05 (1.00e-03)	Tok/s 59404 (54064)	Loss/tok 2.9393 (3.0974)	LR 1.000e-03
0: TRAIN [5][380/484]	Time 0.040 (0.033)	Data 1.13e-04 (9.78e-04)	Tok/s 74651 (54132)	Loss/tok 3.0152 (3.0951)	LR 1.000e-03
0: TRAIN [5][390/484]	Time 0.034 (0.033)	Data 7.84e-05 (9.55e-04)	Tok/s 60996 (54070)	Loss/tok 2.8991 (3.0919)	LR 1.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [5][400/484]	Time 0.035 (0.033)	Data 7.80e-05 (9.34e-04)	Tok/s 59596 (53884)	Loss/tok 3.1119 (3.0901)	LR 1.000e-03
0: TRAIN [5][410/484]	Time 0.029 (0.033)	Data 7.96e-05 (9.13e-04)	Tok/s 46369 (53908)	Loss/tok 2.8207 (3.0947)	LR 1.000e-03
0: TRAIN [5][420/484]	Time 0.039 (0.033)	Data 1.90e-04 (8.94e-04)	Tok/s 73728 (53952)	Loss/tok 3.3940 (3.0952)	LR 1.000e-03
0: TRAIN [5][430/484]	Time 0.035 (0.033)	Data 7.82e-05 (8.75e-04)	Tok/s 60467 (53989)	Loss/tok 2.8182 (3.0933)	LR 5.000e-04
0: TRAIN [5][440/484]	Time 0.035 (0.033)	Data 7.82e-05 (8.58e-04)	Tok/s 61739 (54027)	Loss/tok 3.0643 (3.0959)	LR 5.000e-04
0: TRAIN [5][450/484]	Time 0.035 (0.033)	Data 8.03e-05 (8.41e-04)	Tok/s 60990 (53875)	Loss/tok 2.9968 (3.0941)	LR 5.000e-04
0: TRAIN [5][460/484]	Time 0.023 (0.033)	Data 8.13e-05 (8.25e-04)	Tok/s 28893 (53826)	Loss/tok 2.6727 (3.0914)	LR 5.000e-04
0: TRAIN [5][470/484]	Time 0.040 (0.033)	Data 1.06e-04 (8.09e-04)	Tok/s 71689 (53926)	Loss/tok 3.3937 (3.0942)	LR 5.000e-04
0: TRAIN [5][480/484]	Time 0.029 (0.033)	Data 7.94e-05 (7.94e-04)	Tok/s 46153 (54005)	Loss/tok 2.8128 (3.0968)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593022160095, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593022160096, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.281 (0.281)	Decoder iters 101.0 (101.0)	Tok/s 9059 (9059)
0: Running moses detokenizer
0: BLEU(score=24.13698369373227, counts=[36922, 18467, 10497, 6213], totals=[64759, 61756, 58754, 55757], precisions=[57.01446903133155, 29.90316730358184, 17.86601763284202, 11.14299549832308], bp=1.0, sys_len=64759, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593022160530, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2414, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593022160531, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.0853	Test BLEU: 24.14
0: Performance: Epoch: 5	Training: 13801412 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1593022160531, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593022160531, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:09:27 AM
RESULT,RNN_TRANSLATOR,,140,nvidia,2020-06-24 11:07:07 AM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 11:09:28 AM
RESULT,RNN_TRANSLATOR,,141,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
ENDING TIMING RUN AT 2020-06-24 11:09:29 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,142,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
slurmstepd: error: _is_a_lwp: open() /proc/76796/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:30 AM
RESULT,RNN_TRANSLATOR,,143,nvidia,2020-06-24 11:07:07 AM
ENDING TIMING RUN AT 2020-06-24 11:09:31 AM
RESULT,RNN_TRANSLATOR,,144,nvidia,2020-06-24 11:07:07 AM
