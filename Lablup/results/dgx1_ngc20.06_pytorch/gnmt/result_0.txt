+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019117722, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019117758, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019117758, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019117759, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019117759, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: Job 467813 step creation temporarily disabled, retrying
srun: Step created for job 467813
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-446
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019124050, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177094/results:/results ./run_and_time.sh
srun: Job 467813 step creation temporarily disabled, retrying
srun: Step created for job 467813
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
running benchmark
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
STARTING TIMING RUN AT 2020-06-24 10:18:46 AM
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ '[' -n 0 ']'
+ DECAY_INTERVAL=809
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019128282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128281, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128297, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019128323, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2197702839
:::MLLOG {"namespace": "", "time_ms": 1593019134167, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2197702839, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1372145621
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019142271, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019142272, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019142272, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019142272, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019142272, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019143852, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019143853, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019143853, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019144121, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019144122, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019144123, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019144123, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019144124, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019144124, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019144124, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019144124, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019144124, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019144124, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019144125, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019144125, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2498723167
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.451 (0.451)	Data 2.98e-01 (2.98e-01)	Tok/s 37007 (37007)	Loss/tok 10.7019 (10.7019)	LR 2.047e-05
0: TRAIN [0][10/1938]	Time 0.105 (0.179)	Data 1.81e-04 (2.72e-02)	Tok/s 99337 (97805)	Loss/tok 9.6102 (10.1416)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.155 (0.156)	Data 1.84e-04 (1.44e-02)	Tok/s 108468 (99996)	Loss/tok 9.3415 (9.8444)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.105 (0.140)	Data 1.31e-04 (9.77e-03)	Tok/s 95698 (99703)	Loss/tok 9.0235 (9.6627)	LR 4.083e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1938]	Time 0.156 (0.139)	Data 1.58e-04 (7.43e-03)	Tok/s 106787 (100158)	Loss/tok 8.9311 (9.5135)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.105 (0.138)	Data 1.60e-04 (6.01e-03)	Tok/s 98044 (100531)	Loss/tok 8.5645 (9.3629)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.208 (0.134)	Data 1.41e-04 (5.05e-03)	Tok/s 111645 (100352)	Loss/tok 8.5204 (9.2308)	LR 7.962e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][70/1938]	Time 0.106 (0.136)	Data 1.67e-04 (4.36e-03)	Tok/s 97865 (100736)	Loss/tok 8.6956 (9.1711)	LR 9.796e-05
0: TRAIN [0][80/1938]	Time 0.105 (0.133)	Data 1.61e-04 (3.85e-03)	Tok/s 98034 (100442)	Loss/tok 8.0606 (9.0756)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.209 (0.135)	Data 1.99e-04 (3.44e-03)	Tok/s 111695 (100921)	Loss/tok 8.1205 (8.9572)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.208 (0.137)	Data 1.80e-04 (3.12e-03)	Tok/s 112426 (101415)	Loss/tok 8.0751 (8.8481)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.106 (0.138)	Data 1.80e-04 (2.85e-03)	Tok/s 96747 (101585)	Loss/tok 7.8189 (8.7618)	LR 2.461e-04
0: TRAIN [0][120/1938]	Time 0.157 (0.139)	Data 1.65e-04 (2.63e-03)	Tok/s 107423 (101745)	Loss/tok 7.8608 (8.6893)	LR 3.098e-04
0: TRAIN [0][130/1938]	Time 0.106 (0.138)	Data 1.45e-04 (2.44e-03)	Tok/s 98325 (101771)	Loss/tok 7.6752 (8.6275)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.060 (0.137)	Data 1.61e-04 (2.28e-03)	Tok/s 89681 (101699)	Loss/tok 6.9217 (8.5723)	LR 4.909e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][150/1938]	Time 0.106 (0.136)	Data 1.81e-04 (2.14e-03)	Tok/s 97360 (101651)	Loss/tok 7.6217 (8.5200)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.157 (0.136)	Data 1.58e-04 (2.02e-03)	Tok/s 106167 (101671)	Loss/tok 7.7573 (8.4719)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.157 (0.137)	Data 1.54e-04 (1.91e-03)	Tok/s 107498 (101676)	Loss/tok 7.6952 (8.4227)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.106 (0.137)	Data 1.60e-04 (1.81e-03)	Tok/s 99752 (101799)	Loss/tok 7.2676 (8.3684)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.106 (0.137)	Data 1.83e-04 (1.73e-03)	Tok/s 97394 (101789)	Loss/tok 7.0734 (8.3160)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.157 (0.137)	Data 2.06e-04 (1.65e-03)	Tok/s 106643 (101769)	Loss/tok 7.1668 (8.2585)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.107 (0.138)	Data 1.94e-04 (1.58e-03)	Tok/s 96574 (101823)	Loss/tok 6.7579 (8.1948)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.060 (0.139)	Data 1.75e-04 (1.52e-03)	Tok/s 86629 (101903)	Loss/tok 5.8646 (8.1300)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.059 (0.137)	Data 1.80e-04 (1.46e-03)	Tok/s 87152 (101713)	Loss/tok 5.6537 (8.0790)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.157 (0.138)	Data 1.46e-04 (1.41e-03)	Tok/s 107195 (101808)	Loss/tok 6.6344 (8.0139)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.107 (0.139)	Data 1.81e-04 (1.36e-03)	Tok/s 96214 (101910)	Loss/tok 6.1230 (7.9460)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.106 (0.138)	Data 1.54e-04 (1.31e-03)	Tok/s 95670 (101781)	Loss/tok 6.0454 (7.8904)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][270/1938]	Time 0.108 (0.138)	Data 1.55e-04 (1.27e-03)	Tok/s 98014 (101778)	Loss/tok 5.9481 (7.8276)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.106 (0.137)	Data 1.67e-04 (1.23e-03)	Tok/s 98137 (101655)	Loss/tok 5.9407 (7.7721)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.159 (0.138)	Data 1.94e-04 (1.19e-03)	Tok/s 106583 (101673)	Loss/tok 6.1496 (7.7075)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.107 (0.138)	Data 1.89e-04 (1.16e-03)	Tok/s 95834 (101625)	Loss/tok 5.6966 (7.6515)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.107 (0.138)	Data 1.67e-04 (1.13e-03)	Tok/s 94175 (101583)	Loss/tok 5.4761 (7.5923)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.159 (0.138)	Data 2.06e-04 (1.10e-03)	Tok/s 106690 (101572)	Loss/tok 5.8199 (7.5367)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.106 (0.138)	Data 1.70e-04 (1.07e-03)	Tok/s 97509 (101579)	Loss/tok 5.3648 (7.4779)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.159 (0.139)	Data 1.59e-04 (1.04e-03)	Tok/s 106779 (101660)	Loss/tok 5.7226 (7.4188)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.211 (0.140)	Data 1.66e-04 (1.02e-03)	Tok/s 111993 (101839)	Loss/tok 5.6310 (7.3480)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.212 (0.140)	Data 2.06e-04 (9.95e-04)	Tok/s 111127 (101852)	Loss/tok 5.5769 (7.2920)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.159 (0.140)	Data 1.47e-04 (9.73e-04)	Tok/s 104689 (101792)	Loss/tok 5.3562 (7.2426)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.158 (0.140)	Data 1.57e-04 (9.52e-04)	Tok/s 106006 (101780)	Loss/tok 5.2997 (7.1891)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.159 (0.140)	Data 1.81e-04 (9.32e-04)	Tok/s 107112 (101782)	Loss/tok 5.1463 (7.1368)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][400/1938]	Time 0.107 (0.140)	Data 1.64e-04 (9.13e-04)	Tok/s 96828 (101739)	Loss/tok 4.6860 (7.0877)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.269 (0.140)	Data 1.87e-04 (8.95e-04)	Tok/s 109772 (101645)	Loss/tok 5.5177 (7.0417)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.107 (0.140)	Data 1.85e-04 (8.78e-04)	Tok/s 96362 (101649)	Loss/tok 4.6769 (6.9939)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.108 (0.140)	Data 1.64e-04 (8.62e-04)	Tok/s 95484 (101640)	Loss/tok 4.4114 (6.9433)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.107 (0.140)	Data 1.60e-04 (8.46e-04)	Tok/s 98398 (101593)	Loss/tok 4.7627 (6.8993)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.270 (0.140)	Data 1.64e-04 (8.31e-04)	Tok/s 109373 (101649)	Loss/tok 5.2362 (6.8448)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.060 (0.141)	Data 1.75e-04 (8.17e-04)	Tok/s 88056 (101677)	Loss/tok 3.5943 (6.7940)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.107 (0.140)	Data 1.71e-04 (8.03e-04)	Tok/s 95396 (101607)	Loss/tok 4.3283 (6.7538)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.107 (0.140)	Data 1.93e-04 (7.90e-04)	Tok/s 96626 (101611)	Loss/tok 4.2117 (6.7088)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.107 (0.141)	Data 1.64e-04 (7.77e-04)	Tok/s 96731 (101655)	Loss/tok 4.1795 (6.6597)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.107 (0.140)	Data 1.75e-04 (7.65e-04)	Tok/s 96261 (101594)	Loss/tok 4.1323 (6.6207)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.060 (0.140)	Data 1.64e-04 (7.53e-04)	Tok/s 87739 (101551)	Loss/tok 3.5604 (6.5798)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.108 (0.140)	Data 1.88e-04 (7.42e-04)	Tok/s 96578 (101508)	Loss/tok 4.1445 (6.5405)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][530/1938]	Time 0.159 (0.140)	Data 1.72e-04 (7.31e-04)	Tok/s 103818 (101440)	Loss/tok 4.5184 (6.5056)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.107 (0.140)	Data 1.68e-04 (7.21e-04)	Tok/s 97100 (101432)	Loss/tok 4.0376 (6.4653)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.107 (0.140)	Data 1.64e-04 (7.11e-04)	Tok/s 98490 (101443)	Loss/tok 4.0662 (6.4261)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.159 (0.140)	Data 1.65e-04 (7.01e-04)	Tok/s 105881 (101455)	Loss/tok 4.5165 (6.3866)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.107 (0.140)	Data 1.39e-04 (6.92e-04)	Tok/s 96414 (101413)	Loss/tok 4.1937 (6.3516)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.211 (0.140)	Data 1.59e-04 (6.83e-04)	Tok/s 111037 (101413)	Loss/tok 4.6374 (6.3154)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.107 (0.141)	Data 1.94e-04 (6.74e-04)	Tok/s 97054 (101450)	Loss/tok 3.9604 (6.2750)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.108 (0.140)	Data 1.56e-04 (6.65e-04)	Tok/s 96737 (101392)	Loss/tok 3.9815 (6.2459)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.212 (0.141)	Data 1.65e-04 (6.57e-04)	Tok/s 108959 (101427)	Loss/tok 4.5258 (6.2073)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.212 (0.142)	Data 1.70e-04 (6.49e-04)	Tok/s 110536 (101535)	Loss/tok 4.2779 (6.1646)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.160 (0.142)	Data 1.88e-04 (6.42e-04)	Tok/s 105459 (101528)	Loss/tok 4.1807 (6.1323)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.107 (0.141)	Data 1.70e-04 (6.34e-04)	Tok/s 94489 (101439)	Loss/tok 3.8778 (6.1075)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.108 (0.142)	Data 1.62e-04 (6.27e-04)	Tok/s 95344 (101482)	Loss/tok 3.8703 (6.0720)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][660/1938]	Time 0.211 (0.141)	Data 1.87e-04 (6.20e-04)	Tok/s 111584 (101447)	Loss/tok 4.2212 (6.0441)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.161 (0.142)	Data 1.52e-04 (6.13e-04)	Tok/s 104601 (101473)	Loss/tok 4.2392 (6.0117)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.107 (0.142)	Data 1.86e-04 (6.07e-04)	Tok/s 96340 (101507)	Loss/tok 3.9064 (5.9807)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][690/1938]	Time 0.060 (0.142)	Data 1.67e-04 (6.00e-04)	Tok/s 88107 (101492)	Loss/tok 3.2447 (5.9530)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][700/1938]	Time 0.108 (0.142)	Data 1.97e-04 (5.94e-04)	Tok/s 93959 (101453)	Loss/tok 3.7521 (5.9286)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.107 (0.142)	Data 1.94e-04 (5.88e-04)	Tok/s 95720 (101378)	Loss/tok 3.8127 (5.9077)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.161 (0.142)	Data 1.63e-04 (5.83e-04)	Tok/s 105479 (101368)	Loss/tok 4.1721 (5.8829)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.108 (0.141)	Data 1.80e-04 (5.77e-04)	Tok/s 96780 (101324)	Loss/tok 3.7862 (5.8608)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.161 (0.141)	Data 1.55e-04 (5.72e-04)	Tok/s 103151 (101317)	Loss/tok 4.0238 (5.8363)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.108 (0.141)	Data 1.43e-04 (5.66e-04)	Tok/s 93926 (101283)	Loss/tok 3.7422 (5.8132)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.060 (0.141)	Data 1.65e-04 (5.61e-04)	Tok/s 88073 (101243)	Loss/tok 3.1617 (5.7922)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.107 (0.141)	Data 1.68e-04 (5.56e-04)	Tok/s 94039 (101213)	Loss/tok 3.5931 (5.7704)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.060 (0.141)	Data 1.61e-04 (5.51e-04)	Tok/s 91199 (101153)	Loss/tok 3.1057 (5.7509)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.060 (0.141)	Data 1.77e-04 (5.46e-04)	Tok/s 86765 (101129)	Loss/tok 3.0218 (5.7302)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.160 (0.141)	Data 1.64e-04 (5.41e-04)	Tok/s 105936 (101127)	Loss/tok 3.9313 (5.7080)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.108 (0.141)	Data 1.45e-04 (5.37e-04)	Tok/s 98183 (101147)	Loss/tok 3.5467 (5.6847)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.160 (0.141)	Data 1.61e-04 (5.32e-04)	Tok/s 103476 (101201)	Loss/tok 3.9134 (5.6591)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][830/1938]	Time 0.212 (0.141)	Data 1.58e-04 (5.28e-04)	Tok/s 108683 (101231)	Loss/tok 4.1619 (5.6361)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.160 (0.141)	Data 1.70e-04 (5.23e-04)	Tok/s 104640 (101223)	Loss/tok 3.9084 (5.6152)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.272 (0.142)	Data 1.72e-04 (5.19e-04)	Tok/s 110326 (101235)	Loss/tok 4.2594 (5.5936)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.107 (0.142)	Data 1.74e-04 (5.15e-04)	Tok/s 96814 (101221)	Loss/tok 3.7107 (5.5742)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.108 (0.142)	Data 1.65e-04 (5.11e-04)	Tok/s 94338 (101198)	Loss/tok 3.7645 (5.5571)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.211 (0.142)	Data 1.60e-04 (5.07e-04)	Tok/s 110655 (101199)	Loss/tok 4.0941 (5.5380)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.108 (0.142)	Data 1.80e-04 (5.04e-04)	Tok/s 95769 (101197)	Loss/tok 3.8107 (5.5199)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.107 (0.142)	Data 1.49e-04 (5.00e-04)	Tok/s 95470 (101192)	Loss/tok 3.6573 (5.5020)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.107 (0.142)	Data 1.72e-04 (4.96e-04)	Tok/s 95457 (101171)	Loss/tok 3.6093 (5.4846)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.108 (0.141)	Data 1.62e-04 (4.93e-04)	Tok/s 94947 (101142)	Loss/tok 3.6012 (5.4690)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.212 (0.142)	Data 1.65e-04 (4.90e-04)	Tok/s 109131 (101177)	Loss/tok 4.0125 (5.4490)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.159 (0.142)	Data 1.61e-04 (4.86e-04)	Tok/s 105731 (101193)	Loss/tok 3.9783 (5.4307)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][950/1938]	Time 0.107 (0.142)	Data 1.34e-04 (4.83e-04)	Tok/s 96258 (101193)	Loss/tok 3.6014 (5.4133)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.160 (0.142)	Data 1.94e-04 (4.80e-04)	Tok/s 104563 (101193)	Loss/tok 3.8685 (5.3969)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.108 (0.142)	Data 1.74e-04 (4.76e-04)	Tok/s 95024 (101175)	Loss/tok 3.6034 (5.3818)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.108 (0.142)	Data 1.63e-04 (4.73e-04)	Tok/s 96919 (101178)	Loss/tok 3.5341 (5.3647)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][990/1938]	Time 0.162 (0.142)	Data 1.70e-04 (4.70e-04)	Tok/s 102934 (101192)	Loss/tok 3.9722 (5.3478)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.160 (0.142)	Data 1.58e-04 (4.67e-04)	Tok/s 106253 (101187)	Loss/tok 3.8272 (5.3320)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.109 (0.142)	Data 1.65e-04 (4.64e-04)	Tok/s 94418 (101139)	Loss/tok 3.5206 (5.3191)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.108 (0.142)	Data 1.83e-04 (4.61e-04)	Tok/s 94141 (101139)	Loss/tok 3.5948 (5.3039)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.212 (0.142)	Data 1.74e-04 (4.59e-04)	Tok/s 110178 (101118)	Loss/tok 4.0094 (5.2899)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.213 (0.142)	Data 2.64e-04 (4.56e-04)	Tok/s 110549 (101137)	Loss/tok 4.0982 (5.2739)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.108 (0.142)	Data 1.60e-04 (4.53e-04)	Tok/s 95314 (101130)	Loss/tok 3.4755 (5.2602)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.107 (0.142)	Data 1.80e-04 (4.51e-04)	Tok/s 95104 (101094)	Loss/tok 3.5243 (5.2478)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.108 (0.142)	Data 1.63e-04 (4.48e-04)	Tok/s 95630 (101059)	Loss/tok 3.7078 (5.2362)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.161 (0.141)	Data 1.65e-04 (4.46e-04)	Tok/s 103716 (101010)	Loss/tok 3.8405 (5.2251)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.108 (0.141)	Data 1.80e-04 (4.43e-04)	Tok/s 93522 (100986)	Loss/tok 3.4485 (5.2130)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.108 (0.141)	Data 1.93e-04 (4.41e-04)	Tok/s 96893 (100985)	Loss/tok 3.4192 (5.1991)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.108 (0.141)	Data 1.87e-04 (4.38e-04)	Tok/s 95081 (100981)	Loss/tok 3.4955 (5.1859)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1120/1938]	Time 0.160 (0.141)	Data 1.73e-04 (4.36e-04)	Tok/s 106818 (100990)	Loss/tok 3.8047 (5.1727)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.108 (0.141)	Data 1.80e-04 (4.34e-04)	Tok/s 94320 (100963)	Loss/tok 3.4528 (5.1613)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.108 (0.141)	Data 1.85e-04 (4.31e-04)	Tok/s 96480 (100929)	Loss/tok 3.4782 (5.1510)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.108 (0.141)	Data 2.16e-04 (4.29e-04)	Tok/s 96393 (100939)	Loss/tok 3.3849 (5.1377)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.213 (0.141)	Data 1.85e-04 (4.27e-04)	Tok/s 109916 (100940)	Loss/tok 3.8622 (5.1250)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.108 (0.141)	Data 1.87e-04 (4.25e-04)	Tok/s 96892 (100927)	Loss/tok 3.4986 (5.1136)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.108 (0.141)	Data 2.05e-04 (4.23e-04)	Tok/s 94448 (100923)	Loss/tok 3.4450 (5.1015)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.060 (0.141)	Data 1.86e-04 (4.21e-04)	Tok/s 88719 (100923)	Loss/tok 2.9886 (5.0900)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.109 (0.141)	Data 1.60e-04 (4.19e-04)	Tok/s 95979 (100917)	Loss/tok 3.4620 (5.0786)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.108 (0.141)	Data 1.67e-04 (4.17e-04)	Tok/s 98552 (100921)	Loss/tok 3.3899 (5.0670)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.212 (0.141)	Data 1.66e-04 (4.15e-04)	Tok/s 108930 (100914)	Loss/tok 3.8549 (5.0556)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.108 (0.141)	Data 1.60e-04 (4.13e-04)	Tok/s 94797 (100893)	Loss/tok 3.3725 (5.0454)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.212 (0.141)	Data 1.68e-04 (4.11e-04)	Tok/s 109770 (100906)	Loss/tok 3.9915 (5.0336)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1250/1938]	Time 0.160 (0.141)	Data 1.64e-04 (4.09e-04)	Tok/s 104423 (100921)	Loss/tok 3.7568 (5.0218)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1260/1938]	Time 0.273 (0.142)	Data 1.71e-04 (4.07e-04)	Tok/s 108538 (100917)	Loss/tok 4.2150 (5.0111)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.108 (0.142)	Data 1.92e-04 (4.05e-04)	Tok/s 95750 (100901)	Loss/tok 3.4793 (5.0011)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.108 (0.142)	Data 1.70e-04 (4.03e-04)	Tok/s 96245 (100916)	Loss/tok 3.5184 (4.9901)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.107 (0.142)	Data 1.72e-04 (4.01e-04)	Tok/s 96917 (100918)	Loss/tok 3.4458 (4.9798)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.212 (0.142)	Data 1.96e-04 (4.00e-04)	Tok/s 109378 (100936)	Loss/tok 3.8500 (4.9686)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.160 (0.142)	Data 1.42e-04 (3.98e-04)	Tok/s 102755 (100941)	Loss/tok 3.7075 (4.9582)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.162 (0.142)	Data 2.36e-04 (3.96e-04)	Tok/s 103038 (100936)	Loss/tok 3.7228 (4.9487)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.214 (0.142)	Data 1.60e-04 (3.95e-04)	Tok/s 108883 (100958)	Loss/tok 3.9232 (4.9374)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.108 (0.142)	Data 1.59e-04 (3.93e-04)	Tok/s 96218 (100956)	Loss/tok 3.3992 (4.9277)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.109 (0.142)	Data 1.80e-04 (3.91e-04)	Tok/s 96063 (100941)	Loss/tok 3.4325 (4.9188)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.108 (0.142)	Data 1.76e-04 (3.90e-04)	Tok/s 93714 (100916)	Loss/tok 3.5177 (4.9104)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.108 (0.142)	Data 1.74e-04 (3.88e-04)	Tok/s 95973 (100910)	Loss/tok 3.6018 (4.9012)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.060 (0.142)	Data 1.57e-04 (3.87e-04)	Tok/s 87299 (100909)	Loss/tok 2.8679 (4.8922)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1390/1938]	Time 0.159 (0.142)	Data 1.65e-04 (3.85e-04)	Tok/s 105605 (100947)	Loss/tok 3.6859 (4.8818)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.109 (0.142)	Data 1.87e-04 (3.84e-04)	Tok/s 95737 (100931)	Loss/tok 3.4754 (4.8736)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.160 (0.142)	Data 1.80e-04 (3.82e-04)	Tok/s 104356 (100905)	Loss/tok 3.5950 (4.8659)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.108 (0.142)	Data 1.74e-04 (3.81e-04)	Tok/s 94911 (100875)	Loss/tok 3.3444 (4.8585)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.161 (0.142)	Data 1.90e-04 (3.79e-04)	Tok/s 104267 (100880)	Loss/tok 3.7177 (4.8498)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.213 (0.142)	Data 2.28e-04 (3.78e-04)	Tok/s 109941 (100894)	Loss/tok 3.8870 (4.8406)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.108 (0.142)	Data 2.08e-04 (3.77e-04)	Tok/s 96565 (100860)	Loss/tok 3.4804 (4.8337)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1460/1938]	Time 0.160 (0.142)	Data 1.74e-04 (3.75e-04)	Tok/s 104831 (100883)	Loss/tok 3.6530 (4.8246)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.160 (0.142)	Data 2.10e-04 (3.74e-04)	Tok/s 105042 (100871)	Loss/tok 3.6490 (4.8165)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.060 (0.142)	Data 2.09e-04 (3.73e-04)	Tok/s 88062 (100849)	Loss/tok 2.8858 (4.8091)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.061 (0.142)	Data 1.85e-04 (3.71e-04)	Tok/s 86741 (100825)	Loss/tok 2.8539 (4.8021)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.212 (0.142)	Data 1.77e-04 (3.70e-04)	Tok/s 111163 (100823)	Loss/tok 3.8193 (4.7940)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.159 (0.142)	Data 1.64e-04 (3.68e-04)	Tok/s 106333 (100835)	Loss/tok 3.6695 (4.7855)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.108 (0.142)	Data 1.64e-04 (3.67e-04)	Tok/s 96761 (100828)	Loss/tok 3.4327 (4.7781)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.108 (0.142)	Data 1.93e-04 (3.66e-04)	Tok/s 95724 (100805)	Loss/tok 3.3902 (4.7714)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.108 (0.142)	Data 1.82e-04 (3.65e-04)	Tok/s 93544 (100790)	Loss/tok 3.3334 (4.7642)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.108 (0.142)	Data 1.41e-04 (3.63e-04)	Tok/s 93947 (100795)	Loss/tok 3.4904 (4.7565)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.214 (0.142)	Data 1.84e-04 (3.62e-04)	Tok/s 108951 (100789)	Loss/tok 3.9190 (4.7497)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.213 (0.142)	Data 3.11e-04 (3.61e-04)	Tok/s 108727 (100781)	Loss/tok 3.8827 (4.7426)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.160 (0.142)	Data 1.74e-04 (3.60e-04)	Tok/s 105504 (100771)	Loss/tok 3.6459 (4.7361)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1590/1938]	Time 0.161 (0.142)	Data 1.65e-04 (3.59e-04)	Tok/s 102605 (100770)	Loss/tok 3.6193 (4.7288)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.109 (0.142)	Data 1.89e-04 (3.58e-04)	Tok/s 94679 (100761)	Loss/tok 3.3751 (4.7221)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.108 (0.141)	Data 1.79e-04 (3.56e-04)	Tok/s 95935 (100748)	Loss/tok 3.3748 (4.7154)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.108 (0.141)	Data 1.76e-04 (3.55e-04)	Tok/s 95286 (100740)	Loss/tok 3.3187 (4.7088)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.109 (0.141)	Data 2.07e-04 (3.54e-04)	Tok/s 93683 (100739)	Loss/tok 3.4302 (4.7018)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.108 (0.141)	Data 1.59e-04 (3.53e-04)	Tok/s 95322 (100700)	Loss/tok 3.4978 (4.6965)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.108 (0.141)	Data 2.38e-04 (3.52e-04)	Tok/s 94344 (100695)	Loss/tok 3.1836 (4.6898)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1660/1938]	Time 0.108 (0.141)	Data 1.59e-04 (3.51e-04)	Tok/s 95872 (100708)	Loss/tok 3.2345 (4.6824)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.160 (0.141)	Data 1.75e-04 (3.50e-04)	Tok/s 105612 (100678)	Loss/tok 3.5514 (4.6767)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.160 (0.141)	Data 1.62e-04 (3.49e-04)	Tok/s 104998 (100703)	Loss/tok 3.5360 (4.6692)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.108 (0.141)	Data 1.87e-04 (3.48e-04)	Tok/s 92834 (100688)	Loss/tok 3.2724 (4.6630)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.108 (0.141)	Data 1.65e-04 (3.47e-04)	Tok/s 92413 (100682)	Loss/tok 3.4116 (4.6565)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.161 (0.142)	Data 1.54e-04 (3.46e-04)	Tok/s 105024 (100704)	Loss/tok 3.5128 (4.6491)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.060 (0.141)	Data 1.57e-04 (3.45e-04)	Tok/s 85136 (100694)	Loss/tok 2.8397 (4.6432)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.109 (0.141)	Data 1.95e-04 (3.44e-04)	Tok/s 94667 (100681)	Loss/tok 3.3093 (4.6371)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.160 (0.141)	Data 1.56e-04 (3.43e-04)	Tok/s 104548 (100675)	Loss/tok 3.5870 (4.6312)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.275 (0.141)	Data 1.87e-04 (3.42e-04)	Tok/s 109328 (100679)	Loss/tok 3.9277 (4.6249)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.214 (0.142)	Data 1.70e-04 (3.41e-04)	Tok/s 108663 (100696)	Loss/tok 3.8181 (4.6182)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.272 (0.142)	Data 1.64e-04 (3.40e-04)	Tok/s 108465 (100685)	Loss/tok 3.9906 (4.6124)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.111 (0.142)	Data 1.71e-04 (3.39e-04)	Tok/s 94193 (100679)	Loss/tok 3.2763 (4.6067)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1790/1938]	Time 0.108 (0.142)	Data 1.70e-04 (3.38e-04)	Tok/s 94897 (100660)	Loss/tok 3.3982 (4.6015)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.109 (0.142)	Data 1.84e-04 (3.37e-04)	Tok/s 92867 (100660)	Loss/tok 3.2917 (4.5955)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.161 (0.142)	Data 2.10e-04 (3.36e-04)	Tok/s 103757 (100664)	Loss/tok 3.5307 (4.5894)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.108 (0.141)	Data 1.92e-04 (3.35e-04)	Tok/s 96412 (100642)	Loss/tok 3.1819 (4.5843)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1830/1938]	Time 0.273 (0.142)	Data 1.76e-04 (3.35e-04)	Tok/s 109521 (100652)	Loss/tok 3.8969 (4.5781)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.160 (0.142)	Data 1.68e-04 (3.34e-04)	Tok/s 105683 (100659)	Loss/tok 3.5190 (4.5718)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.275 (0.142)	Data 2.02e-04 (3.33e-04)	Tok/s 108411 (100648)	Loss/tok 3.8705 (4.5665)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.160 (0.142)	Data 1.76e-04 (3.32e-04)	Tok/s 105686 (100672)	Loss/tok 3.5142 (4.5599)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.162 (0.142)	Data 1.88e-04 (3.31e-04)	Tok/s 104014 (100678)	Loss/tok 3.5199 (4.5539)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.160 (0.142)	Data 1.76e-04 (3.30e-04)	Tok/s 105537 (100693)	Loss/tok 3.4685 (4.5476)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.162 (0.142)	Data 1.68e-04 (3.29e-04)	Tok/s 103942 (100676)	Loss/tok 3.5335 (4.5427)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.109 (0.142)	Data 2.09e-04 (3.29e-04)	Tok/s 96911 (100641)	Loss/tok 3.3367 (4.5383)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.108 (0.142)	Data 1.71e-04 (3.28e-04)	Tok/s 96787 (100648)	Loss/tok 3.2172 (4.5329)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.060 (0.142)	Data 1.98e-04 (3.27e-04)	Tok/s 88922 (100636)	Loss/tok 2.8760 (4.5280)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.161 (0.142)	Data 1.78e-04 (3.26e-04)	Tok/s 104936 (100628)	Loss/tok 3.5593 (4.5230)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019419714, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019419715, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.604 (0.604)	Decoder iters 103.0 (103.0)	Tok/s 26546 (26546)
0: Running moses detokenizer
0: BLEU(score=20.51748579580915, counts=[34759, 16045, 8601, 4812], totals=[64629, 61626, 58623, 55624], precisions=[53.78235776508998, 26.03608866387564, 14.671715879432988, 8.65094203940745], bp=0.9992730366508737, sys_len=64629, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019421784, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2052, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019421785, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.5185	Test BLEU: 20.52
0: Performance: Epoch: 0	Training: 804957 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019421785, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019421785, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019421786, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 173887892
0: TRAIN [1][0/1938]	Time 0.364 (0.364)	Data 2.58e-01 (2.58e-01)	Tok/s 28341 (28341)	Loss/tok 3.1581 (3.1581)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.271 (0.182)	Data 1.99e-04 (2.36e-02)	Tok/s 108811 (98085)	Loss/tok 3.9776 (3.5258)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][20/1938]	Time 0.107 (0.156)	Data 1.59e-04 (1.24e-02)	Tok/s 95508 (98954)	Loss/tok 3.2119 (3.4539)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.107 (0.154)	Data 2.01e-04 (8.49e-03)	Tok/s 96144 (100405)	Loss/tok 3.1131 (3.4636)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.158 (0.156)	Data 1.62e-04 (6.46e-03)	Tok/s 105821 (101502)	Loss/tok 3.5084 (3.4911)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.107 (0.155)	Data 1.49e-04 (5.23e-03)	Tok/s 99346 (101811)	Loss/tok 3.2560 (3.4881)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.107 (0.146)	Data 1.70e-04 (4.40e-03)	Tok/s 96369 (100886)	Loss/tok 3.3515 (3.4584)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.159 (0.145)	Data 1.77e-04 (3.80e-03)	Tok/s 106218 (101058)	Loss/tok 3.4568 (3.4557)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.106 (0.145)	Data 1.65e-04 (3.36e-03)	Tok/s 97182 (101115)	Loss/tok 3.2326 (3.4614)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][90/1938]	Time 0.159 (0.147)	Data 1.62e-04 (3.00e-03)	Tok/s 107125 (101311)	Loss/tok 3.4114 (3.4763)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.106 (0.146)	Data 1.72e-04 (2.72e-03)	Tok/s 97170 (101215)	Loss/tok 3.2925 (3.4746)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.211 (0.148)	Data 1.65e-04 (2.49e-03)	Tok/s 109830 (101507)	Loss/tok 3.6134 (3.4802)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.158 (0.144)	Data 1.88e-04 (2.30e-03)	Tok/s 106228 (101138)	Loss/tok 3.5401 (3.4691)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.213 (0.145)	Data 2.17e-04 (2.14e-03)	Tok/s 109684 (101292)	Loss/tok 3.6680 (3.4743)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.059 (0.145)	Data 1.80e-04 (2.00e-03)	Tok/s 91589 (101237)	Loss/tok 2.8009 (3.4720)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.107 (0.146)	Data 1.67e-04 (1.88e-03)	Tok/s 95811 (101303)	Loss/tok 3.1422 (3.4773)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.107 (0.145)	Data 1.97e-04 (1.77e-03)	Tok/s 96458 (101318)	Loss/tok 3.2849 (3.4740)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.107 (0.145)	Data 1.58e-04 (1.68e-03)	Tok/s 95851 (101241)	Loss/tok 3.2621 (3.4709)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.106 (0.144)	Data 1.28e-04 (1.60e-03)	Tok/s 98708 (101226)	Loss/tok 3.4009 (3.4684)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.159 (0.145)	Data 1.71e-04 (1.52e-03)	Tok/s 103797 (101361)	Loss/tok 3.6026 (3.4719)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.211 (0.145)	Data 1.77e-04 (1.45e-03)	Tok/s 112122 (101376)	Loss/tok 3.6662 (3.4697)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.107 (0.144)	Data 1.85e-04 (1.39e-03)	Tok/s 98164 (101322)	Loss/tok 3.2357 (3.4655)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][220/1938]	Time 0.107 (0.144)	Data 1.80e-04 (1.34e-03)	Tok/s 94321 (101308)	Loss/tok 3.1185 (3.4641)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.159 (0.145)	Data 2.06e-04 (1.29e-03)	Tok/s 104827 (101428)	Loss/tok 3.4551 (3.4659)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.109 (0.144)	Data 2.45e-04 (1.24e-03)	Tok/s 93120 (101345)	Loss/tok 3.2278 (3.4630)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.108 (0.144)	Data 1.91e-04 (1.20e-03)	Tok/s 93617 (101267)	Loss/tok 3.1795 (3.4615)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.107 (0.143)	Data 1.67e-04 (1.16e-03)	Tok/s 94111 (101200)	Loss/tok 3.1796 (3.4600)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.159 (0.143)	Data 1.66e-04 (1.12e-03)	Tok/s 105207 (101190)	Loss/tok 3.4335 (3.4615)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.107 (0.143)	Data 1.64e-04 (1.09e-03)	Tok/s 97149 (101176)	Loss/tok 3.1643 (3.4576)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.106 (0.143)	Data 1.60e-04 (1.06e-03)	Tok/s 98929 (101241)	Loss/tok 3.3207 (3.4599)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.272 (0.144)	Data 1.70e-04 (1.03e-03)	Tok/s 109904 (101283)	Loss/tok 3.7609 (3.4629)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.211 (0.145)	Data 1.52e-04 (1.00e-03)	Tok/s 110695 (101388)	Loss/tok 3.5232 (3.4661)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.212 (0.145)	Data 1.69e-04 (9.76e-04)	Tok/s 109810 (101484)	Loss/tok 3.7647 (3.4676)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.107 (0.146)	Data 1.87e-04 (9.52e-04)	Tok/s 96081 (101590)	Loss/tok 3.2000 (3.4691)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.059 (0.146)	Data 1.56e-04 (9.29e-04)	Tok/s 88971 (101541)	Loss/tok 2.6943 (3.4669)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][350/1938]	Time 0.106 (0.146)	Data 1.65e-04 (9.07e-04)	Tok/s 95934 (101584)	Loss/tok 3.3555 (3.4700)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.106 (0.146)	Data 1.96e-04 (8.87e-04)	Tok/s 98770 (101623)	Loss/tok 3.1962 (3.4699)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.107 (0.147)	Data 1.65e-04 (8.68e-04)	Tok/s 97147 (101626)	Loss/tok 3.2463 (3.4719)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.107 (0.147)	Data 1.57e-04 (8.50e-04)	Tok/s 97791 (101629)	Loss/tok 3.0987 (3.4711)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.212 (0.147)	Data 2.05e-04 (8.32e-04)	Tok/s 109495 (101697)	Loss/tok 3.6727 (3.4725)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.271 (0.147)	Data 1.83e-04 (8.16e-04)	Tok/s 110677 (101630)	Loss/tok 3.8387 (3.4711)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.107 (0.147)	Data 1.71e-04 (8.00e-04)	Tok/s 97763 (101660)	Loss/tok 3.2224 (3.4708)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.106 (0.146)	Data 1.49e-04 (7.85e-04)	Tok/s 96769 (101646)	Loss/tok 3.3105 (3.4694)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.107 (0.147)	Data 1.66e-04 (7.71e-04)	Tok/s 97492 (101680)	Loss/tok 3.2020 (3.4701)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.107 (0.146)	Data 1.76e-04 (7.57e-04)	Tok/s 97320 (101638)	Loss/tok 3.3034 (3.4680)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.272 (0.146)	Data 1.68e-04 (7.44e-04)	Tok/s 110586 (101523)	Loss/tok 3.8302 (3.4660)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.159 (0.146)	Data 1.45e-04 (7.32e-04)	Tok/s 105638 (101525)	Loss/tok 3.3045 (3.4659)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.060 (0.146)	Data 1.37e-04 (7.20e-04)	Tok/s 88101 (101540)	Loss/tok 2.7715 (3.4655)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][480/1938]	Time 0.160 (0.146)	Data 1.72e-04 (7.09e-04)	Tok/s 105878 (101553)	Loss/tok 3.4572 (3.4669)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.107 (0.146)	Data 1.69e-04 (6.98e-04)	Tok/s 97166 (101522)	Loss/tok 3.1711 (3.4660)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.160 (0.146)	Data 1.71e-04 (6.87e-04)	Tok/s 105562 (101626)	Loss/tok 3.3595 (3.4669)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.107 (0.146)	Data 1.54e-04 (6.78e-04)	Tok/s 97052 (101610)	Loss/tok 3.1800 (3.4654)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.160 (0.146)	Data 1.62e-04 (6.68e-04)	Tok/s 105330 (101551)	Loss/tok 3.4378 (3.4635)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.059 (0.145)	Data 1.63e-04 (6.59e-04)	Tok/s 89294 (101460)	Loss/tok 2.7539 (3.4602)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.159 (0.144)	Data 1.59e-04 (6.50e-04)	Tok/s 105114 (101431)	Loss/tok 3.4059 (3.4586)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.060 (0.144)	Data 1.98e-04 (6.41e-04)	Tok/s 89322 (101368)	Loss/tok 2.7600 (3.4568)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.106 (0.144)	Data 2.01e-04 (6.33e-04)	Tok/s 96733 (101385)	Loss/tok 3.0172 (3.4582)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.107 (0.144)	Data 1.42e-04 (6.24e-04)	Tok/s 95194 (101358)	Loss/tok 3.2373 (3.4560)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][580/1938]	Time 0.107 (0.144)	Data 1.63e-04 (6.17e-04)	Tok/s 97094 (101356)	Loss/tok 3.2168 (3.4560)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.060 (0.143)	Data 1.80e-04 (6.10e-04)	Tok/s 84856 (101284)	Loss/tok 2.7416 (3.4534)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.212 (0.143)	Data 1.71e-04 (6.02e-04)	Tok/s 111371 (101262)	Loss/tok 3.6415 (3.4526)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.159 (0.143)	Data 2.32e-04 (5.95e-04)	Tok/s 105599 (101254)	Loss/tok 3.4398 (3.4509)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.160 (0.143)	Data 1.49e-04 (5.89e-04)	Tok/s 105108 (101303)	Loss/tok 3.3774 (3.4505)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.159 (0.143)	Data 1.22e-04 (5.82e-04)	Tok/s 107207 (101296)	Loss/tok 3.4655 (3.4496)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.107 (0.143)	Data 1.61e-04 (5.76e-04)	Tok/s 96271 (101289)	Loss/tok 3.2197 (3.4494)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.158 (0.143)	Data 1.74e-04 (5.70e-04)	Tok/s 107639 (101273)	Loss/tok 3.3976 (3.4483)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.212 (0.143)	Data 1.72e-04 (5.65e-04)	Tok/s 110585 (101268)	Loss/tok 3.5449 (3.4484)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.107 (0.143)	Data 1.61e-04 (5.59e-04)	Tok/s 96080 (101292)	Loss/tok 3.1028 (3.4501)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.106 (0.143)	Data 2.94e-04 (5.53e-04)	Tok/s 96667 (101222)	Loss/tok 3.1999 (3.4490)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.107 (0.143)	Data 1.77e-04 (5.48e-04)	Tok/s 98082 (101233)	Loss/tok 3.2964 (3.4499)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.107 (0.143)	Data 1.80e-04 (5.43e-04)	Tok/s 98387 (101238)	Loss/tok 3.1795 (3.4500)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][710/1938]	Time 0.107 (0.142)	Data 1.23e-04 (5.37e-04)	Tok/s 95044 (101181)	Loss/tok 3.1497 (3.4487)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.272 (0.142)	Data 1.50e-04 (5.33e-04)	Tok/s 110428 (101173)	Loss/tok 3.7121 (3.4482)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.107 (0.142)	Data 1.58e-04 (5.28e-04)	Tok/s 96605 (101117)	Loss/tok 3.2038 (3.4461)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.213 (0.142)	Data 1.61e-04 (5.23e-04)	Tok/s 109416 (101100)	Loss/tok 3.5256 (3.4448)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.160 (0.142)	Data 1.90e-04 (5.18e-04)	Tok/s 103852 (101120)	Loss/tok 3.5152 (3.4440)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.159 (0.142)	Data 1.75e-04 (5.14e-04)	Tok/s 105572 (101149)	Loss/tok 3.4381 (3.4444)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.107 (0.142)	Data 2.06e-04 (5.09e-04)	Tok/s 93982 (101117)	Loss/tok 3.1355 (3.4429)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.107 (0.142)	Data 1.67e-04 (5.05e-04)	Tok/s 95801 (101117)	Loss/tok 3.1782 (3.4425)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.211 (0.142)	Data 1.24e-04 (5.01e-04)	Tok/s 110112 (101088)	Loss/tok 3.6536 (3.4424)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.108 (0.142)	Data 1.54e-04 (4.97e-04)	Tok/s 95584 (101081)	Loss/tok 3.2730 (3.4417)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.108 (0.142)	Data 1.63e-04 (4.93e-04)	Tok/s 93023 (101096)	Loss/tok 3.0922 (3.4418)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.108 (0.142)	Data 1.80e-04 (4.89e-04)	Tok/s 94352 (101118)	Loss/tok 3.1902 (3.4414)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][830/1938]	Time 0.059 (0.142)	Data 1.33e-04 (4.85e-04)	Tok/s 86446 (101067)	Loss/tok 2.6707 (3.4395)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.212 (0.141)	Data 2.04e-04 (4.82e-04)	Tok/s 110963 (101044)	Loss/tok 3.4964 (3.4379)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.108 (0.142)	Data 1.60e-04 (4.78e-04)	Tok/s 93944 (101056)	Loss/tok 3.1111 (3.4379)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.107 (0.142)	Data 1.87e-04 (4.75e-04)	Tok/s 96343 (101066)	Loss/tok 3.1440 (3.4375)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][870/1938]	Time 0.106 (0.141)	Data 2.22e-04 (4.71e-04)	Tok/s 97875 (101045)	Loss/tok 3.1836 (3.4366)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.160 (0.141)	Data 1.66e-04 (4.68e-04)	Tok/s 105395 (101008)	Loss/tok 3.3768 (3.4353)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][890/1938]	Time 0.212 (0.141)	Data 1.72e-04 (4.65e-04)	Tok/s 110939 (101015)	Loss/tok 3.5260 (3.4355)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.107 (0.141)	Data 1.70e-04 (4.62e-04)	Tok/s 96745 (101021)	Loss/tok 3.1425 (3.4354)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.159 (0.141)	Data 1.73e-04 (4.59e-04)	Tok/s 107201 (101032)	Loss/tok 3.3402 (3.4352)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.106 (0.141)	Data 2.79e-04 (4.56e-04)	Tok/s 97688 (101016)	Loss/tok 3.1096 (3.4338)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.158 (0.141)	Data 1.64e-04 (4.53e-04)	Tok/s 106607 (101008)	Loss/tok 3.3863 (3.4336)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.159 (0.141)	Data 1.76e-04 (4.49e-04)	Tok/s 104715 (101000)	Loss/tok 3.3751 (3.4325)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.107 (0.141)	Data 1.57e-04 (4.46e-04)	Tok/s 96644 (101013)	Loss/tok 3.2952 (3.4322)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.107 (0.141)	Data 1.70e-04 (4.44e-04)	Tok/s 96133 (101048)	Loss/tok 3.1812 (3.4332)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.160 (0.141)	Data 1.46e-04 (4.41e-04)	Tok/s 105656 (101078)	Loss/tok 3.4356 (3.4325)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.159 (0.142)	Data 1.71e-04 (4.38e-04)	Tok/s 105933 (101101)	Loss/tok 3.3686 (3.4331)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.272 (0.142)	Data 1.59e-04 (4.36e-04)	Tok/s 111122 (101137)	Loss/tok 3.7007 (3.4336)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.106 (0.142)	Data 1.68e-04 (4.33e-04)	Tok/s 99111 (101173)	Loss/tok 3.1080 (3.4335)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1010/1938]	Time 0.107 (0.142)	Data 1.63e-04 (4.30e-04)	Tok/s 97327 (101154)	Loss/tok 3.2584 (3.4323)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.107 (0.142)	Data 1.43e-04 (4.28e-04)	Tok/s 98872 (101122)	Loss/tok 3.2363 (3.4313)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.059 (0.142)	Data 1.69e-04 (4.26e-04)	Tok/s 90494 (101142)	Loss/tok 2.7657 (3.4320)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1040/1938]	Time 0.158 (0.142)	Data 2.31e-04 (4.23e-04)	Tok/s 104147 (101164)	Loss/tok 3.3995 (3.4322)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.160 (0.142)	Data 1.76e-04 (4.21e-04)	Tok/s 104100 (101133)	Loss/tok 3.4801 (3.4312)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.159 (0.142)	Data 1.24e-04 (4.18e-04)	Tok/s 105921 (101160)	Loss/tok 3.4780 (3.4321)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.106 (0.142)	Data 1.75e-04 (4.16e-04)	Tok/s 98282 (101160)	Loss/tok 3.1034 (3.4316)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.159 (0.142)	Data 2.25e-04 (4.14e-04)	Tok/s 105005 (101145)	Loss/tok 3.3577 (3.4308)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.160 (0.142)	Data 1.60e-04 (4.12e-04)	Tok/s 105774 (101134)	Loss/tok 3.3769 (3.4299)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.107 (0.142)	Data 1.74e-04 (4.10e-04)	Tok/s 95749 (101148)	Loss/tok 3.0993 (3.4297)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.106 (0.142)	Data 1.40e-04 (4.08e-04)	Tok/s 97289 (101172)	Loss/tok 3.2641 (3.4310)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.270 (0.143)	Data 1.39e-04 (4.05e-04)	Tok/s 110335 (101201)	Loss/tok 3.6583 (3.4319)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.106 (0.142)	Data 1.79e-04 (4.04e-04)	Tok/s 98219 (101164)	Loss/tok 3.1671 (3.4306)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.159 (0.142)	Data 1.58e-04 (4.02e-04)	Tok/s 105816 (101166)	Loss/tok 3.3321 (3.4298)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.211 (0.142)	Data 1.39e-04 (4.00e-04)	Tok/s 111542 (101153)	Loss/tok 3.5664 (3.4289)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.159 (0.142)	Data 1.65e-04 (3.98e-04)	Tok/s 103956 (101144)	Loss/tok 3.4920 (3.4280)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1170/1938]	Time 0.107 (0.142)	Data 1.45e-04 (3.96e-04)	Tok/s 97759 (101133)	Loss/tok 3.1374 (3.4266)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.107 (0.142)	Data 1.50e-04 (3.94e-04)	Tok/s 95826 (101140)	Loss/tok 3.1581 (3.4270)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.159 (0.142)	Data 1.73e-04 (3.92e-04)	Tok/s 107005 (101153)	Loss/tok 3.3233 (3.4267)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.159 (0.142)	Data 1.55e-04 (3.90e-04)	Tok/s 103888 (101135)	Loss/tok 3.2996 (3.4260)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.107 (0.142)	Data 2.41e-04 (3.89e-04)	Tok/s 95712 (101134)	Loss/tok 3.2368 (3.4256)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.212 (0.142)	Data 1.46e-04 (3.87e-04)	Tok/s 109262 (101114)	Loss/tok 3.5872 (3.4247)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.212 (0.142)	Data 2.21e-04 (3.85e-04)	Tok/s 109778 (101119)	Loss/tok 3.5344 (3.4244)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.107 (0.142)	Data 1.64e-04 (3.84e-04)	Tok/s 92707 (101112)	Loss/tok 3.0408 (3.4238)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.108 (0.142)	Data 1.71e-04 (3.82e-04)	Tok/s 95904 (101131)	Loss/tok 3.0465 (3.4234)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.107 (0.142)	Data 1.66e-04 (3.80e-04)	Tok/s 95733 (101113)	Loss/tok 3.0997 (3.4224)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.107 (0.142)	Data 1.81e-04 (3.79e-04)	Tok/s 96009 (101098)	Loss/tok 3.2569 (3.4216)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.211 (0.141)	Data 1.44e-04 (3.77e-04)	Tok/s 111118 (101080)	Loss/tok 3.4487 (3.4210)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.212 (0.142)	Data 1.98e-04 (3.76e-04)	Tok/s 109689 (101088)	Loss/tok 3.5848 (3.4209)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1300/1938]	Time 0.158 (0.142)	Data 1.75e-04 (3.74e-04)	Tok/s 106016 (101103)	Loss/tok 3.4420 (3.4211)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.106 (0.142)	Data 1.87e-04 (3.73e-04)	Tok/s 94422 (101131)	Loss/tok 3.1380 (3.4214)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.059 (0.142)	Data 1.90e-04 (3.71e-04)	Tok/s 87647 (101123)	Loss/tok 2.6249 (3.4210)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.160 (0.142)	Data 2.32e-04 (3.70e-04)	Tok/s 105579 (101151)	Loss/tok 3.3041 (3.4219)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.159 (0.142)	Data 1.69e-04 (3.68e-04)	Tok/s 104392 (101149)	Loss/tok 3.3815 (3.4212)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.107 (0.142)	Data 1.73e-04 (3.67e-04)	Tok/s 96373 (101142)	Loss/tok 3.1528 (3.4202)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.107 (0.142)	Data 2.45e-04 (3.65e-04)	Tok/s 97612 (101128)	Loss/tok 3.2664 (3.4190)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.159 (0.142)	Data 1.85e-04 (3.64e-04)	Tok/s 106542 (101124)	Loss/tok 3.3323 (3.4181)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.159 (0.142)	Data 1.62e-04 (3.63e-04)	Tok/s 106225 (101124)	Loss/tok 3.4299 (3.4175)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.107 (0.141)	Data 1.77e-04 (3.61e-04)	Tok/s 95553 (101107)	Loss/tok 3.1364 (3.4164)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.106 (0.141)	Data 2.22e-04 (3.60e-04)	Tok/s 99171 (101118)	Loss/tok 3.0264 (3.4162)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.108 (0.142)	Data 1.95e-04 (3.59e-04)	Tok/s 94635 (101129)	Loss/tok 3.1954 (3.4162)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.107 (0.142)	Data 1.42e-04 (3.58e-04)	Tok/s 97325 (101151)	Loss/tok 3.0487 (3.4162)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1430/1938]	Time 0.106 (0.142)	Data 1.64e-04 (3.56e-04)	Tok/s 97149 (101134)	Loss/tok 3.1586 (3.4152)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.107 (0.142)	Data 1.85e-04 (3.55e-04)	Tok/s 97369 (101141)	Loss/tok 3.0414 (3.4152)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.159 (0.142)	Data 1.23e-04 (3.54e-04)	Tok/s 107829 (101130)	Loss/tok 3.3972 (3.4141)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.059 (0.141)	Data 1.66e-04 (3.53e-04)	Tok/s 89428 (101113)	Loss/tok 2.6745 (3.4135)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.159 (0.141)	Data 1.96e-04 (3.51e-04)	Tok/s 108332 (101113)	Loss/tok 3.3709 (3.4126)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.107 (0.141)	Data 1.76e-04 (3.50e-04)	Tok/s 97624 (101102)	Loss/tok 2.9958 (3.4117)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1490/1938]	Time 0.106 (0.141)	Data 2.34e-04 (3.49e-04)	Tok/s 100632 (101129)	Loss/tok 3.0680 (3.4121)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.106 (0.141)	Data 1.58e-04 (3.48e-04)	Tok/s 99119 (101127)	Loss/tok 3.0982 (3.4115)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.107 (0.142)	Data 1.82e-04 (3.47e-04)	Tok/s 96347 (101150)	Loss/tok 3.1822 (3.4113)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.160 (0.142)	Data 1.46e-04 (3.46e-04)	Tok/s 105225 (101157)	Loss/tok 3.3533 (3.4112)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.160 (0.142)	Data 1.53e-04 (3.45e-04)	Tok/s 106568 (101170)	Loss/tok 3.2350 (3.4107)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.107 (0.142)	Data 1.63e-04 (3.44e-04)	Tok/s 96042 (101159)	Loss/tok 3.0591 (3.4104)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.160 (0.141)	Data 1.61e-04 (3.43e-04)	Tok/s 105162 (101145)	Loss/tok 3.2587 (3.4096)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.211 (0.141)	Data 1.75e-04 (3.42e-04)	Tok/s 112810 (101129)	Loss/tok 3.4773 (3.4090)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.107 (0.141)	Data 1.95e-04 (3.41e-04)	Tok/s 94401 (101114)	Loss/tok 3.1090 (3.4080)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.106 (0.141)	Data 2.19e-04 (3.40e-04)	Tok/s 97396 (101108)	Loss/tok 3.1140 (3.4072)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.107 (0.141)	Data 1.57e-04 (3.39e-04)	Tok/s 95902 (101101)	Loss/tok 3.0389 (3.4065)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.160 (0.141)	Data 1.76e-04 (3.38e-04)	Tok/s 105497 (101108)	Loss/tok 3.3815 (3.4064)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1610/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.37e-04)	Tok/s 96951 (101121)	Loss/tok 3.1832 (3.4057)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.160 (0.141)	Data 1.78e-04 (3.36e-04)	Tok/s 104882 (101111)	Loss/tok 3.3129 (3.4051)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.107 (0.141)	Data 1.75e-04 (3.35e-04)	Tok/s 96657 (101087)	Loss/tok 3.0911 (3.4042)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.272 (0.141)	Data 1.46e-04 (3.33e-04)	Tok/s 107659 (101106)	Loss/tok 3.7430 (3.4043)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.159 (0.141)	Data 1.39e-04 (3.33e-04)	Tok/s 106043 (101110)	Loss/tok 3.3757 (3.4047)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1660/1938]	Time 0.160 (0.141)	Data 1.47e-04 (3.32e-04)	Tok/s 105031 (101104)	Loss/tok 3.3639 (3.4046)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.108 (0.141)	Data 1.61e-04 (3.31e-04)	Tok/s 97692 (101101)	Loss/tok 3.1746 (3.4040)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.159 (0.141)	Data 1.56e-04 (3.30e-04)	Tok/s 104690 (101099)	Loss/tok 3.4859 (3.4039)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.159 (0.141)	Data 1.45e-04 (3.29e-04)	Tok/s 105005 (101109)	Loss/tok 3.3180 (3.4040)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.158 (0.141)	Data 1.73e-04 (3.28e-04)	Tok/s 106545 (101088)	Loss/tok 3.4138 (3.4034)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.108 (0.141)	Data 1.27e-04 (3.27e-04)	Tok/s 95718 (101084)	Loss/tok 3.2190 (3.4028)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.107 (0.141)	Data 1.46e-04 (3.26e-04)	Tok/s 94578 (101094)	Loss/tok 3.2164 (3.4034)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.059 (0.141)	Data 2.00e-04 (3.25e-04)	Tok/s 89718 (101080)	Loss/tok 2.5651 (3.4025)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.108 (0.141)	Data 1.44e-04 (3.24e-04)	Tok/s 96593 (101080)	Loss/tok 3.0807 (3.4027)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.108 (0.141)	Data 1.66e-04 (3.23e-04)	Tok/s 95496 (101075)	Loss/tok 3.1434 (3.4022)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1760/1938]	Time 0.160 (0.141)	Data 1.39e-04 (3.22e-04)	Tok/s 105340 (101072)	Loss/tok 3.3875 (3.4022)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.107 (0.141)	Data 1.33e-04 (3.21e-04)	Tok/s 95660 (101045)	Loss/tok 3.1042 (3.4012)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.109 (0.141)	Data 1.76e-04 (3.21e-04)	Tok/s 94757 (101018)	Loss/tok 3.1176 (3.4003)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.212 (0.141)	Data 1.89e-04 (3.20e-04)	Tok/s 110827 (101033)	Loss/tok 3.3848 (3.4005)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.212 (0.141)	Data 1.75e-04 (3.19e-04)	Tok/s 110149 (101057)	Loss/tok 3.5403 (3.4006)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.214 (0.141)	Data 1.57e-04 (3.18e-04)	Tok/s 108973 (101076)	Loss/tok 3.5420 (3.4013)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.273 (0.141)	Data 1.28e-04 (3.17e-04)	Tok/s 109822 (101070)	Loss/tok 3.6085 (3.4010)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.108 (0.141)	Data 1.41e-04 (3.16e-04)	Tok/s 96585 (101070)	Loss/tok 3.0762 (3.4011)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.108 (0.141)	Data 1.25e-04 (3.16e-04)	Tok/s 97657 (101061)	Loss/tok 3.0596 (3.4006)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.161 (0.141)	Data 1.44e-04 (3.15e-04)	Tok/s 104550 (101052)	Loss/tok 3.5259 (3.4009)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.213 (0.141)	Data 1.81e-04 (3.14e-04)	Tok/s 109786 (101047)	Loss/tok 3.5255 (3.4002)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.110 (0.141)	Data 1.36e-04 (3.13e-04)	Tok/s 94162 (101045)	Loss/tok 2.9722 (3.3997)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.161 (0.141)	Data 1.28e-04 (3.13e-04)	Tok/s 105142 (101036)	Loss/tok 3.3676 (3.3996)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1890/1938]	Time 0.213 (0.141)	Data 1.72e-04 (3.12e-04)	Tok/s 110594 (101025)	Loss/tok 3.6475 (3.3992)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.160 (0.141)	Data 1.46e-04 (3.11e-04)	Tok/s 105685 (101025)	Loss/tok 3.3690 (3.3989)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.109 (0.141)	Data 1.78e-04 (3.10e-04)	Tok/s 93310 (101016)	Loss/tok 3.2202 (3.3984)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.108 (0.141)	Data 1.45e-04 (3.09e-04)	Tok/s 96462 (101023)	Loss/tok 3.1004 (3.3983)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.108 (0.142)	Data 1.70e-04 (3.09e-04)	Tok/s 94205 (101031)	Loss/tok 3.1511 (3.3983)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019696665, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019696666, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.588 (0.588)	Decoder iters 98.0 (98.0)	Tok/s 27519 (27519)
0: Running moses detokenizer
0: BLEU(score=22.10383499930116, counts=[35773, 17230, 9504, 5426], totals=[65004, 62001, 58998, 55999], precisions=[55.031998030890406, 27.789874356865212, 16.109020644767618, 9.689458740334649], bp=1.0, sys_len=65004, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019698365, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.221, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019698366, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3968	Test BLEU: 22.10
0: Performance: Epoch: 1	Training: 808205 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019698366, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019698367, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019698367, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2227032367
0: TRAIN [2][0/1938]	Time 0.528 (0.528)	Data 2.59e-01 (2.59e-01)	Tok/s 56194 (56194)	Loss/tok 3.6198 (3.6198)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.106 (0.178)	Data 1.75e-04 (2.37e-02)	Tok/s 97227 (98371)	Loss/tok 3.1013 (3.2827)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.108 (0.144)	Data 1.76e-04 (1.25e-02)	Tok/s 96536 (97423)	Loss/tok 2.9123 (3.1878)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.059 (0.139)	Data 1.47e-04 (8.54e-03)	Tok/s 88367 (98114)	Loss/tok 2.5746 (3.1771)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.159 (0.135)	Data 2.10e-04 (6.50e-03)	Tok/s 105329 (98264)	Loss/tok 3.3377 (3.1824)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.106 (0.140)	Data 2.00e-04 (5.26e-03)	Tok/s 95306 (99113)	Loss/tok 3.0086 (3.2138)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.112 (0.138)	Data 1.86e-04 (4.42e-03)	Tok/s 94131 (99221)	Loss/tok 3.1695 (3.2035)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.213 (0.136)	Data 1.66e-04 (3.83e-03)	Tok/s 109495 (99290)	Loss/tok 3.3226 (3.1948)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][80/1938]	Time 0.159 (0.136)	Data 1.47e-04 (3.37e-03)	Tok/s 106237 (99474)	Loss/tok 3.2740 (3.1971)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.212 (0.135)	Data 1.37e-04 (3.02e-03)	Tok/s 108975 (99436)	Loss/tok 3.4661 (3.1956)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.213 (0.135)	Data 1.77e-04 (2.74e-03)	Tok/s 110659 (99669)	Loss/tok 3.3747 (3.2019)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.106 (0.136)	Data 1.22e-04 (2.51e-03)	Tok/s 96687 (99876)	Loss/tok 3.0769 (3.2106)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.159 (0.135)	Data 1.98e-04 (2.31e-03)	Tok/s 104457 (99816)	Loss/tok 3.2758 (3.2063)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.059 (0.134)	Data 1.24e-04 (2.15e-03)	Tok/s 86823 (99668)	Loss/tok 2.6031 (3.2001)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.212 (0.135)	Data 1.55e-04 (2.01e-03)	Tok/s 109202 (99690)	Loss/tok 3.5340 (3.2134)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.212 (0.136)	Data 1.56e-04 (1.89e-03)	Tok/s 110916 (99869)	Loss/tok 3.3214 (3.2172)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.107 (0.137)	Data 1.61e-04 (1.78e-03)	Tok/s 96740 (100111)	Loss/tok 3.0332 (3.2190)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.213 (0.138)	Data 1.29e-04 (1.69e-03)	Tok/s 109525 (100343)	Loss/tok 3.4065 (3.2238)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.159 (0.139)	Data 1.76e-04 (1.60e-03)	Tok/s 104397 (100494)	Loss/tok 3.2182 (3.2310)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.212 (0.140)	Data 2.14e-04 (1.53e-03)	Tok/s 109518 (100685)	Loss/tok 3.4896 (3.2363)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.107 (0.140)	Data 1.46e-04 (1.46e-03)	Tok/s 98392 (100711)	Loss/tok 2.9868 (3.2339)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][210/1938]	Time 0.062 (0.141)	Data 1.31e-04 (1.40e-03)	Tok/s 84871 (100733)	Loss/tok 2.6343 (3.2415)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.107 (0.142)	Data 2.20e-04 (1.34e-03)	Tok/s 97562 (100797)	Loss/tok 3.0281 (3.2448)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.160 (0.141)	Data 1.54e-04 (1.29e-03)	Tok/s 106044 (100780)	Loss/tok 3.2549 (3.2448)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.159 (0.142)	Data 1.49e-04 (1.25e-03)	Tok/s 105529 (100905)	Loss/tok 3.2517 (3.2469)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.108 (0.142)	Data 1.46e-04 (1.20e-03)	Tok/s 96123 (100940)	Loss/tok 3.1459 (3.2481)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.159 (0.142)	Data 1.46e-04 (1.16e-03)	Tok/s 105857 (100943)	Loss/tok 3.2556 (3.2467)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.107 (0.142)	Data 1.38e-04 (1.13e-03)	Tok/s 96614 (101005)	Loss/tok 3.0593 (3.2496)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.160 (0.142)	Data 1.84e-04 (1.09e-03)	Tok/s 105087 (101019)	Loss/tok 3.1459 (3.2491)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.059 (0.141)	Data 1.96e-04 (1.06e-03)	Tok/s 90402 (100909)	Loss/tok 2.5775 (3.2459)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.107 (0.141)	Data 1.81e-04 (1.03e-03)	Tok/s 97666 (100845)	Loss/tok 3.0636 (3.2462)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.106 (0.141)	Data 1.37e-04 (1.00e-03)	Tok/s 97533 (100901)	Loss/tok 3.0291 (3.2469)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.106 (0.142)	Data 1.45e-04 (9.78e-04)	Tok/s 98404 (100926)	Loss/tok 3.0762 (3.2476)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.106 (0.141)	Data 1.37e-04 (9.54e-04)	Tok/s 97966 (100902)	Loss/tok 3.0155 (3.2451)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][340/1938]	Time 0.106 (0.142)	Data 1.63e-04 (9.31e-04)	Tok/s 96124 (101014)	Loss/tok 3.0916 (3.2522)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.159 (0.143)	Data 1.74e-04 (9.09e-04)	Tok/s 104761 (101098)	Loss/tok 3.2786 (3.2538)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.106 (0.143)	Data 1.80e-04 (8.89e-04)	Tok/s 98948 (101144)	Loss/tok 3.0127 (3.2552)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.106 (0.142)	Data 1.47e-04 (8.69e-04)	Tok/s 95970 (101052)	Loss/tok 3.0371 (3.2522)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.159 (0.143)	Data 1.60e-04 (8.51e-04)	Tok/s 105503 (101140)	Loss/tok 3.2684 (3.2548)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.160 (0.143)	Data 2.01e-04 (8.34e-04)	Tok/s 103818 (101170)	Loss/tok 3.3612 (3.2575)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.059 (0.143)	Data 1.90e-04 (8.17e-04)	Tok/s 89503 (101159)	Loss/tok 2.6346 (3.2567)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.272 (0.144)	Data 1.84e-04 (8.02e-04)	Tok/s 109744 (101242)	Loss/tok 3.5057 (3.2613)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.107 (0.144)	Data 1.87e-04 (7.87e-04)	Tok/s 96101 (101238)	Loss/tok 3.0180 (3.2626)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.159 (0.143)	Data 1.59e-04 (7.73e-04)	Tok/s 105702 (101160)	Loss/tok 3.2600 (3.2599)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.107 (0.143)	Data 1.65e-04 (7.59e-04)	Tok/s 96195 (101190)	Loss/tok 3.0062 (3.2586)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.058 (0.143)	Data 1.64e-04 (7.46e-04)	Tok/s 91292 (101152)	Loss/tok 2.6580 (3.2584)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.213 (0.143)	Data 2.24e-04 (7.34e-04)	Tok/s 108392 (101144)	Loss/tok 3.4952 (3.2572)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][470/1938]	Time 0.059 (0.143)	Data 1.70e-04 (7.22e-04)	Tok/s 87366 (101166)	Loss/tok 2.5817 (3.2582)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.106 (0.143)	Data 1.66e-04 (7.11e-04)	Tok/s 96435 (101143)	Loss/tok 3.0695 (3.2580)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.159 (0.143)	Data 1.50e-04 (7.00e-04)	Tok/s 103215 (101123)	Loss/tok 3.3163 (3.2576)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.107 (0.142)	Data 1.47e-04 (6.89e-04)	Tok/s 99221 (101092)	Loss/tok 3.1190 (3.2571)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.107 (0.142)	Data 1.65e-04 (6.79e-04)	Tok/s 95076 (101064)	Loss/tok 3.0673 (3.2547)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.160 (0.142)	Data 1.50e-04 (6.69e-04)	Tok/s 106365 (101084)	Loss/tok 3.2162 (3.2543)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.107 (0.142)	Data 2.39e-04 (6.60e-04)	Tok/s 98570 (101054)	Loss/tok 3.1303 (3.2545)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.160 (0.142)	Data 1.80e-04 (6.51e-04)	Tok/s 104341 (101028)	Loss/tok 3.3158 (3.2532)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.107 (0.141)	Data 1.63e-04 (6.42e-04)	Tok/s 97974 (100952)	Loss/tok 3.1158 (3.2515)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.107 (0.141)	Data 1.52e-04 (6.34e-04)	Tok/s 97669 (100907)	Loss/tok 3.1112 (3.2501)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.107 (0.141)	Data 1.81e-04 (6.26e-04)	Tok/s 96203 (100942)	Loss/tok 3.0702 (3.2492)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.159 (0.140)	Data 2.04e-04 (6.18e-04)	Tok/s 104399 (100895)	Loss/tok 3.3024 (3.2472)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.159 (0.141)	Data 1.46e-04 (6.10e-04)	Tok/s 105478 (101011)	Loss/tok 3.2561 (3.2505)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][600/1938]	Time 0.159 (0.141)	Data 1.60e-04 (6.03e-04)	Tok/s 105290 (101023)	Loss/tok 3.3172 (3.2504)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.160 (0.141)	Data 1.31e-04 (5.96e-04)	Tok/s 105165 (101033)	Loss/tok 3.2337 (3.2510)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.060 (0.141)	Data 1.65e-04 (5.89e-04)	Tok/s 87822 (100959)	Loss/tok 2.6577 (3.2510)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.160 (0.141)	Data 2.01e-04 (5.83e-04)	Tok/s 104493 (100977)	Loss/tok 3.2836 (3.2516)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.273 (0.141)	Data 1.79e-04 (5.77e-04)	Tok/s 109779 (101038)	Loss/tok 3.7130 (3.2554)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.059 (0.142)	Data 1.80e-04 (5.71e-04)	Tok/s 89981 (101075)	Loss/tok 2.6270 (3.2567)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.060 (0.141)	Data 1.79e-04 (5.65e-04)	Tok/s 84654 (101046)	Loss/tok 2.6598 (3.2560)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.107 (0.141)	Data 1.61e-04 (5.59e-04)	Tok/s 97743 (101043)	Loss/tok 3.0225 (3.2550)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.107 (0.141)	Data 1.63e-04 (5.53e-04)	Tok/s 96955 (101014)	Loss/tok 2.9529 (3.2533)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.107 (0.141)	Data 1.46e-04 (5.48e-04)	Tok/s 96494 (101006)	Loss/tok 3.0120 (3.2527)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.107 (0.141)	Data 1.70e-04 (5.43e-04)	Tok/s 95704 (100995)	Loss/tok 3.0961 (3.2538)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.213 (0.141)	Data 1.75e-04 (5.37e-04)	Tok/s 107948 (101036)	Loss/tok 3.5352 (3.2546)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.158 (0.142)	Data 1.59e-04 (5.32e-04)	Tok/s 106913 (101089)	Loss/tok 3.1722 (3.2573)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][730/1938]	Time 0.160 (0.142)	Data 2.11e-04 (5.28e-04)	Tok/s 105873 (101087)	Loss/tok 3.1826 (3.2565)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][740/1938]	Time 0.106 (0.142)	Data 1.84e-04 (5.23e-04)	Tok/s 98196 (101116)	Loss/tok 3.1340 (3.2593)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.059 (0.142)	Data 1.81e-04 (5.18e-04)	Tok/s 89399 (101119)	Loss/tok 2.7097 (3.2601)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.271 (0.143)	Data 1.81e-04 (5.14e-04)	Tok/s 109673 (101165)	Loss/tok 3.5502 (3.2623)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.107 (0.142)	Data 2.02e-04 (5.10e-04)	Tok/s 96830 (101159)	Loss/tok 3.0086 (3.2618)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.059 (0.142)	Data 1.95e-04 (5.06e-04)	Tok/s 89037 (101136)	Loss/tok 2.5917 (3.2616)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.212 (0.142)	Data 2.59e-04 (5.02e-04)	Tok/s 109173 (101147)	Loss/tok 3.5167 (3.2621)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.108 (0.142)	Data 1.80e-04 (4.98e-04)	Tok/s 95633 (101138)	Loss/tok 2.9724 (3.2613)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.108 (0.142)	Data 1.73e-04 (4.94e-04)	Tok/s 96767 (101148)	Loss/tok 2.9695 (3.2620)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.272 (0.142)	Data 2.88e-04 (4.90e-04)	Tok/s 109942 (101132)	Loss/tok 3.5512 (3.2621)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.108 (0.142)	Data 1.81e-04 (4.86e-04)	Tok/s 96437 (101118)	Loss/tok 3.0475 (3.2624)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.107 (0.143)	Data 1.90e-04 (4.83e-04)	Tok/s 98376 (101153)	Loss/tok 3.1187 (3.2634)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.274 (0.143)	Data 2.01e-04 (4.79e-04)	Tok/s 108895 (101165)	Loss/tok 3.6037 (3.2646)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.060 (0.143)	Data 2.01e-04 (4.76e-04)	Tok/s 88445 (101187)	Loss/tok 2.5586 (3.2660)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][870/1938]	Time 0.273 (0.143)	Data 2.02e-04 (4.73e-04)	Tok/s 108307 (101175)	Loss/tok 3.6958 (3.2663)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.107 (0.143)	Data 2.41e-04 (4.70e-04)	Tok/s 96803 (101123)	Loss/tok 3.0625 (3.2656)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.158 (0.142)	Data 1.66e-04 (4.66e-04)	Tok/s 106173 (101106)	Loss/tok 3.2594 (3.2651)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.213 (0.142)	Data 1.78e-04 (4.63e-04)	Tok/s 109933 (101092)	Loss/tok 3.3919 (3.2644)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.108 (0.142)	Data 1.93e-04 (4.60e-04)	Tok/s 94921 (101036)	Loss/tok 3.0893 (3.2630)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.107 (0.142)	Data 1.71e-04 (4.57e-04)	Tok/s 97564 (101025)	Loss/tok 3.0549 (3.2622)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][930/1938]	Time 0.107 (0.142)	Data 1.86e-04 (4.54e-04)	Tok/s 96864 (101011)	Loss/tok 2.9938 (3.2618)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.107 (0.141)	Data 1.83e-04 (4.51e-04)	Tok/s 97976 (100951)	Loss/tok 2.9864 (3.2606)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.160 (0.141)	Data 1.70e-04 (4.48e-04)	Tok/s 105553 (100939)	Loss/tok 3.1274 (3.2600)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.160 (0.141)	Data 2.06e-04 (4.45e-04)	Tok/s 103997 (100918)	Loss/tok 3.2571 (3.2598)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.108 (0.142)	Data 1.76e-04 (4.43e-04)	Tok/s 97761 (100929)	Loss/tok 2.9528 (3.2609)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.059 (0.141)	Data 1.98e-04 (4.40e-04)	Tok/s 90883 (100903)	Loss/tok 2.6715 (3.2600)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.160 (0.141)	Data 1.68e-04 (4.37e-04)	Tok/s 105719 (100862)	Loss/tok 3.2820 (3.2597)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.109 (0.141)	Data 1.64e-04 (4.35e-04)	Tok/s 94259 (100862)	Loss/tok 3.1072 (3.2598)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.160 (0.141)	Data 1.90e-04 (4.32e-04)	Tok/s 104232 (100879)	Loss/tok 3.2930 (3.2604)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.161 (0.141)	Data 1.91e-04 (4.30e-04)	Tok/s 102706 (100890)	Loss/tok 3.3363 (3.2605)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.213 (0.142)	Data 1.65e-04 (4.27e-04)	Tok/s 108924 (100895)	Loss/tok 3.4534 (3.2614)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.108 (0.141)	Data 1.54e-04 (4.25e-04)	Tok/s 96524 (100878)	Loss/tok 3.0887 (3.2606)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.110 (0.141)	Data 1.76e-04 (4.23e-04)	Tok/s 94271 (100829)	Loss/tok 3.0273 (3.2596)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1060/1938]	Time 0.108 (0.141)	Data 1.77e-04 (4.20e-04)	Tok/s 94565 (100828)	Loss/tok 3.0219 (3.2594)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.109 (0.141)	Data 1.95e-04 (4.18e-04)	Tok/s 96351 (100815)	Loss/tok 3.0058 (3.2605)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1080/1938]	Time 0.275 (0.142)	Data 1.93e-04 (4.16e-04)	Tok/s 109907 (100823)	Loss/tok 3.5982 (3.2614)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.161 (0.141)	Data 1.86e-04 (4.14e-04)	Tok/s 103341 (100788)	Loss/tok 3.2877 (3.2603)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.109 (0.141)	Data 1.75e-04 (4.12e-04)	Tok/s 93612 (100764)	Loss/tok 2.9783 (3.2595)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.161 (0.142)	Data 1.74e-04 (4.09e-04)	Tok/s 105240 (100782)	Loss/tok 3.3135 (3.2606)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.214 (0.141)	Data 2.05e-04 (4.07e-04)	Tok/s 108904 (100757)	Loss/tok 3.3748 (3.2598)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.214 (0.141)	Data 1.86e-04 (4.05e-04)	Tok/s 106716 (100746)	Loss/tok 3.5142 (3.2597)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.160 (0.141)	Data 1.77e-04 (4.03e-04)	Tok/s 104751 (100718)	Loss/tok 3.3410 (3.2591)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.108 (0.141)	Data 1.46e-04 (4.01e-04)	Tok/s 95433 (100739)	Loss/tok 3.0306 (3.2598)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.108 (0.141)	Data 1.97e-04 (3.99e-04)	Tok/s 94807 (100734)	Loss/tok 2.9946 (3.2596)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.109 (0.141)	Data 2.07e-04 (3.98e-04)	Tok/s 95059 (100736)	Loss/tok 3.0227 (3.2590)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.161 (0.141)	Data 1.96e-04 (3.95e-04)	Tok/s 103479 (100733)	Loss/tok 3.2188 (3.2601)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.273 (0.142)	Data 1.87e-04 (3.93e-04)	Tok/s 108615 (100753)	Loss/tok 3.6289 (3.2609)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1200/1938]	Time 0.162 (0.142)	Data 2.06e-04 (3.91e-04)	Tok/s 104842 (100760)	Loss/tok 3.1753 (3.2616)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.213 (0.142)	Data 1.64e-04 (3.89e-04)	Tok/s 108021 (100755)	Loss/tok 3.4577 (3.2619)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.160 (0.142)	Data 1.53e-04 (3.88e-04)	Tok/s 105077 (100771)	Loss/tok 3.3484 (3.2623)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.107 (0.142)	Data 1.54e-04 (3.86e-04)	Tok/s 94822 (100785)	Loss/tok 2.9362 (3.2625)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.107 (0.142)	Data 1.44e-04 (3.84e-04)	Tok/s 96520 (100815)	Loss/tok 3.0173 (3.2637)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.108 (0.142)	Data 1.48e-04 (3.82e-04)	Tok/s 97898 (100800)	Loss/tok 3.1619 (3.2634)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.060 (0.142)	Data 1.49e-04 (3.80e-04)	Tok/s 88234 (100791)	Loss/tok 2.6190 (3.2628)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.158 (0.142)	Data 1.84e-04 (3.78e-04)	Tok/s 105461 (100831)	Loss/tok 3.2957 (3.2634)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.159 (0.142)	Data 1.45e-04 (3.77e-04)	Tok/s 106700 (100841)	Loss/tok 3.1333 (3.2630)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.213 (0.142)	Data 1.19e-04 (3.75e-04)	Tok/s 110128 (100839)	Loss/tok 3.2330 (3.2622)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.107 (0.142)	Data 1.22e-04 (3.73e-04)	Tok/s 98980 (100832)	Loss/tok 3.0789 (3.2615)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.271 (0.142)	Data 1.56e-04 (3.71e-04)	Tok/s 110269 (100837)	Loss/tok 3.6899 (3.2618)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.107 (0.142)	Data 1.54e-04 (3.70e-04)	Tok/s 98280 (100826)	Loss/tok 2.9382 (3.2610)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1330/1938]	Time 0.059 (0.142)	Data 1.06e-04 (3.68e-04)	Tok/s 89820 (100834)	Loss/tok 2.5728 (3.2620)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.159 (0.142)	Data 1.40e-04 (3.66e-04)	Tok/s 104631 (100838)	Loss/tok 3.2108 (3.2620)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.107 (0.142)	Data 1.96e-04 (3.65e-04)	Tok/s 98036 (100827)	Loss/tok 3.0476 (3.2620)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.107 (0.142)	Data 1.65e-04 (3.63e-04)	Tok/s 96129 (100801)	Loss/tok 3.0104 (3.2612)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.107 (0.142)	Data 1.80e-04 (3.62e-04)	Tok/s 95041 (100776)	Loss/tok 3.0220 (3.2601)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.107 (0.142)	Data 1.99e-04 (3.61e-04)	Tok/s 98411 (100796)	Loss/tok 2.9044 (3.2600)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.159 (0.142)	Data 1.51e-04 (3.59e-04)	Tok/s 105308 (100812)	Loss/tok 3.2698 (3.2600)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.160 (0.142)	Data 1.83e-04 (3.58e-04)	Tok/s 104459 (100802)	Loss/tok 3.2925 (3.2591)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.159 (0.142)	Data 1.83e-04 (3.57e-04)	Tok/s 104964 (100807)	Loss/tok 3.2543 (3.2594)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.272 (0.142)	Data 1.71e-04 (3.56e-04)	Tok/s 109122 (100804)	Loss/tok 3.4916 (3.2590)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.159 (0.142)	Data 1.86e-04 (3.54e-04)	Tok/s 106795 (100813)	Loss/tok 3.2763 (3.2592)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.107 (0.142)	Data 1.94e-04 (3.53e-04)	Tok/s 98557 (100841)	Loss/tok 3.0498 (3.2598)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1450/1938]	Time 0.108 (0.142)	Data 1.47e-04 (3.52e-04)	Tok/s 95210 (100833)	Loss/tok 3.1914 (3.2593)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.272 (0.142)	Data 2.15e-04 (3.51e-04)	Tok/s 107830 (100862)	Loss/tok 3.6500 (3.2598)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.159 (0.142)	Data 2.15e-04 (3.50e-04)	Tok/s 103956 (100883)	Loss/tok 3.3275 (3.2596)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1480/1938]	Time 0.059 (0.142)	Data 1.74e-04 (3.48e-04)	Tok/s 86667 (100889)	Loss/tok 2.6611 (3.2604)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.160 (0.142)	Data 1.72e-04 (3.47e-04)	Tok/s 105069 (100917)	Loss/tok 3.2737 (3.2609)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.159 (0.142)	Data 1.72e-04 (3.46e-04)	Tok/s 105366 (100937)	Loss/tok 3.2886 (3.2612)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.160 (0.142)	Data 1.79e-04 (3.45e-04)	Tok/s 104052 (100926)	Loss/tok 3.2302 (3.2608)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.108 (0.142)	Data 1.87e-04 (3.44e-04)	Tok/s 96203 (100907)	Loss/tok 3.0275 (3.2605)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.160 (0.142)	Data 1.66e-04 (3.43e-04)	Tok/s 105728 (100893)	Loss/tok 3.2965 (3.2602)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.213 (0.142)	Data 1.78e-04 (3.42e-04)	Tok/s 108460 (100867)	Loss/tok 3.4805 (3.2600)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.059 (0.142)	Data 2.19e-04 (3.41e-04)	Tok/s 88412 (100862)	Loss/tok 2.6130 (3.2594)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.160 (0.142)	Data 1.62e-04 (3.39e-04)	Tok/s 104787 (100866)	Loss/tok 3.2436 (3.2595)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.159 (0.142)	Data 1.61e-04 (3.38e-04)	Tok/s 105970 (100860)	Loss/tok 3.2629 (3.2590)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.271 (0.142)	Data 2.08e-04 (3.37e-04)	Tok/s 109728 (100865)	Loss/tok 3.6148 (3.2595)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.160 (0.142)	Data 1.75e-04 (3.36e-04)	Tok/s 103962 (100868)	Loss/tok 3.2939 (3.2599)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.107 (0.142)	Data 1.56e-04 (3.35e-04)	Tok/s 95918 (100885)	Loss/tok 2.9327 (3.2594)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1610/1938]	Time 0.213 (0.142)	Data 1.97e-04 (3.34e-04)	Tok/s 109366 (100900)	Loss/tok 3.3566 (3.2598)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.159 (0.142)	Data 1.80e-04 (3.34e-04)	Tok/s 104886 (100882)	Loss/tok 3.3333 (3.2589)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1630/1938]	Time 0.108 (0.142)	Data 1.76e-04 (3.33e-04)	Tok/s 94669 (100885)	Loss/tok 3.0684 (3.2591)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.108 (0.142)	Data 1.88e-04 (3.32e-04)	Tok/s 96214 (100882)	Loss/tok 3.0814 (3.2591)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.159 (0.142)	Data 2.05e-04 (3.31e-04)	Tok/s 106209 (100901)	Loss/tok 3.3201 (3.2593)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.160 (0.142)	Data 2.06e-04 (3.30e-04)	Tok/s 105997 (100900)	Loss/tok 3.2106 (3.2587)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.059 (0.142)	Data 1.46e-04 (3.29e-04)	Tok/s 90051 (100900)	Loss/tok 2.6378 (3.2585)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.107 (0.142)	Data 1.98e-04 (3.28e-04)	Tok/s 96036 (100891)	Loss/tok 3.1151 (3.2582)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.213 (0.142)	Data 1.78e-04 (3.27e-04)	Tok/s 109963 (100886)	Loss/tok 3.3534 (3.2577)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.107 (0.142)	Data 1.66e-04 (3.26e-04)	Tok/s 98984 (100899)	Loss/tok 3.1958 (3.2577)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.160 (0.142)	Data 1.90e-04 (3.25e-04)	Tok/s 104442 (100905)	Loss/tok 3.2100 (3.2575)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.107 (0.142)	Data 1.70e-04 (3.25e-04)	Tok/s 98368 (100916)	Loss/tok 3.0282 (3.2573)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.211 (0.142)	Data 1.59e-04 (3.24e-04)	Tok/s 109380 (100904)	Loss/tok 3.4567 (3.2568)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.158 (0.142)	Data 1.44e-04 (3.23e-04)	Tok/s 104999 (100909)	Loss/tok 3.4254 (3.2571)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.107 (0.142)	Data 1.72e-04 (3.22e-04)	Tok/s 96924 (100893)	Loss/tok 3.1331 (3.2567)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1760/1938]	Time 0.212 (0.142)	Data 1.76e-04 (3.21e-04)	Tok/s 109787 (100898)	Loss/tok 3.3260 (3.2567)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.107 (0.142)	Data 1.48e-04 (3.20e-04)	Tok/s 95521 (100891)	Loss/tok 3.0176 (3.2564)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.108 (0.142)	Data 1.77e-04 (3.19e-04)	Tok/s 94598 (100886)	Loss/tok 3.0856 (3.2559)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1790/1938]	Time 0.059 (0.142)	Data 1.72e-04 (3.19e-04)	Tok/s 90781 (100891)	Loss/tok 2.6329 (3.2560)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.106 (0.142)	Data 1.85e-04 (3.18e-04)	Tok/s 96357 (100895)	Loss/tok 3.0021 (3.2559)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.160 (0.142)	Data 1.85e-04 (3.17e-04)	Tok/s 104536 (100904)	Loss/tok 3.3535 (3.2557)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.160 (0.142)	Data 1.84e-04 (3.16e-04)	Tok/s 104000 (100893)	Loss/tok 3.2612 (3.2551)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.159 (0.142)	Data 1.76e-04 (3.15e-04)	Tok/s 106614 (100900)	Loss/tok 3.2278 (3.2547)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.107 (0.142)	Data 2.05e-04 (3.15e-04)	Tok/s 95476 (100909)	Loss/tok 3.1655 (3.2543)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.273 (0.142)	Data 1.78e-04 (3.14e-04)	Tok/s 108114 (100917)	Loss/tok 3.6822 (3.2546)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.159 (0.142)	Data 1.65e-04 (3.13e-04)	Tok/s 105003 (100925)	Loss/tok 3.2942 (3.2546)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.108 (0.142)	Data 2.14e-04 (3.13e-04)	Tok/s 96604 (100938)	Loss/tok 3.0502 (3.2547)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.106 (0.142)	Data 1.71e-04 (3.12e-04)	Tok/s 96382 (100932)	Loss/tok 3.0067 (3.2544)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.106 (0.142)	Data 1.64e-04 (3.11e-04)	Tok/s 96830 (100935)	Loss/tok 3.0554 (3.2546)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.061 (0.142)	Data 1.85e-04 (3.10e-04)	Tok/s 88018 (100910)	Loss/tok 2.6000 (3.2539)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.161 (0.142)	Data 1.77e-04 (3.10e-04)	Tok/s 104419 (100920)	Loss/tok 3.2136 (3.2545)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1920/1938]	Time 0.060 (0.142)	Data 1.92e-04 (3.09e-04)	Tok/s 87323 (100921)	Loss/tok 2.6440 (3.2544)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1930/1938]	Time 0.161 (0.142)	Data 2.01e-04 (3.08e-04)	Tok/s 105596 (100938)	Loss/tok 3.2976 (3.2547)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019973686, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019973687, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.738 (0.738)	Decoder iters 149.0 (149.0)	Tok/s 21886 (21886)
0: Running moses detokenizer
0: BLEU(score=23.179977550877677, counts=[36470, 17932, 10081, 5878], totals=[65125, 62122, 59119, 56121], precisions=[56.0, 28.865780238884774, 17.0520475650806, 10.473797687140285], bp=1.0, sys_len=65125, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019975567, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2318, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019975567, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2579	Test BLEU: 23.18
0: Performance: Epoch: 2	Training: 806956 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593019975568, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019975568, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019975568, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 2310404600
0: TRAIN [3][0/1938]	Time 0.319 (0.319)	Data 2.59e-01 (2.59e-01)	Tok/s 16565 (16565)	Loss/tok 2.5862 (2.5862)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.107 (0.174)	Data 1.37e-04 (2.37e-02)	Tok/s 95824 (96238)	Loss/tok 2.8378 (3.1854)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.105 (0.158)	Data 1.45e-04 (1.25e-02)	Tok/s 97267 (97784)	Loss/tok 2.9539 (3.1959)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.270 (0.157)	Data 1.62e-04 (8.50e-03)	Tok/s 110146 (99020)	Loss/tok 3.5617 (3.2102)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.107 (0.147)	Data 1.20e-04 (6.46e-03)	Tok/s 98172 (99015)	Loss/tok 3.0658 (3.1768)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.059 (0.142)	Data 1.59e-04 (5.23e-03)	Tok/s 91183 (98862)	Loss/tok 2.6239 (3.1589)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.111 (0.140)	Data 1.50e-04 (4.39e-03)	Tok/s 91782 (98956)	Loss/tok 2.9017 (3.1507)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.158 (0.140)	Data 1.41e-04 (3.80e-03)	Tok/s 107037 (99341)	Loss/tok 3.3079 (3.1535)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.159 (0.141)	Data 1.54e-04 (3.35e-03)	Tok/s 105900 (99765)	Loss/tok 3.0528 (3.1646)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.108 (0.142)	Data 1.32e-04 (3.00e-03)	Tok/s 95146 (100070)	Loss/tok 2.9925 (3.1634)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.213 (0.143)	Data 1.56e-04 (2.72e-03)	Tok/s 109745 (100166)	Loss/tok 3.2352 (3.1634)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.107 (0.144)	Data 1.55e-04 (2.48e-03)	Tok/s 97072 (100442)	Loss/tok 2.8652 (3.1650)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][120/1938]	Time 0.107 (0.142)	Data 1.65e-04 (2.29e-03)	Tok/s 97529 (100252)	Loss/tok 2.9466 (3.1567)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.107 (0.140)	Data 1.50e-04 (2.13e-03)	Tok/s 95502 (100092)	Loss/tok 2.9740 (3.1528)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.107 (0.142)	Data 1.54e-04 (1.99e-03)	Tok/s 95813 (100341)	Loss/tok 2.8871 (3.1604)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.060 (0.141)	Data 2.04e-04 (1.87e-03)	Tok/s 87699 (100260)	Loss/tok 2.5389 (3.1551)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.159 (0.141)	Data 1.55e-04 (1.76e-03)	Tok/s 105474 (100250)	Loss/tok 3.1818 (3.1583)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.107 (0.141)	Data 1.68e-04 (1.67e-03)	Tok/s 94719 (100341)	Loss/tok 2.9869 (3.1596)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.108 (0.141)	Data 1.48e-04 (1.58e-03)	Tok/s 95443 (100287)	Loss/tok 2.9367 (3.1590)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.059 (0.140)	Data 1.50e-04 (1.51e-03)	Tok/s 90587 (100276)	Loss/tok 2.7122 (3.1563)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.212 (0.140)	Data 1.47e-04 (1.44e-03)	Tok/s 110940 (100300)	Loss/tok 3.2927 (3.1577)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][210/1938]	Time 0.107 (0.141)	Data 1.29e-04 (1.38e-03)	Tok/s 96859 (100325)	Loss/tok 3.0778 (3.1647)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.107 (0.141)	Data 1.53e-04 (1.32e-03)	Tok/s 94527 (100444)	Loss/tok 2.9501 (3.1656)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.108 (0.141)	Data 1.25e-04 (1.27e-03)	Tok/s 95267 (100407)	Loss/tok 3.0694 (3.1635)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.108 (0.140)	Data 1.33e-04 (1.23e-03)	Tok/s 95847 (100314)	Loss/tok 2.9647 (3.1591)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.160 (0.140)	Data 1.46e-04 (1.19e-03)	Tok/s 104688 (100303)	Loss/tok 3.1028 (3.1557)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.161 (0.141)	Data 1.42e-04 (1.15e-03)	Tok/s 103605 (100464)	Loss/tok 3.2154 (3.1622)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.214 (0.142)	Data 1.62e-04 (1.11e-03)	Tok/s 108853 (100531)	Loss/tok 3.2438 (3.1612)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.107 (0.143)	Data 1.48e-04 (1.08e-03)	Tok/s 94436 (100601)	Loss/tok 3.0591 (3.1662)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.273 (0.144)	Data 1.32e-04 (1.04e-03)	Tok/s 108998 (100701)	Loss/tok 3.5464 (3.1698)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.109 (0.144)	Data 1.43e-04 (1.01e-03)	Tok/s 94967 (100797)	Loss/tok 2.9322 (3.1713)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.160 (0.143)	Data 1.26e-04 (9.87e-04)	Tok/s 105823 (100662)	Loss/tok 3.1224 (3.1689)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.161 (0.143)	Data 1.85e-04 (9.61e-04)	Tok/s 103598 (100651)	Loss/tok 3.1975 (3.1690)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.160 (0.144)	Data 1.77e-04 (9.37e-04)	Tok/s 105107 (100659)	Loss/tok 3.1535 (3.1705)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][340/1938]	Time 0.161 (0.144)	Data 1.73e-04 (9.14e-04)	Tok/s 104879 (100725)	Loss/tok 3.1576 (3.1701)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.108 (0.144)	Data 1.73e-04 (8.93e-04)	Tok/s 95502 (100708)	Loss/tok 3.0802 (3.1692)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.109 (0.144)	Data 1.80e-04 (8.73e-04)	Tok/s 95637 (100665)	Loss/tok 2.9273 (3.1677)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.109 (0.143)	Data 1.58e-04 (8.54e-04)	Tok/s 94929 (100608)	Loss/tok 3.0745 (3.1654)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.108 (0.143)	Data 1.71e-04 (8.35e-04)	Tok/s 93826 (100596)	Loss/tok 3.0528 (3.1648)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.160 (0.142)	Data 1.45e-04 (8.18e-04)	Tok/s 104302 (100510)	Loss/tok 3.1455 (3.1621)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.162 (0.142)	Data 1.35e-04 (8.01e-04)	Tok/s 103272 (100488)	Loss/tok 3.1678 (3.1610)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.162 (0.142)	Data 1.53e-04 (7.85e-04)	Tok/s 104942 (100431)	Loss/tok 3.1807 (3.1597)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.160 (0.142)	Data 2.08e-04 (7.70e-04)	Tok/s 104467 (100423)	Loss/tok 3.2496 (3.1587)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][430/1938]	Time 0.274 (0.143)	Data 1.48e-04 (7.56e-04)	Tok/s 109519 (100494)	Loss/tok 3.5104 (3.1614)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.109 (0.142)	Data 1.56e-04 (7.43e-04)	Tok/s 96147 (100412)	Loss/tok 3.0209 (3.1589)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.161 (0.142)	Data 1.58e-04 (7.30e-04)	Tok/s 104361 (100423)	Loss/tok 3.2032 (3.1598)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.109 (0.142)	Data 1.50e-04 (7.17e-04)	Tok/s 95123 (100411)	Loss/tok 2.9141 (3.1597)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.161 (0.142)	Data 1.42e-04 (7.05e-04)	Tok/s 105203 (100425)	Loss/tok 3.1016 (3.1598)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.108 (0.142)	Data 1.77e-04 (6.94e-04)	Tok/s 96078 (100426)	Loss/tok 3.0076 (3.1618)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.109 (0.142)	Data 1.52e-04 (6.83e-04)	Tok/s 94284 (100313)	Loss/tok 3.0564 (3.1606)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.159 (0.142)	Data 1.58e-04 (6.72e-04)	Tok/s 106150 (100335)	Loss/tok 3.1304 (3.1596)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.108 (0.141)	Data 2.15e-04 (6.62e-04)	Tok/s 93859 (100268)	Loss/tok 3.0688 (3.1595)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.160 (0.142)	Data 1.43e-04 (6.53e-04)	Tok/s 102493 (100323)	Loss/tok 3.2548 (3.1617)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.108 (0.142)	Data 1.45e-04 (6.43e-04)	Tok/s 95462 (100339)	Loss/tok 3.0712 (3.1626)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.109 (0.142)	Data 1.59e-04 (6.34e-04)	Tok/s 95276 (100402)	Loss/tok 2.8565 (3.1647)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.059 (0.143)	Data 1.44e-04 (6.26e-04)	Tok/s 89828 (100389)	Loss/tok 2.5549 (3.1656)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1938]	Time 0.109 (0.143)	Data 1.81e-04 (6.17e-04)	Tok/s 96094 (100449)	Loss/tok 2.9686 (3.1681)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.215 (0.143)	Data 2.32e-04 (6.10e-04)	Tok/s 108963 (100409)	Loss/tok 3.3814 (3.1673)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.162 (0.142)	Data 1.68e-04 (6.02e-04)	Tok/s 103428 (100341)	Loss/tok 3.2090 (3.1664)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.216 (0.142)	Data 1.55e-04 (5.94e-04)	Tok/s 108865 (100320)	Loss/tok 3.2934 (3.1664)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.215 (0.143)	Data 1.52e-04 (5.87e-04)	Tok/s 109846 (100359)	Loss/tok 3.4349 (3.1690)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][610/1938]	Time 0.214 (0.143)	Data 1.85e-04 (5.80e-04)	Tok/s 108519 (100352)	Loss/tok 3.3941 (3.1702)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.160 (0.143)	Data 1.46e-04 (5.73e-04)	Tok/s 105263 (100394)	Loss/tok 3.0654 (3.1727)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.108 (0.143)	Data 1.43e-04 (5.67e-04)	Tok/s 94646 (100402)	Loss/tok 2.8247 (3.1722)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.160 (0.143)	Data 1.30e-04 (5.60e-04)	Tok/s 105762 (100411)	Loss/tok 3.1754 (3.1730)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.059 (0.143)	Data 1.56e-04 (5.54e-04)	Tok/s 91851 (100386)	Loss/tok 2.6803 (3.1731)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.273 (0.143)	Data 1.43e-04 (5.48e-04)	Tok/s 109878 (100408)	Loss/tok 3.6168 (3.1742)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.275 (0.143)	Data 1.47e-04 (5.42e-04)	Tok/s 107928 (100380)	Loss/tok 3.5477 (3.1745)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.161 (0.143)	Data 1.44e-04 (5.36e-04)	Tok/s 104766 (100385)	Loss/tok 3.2194 (3.1738)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.161 (0.143)	Data 1.43e-04 (5.31e-04)	Tok/s 104745 (100365)	Loss/tok 3.2406 (3.1725)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.214 (0.143)	Data 1.41e-04 (5.25e-04)	Tok/s 107485 (100346)	Loss/tok 3.3341 (3.1722)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.214 (0.143)	Data 1.85e-04 (5.20e-04)	Tok/s 110114 (100322)	Loss/tok 3.3291 (3.1724)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.161 (0.143)	Data 1.44e-04 (5.14e-04)	Tok/s 103958 (100334)	Loss/tok 3.1791 (3.1722)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.215 (0.143)	Data 1.45e-04 (5.10e-04)	Tok/s 109946 (100300)	Loss/tok 3.3406 (3.1710)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][740/1938]	Time 0.161 (0.143)	Data 1.50e-04 (5.05e-04)	Tok/s 102519 (100278)	Loss/tok 3.3416 (3.1700)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.108 (0.142)	Data 1.33e-04 (5.00e-04)	Tok/s 94009 (100249)	Loss/tok 3.0098 (3.1694)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.109 (0.143)	Data 1.22e-04 (4.95e-04)	Tok/s 94566 (100264)	Loss/tok 2.9629 (3.1696)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.060 (0.143)	Data 1.35e-04 (4.91e-04)	Tok/s 86327 (100249)	Loss/tok 2.5687 (3.1694)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.110 (0.143)	Data 1.81e-04 (4.86e-04)	Tok/s 94395 (100278)	Loss/tok 2.9102 (3.1710)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.161 (0.143)	Data 1.53e-04 (4.82e-04)	Tok/s 104419 (100312)	Loss/tok 3.2001 (3.1728)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.161 (0.144)	Data 1.45e-04 (4.78e-04)	Tok/s 104269 (100326)	Loss/tok 3.2397 (3.1745)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.109 (0.143)	Data 1.35e-04 (4.74e-04)	Tok/s 94412 (100292)	Loss/tok 2.9444 (3.1732)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.108 (0.143)	Data 1.32e-04 (4.70e-04)	Tok/s 94556 (100308)	Loss/tok 2.9921 (3.1738)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.060 (0.143)	Data 1.73e-04 (4.66e-04)	Tok/s 89488 (100275)	Loss/tok 2.5106 (3.1725)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.059 (0.143)	Data 1.63e-04 (4.62e-04)	Tok/s 90706 (100244)	Loss/tok 2.5590 (3.1708)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.108 (0.143)	Data 1.33e-04 (4.58e-04)	Tok/s 96226 (100225)	Loss/tok 2.9741 (3.1708)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.161 (0.143)	Data 1.39e-04 (4.54e-04)	Tok/s 103854 (100206)	Loss/tok 3.1295 (3.1696)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][870/1938]	Time 0.272 (0.143)	Data 1.26e-04 (4.51e-04)	Tok/s 111179 (100195)	Loss/tok 3.5874 (3.1698)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.109 (0.143)	Data 1.51e-04 (4.47e-04)	Tok/s 95684 (100210)	Loss/tok 2.9000 (3.1694)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.274 (0.142)	Data 1.25e-04 (4.44e-04)	Tok/s 107757 (100180)	Loss/tok 3.5323 (3.1692)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.161 (0.142)	Data 1.46e-04 (4.41e-04)	Tok/s 104009 (100173)	Loss/tok 3.0286 (3.1686)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.108 (0.143)	Data 1.28e-04 (4.37e-04)	Tok/s 97092 (100173)	Loss/tok 2.9047 (3.1686)	LR 1.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][920/1938]	Time 0.109 (0.143)	Data 1.25e-04 (4.34e-04)	Tok/s 95150 (100153)	Loss/tok 3.0403 (3.1684)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.109 (0.142)	Data 1.24e-04 (4.31e-04)	Tok/s 94496 (100124)	Loss/tok 2.8748 (3.1681)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.161 (0.143)	Data 1.25e-04 (4.28e-04)	Tok/s 104303 (100190)	Loss/tok 3.0494 (3.1693)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.108 (0.143)	Data 1.29e-04 (4.25e-04)	Tok/s 93168 (100161)	Loss/tok 2.9928 (3.1680)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.108 (0.142)	Data 1.46e-04 (4.22e-04)	Tok/s 95205 (100143)	Loss/tok 2.8771 (3.1674)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.212 (0.143)	Data 1.26e-04 (4.19e-04)	Tok/s 108793 (100152)	Loss/tok 3.3210 (3.1680)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.162 (0.142)	Data 1.21e-04 (4.16e-04)	Tok/s 103609 (100125)	Loss/tok 3.0704 (3.1667)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.214 (0.142)	Data 1.52e-04 (4.13e-04)	Tok/s 109343 (100112)	Loss/tok 3.2448 (3.1657)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.161 (0.143)	Data 1.26e-04 (4.11e-04)	Tok/s 105007 (100138)	Loss/tok 3.0911 (3.1664)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.214 (0.143)	Data 1.51e-04 (4.08e-04)	Tok/s 108811 (100143)	Loss/tok 3.2962 (3.1674)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.160 (0.143)	Data 1.40e-04 (4.06e-04)	Tok/s 105467 (100144)	Loss/tok 3.1461 (3.1666)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.108 (0.142)	Data 1.44e-04 (4.03e-04)	Tok/s 97022 (100110)	Loss/tok 2.8894 (3.1654)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.108 (0.142)	Data 1.24e-04 (4.01e-04)	Tok/s 93989 (100112)	Loss/tok 2.9476 (3.1647)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1050/1938]	Time 0.160 (0.142)	Data 2.16e-04 (3.98e-04)	Tok/s 104208 (100112)	Loss/tok 3.0998 (3.1645)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.060 (0.142)	Data 1.41e-04 (3.96e-04)	Tok/s 87234 (100108)	Loss/tok 2.5104 (3.1646)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.110 (0.142)	Data 1.30e-04 (3.93e-04)	Tok/s 94697 (100082)	Loss/tok 3.0226 (3.1637)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.109 (0.142)	Data 1.24e-04 (3.91e-04)	Tok/s 93343 (100053)	Loss/tok 2.9199 (3.1624)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.162 (0.142)	Data 1.29e-04 (3.89e-04)	Tok/s 103735 (100052)	Loss/tok 3.1058 (3.1618)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.160 (0.142)	Data 1.49e-04 (3.86e-04)	Tok/s 104708 (100036)	Loss/tok 3.1523 (3.1607)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.109 (0.142)	Data 1.37e-04 (3.84e-04)	Tok/s 95336 (100041)	Loss/tok 3.0005 (3.1606)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.161 (0.142)	Data 1.28e-04 (3.82e-04)	Tok/s 104952 (100062)	Loss/tok 3.1596 (3.1611)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.161 (0.142)	Data 1.70e-04 (3.80e-04)	Tok/s 105046 (100084)	Loss/tok 3.1575 (3.1615)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.108 (0.142)	Data 2.14e-04 (3.78e-04)	Tok/s 95405 (100072)	Loss/tok 2.9164 (3.1614)	LR 1.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1150/1938]	Time 0.107 (0.142)	Data 1.76e-04 (3.77e-04)	Tok/s 96425 (100068)	Loss/tok 2.9560 (3.1622)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.108 (0.142)	Data 1.89e-04 (3.75e-04)	Tok/s 94359 (100087)	Loss/tok 2.9636 (3.1627)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.108 (0.142)	Data 1.73e-04 (3.73e-04)	Tok/s 94088 (100072)	Loss/tok 2.8843 (3.1617)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.160 (0.142)	Data 1.45e-04 (3.72e-04)	Tok/s 105149 (100073)	Loss/tok 3.1167 (3.1611)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.109 (0.142)	Data 1.61e-04 (3.70e-04)	Tok/s 94919 (100081)	Loss/tok 2.9984 (3.1607)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.108 (0.142)	Data 1.41e-04 (3.68e-04)	Tok/s 95081 (100052)	Loss/tok 2.9366 (3.1599)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.108 (0.142)	Data 2.42e-04 (3.67e-04)	Tok/s 95780 (100036)	Loss/tok 2.9381 (3.1591)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.108 (0.142)	Data 1.54e-04 (3.65e-04)	Tok/s 96487 (99988)	Loss/tok 2.9754 (3.1592)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.162 (0.142)	Data 2.27e-04 (3.63e-04)	Tok/s 105098 (99993)	Loss/tok 3.1471 (3.1597)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.109 (0.141)	Data 1.63e-04 (3.62e-04)	Tok/s 91979 (99969)	Loss/tok 2.8703 (3.1587)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.108 (0.141)	Data 2.02e-04 (3.60e-04)	Tok/s 96276 (99957)	Loss/tok 2.9171 (3.1584)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.109 (0.141)	Data 2.22e-04 (3.59e-04)	Tok/s 94050 (99944)	Loss/tok 2.8985 (3.1573)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.161 (0.142)	Data 1.87e-04 (3.57e-04)	Tok/s 105363 (99962)	Loss/tok 3.1185 (3.1574)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1280/1938]	Time 0.161 (0.142)	Data 1.82e-04 (3.56e-04)	Tok/s 105229 (99961)	Loss/tok 3.1755 (3.1576)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.161 (0.142)	Data 1.63e-04 (3.55e-04)	Tok/s 102709 (99991)	Loss/tok 3.1663 (3.1582)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.160 (0.142)	Data 1.71e-04 (3.53e-04)	Tok/s 104915 (99993)	Loss/tok 3.1266 (3.1578)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.109 (0.142)	Data 1.57e-04 (3.52e-04)	Tok/s 93248 (99987)	Loss/tok 2.8817 (3.1570)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.109 (0.142)	Data 1.87e-04 (3.50e-04)	Tok/s 94130 (99993)	Loss/tok 2.9796 (3.1575)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.060 (0.142)	Data 2.21e-04 (3.49e-04)	Tok/s 88980 (100008)	Loss/tok 2.5304 (3.1577)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.059 (0.142)	Data 1.41e-04 (3.48e-04)	Tok/s 88200 (100006)	Loss/tok 2.4653 (3.1572)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.110 (0.142)	Data 2.23e-04 (3.47e-04)	Tok/s 93577 (99997)	Loss/tok 3.0127 (3.1570)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.108 (0.142)	Data 1.80e-04 (3.45e-04)	Tok/s 96228 (99990)	Loss/tok 2.9487 (3.1567)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.109 (0.142)	Data 1.74e-04 (3.44e-04)	Tok/s 95024 (99965)	Loss/tok 2.8567 (3.1556)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.161 (0.142)	Data 1.46e-04 (3.43e-04)	Tok/s 106532 (99979)	Loss/tok 3.0681 (3.1556)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.215 (0.142)	Data 1.55e-04 (3.41e-04)	Tok/s 107334 (100005)	Loss/tok 3.4340 (3.1560)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.160 (0.142)	Data 1.44e-04 (3.40e-04)	Tok/s 104367 (100001)	Loss/tok 3.1336 (3.1553)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1410/1938]	Time 0.273 (0.142)	Data 1.27e-04 (3.39e-04)	Tok/s 109397 (100045)	Loss/tok 3.4665 (3.1564)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.108 (0.142)	Data 1.51e-04 (3.37e-04)	Tok/s 95825 (100019)	Loss/tok 2.9249 (3.1557)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.108 (0.142)	Data 1.56e-04 (3.36e-04)	Tok/s 96516 (100014)	Loss/tok 2.9908 (3.1551)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.108 (0.142)	Data 1.30e-04 (3.35e-04)	Tok/s 94835 (100032)	Loss/tok 3.0156 (3.1555)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.161 (0.142)	Data 1.96e-04 (3.33e-04)	Tok/s 104335 (100049)	Loss/tok 3.1684 (3.1554)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.108 (0.142)	Data 1.26e-04 (3.32e-04)	Tok/s 95089 (100042)	Loss/tok 2.7847 (3.1551)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.160 (0.142)	Data 1.27e-04 (3.31e-04)	Tok/s 103701 (100046)	Loss/tok 3.1593 (3.1551)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.110 (0.142)	Data 1.37e-04 (3.29e-04)	Tok/s 94854 (100043)	Loss/tok 2.9963 (3.1551)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.214 (0.142)	Data 1.45e-04 (3.28e-04)	Tok/s 108271 (100034)	Loss/tok 3.2878 (3.1557)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.162 (0.142)	Data 1.63e-04 (3.27e-04)	Tok/s 103664 (100028)	Loss/tok 3.1957 (3.1553)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.162 (0.142)	Data 1.25e-04 (3.26e-04)	Tok/s 104390 (100018)	Loss/tok 3.1155 (3.1548)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.109 (0.142)	Data 1.24e-04 (3.25e-04)	Tok/s 93067 (100033)	Loss/tok 2.9254 (3.1550)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1530/1938]	Time 0.108 (0.142)	Data 1.22e-04 (3.23e-04)	Tok/s 94001 (100028)	Loss/tok 2.8616 (3.1547)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.108 (0.142)	Data 1.39e-04 (3.22e-04)	Tok/s 94665 (100023)	Loss/tok 2.9314 (3.1544)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.109 (0.143)	Data 1.27e-04 (3.21e-04)	Tok/s 97244 (100029)	Loss/tok 2.9251 (3.1543)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.212 (0.143)	Data 1.26e-04 (3.20e-04)	Tok/s 110559 (100046)	Loss/tok 3.3416 (3.1549)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.109 (0.143)	Data 1.25e-04 (3.19e-04)	Tok/s 95693 (100040)	Loss/tok 2.8852 (3.1549)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.108 (0.143)	Data 1.27e-04 (3.18e-04)	Tok/s 93609 (100039)	Loss/tok 2.9915 (3.1553)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.059 (0.143)	Data 1.20e-04 (3.16e-04)	Tok/s 91255 (100049)	Loss/tok 2.4357 (3.1550)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.109 (0.143)	Data 1.57e-04 (3.15e-04)	Tok/s 97234 (100049)	Loss/tok 2.8858 (3.1541)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.109 (0.143)	Data 1.34e-04 (3.14e-04)	Tok/s 95271 (100062)	Loss/tok 2.9291 (3.1546)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.059 (0.143)	Data 1.22e-04 (3.13e-04)	Tok/s 91668 (100035)	Loss/tok 2.5363 (3.1538)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.109 (0.143)	Data 1.23e-04 (3.12e-04)	Tok/s 92609 (100032)	Loss/tok 2.9127 (3.1531)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.108 (0.143)	Data 1.27e-04 (3.11e-04)	Tok/s 94169 (100042)	Loss/tok 2.9216 (3.1533)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.160 (0.143)	Data 1.29e-04 (3.10e-04)	Tok/s 105618 (100035)	Loss/tok 3.0566 (3.1525)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1660/1938]	Time 0.214 (0.143)	Data 1.54e-04 (3.09e-04)	Tok/s 106976 (100042)	Loss/tok 3.4258 (3.1527)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.160 (0.143)	Data 1.27e-04 (3.08e-04)	Tok/s 105414 (100044)	Loss/tok 3.2132 (3.1526)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.160 (0.143)	Data 1.37e-04 (3.07e-04)	Tok/s 105683 (100061)	Loss/tok 2.9367 (3.1526)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.060 (0.143)	Data 1.22e-04 (3.07e-04)	Tok/s 89821 (100048)	Loss/tok 2.5228 (3.1519)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.108 (0.143)	Data 1.23e-04 (3.06e-04)	Tok/s 95394 (100044)	Loss/tok 2.8644 (3.1513)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.108 (0.143)	Data 1.43e-04 (3.05e-04)	Tok/s 93558 (100053)	Loss/tok 2.8144 (3.1512)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.160 (0.143)	Data 1.24e-04 (3.04e-04)	Tok/s 105624 (100067)	Loss/tok 3.1328 (3.1514)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.162 (0.143)	Data 1.22e-04 (3.03e-04)	Tok/s 102869 (100050)	Loss/tok 3.1571 (3.1506)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.108 (0.143)	Data 1.27e-04 (3.02e-04)	Tok/s 93896 (100053)	Loss/tok 2.9012 (3.1502)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.109 (0.143)	Data 1.22e-04 (3.01e-04)	Tok/s 96456 (100029)	Loss/tok 2.9435 (3.1496)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.109 (0.142)	Data 1.33e-04 (3.00e-04)	Tok/s 94685 (100019)	Loss/tok 2.8520 (3.1491)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.214 (0.143)	Data 1.24e-04 (2.99e-04)	Tok/s 108045 (100039)	Loss/tok 3.1813 (3.1493)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.161 (0.143)	Data 1.33e-04 (2.98e-04)	Tok/s 104163 (100047)	Loss/tok 3.1206 (3.1494)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1790/1938]	Time 0.275 (0.143)	Data 1.68e-04 (2.98e-04)	Tok/s 108977 (100054)	Loss/tok 3.3897 (3.1497)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.162 (0.143)	Data 1.40e-04 (2.97e-04)	Tok/s 102911 (100048)	Loss/tok 3.0051 (3.1493)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.213 (0.143)	Data 1.40e-04 (2.96e-04)	Tok/s 107832 (100049)	Loss/tok 3.3591 (3.1493)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.162 (0.143)	Data 1.38e-04 (2.95e-04)	Tok/s 104434 (100064)	Loss/tok 3.0973 (3.1500)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.160 (0.143)	Data 2.18e-04 (2.94e-04)	Tok/s 104416 (100079)	Loss/tok 3.0596 (3.1503)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.274 (0.143)	Data 1.36e-04 (2.94e-04)	Tok/s 108034 (100071)	Loss/tok 3.3752 (3.1501)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.213 (0.143)	Data 1.66e-04 (2.93e-04)	Tok/s 110196 (100086)	Loss/tok 3.3109 (3.1501)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.214 (0.143)	Data 1.37e-04 (2.92e-04)	Tok/s 109354 (100077)	Loss/tok 3.2841 (3.1500)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.059 (0.143)	Data 1.35e-04 (2.91e-04)	Tok/s 88050 (100062)	Loss/tok 2.5305 (3.1494)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.109 (0.143)	Data 1.30e-04 (2.90e-04)	Tok/s 94480 (100058)	Loss/tok 2.9536 (3.1488)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.107 (0.143)	Data 1.26e-04 (2.90e-04)	Tok/s 96238 (100056)	Loss/tok 2.9590 (3.1483)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.272 (0.143)	Data 1.26e-04 (2.89e-04)	Tok/s 110059 (100063)	Loss/tok 3.3954 (3.1481)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.107 (0.143)	Data 1.26e-04 (2.88e-04)	Tok/s 96343 (100066)	Loss/tok 2.7996 (3.1479)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1920/1938]	Time 0.213 (0.143)	Data 1.36e-04 (2.87e-04)	Tok/s 111036 (100057)	Loss/tok 3.2423 (3.1472)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.107 (0.143)	Data 1.24e-04 (2.87e-04)	Tok/s 95043 (100060)	Loss/tok 3.0361 (3.1472)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020252980, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020252980, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.742 (0.742)	Decoder iters 149.0 (149.0)	Tok/s 22001 (22001)
0: Running moses detokenizer
0: BLEU(score=24.347026580561636, counts=[37202, 18712, 10691, 6340], totals=[65131, 62128, 59125, 56126], precisions=[57.11872994426617, 30.1184651043008, 18.08202959830867, 11.296012543206357], bp=1.0, sys_len=65131, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020254857, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24350000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020254857, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1478	Test BLEU: 24.35
0: Performance: Epoch: 3	Training: 800562 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020254858, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020254858, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:37:39 AM
RESULT,RNN_TRANSLATOR,,1133,nvidia,2020-06-24 10:18:46 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
ENDING TIMING RUN AT 2020-06-24 10:37:40 AM
RESULT,RNN_TRANSLATOR,,1134,nvidia,2020-06-24 10:18:46 AM
