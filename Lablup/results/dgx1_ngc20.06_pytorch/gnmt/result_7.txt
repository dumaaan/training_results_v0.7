+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
srun: Job 467821 step creation temporarily disabled, retrying
srun: Step created for job 467821
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019162159, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019162196, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019162197, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019162197, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019162197, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-323
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019166934, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177105/results:/results ./run_and_time.sh
srun: Job 467821 step creation temporarily disabled, retrying
srun: Step created for job 467821
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -n 3 ']'
+ '[' 8 -gt 1 ']'
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=256
running benchmark
+ TEST_BATCH_SIZE=128
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=128
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
running benchmark
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019171290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171310, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 4196091661
:::MLLOG {"namespace": "", "time_ms": 1593019177125, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4196091661, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3889808251
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019185106, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019185106, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019185107, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019185107, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019185107, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019186634, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019186634, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019186635, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019186910, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019186911, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019186912, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019186912, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019186913, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019186913, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019186913, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019186913, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019186913, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019186913, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019186914, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019186914, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3568317537
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.503 (0.503)	Data 2.95e-01 (2.95e-01)	Tok/s 46933 (46933)	Loss/tok 10.6792 (10.6792)	LR 2.047e-05
0: TRAIN [0][10/1938]	Time 0.104 (0.160)	Data 1.44e-04 (2.70e-02)	Tok/s 96817 (96335)	Loss/tok 9.5167 (10.1092)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.262 (0.149)	Data 2.96e-04 (1.42e-02)	Tok/s 114720 (98890)	Loss/tok 9.3971 (9.7715)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.104 (0.142)	Data 1.31e-04 (9.68e-03)	Tok/s 97812 (99970)	Loss/tok 8.9163 (9.5712)	LR 4.083e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1938]	Time 0.104 (0.140)	Data 1.64e-04 (7.36e-03)	Tok/s 100001 (100859)	Loss/tok 8.7456 (9.4002)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.262 (0.148)	Data 1.77e-04 (5.96e-03)	Tok/s 112946 (102328)	Loss/tok 8.6703 (9.2314)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.154 (0.146)	Data 1.55e-04 (5.01e-03)	Tok/s 109227 (102562)	Loss/tok 8.4079 (9.1086)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.154 (0.145)	Data 2.09e-04 (4.33e-03)	Tok/s 109988 (102905)	Loss/tok 8.2145 (8.9935)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.205 (0.145)	Data 1.22e-04 (3.81e-03)	Tok/s 113517 (103268)	Loss/tok 8.2033 (8.8931)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.103 (0.146)	Data 1.56e-04 (3.41e-03)	Tok/s 101756 (103500)	Loss/tok 7.8809 (8.7938)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.104 (0.146)	Data 1.98e-04 (3.09e-03)	Tok/s 101548 (103608)	Loss/tok 7.7494 (8.7117)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.105 (0.146)	Data 1.24e-04 (2.83e-03)	Tok/s 97866 (103749)	Loss/tok 7.7318 (8.6396)	LR 2.518e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][120/1938]	Time 0.154 (0.146)	Data 1.43e-04 (2.61e-03)	Tok/s 110912 (103941)	Loss/tok 7.9102 (8.5782)	LR 3.098e-04
0: TRAIN [0][130/1938]	Time 0.154 (0.145)	Data 1.57e-04 (2.42e-03)	Tok/s 108614 (103992)	Loss/tok 7.8725 (8.5280)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.104 (0.144)	Data 1.50e-04 (2.26e-03)	Tok/s 99294 (104025)	Loss/tok 7.5552 (8.4789)	LR 4.909e-04
0: TRAIN [0][150/1938]	Time 0.207 (0.143)	Data 1.80e-04 (2.12e-03)	Tok/s 113235 (103839)	Loss/tok 7.9628 (8.4331)	LR 6.181e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][160/1938]	Time 0.156 (0.141)	Data 1.66e-04 (2.00e-03)	Tok/s 110097 (103600)	Loss/tok 8.2877 (8.3962)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.206 (0.140)	Data 2.41e-04 (1.89e-03)	Tok/s 112812 (103453)	Loss/tok 7.8217 (8.3599)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.105 (0.141)	Data 2.06e-04 (1.80e-03)	Tok/s 96422 (103565)	Loss/tok 7.3139 (8.3135)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.105 (0.140)	Data 1.76e-04 (1.71e-03)	Tok/s 98923 (103531)	Loss/tok 7.1838 (8.2672)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.156 (0.140)	Data 1.45e-04 (1.63e-03)	Tok/s 105517 (103486)	Loss/tok 7.2664 (8.2164)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.155 (0.140)	Data 3.00e-04 (1.57e-03)	Tok/s 107375 (103529)	Loss/tok 6.8613 (8.1571)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.207 (0.140)	Data 1.54e-04 (1.50e-03)	Tok/s 112699 (103510)	Loss/tok 7.1102 (8.0993)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.106 (0.140)	Data 1.45e-04 (1.44e-03)	Tok/s 97916 (103561)	Loss/tok 6.4384 (8.0365)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.105 (0.139)	Data 1.19e-04 (1.39e-03)	Tok/s 101655 (103408)	Loss/tok 6.3583 (7.9850)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.106 (0.139)	Data 1.75e-04 (1.34e-03)	Tok/s 97413 (103376)	Loss/tok 6.1332 (7.9262)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.265 (0.140)	Data 1.79e-04 (1.30e-03)	Tok/s 111014 (103449)	Loss/tok 6.6687 (7.8563)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.265 (0.141)	Data 1.55e-04 (1.26e-03)	Tok/s 110344 (103431)	Loss/tok 6.5481 (7.7948)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.105 (0.140)	Data 1.79e-04 (1.22e-03)	Tok/s 99856 (103350)	Loss/tok 5.7575 (7.7382)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][290/1938]	Time 0.105 (0.140)	Data 1.44e-04 (1.18e-03)	Tok/s 98434 (103366)	Loss/tok 5.7051 (7.6798)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.106 (0.139)	Data 1.83e-04 (1.15e-03)	Tok/s 95407 (103266)	Loss/tok 5.6939 (7.6286)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.058 (0.140)	Data 1.79e-04 (1.12e-03)	Tok/s 90601 (103428)	Loss/tok 4.8111 (7.5608)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.106 (0.140)	Data 1.65e-04 (1.09e-03)	Tok/s 97570 (103436)	Loss/tok 5.4024 (7.5033)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.105 (0.140)	Data 1.67e-04 (1.06e-03)	Tok/s 97234 (103368)	Loss/tok 5.2668 (7.4515)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.106 (0.140)	Data 1.69e-04 (1.03e-03)	Tok/s 97999 (103335)	Loss/tok 5.2714 (7.3981)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.059 (0.139)	Data 1.50e-04 (1.01e-03)	Tok/s 90520 (103249)	Loss/tok 4.3085 (7.3498)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.156 (0.139)	Data 2.18e-04 (9.86e-04)	Tok/s 107873 (103214)	Loss/tok 5.3345 (7.2957)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.106 (0.139)	Data 1.65e-04 (9.64e-04)	Tok/s 96733 (103186)	Loss/tok 5.0085 (7.2448)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.106 (0.139)	Data 1.42e-04 (9.44e-04)	Tok/s 94960 (103220)	Loss/tok 4.9476 (7.1886)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.108 (0.139)	Data 1.81e-04 (9.24e-04)	Tok/s 96642 (103165)	Loss/tok 4.8955 (7.1383)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.107 (0.139)	Data 1.42e-04 (9.05e-04)	Tok/s 96915 (103093)	Loss/tok 4.9092 (7.0913)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][410/1938]	Time 0.106 (0.139)	Data 2.02e-04 (8.87e-04)	Tok/s 96672 (103099)	Loss/tok 4.7489 (7.0404)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.208 (0.140)	Data 1.76e-04 (8.70e-04)	Tok/s 111558 (103173)	Loss/tok 5.1173 (6.9797)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.105 (0.140)	Data 1.19e-04 (8.54e-04)	Tok/s 98651 (103103)	Loss/tok 4.5277 (6.9369)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.207 (0.139)	Data 1.68e-04 (8.38e-04)	Tok/s 113294 (103057)	Loss/tok 5.2143 (6.8928)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.105 (0.139)	Data 1.30e-04 (8.23e-04)	Tok/s 98516 (103073)	Loss/tok 4.4311 (6.8448)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.208 (0.139)	Data 1.51e-04 (8.09e-04)	Tok/s 112917 (103083)	Loss/tok 4.8524 (6.7992)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.157 (0.139)	Data 1.65e-04 (7.95e-04)	Tok/s 107477 (103054)	Loss/tok 4.7366 (6.7559)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.107 (0.139)	Data 1.35e-04 (7.82e-04)	Tok/s 96442 (103036)	Loss/tok 4.3166 (6.7109)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.210 (0.139)	Data 1.48e-04 (7.70e-04)	Tok/s 111862 (103039)	Loss/tok 4.8669 (6.6656)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.158 (0.140)	Data 1.52e-04 (7.57e-04)	Tok/s 106891 (103093)	Loss/tok 4.5035 (6.6162)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.156 (0.140)	Data 1.32e-04 (7.46e-04)	Tok/s 107513 (103074)	Loss/tok 4.4494 (6.5745)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.106 (0.140)	Data 1.29e-04 (7.35e-04)	Tok/s 95621 (103032)	Loss/tok 4.0259 (6.5343)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.059 (0.139)	Data 1.44e-04 (7.24e-04)	Tok/s 89187 (102982)	Loss/tok 3.4459 (6.4979)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][540/1938]	Time 0.157 (0.140)	Data 2.51e-04 (7.14e-04)	Tok/s 107853 (103012)	Loss/tok 4.4253 (6.4537)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.157 (0.140)	Data 1.67e-04 (7.04e-04)	Tok/s 107586 (102977)	Loss/tok 4.3959 (6.4178)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.107 (0.139)	Data 1.27e-04 (6.95e-04)	Tok/s 97149 (102879)	Loss/tok 3.9352 (6.3875)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][570/1938]	Time 0.210 (0.140)	Data 2.27e-04 (6.85e-04)	Tok/s 112408 (102964)	Loss/tok 4.5015 (6.3418)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.158 (0.140)	Data 1.71e-04 (6.76e-04)	Tok/s 106450 (102940)	Loss/tok 4.1973 (6.3061)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.209 (0.140)	Data 1.43e-04 (6.68e-04)	Tok/s 110124 (102966)	Loss/tok 4.5413 (6.2686)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.158 (0.140)	Data 1.76e-04 (6.59e-04)	Tok/s 103494 (102914)	Loss/tok 4.2185 (6.2383)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.107 (0.139)	Data 1.53e-04 (6.51e-04)	Tok/s 97668 (102842)	Loss/tok 3.9962 (6.2089)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.106 (0.139)	Data 1.65e-04 (6.43e-04)	Tok/s 95951 (102785)	Loss/tok 3.8397 (6.1788)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.158 (0.139)	Data 1.77e-04 (6.36e-04)	Tok/s 107468 (102720)	Loss/tok 4.1207 (6.1515)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.059 (0.139)	Data 1.56e-04 (6.28e-04)	Tok/s 87822 (102713)	Loss/tok 3.2595 (6.1203)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.106 (0.138)	Data 1.33e-04 (6.21e-04)	Tok/s 100110 (102665)	Loss/tok 3.7715 (6.0940)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.106 (0.138)	Data 1.49e-04 (6.14e-04)	Tok/s 98740 (102645)	Loss/tok 3.9063 (6.0654)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.267 (0.138)	Data 1.50e-04 (6.07e-04)	Tok/s 111061 (102617)	Loss/tok 4.7354 (6.0388)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.209 (0.139)	Data 1.72e-04 (6.01e-04)	Tok/s 110022 (102652)	Loss/tok 4.4761 (6.0074)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.156 (0.139)	Data 1.75e-04 (5.94e-04)	Tok/s 108432 (102680)	Loss/tok 4.0494 (5.9770)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][700/1938]	Time 0.058 (0.139)	Data 1.77e-04 (5.88e-04)	Tok/s 90045 (102675)	Loss/tok 3.1760 (5.9495)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.058 (0.139)	Data 1.69e-04 (5.82e-04)	Tok/s 90247 (102658)	Loss/tok 3.2184 (5.9251)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.207 (0.139)	Data 1.57e-04 (5.76e-04)	Tok/s 113067 (102647)	Loss/tok 4.2565 (5.9004)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.156 (0.139)	Data 1.75e-04 (5.70e-04)	Tok/s 107759 (102707)	Loss/tok 4.0666 (5.8702)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.157 (0.139)	Data 1.26e-04 (5.65e-04)	Tok/s 108261 (102741)	Loss/tok 4.1341 (5.8437)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.105 (0.139)	Data 1.63e-04 (5.60e-04)	Tok/s 97398 (102777)	Loss/tok 3.7993 (5.8170)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.156 (0.139)	Data 1.29e-04 (5.55e-04)	Tok/s 106695 (102797)	Loss/tok 4.1139 (5.7916)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.058 (0.139)	Data 1.57e-04 (5.50e-04)	Tok/s 90942 (102791)	Loss/tok 3.1200 (5.7683)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.058 (0.140)	Data 1.42e-04 (5.45e-04)	Tok/s 91497 (102813)	Loss/tok 3.0422 (5.7446)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.105 (0.140)	Data 1.41e-04 (5.40e-04)	Tok/s 98834 (102817)	Loss/tok 3.6138 (5.7218)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.059 (0.139)	Data 1.66e-04 (5.35e-04)	Tok/s 89631 (102776)	Loss/tok 3.1847 (5.7028)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.207 (0.139)	Data 1.83e-04 (5.31e-04)	Tok/s 112803 (102792)	Loss/tok 4.0260 (5.6799)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][820/1938]	Time 0.157 (0.139)	Data 1.47e-04 (5.26e-04)	Tok/s 107763 (102754)	Loss/tok 3.9773 (5.6619)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][830/1938]	Time 0.157 (0.139)	Data 1.57e-04 (5.22e-04)	Tok/s 108593 (102768)	Loss/tok 3.9485 (5.6399)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.209 (0.139)	Data 1.75e-04 (5.18e-04)	Tok/s 111988 (102802)	Loss/tok 4.1389 (5.6176)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.268 (0.139)	Data 1.54e-04 (5.14e-04)	Tok/s 111634 (102791)	Loss/tok 4.3514 (5.5978)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.267 (0.140)	Data 1.80e-04 (5.10e-04)	Tok/s 112650 (102845)	Loss/tok 4.1142 (5.5738)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.105 (0.140)	Data 1.87e-04 (5.06e-04)	Tok/s 99582 (102812)	Loss/tok 3.8282 (5.5571)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.157 (0.140)	Data 1.67e-04 (5.02e-04)	Tok/s 106597 (102817)	Loss/tok 3.8484 (5.5379)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.156 (0.140)	Data 1.85e-04 (4.98e-04)	Tok/s 106906 (102831)	Loss/tok 3.8265 (5.5183)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.106 (0.140)	Data 1.54e-04 (4.94e-04)	Tok/s 97389 (102821)	Loss/tok 3.5737 (5.5004)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.105 (0.140)	Data 1.24e-04 (4.91e-04)	Tok/s 98420 (102787)	Loss/tok 3.6233 (5.4852)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.209 (0.139)	Data 1.84e-04 (4.87e-04)	Tok/s 111960 (102760)	Loss/tok 3.9789 (5.4695)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.156 (0.139)	Data 1.88e-04 (4.84e-04)	Tok/s 106982 (102718)	Loss/tok 3.8821 (5.4550)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.105 (0.139)	Data 1.56e-04 (4.80e-04)	Tok/s 98241 (102727)	Loss/tok 3.4808 (5.4382)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.268 (0.139)	Data 1.98e-04 (4.77e-04)	Tok/s 110432 (102687)	Loss/tok 4.2992 (5.4236)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][960/1938]	Time 0.106 (0.139)	Data 1.68e-04 (4.74e-04)	Tok/s 98735 (102726)	Loss/tok 3.6372 (5.4041)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.157 (0.139)	Data 1.38e-04 (4.71e-04)	Tok/s 105278 (102727)	Loss/tok 3.8900 (5.3883)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.106 (0.139)	Data 1.79e-04 (4.68e-04)	Tok/s 98225 (102742)	Loss/tok 3.6446 (5.3717)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.157 (0.139)	Data 1.75e-04 (4.65e-04)	Tok/s 106640 (102753)	Loss/tok 3.8807 (5.3549)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.105 (0.139)	Data 2.32e-04 (4.62e-04)	Tok/s 99373 (102747)	Loss/tok 3.5084 (5.3403)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.156 (0.139)	Data 1.62e-04 (4.60e-04)	Tok/s 108761 (102764)	Loss/tok 3.8547 (5.3246)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.157 (0.139)	Data 1.92e-04 (4.57e-04)	Tok/s 105603 (102776)	Loss/tok 3.8791 (5.3093)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.157 (0.139)	Data 1.57e-04 (4.54e-04)	Tok/s 106108 (102788)	Loss/tok 3.7932 (5.2940)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.105 (0.139)	Data 1.96e-04 (4.52e-04)	Tok/s 100653 (102775)	Loss/tok 3.6675 (5.2803)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.209 (0.139)	Data 1.37e-04 (4.49e-04)	Tok/s 112224 (102782)	Loss/tok 3.9234 (5.2652)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.157 (0.140)	Data 1.70e-04 (4.46e-04)	Tok/s 108604 (102829)	Loss/tok 3.8209 (5.2481)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.058 (0.140)	Data 1.80e-04 (4.44e-04)	Tok/s 92504 (102823)	Loss/tok 3.0292 (5.2348)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.105 (0.140)	Data 1.85e-04 (4.41e-04)	Tok/s 98772 (102834)	Loss/tok 3.5237 (5.2199)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1090/1938]	Time 0.208 (0.140)	Data 1.74e-04 (4.39e-04)	Tok/s 111345 (102821)	Loss/tok 3.9808 (5.2071)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.209 (0.140)	Data 1.46e-04 (4.37e-04)	Tok/s 111917 (102830)	Loss/tok 3.8291 (5.1935)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1110/1938]	Time 0.105 (0.140)	Data 2.52e-04 (4.35e-04)	Tok/s 102499 (102813)	Loss/tok 3.6597 (5.1821)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.209 (0.140)	Data 1.88e-04 (4.32e-04)	Tok/s 111960 (102828)	Loss/tok 4.0556 (5.1692)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.106 (0.140)	Data 2.40e-04 (4.30e-04)	Tok/s 98529 (102835)	Loss/tok 3.4382 (5.1566)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.156 (0.140)	Data 2.43e-04 (4.28e-04)	Tok/s 107629 (102836)	Loss/tok 3.7040 (5.1443)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.207 (0.140)	Data 1.72e-04 (4.26e-04)	Tok/s 111671 (102880)	Loss/tok 4.0511 (5.1300)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.105 (0.140)	Data 1.60e-04 (4.24e-04)	Tok/s 96719 (102849)	Loss/tok 3.4462 (5.1197)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.105 (0.140)	Data 2.32e-04 (4.22e-04)	Tok/s 98586 (102848)	Loss/tok 3.5410 (5.1078)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.105 (0.140)	Data 1.49e-04 (4.19e-04)	Tok/s 97352 (102858)	Loss/tok 3.3976 (5.0956)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.105 (0.140)	Data 2.07e-04 (4.18e-04)	Tok/s 97482 (102861)	Loss/tok 3.4335 (5.0836)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.106 (0.140)	Data 1.60e-04 (4.16e-04)	Tok/s 97741 (102830)	Loss/tok 3.4810 (5.0736)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.157 (0.139)	Data 2.38e-04 (4.14e-04)	Tok/s 108275 (102816)	Loss/tok 3.6703 (5.0630)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.105 (0.139)	Data 1.90e-04 (4.12e-04)	Tok/s 97920 (102786)	Loss/tok 3.4713 (5.0534)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1230/1938]	Time 0.209 (0.140)	Data 2.63e-04 (4.10e-04)	Tok/s 114328 (102813)	Loss/tok 3.9155 (5.0413)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.156 (0.140)	Data 1.21e-04 (4.08e-04)	Tok/s 107477 (102818)	Loss/tok 3.6329 (5.0306)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.058 (0.139)	Data 1.82e-04 (4.06e-04)	Tok/s 92101 (102800)	Loss/tok 2.9097 (5.0213)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.268 (0.139)	Data 1.76e-04 (4.04e-04)	Tok/s 110673 (102792)	Loss/tok 4.0806 (5.0110)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.157 (0.139)	Data 2.21e-04 (4.03e-04)	Tok/s 107741 (102798)	Loss/tok 3.8251 (5.0006)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.157 (0.139)	Data 1.65e-04 (4.01e-04)	Tok/s 106405 (102792)	Loss/tok 3.7678 (4.9904)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.208 (0.140)	Data 2.16e-04 (3.99e-04)	Tok/s 111052 (102820)	Loss/tok 3.9021 (4.9787)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.058 (0.140)	Data 1.84e-04 (3.97e-04)	Tok/s 90173 (102829)	Loss/tok 2.8303 (4.9682)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.104 (0.140)	Data 2.39e-04 (3.96e-04)	Tok/s 100454 (102844)	Loss/tok 3.3650 (4.9571)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.156 (0.140)	Data 2.29e-04 (3.94e-04)	Tok/s 107859 (102827)	Loss/tok 3.7409 (4.9484)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.267 (0.140)	Data 2.19e-04 (3.92e-04)	Tok/s 112772 (102833)	Loss/tok 4.0838 (4.9384)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.106 (0.140)	Data 1.65e-04 (3.91e-04)	Tok/s 95728 (102812)	Loss/tok 3.3870 (4.9300)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.208 (0.140)	Data 2.06e-04 (3.89e-04)	Tok/s 112468 (102792)	Loss/tok 3.8642 (4.9215)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1360/1938]	Time 0.157 (0.140)	Data 1.99e-04 (3.88e-04)	Tok/s 108107 (102776)	Loss/tok 3.6730 (4.9128)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.105 (0.140)	Data 1.95e-04 (3.86e-04)	Tok/s 98550 (102783)	Loss/tok 3.5351 (4.9031)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1380/1938]	Time 0.156 (0.140)	Data 1.45e-04 (3.85e-04)	Tok/s 106536 (102763)	Loss/tok 3.6259 (4.8946)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.106 (0.139)	Data 2.31e-04 (3.83e-04)	Tok/s 96543 (102751)	Loss/tok 3.4542 (4.8860)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.158 (0.139)	Data 2.41e-04 (3.82e-04)	Tok/s 105355 (102742)	Loss/tok 3.7670 (4.8773)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.058 (0.139)	Data 1.35e-04 (3.80e-04)	Tok/s 90040 (102747)	Loss/tok 3.0868 (4.8682)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.157 (0.139)	Data 1.66e-04 (3.79e-04)	Tok/s 105617 (102744)	Loss/tok 3.7398 (4.8597)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.060 (0.139)	Data 1.61e-04 (3.77e-04)	Tok/s 87647 (102686)	Loss/tok 2.9323 (4.8534)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.059 (0.139)	Data 1.97e-04 (3.76e-04)	Tok/s 91104 (102652)	Loss/tok 3.0352 (4.8462)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.158 (0.139)	Data 2.09e-04 (3.74e-04)	Tok/s 106331 (102675)	Loss/tok 3.5582 (4.8367)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.210 (0.139)	Data 1.44e-04 (3.73e-04)	Tok/s 111883 (102669)	Loss/tok 3.7721 (4.8283)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.157 (0.139)	Data 1.98e-04 (3.72e-04)	Tok/s 107287 (102647)	Loss/tok 3.5530 (4.8209)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.211 (0.139)	Data 2.45e-04 (3.71e-04)	Tok/s 109461 (102657)	Loss/tok 3.8476 (4.8122)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.158 (0.139)	Data 1.89e-04 (3.69e-04)	Tok/s 107228 (102671)	Loss/tok 3.6222 (4.8033)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1500/1938]	Time 0.210 (0.139)	Data 1.83e-04 (3.68e-04)	Tok/s 112374 (102681)	Loss/tok 3.8697 (4.7948)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1510/1938]	Time 0.158 (0.139)	Data 1.69e-04 (3.67e-04)	Tok/s 107714 (102683)	Loss/tok 3.6356 (4.7867)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.108 (0.139)	Data 1.59e-04 (3.66e-04)	Tok/s 97372 (102678)	Loss/tok 3.2396 (4.7790)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.106 (0.140)	Data 1.45e-04 (3.64e-04)	Tok/s 97074 (102698)	Loss/tok 3.3433 (4.7700)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.210 (0.139)	Data 1.94e-04 (3.63e-04)	Tok/s 112624 (102666)	Loss/tok 3.8299 (4.7635)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.208 (0.140)	Data 2.57e-04 (3.62e-04)	Tok/s 112634 (102691)	Loss/tok 3.6756 (4.7545)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.158 (0.140)	Data 1.57e-04 (3.61e-04)	Tok/s 106117 (102701)	Loss/tok 3.5756 (4.7464)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.209 (0.140)	Data 1.77e-04 (3.60e-04)	Tok/s 111441 (102685)	Loss/tok 3.8005 (4.7394)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.159 (0.140)	Data 1.37e-04 (3.59e-04)	Tok/s 104887 (102676)	Loss/tok 3.5695 (4.7321)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.157 (0.140)	Data 1.71e-04 (3.58e-04)	Tok/s 107483 (102666)	Loss/tok 3.4595 (4.7251)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.106 (0.140)	Data 1.81e-04 (3.56e-04)	Tok/s 95519 (102637)	Loss/tok 3.2964 (4.7187)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.108 (0.140)	Data 1.52e-04 (3.55e-04)	Tok/s 93624 (102634)	Loss/tok 3.2739 (4.7113)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.107 (0.140)	Data 1.72e-04 (3.54e-04)	Tok/s 96579 (102615)	Loss/tok 3.3124 (4.7050)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.106 (0.140)	Data 3.36e-04 (3.53e-04)	Tok/s 98294 (102614)	Loss/tok 3.2961 (4.6979)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1640/1938]	Time 0.159 (0.140)	Data 1.64e-04 (3.52e-04)	Tok/s 106425 (102619)	Loss/tok 3.4367 (4.6905)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.106 (0.140)	Data 1.65e-04 (3.51e-04)	Tok/s 99561 (102599)	Loss/tok 3.4249 (4.6843)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.107 (0.139)	Data 1.83e-04 (3.50e-04)	Tok/s 95066 (102578)	Loss/tok 3.3003 (4.6782)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.059 (0.139)	Data 2.36e-04 (3.49e-04)	Tok/s 88980 (102569)	Loss/tok 2.9015 (4.6718)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1680/1938]	Time 0.209 (0.140)	Data 1.59e-04 (3.48e-04)	Tok/s 110509 (102575)	Loss/tok 3.8944 (4.6646)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.158 (0.140)	Data 2.42e-04 (3.47e-04)	Tok/s 106719 (102578)	Loss/tok 3.5457 (4.6577)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.107 (0.140)	Data 1.25e-04 (3.46e-04)	Tok/s 96710 (102550)	Loss/tok 3.3588 (4.6521)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.106 (0.139)	Data 1.49e-04 (3.45e-04)	Tok/s 97734 (102515)	Loss/tok 3.4451 (4.6468)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.059 (0.139)	Data 1.50e-04 (3.44e-04)	Tok/s 90283 (102509)	Loss/tok 2.8424 (4.6408)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.210 (0.139)	Data 1.87e-04 (3.43e-04)	Tok/s 112433 (102512)	Loss/tok 3.6965 (4.6341)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.107 (0.139)	Data 2.50e-04 (3.42e-04)	Tok/s 97739 (102498)	Loss/tok 3.3075 (4.6285)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.270 (0.139)	Data 1.73e-04 (3.41e-04)	Tok/s 110171 (102510)	Loss/tok 3.9078 (4.6216)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.059 (0.139)	Data 1.88e-04 (3.41e-04)	Tok/s 87673 (102506)	Loss/tok 2.8100 (4.6157)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.268 (0.139)	Data 1.78e-04 (3.40e-04)	Tok/s 110404 (102487)	Loss/tok 3.9803 (4.6104)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.105 (0.139)	Data 1.97e-04 (3.39e-04)	Tok/s 98825 (102489)	Loss/tok 3.3531 (4.6044)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.157 (0.139)	Data 1.79e-04 (3.38e-04)	Tok/s 104845 (102479)	Loss/tok 3.5834 (4.5990)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.157 (0.139)	Data 2.08e-04 (3.37e-04)	Tok/s 106567 (102464)	Loss/tok 3.5230 (4.5936)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1810/1938]	Time 0.269 (0.139)	Data 1.86e-04 (3.36e-04)	Tok/s 111207 (102474)	Loss/tok 3.9266 (4.5874)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.106 (0.139)	Data 2.09e-04 (3.35e-04)	Tok/s 96551 (102476)	Loss/tok 3.3225 (4.5813)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.106 (0.139)	Data 2.40e-04 (3.35e-04)	Tok/s 98742 (102470)	Loss/tok 3.3382 (4.5758)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.157 (0.139)	Data 1.25e-04 (3.34e-04)	Tok/s 107310 (102474)	Loss/tok 3.5322 (4.5701)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.106 (0.139)	Data 1.98e-04 (3.33e-04)	Tok/s 97882 (102469)	Loss/tok 3.4180 (4.5646)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.107 (0.139)	Data 1.51e-04 (3.32e-04)	Tok/s 96607 (102462)	Loss/tok 3.2886 (4.5593)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.210 (0.139)	Data 2.65e-04 (3.31e-04)	Tok/s 111163 (102482)	Loss/tok 3.6502 (4.5529)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.107 (0.139)	Data 1.90e-04 (3.30e-04)	Tok/s 97267 (102478)	Loss/tok 3.2344 (4.5473)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.210 (0.139)	Data 1.30e-04 (3.30e-04)	Tok/s 110781 (102478)	Loss/tok 3.8128 (4.5419)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.158 (0.139)	Data 1.55e-04 (3.29e-04)	Tok/s 108346 (102468)	Loss/tok 3.6551 (4.5369)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1910/1938]	Time 0.106 (0.139)	Data 1.72e-04 (3.28e-04)	Tok/s 97572 (102458)	Loss/tok 3.3674 (4.5323)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.108 (0.140)	Data 1.74e-04 (3.27e-04)	Tok/s 95168 (102458)	Loss/tok 3.1423 (4.5267)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.106 (0.139)	Data 1.64e-04 (3.26e-04)	Tok/s 96433 (102450)	Loss/tok 3.3052 (4.5217)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019457804, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019457805, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.743 (0.743)	Decoder iters 149.0 (149.0)	Tok/s 21664 (21664)
0: Running moses detokenizer
0: BLEU(score=20.278701579702066, counts=[34742, 15953, 8488, 4724], totals=[64806, 61803, 58800, 55802], precisions=[53.60923371292782, 25.81266281572092, 14.435374149659864, 8.465646392602416], bp=1.0, sys_len=64806, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019459731, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2028, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019459732, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.5227	Test BLEU: 20.28
0: Performance: Epoch: 0	Training: 819442 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019459732, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019459733, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019459733, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3634744642
0: TRAIN [1][0/1938]	Time 0.367 (0.367)	Data 2.62e-01 (2.62e-01)	Tok/s 28316 (28316)	Loss/tok 3.1970 (3.1970)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.107 (0.165)	Data 1.86e-04 (2.40e-02)	Tok/s 97891 (94026)	Loss/tok 3.2024 (3.5406)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.106 (0.137)	Data 2.51e-04 (1.27e-02)	Tok/s 96442 (96075)	Loss/tok 3.1784 (3.4304)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.108 (0.131)	Data 1.41e-04 (8.63e-03)	Tok/s 95754 (97129)	Loss/tok 3.2378 (3.4019)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.158 (0.134)	Data 1.65e-04 (6.57e-03)	Tok/s 107028 (98378)	Loss/tok 3.4272 (3.4011)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.159 (0.141)	Data 1.61e-04 (5.31e-03)	Tok/s 105112 (99597)	Loss/tok 3.4266 (3.4423)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.268 (0.142)	Data 2.22e-04 (4.48e-03)	Tok/s 111928 (99944)	Loss/tok 3.9172 (3.4563)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.158 (0.143)	Data 2.06e-04 (3.87e-03)	Tok/s 108045 (100287)	Loss/tok 3.4692 (3.4644)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.059 (0.141)	Data 1.77e-04 (3.42e-03)	Tok/s 89589 (100340)	Loss/tok 2.8018 (3.4616)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.268 (0.141)	Data 2.69e-04 (3.06e-03)	Tok/s 110871 (100078)	Loss/tok 3.8765 (3.4664)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][100/1938]	Time 0.107 (0.139)	Data 1.97e-04 (2.77e-03)	Tok/s 96964 (100015)	Loss/tok 3.3040 (3.4604)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.108 (0.139)	Data 1.44e-04 (2.54e-03)	Tok/s 95348 (100251)	Loss/tok 3.1626 (3.4567)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.108 (0.141)	Data 1.41e-04 (2.34e-03)	Tok/s 98050 (100443)	Loss/tok 3.1718 (3.4659)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.159 (0.140)	Data 1.42e-04 (2.18e-03)	Tok/s 104684 (100403)	Loss/tok 3.4859 (3.4571)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.107 (0.141)	Data 1.53e-04 (2.03e-03)	Tok/s 96241 (100615)	Loss/tok 3.1349 (3.4573)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.211 (0.141)	Data 2.26e-04 (1.91e-03)	Tok/s 109793 (100725)	Loss/tok 3.6564 (3.4600)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.211 (0.141)	Data 1.22e-04 (1.80e-03)	Tok/s 110417 (100754)	Loss/tok 3.6454 (3.4590)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.108 (0.142)	Data 1.39e-04 (1.71e-03)	Tok/s 96755 (100814)	Loss/tok 3.1308 (3.4645)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][180/1938]	Time 0.268 (0.144)	Data 1.73e-04 (1.62e-03)	Tok/s 111862 (101078)	Loss/tok 3.8448 (3.4766)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.058 (0.143)	Data 1.87e-04 (1.55e-03)	Tok/s 91904 (101041)	Loss/tok 2.7969 (3.4694)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.105 (0.141)	Data 1.39e-04 (1.48e-03)	Tok/s 96527 (100895)	Loss/tok 3.4114 (3.4652)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.058 (0.142)	Data 1.67e-04 (1.42e-03)	Tok/s 89597 (101103)	Loss/tok 2.8383 (3.4684)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.158 (0.143)	Data 1.59e-04 (1.36e-03)	Tok/s 106225 (101307)	Loss/tok 3.5077 (3.4692)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.159 (0.143)	Data 1.40e-04 (1.31e-03)	Tok/s 105496 (101306)	Loss/tok 3.4840 (3.4710)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.108 (0.144)	Data 1.63e-04 (1.26e-03)	Tok/s 97301 (101402)	Loss/tok 3.2599 (3.4749)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.059 (0.144)	Data 1.71e-04 (1.22e-03)	Tok/s 89275 (101381)	Loss/tok 2.7093 (3.4733)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.272 (0.144)	Data 1.27e-04 (1.18e-03)	Tok/s 108476 (101326)	Loss/tok 3.9664 (3.4764)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.160 (0.144)	Data 1.61e-04 (1.14e-03)	Tok/s 104759 (101417)	Loss/tok 3.4685 (3.4764)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.107 (0.143)	Data 1.53e-04 (1.10e-03)	Tok/s 96196 (101295)	Loss/tok 3.2163 (3.4726)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.060 (0.143)	Data 1.47e-04 (1.07e-03)	Tok/s 86230 (101238)	Loss/tok 2.7449 (3.4723)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.108 (0.142)	Data 1.29e-04 (1.04e-03)	Tok/s 95731 (101112)	Loss/tok 3.1927 (3.4666)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][310/1938]	Time 0.107 (0.142)	Data 1.93e-04 (1.01e-03)	Tok/s 94470 (101050)	Loss/tok 3.1931 (3.4669)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.210 (0.142)	Data 1.64e-04 (9.86e-04)	Tok/s 110224 (100993)	Loss/tok 3.5720 (3.4672)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.159 (0.142)	Data 2.64e-04 (9.61e-04)	Tok/s 104300 (101081)	Loss/tok 3.3644 (3.4653)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.107 (0.141)	Data 1.73e-04 (9.38e-04)	Tok/s 96242 (100955)	Loss/tok 3.3040 (3.4606)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.158 (0.141)	Data 1.32e-04 (9.16e-04)	Tok/s 106044 (100922)	Loss/tok 3.4157 (3.4591)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.106 (0.141)	Data 1.28e-04 (8.95e-04)	Tok/s 96419 (100905)	Loss/tok 3.3456 (3.4580)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.211 (0.141)	Data 2.69e-04 (8.75e-04)	Tok/s 110636 (100922)	Loss/tok 3.7284 (3.4573)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.160 (0.140)	Data 1.24e-04 (8.56e-04)	Tok/s 102934 (100873)	Loss/tok 3.5430 (3.4545)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.157 (0.140)	Data 2.07e-04 (8.39e-04)	Tok/s 106375 (100842)	Loss/tok 3.4925 (3.4529)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.209 (0.140)	Data 1.22e-04 (8.22e-04)	Tok/s 111847 (100904)	Loss/tok 3.5395 (3.4519)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.159 (0.140)	Data 1.23e-04 (8.07e-04)	Tok/s 105663 (100917)	Loss/tok 3.3823 (3.4496)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.107 (0.140)	Data 1.25e-04 (7.91e-04)	Tok/s 97289 (100919)	Loss/tok 3.1681 (3.4492)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][430/1938]	Time 0.107 (0.140)	Data 1.57e-04 (7.76e-04)	Tok/s 96385 (100927)	Loss/tok 3.2005 (3.4491)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.158 (0.140)	Data 1.25e-04 (7.62e-04)	Tok/s 105214 (100959)	Loss/tok 3.5248 (3.4478)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][450/1938]	Time 0.059 (0.140)	Data 1.88e-04 (7.49e-04)	Tok/s 90158 (100961)	Loss/tok 2.7261 (3.4479)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.059 (0.140)	Data 1.84e-04 (7.36e-04)	Tok/s 90741 (100939)	Loss/tok 2.8558 (3.4473)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.106 (0.140)	Data 1.42e-04 (7.24e-04)	Tok/s 96777 (100946)	Loss/tok 3.2319 (3.4473)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.212 (0.140)	Data 1.28e-04 (7.13e-04)	Tok/s 109511 (101007)	Loss/tok 3.6708 (3.4478)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.159 (0.140)	Data 1.29e-04 (7.01e-04)	Tok/s 105698 (101013)	Loss/tok 3.4529 (3.4470)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.158 (0.140)	Data 2.16e-04 (6.91e-04)	Tok/s 105494 (101084)	Loss/tok 3.4842 (3.4473)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.107 (0.140)	Data 1.18e-04 (6.80e-04)	Tok/s 95124 (101087)	Loss/tok 3.2072 (3.4458)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.210 (0.141)	Data 1.11e-04 (6.70e-04)	Tok/s 113163 (101187)	Loss/tok 3.6372 (3.4489)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][530/1938]	Time 0.269 (0.140)	Data 2.07e-04 (6.61e-04)	Tok/s 111853 (101113)	Loss/tok 3.7986 (3.4472)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.158 (0.140)	Data 1.24e-04 (6.52e-04)	Tok/s 107124 (101153)	Loss/tok 3.4006 (3.4481)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.158 (0.140)	Data 2.18e-04 (6.43e-04)	Tok/s 105934 (101186)	Loss/tok 3.5014 (3.4490)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.107 (0.141)	Data 1.36e-04 (6.34e-04)	Tok/s 98247 (101202)	Loss/tok 3.3511 (3.4491)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.157 (0.141)	Data 1.22e-04 (6.27e-04)	Tok/s 105949 (101221)	Loss/tok 3.3781 (3.4477)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.107 (0.141)	Data 1.40e-04 (6.19e-04)	Tok/s 97945 (101246)	Loss/tok 3.0675 (3.4470)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.160 (0.141)	Data 1.27e-04 (6.11e-04)	Tok/s 104842 (101271)	Loss/tok 3.3319 (3.4467)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.272 (0.141)	Data 1.70e-04 (6.04e-04)	Tok/s 107637 (101309)	Loss/tok 3.8248 (3.4478)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.108 (0.141)	Data 1.30e-04 (5.96e-04)	Tok/s 95614 (101284)	Loss/tok 3.2154 (3.4480)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.108 (0.141)	Data 1.37e-04 (5.89e-04)	Tok/s 95050 (101209)	Loss/tok 3.3074 (3.4456)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.059 (0.140)	Data 1.24e-04 (5.82e-04)	Tok/s 88839 (101155)	Loss/tok 2.6822 (3.4436)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.109 (0.141)	Data 1.29e-04 (5.76e-04)	Tok/s 94677 (101157)	Loss/tok 3.0928 (3.4442)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.159 (0.141)	Data 1.50e-04 (5.69e-04)	Tok/s 106104 (101197)	Loss/tok 3.5220 (3.4443)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][660/1938]	Time 0.108 (0.141)	Data 1.29e-04 (5.63e-04)	Tok/s 95618 (101200)	Loss/tok 3.1544 (3.4440)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.159 (0.141)	Data 1.24e-04 (5.57e-04)	Tok/s 105359 (101226)	Loss/tok 3.4385 (3.4442)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.159 (0.141)	Data 1.27e-04 (5.51e-04)	Tok/s 105418 (101199)	Loss/tok 3.3887 (3.4419)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.157 (0.141)	Data 1.97e-04 (5.46e-04)	Tok/s 107991 (101225)	Loss/tok 3.3040 (3.4417)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.108 (0.141)	Data 1.16e-04 (5.40e-04)	Tok/s 96695 (101238)	Loss/tok 3.1897 (3.4421)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.211 (0.141)	Data 1.47e-04 (5.35e-04)	Tok/s 111448 (101281)	Loss/tok 3.5776 (3.4422)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.107 (0.141)	Data 1.29e-04 (5.30e-04)	Tok/s 95813 (101242)	Loss/tok 3.1409 (3.4404)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.211 (0.141)	Data 1.73e-04 (5.25e-04)	Tok/s 110917 (101242)	Loss/tok 3.4769 (3.4393)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.107 (0.141)	Data 1.33e-04 (5.20e-04)	Tok/s 96972 (101250)	Loss/tok 3.1719 (3.4383)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.159 (0.141)	Data 1.79e-04 (5.15e-04)	Tok/s 105224 (101276)	Loss/tok 3.4265 (3.4377)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.106 (0.141)	Data 2.27e-04 (5.11e-04)	Tok/s 95739 (101257)	Loss/tok 3.2501 (3.4358)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.268 (0.141)	Data 1.38e-04 (5.06e-04)	Tok/s 110275 (101283)	Loss/tok 3.7343 (3.4379)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.157 (0.141)	Data 1.32e-04 (5.02e-04)	Tok/s 104749 (101323)	Loss/tok 3.5159 (3.4387)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][790/1938]	Time 0.211 (0.141)	Data 1.27e-04 (4.98e-04)	Tok/s 109588 (101318)	Loss/tok 3.5657 (3.4379)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.210 (0.141)	Data 2.09e-04 (4.94e-04)	Tok/s 112433 (101317)	Loss/tok 3.6670 (3.4383)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.157 (0.142)	Data 2.11e-04 (4.90e-04)	Tok/s 107911 (101375)	Loss/tok 3.3990 (3.4393)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.107 (0.141)	Data 1.70e-04 (4.86e-04)	Tok/s 97340 (101337)	Loss/tok 3.1356 (3.4375)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.107 (0.141)	Data 1.26e-04 (4.82e-04)	Tok/s 95544 (101308)	Loss/tok 3.0921 (3.4355)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.107 (0.141)	Data 1.39e-04 (4.78e-04)	Tok/s 98113 (101297)	Loss/tok 3.1148 (3.4345)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][850/1938]	Time 0.210 (0.141)	Data 1.33e-04 (4.74e-04)	Tok/s 112650 (101279)	Loss/tok 3.6637 (3.4335)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.159 (0.141)	Data 2.05e-04 (4.71e-04)	Tok/s 106704 (101259)	Loss/tok 3.3336 (3.4321)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.269 (0.141)	Data 2.01e-04 (4.67e-04)	Tok/s 109687 (101248)	Loss/tok 3.8391 (3.4316)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.158 (0.141)	Data 1.21e-04 (4.64e-04)	Tok/s 104868 (101245)	Loss/tok 3.3106 (3.4312)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.160 (0.140)	Data 1.37e-04 (4.60e-04)	Tok/s 104695 (101225)	Loss/tok 3.3954 (3.4296)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.159 (0.140)	Data 1.24e-04 (4.57e-04)	Tok/s 104228 (101211)	Loss/tok 3.4249 (3.4292)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.107 (0.140)	Data 1.26e-04 (4.53e-04)	Tok/s 95066 (101167)	Loss/tok 3.2004 (3.4284)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.270 (0.140)	Data 2.36e-04 (4.50e-04)	Tok/s 111035 (101192)	Loss/tok 3.6141 (3.4292)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.210 (0.140)	Data 1.83e-04 (4.47e-04)	Tok/s 111914 (101184)	Loss/tok 3.6565 (3.4294)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.158 (0.141)	Data 1.55e-04 (4.44e-04)	Tok/s 107204 (101221)	Loss/tok 3.4135 (3.4298)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.159 (0.141)	Data 1.28e-04 (4.41e-04)	Tok/s 107009 (101215)	Loss/tok 3.4671 (3.4299)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.107 (0.140)	Data 1.20e-04 (4.38e-04)	Tok/s 95527 (101175)	Loss/tok 3.3234 (3.4284)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.158 (0.141)	Data 1.43e-04 (4.35e-04)	Tok/s 106324 (101196)	Loss/tok 3.2909 (3.4283)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][980/1938]	Time 0.107 (0.141)	Data 2.74e-04 (4.32e-04)	Tok/s 94624 (101201)	Loss/tok 3.1626 (3.4290)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.108 (0.141)	Data 1.59e-04 (4.29e-04)	Tok/s 95682 (101181)	Loss/tok 3.1671 (3.4280)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.107 (0.140)	Data 2.12e-04 (4.27e-04)	Tok/s 97353 (101149)	Loss/tok 3.2060 (3.4268)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.107 (0.140)	Data 1.25e-04 (4.24e-04)	Tok/s 96185 (101159)	Loss/tok 3.1340 (3.4256)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.158 (0.140)	Data 1.27e-04 (4.21e-04)	Tok/s 105668 (101162)	Loss/tok 3.4565 (3.4258)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.159 (0.140)	Data 1.13e-04 (4.19e-04)	Tok/s 104910 (101151)	Loss/tok 3.4054 (3.4251)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.108 (0.140)	Data 1.55e-04 (4.16e-04)	Tok/s 96637 (101134)	Loss/tok 3.2410 (3.4246)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.107 (0.141)	Data 1.26e-04 (4.14e-04)	Tok/s 93064 (101177)	Loss/tok 3.1757 (3.4255)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.107 (0.141)	Data 1.61e-04 (4.11e-04)	Tok/s 95938 (101163)	Loss/tok 3.0893 (3.4246)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.059 (0.140)	Data 2.11e-04 (4.09e-04)	Tok/s 88839 (101164)	Loss/tok 2.6005 (3.4242)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1080/1938]	Time 0.106 (0.140)	Data 1.34e-04 (4.07e-04)	Tok/s 95079 (101154)	Loss/tok 3.2250 (3.4239)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.059 (0.140)	Data 1.28e-04 (4.05e-04)	Tok/s 87559 (101169)	Loss/tok 2.7349 (3.4237)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.107 (0.140)	Data 1.13e-04 (4.02e-04)	Tok/s 96159 (101116)	Loss/tok 3.1982 (3.4224)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.157 (0.140)	Data 1.25e-04 (4.00e-04)	Tok/s 106671 (101112)	Loss/tok 3.4807 (3.4217)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.210 (0.140)	Data 1.78e-04 (3.98e-04)	Tok/s 110872 (101094)	Loss/tok 3.7103 (3.4208)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.106 (0.140)	Data 1.29e-04 (3.96e-04)	Tok/s 98622 (101070)	Loss/tok 3.1693 (3.4192)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.271 (0.140)	Data 1.26e-04 (3.93e-04)	Tok/s 111016 (101109)	Loss/tok 3.7314 (3.4195)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.158 (0.140)	Data 2.40e-04 (3.92e-04)	Tok/s 107936 (101111)	Loss/tok 3.3240 (3.4203)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.107 (0.140)	Data 1.22e-04 (3.90e-04)	Tok/s 95436 (101133)	Loss/tok 3.1452 (3.4201)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.107 (0.140)	Data 1.96e-04 (3.88e-04)	Tok/s 95161 (101133)	Loss/tok 3.1982 (3.4198)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.157 (0.140)	Data 1.91e-04 (3.86e-04)	Tok/s 106564 (101128)	Loss/tok 3.3775 (3.4188)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.157 (0.140)	Data 1.87e-04 (3.84e-04)	Tok/s 107805 (101156)	Loss/tok 3.3188 (3.4187)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.158 (0.140)	Data 1.36e-04 (3.82e-04)	Tok/s 105071 (101170)	Loss/tok 3.5299 (3.4193)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1210/1938]	Time 0.106 (0.140)	Data 1.75e-04 (3.80e-04)	Tok/s 97866 (101181)	Loss/tok 3.2049 (3.4192)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.157 (0.140)	Data 1.32e-04 (3.79e-04)	Tok/s 105621 (101191)	Loss/tok 3.3622 (3.4189)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.106 (0.140)	Data 1.65e-04 (3.77e-04)	Tok/s 98126 (101198)	Loss/tok 3.1986 (3.4183)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.157 (0.140)	Data 2.10e-04 (3.75e-04)	Tok/s 105816 (101219)	Loss/tok 3.4729 (3.4183)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.209 (0.140)	Data 1.47e-04 (3.74e-04)	Tok/s 110186 (101246)	Loss/tok 3.6142 (3.4188)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.107 (0.141)	Data 1.51e-04 (3.72e-04)	Tok/s 95334 (101264)	Loss/tok 3.1276 (3.4192)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.106 (0.141)	Data 1.55e-04 (3.70e-04)	Tok/s 97599 (101261)	Loss/tok 3.1415 (3.4184)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.106 (0.140)	Data 1.41e-04 (3.68e-04)	Tok/s 96846 (101224)	Loss/tok 3.1541 (3.4170)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.159 (0.140)	Data 1.32e-04 (3.67e-04)	Tok/s 106636 (101216)	Loss/tok 3.4558 (3.4159)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.107 (0.140)	Data 2.11e-04 (3.65e-04)	Tok/s 96733 (101219)	Loss/tok 3.1000 (3.4152)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.157 (0.140)	Data 2.46e-04 (3.64e-04)	Tok/s 107532 (101265)	Loss/tok 3.3678 (3.4160)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.158 (0.140)	Data 1.29e-04 (3.62e-04)	Tok/s 105520 (101283)	Loss/tok 3.3415 (3.4159)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.211 (0.140)	Data 1.22e-04 (3.61e-04)	Tok/s 111224 (101285)	Loss/tok 3.6311 (3.4156)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1340/1938]	Time 0.058 (0.140)	Data 1.39e-04 (3.59e-04)	Tok/s 90143 (101283)	Loss/tok 2.6636 (3.4152)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1350/1938]	Time 0.159 (0.140)	Data 1.72e-04 (3.58e-04)	Tok/s 105355 (101271)	Loss/tok 3.3486 (3.4147)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.107 (0.140)	Data 1.69e-04 (3.56e-04)	Tok/s 94559 (101278)	Loss/tok 3.0806 (3.4143)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.106 (0.140)	Data 2.25e-04 (3.55e-04)	Tok/s 98313 (101268)	Loss/tok 3.2187 (3.4132)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.106 (0.140)	Data 1.81e-04 (3.53e-04)	Tok/s 97022 (101230)	Loss/tok 3.3210 (3.4118)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.059 (0.140)	Data 1.93e-04 (3.52e-04)	Tok/s 91113 (101223)	Loss/tok 2.6794 (3.4110)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.106 (0.140)	Data 1.40e-04 (3.51e-04)	Tok/s 97042 (101232)	Loss/tok 3.0876 (3.4107)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.107 (0.140)	Data 2.02e-04 (3.49e-04)	Tok/s 95321 (101259)	Loss/tok 3.0900 (3.4105)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.107 (0.140)	Data 1.91e-04 (3.48e-04)	Tok/s 96440 (101237)	Loss/tok 3.0274 (3.4096)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.211 (0.140)	Data 1.43e-04 (3.47e-04)	Tok/s 111460 (101233)	Loss/tok 3.6770 (3.4093)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.106 (0.140)	Data 1.34e-04 (3.45e-04)	Tok/s 96857 (101255)	Loss/tok 3.1629 (3.4099)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.107 (0.140)	Data 1.51e-04 (3.44e-04)	Tok/s 98562 (101280)	Loss/tok 3.1921 (3.4096)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.106 (0.140)	Data 1.55e-04 (3.43e-04)	Tok/s 99041 (101303)	Loss/tok 3.1481 (3.4101)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.106 (0.140)	Data 1.26e-04 (3.41e-04)	Tok/s 97175 (101314)	Loss/tok 3.2038 (3.4099)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1480/1938]	Time 0.107 (0.140)	Data 1.44e-04 (3.40e-04)	Tok/s 95131 (101310)	Loss/tok 3.2570 (3.4095)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.211 (0.140)	Data 1.32e-04 (3.39e-04)	Tok/s 109073 (101333)	Loss/tok 3.5702 (3.4099)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.106 (0.141)	Data 1.99e-04 (3.38e-04)	Tok/s 98228 (101351)	Loss/tok 3.1254 (3.4097)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.059 (0.140)	Data 1.20e-04 (3.37e-04)	Tok/s 91625 (101327)	Loss/tok 2.7531 (3.4086)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.270 (0.140)	Data 2.76e-04 (3.36e-04)	Tok/s 110506 (101330)	Loss/tok 3.6361 (3.4084)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.158 (0.140)	Data 1.28e-04 (3.35e-04)	Tok/s 105730 (101325)	Loss/tok 3.3670 (3.4085)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.107 (0.140)	Data 1.12e-04 (3.33e-04)	Tok/s 95974 (101330)	Loss/tok 3.0997 (3.4083)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.157 (0.141)	Data 1.48e-04 (3.32e-04)	Tok/s 107216 (101334)	Loss/tok 3.3962 (3.4089)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1560/1938]	Time 0.107 (0.140)	Data 1.26e-04 (3.31e-04)	Tok/s 98077 (101326)	Loss/tok 3.2401 (3.4082)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.106 (0.140)	Data 1.68e-04 (3.30e-04)	Tok/s 96151 (101314)	Loss/tok 3.1563 (3.4076)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.210 (0.141)	Data 2.86e-04 (3.29e-04)	Tok/s 112641 (101337)	Loss/tok 3.3862 (3.4082)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.209 (0.141)	Data 2.16e-04 (3.28e-04)	Tok/s 110544 (101343)	Loss/tok 3.4063 (3.4076)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.106 (0.141)	Data 2.82e-04 (3.27e-04)	Tok/s 96613 (101343)	Loss/tok 3.1493 (3.4070)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.106 (0.141)	Data 1.79e-04 (3.26e-04)	Tok/s 95034 (101343)	Loss/tok 3.1368 (3.4070)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.106 (0.141)	Data 2.54e-04 (3.25e-04)	Tok/s 98379 (101356)	Loss/tok 3.0792 (3.4070)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.107 (0.140)	Data 1.39e-04 (3.24e-04)	Tok/s 97031 (101338)	Loss/tok 3.0616 (3.4059)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.106 (0.140)	Data 1.83e-04 (3.23e-04)	Tok/s 99294 (101332)	Loss/tok 3.1838 (3.4052)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.157 (0.140)	Data 1.33e-04 (3.22e-04)	Tok/s 107011 (101338)	Loss/tok 3.3307 (3.4044)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.158 (0.140)	Data 2.05e-04 (3.21e-04)	Tok/s 105759 (101338)	Loss/tok 3.3509 (3.4044)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.211 (0.141)	Data 1.45e-04 (3.20e-04)	Tok/s 110987 (101369)	Loss/tok 3.5920 (3.4049)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1680/1938]	Time 0.158 (0.141)	Data 1.47e-04 (3.19e-04)	Tok/s 106350 (101378)	Loss/tok 3.3563 (3.4046)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.106 (0.141)	Data 1.45e-04 (3.18e-04)	Tok/s 97366 (101377)	Loss/tok 3.2037 (3.4042)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.108 (0.140)	Data 1.83e-04 (3.17e-04)	Tok/s 98147 (101364)	Loss/tok 3.1811 (3.4033)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.107 (0.141)	Data 1.58e-04 (3.16e-04)	Tok/s 95823 (101386)	Loss/tok 3.1027 (3.4033)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1720/1938]	Time 0.158 (0.141)	Data 1.27e-04 (3.15e-04)	Tok/s 104956 (101380)	Loss/tok 3.3506 (3.4032)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.159 (0.140)	Data 1.92e-04 (3.14e-04)	Tok/s 105871 (101372)	Loss/tok 3.3017 (3.4023)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.158 (0.140)	Data 1.74e-04 (3.14e-04)	Tok/s 106500 (101380)	Loss/tok 3.3648 (3.4021)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.107 (0.141)	Data 1.56e-04 (3.13e-04)	Tok/s 96040 (101388)	Loss/tok 3.1666 (3.4021)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.210 (0.141)	Data 1.28e-04 (3.12e-04)	Tok/s 111017 (101397)	Loss/tok 3.5599 (3.4023)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.11e-04)	Tok/s 95378 (101400)	Loss/tok 3.2009 (3.4024)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.106 (0.141)	Data 1.48e-04 (3.10e-04)	Tok/s 96335 (101411)	Loss/tok 3.1131 (3.4026)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.158 (0.141)	Data 1.42e-04 (3.09e-04)	Tok/s 106166 (101398)	Loss/tok 3.3628 (3.4018)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.270 (0.141)	Data 1.79e-04 (3.08e-04)	Tok/s 110208 (101415)	Loss/tok 3.7104 (3.4019)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.158 (0.141)	Data 1.25e-04 (3.07e-04)	Tok/s 106983 (101411)	Loss/tok 3.2212 (3.4009)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.158 (0.141)	Data 1.48e-04 (3.06e-04)	Tok/s 106621 (101403)	Loss/tok 3.3823 (3.4004)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.106 (0.141)	Data 1.33e-04 (3.06e-04)	Tok/s 97515 (101394)	Loss/tok 3.0901 (3.3994)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.106 (0.141)	Data 1.30e-04 (3.05e-04)	Tok/s 98324 (101398)	Loss/tok 3.1843 (3.3996)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1850/1938]	Time 0.210 (0.141)	Data 1.69e-04 (3.04e-04)	Tok/s 109561 (101391)	Loss/tok 3.6483 (3.3990)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.157 (0.141)	Data 1.71e-04 (3.03e-04)	Tok/s 106898 (101408)	Loss/tok 3.3410 (3.3990)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.059 (0.141)	Data 1.67e-04 (3.03e-04)	Tok/s 91245 (101383)	Loss/tok 2.6790 (3.3982)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.107 (0.140)	Data 1.25e-04 (3.02e-04)	Tok/s 97770 (101380)	Loss/tok 3.0576 (3.3976)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.271 (0.141)	Data 1.74e-04 (3.01e-04)	Tok/s 109635 (101399)	Loss/tok 3.6100 (3.3977)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.210 (0.141)	Data 2.20e-04 (3.01e-04)	Tok/s 109027 (101407)	Loss/tok 3.5384 (3.3976)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.158 (0.141)	Data 1.94e-04 (3.00e-04)	Tok/s 105478 (101399)	Loss/tok 3.3751 (3.3970)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.107 (0.141)	Data 1.24e-04 (2.99e-04)	Tok/s 96515 (101385)	Loss/tok 3.1229 (3.3961)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1930/1938]	Time 0.157 (0.141)	Data 1.39e-04 (2.99e-04)	Tok/s 105796 (101402)	Loss/tok 3.3334 (3.3964)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019733317, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019733317, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.754 (0.754)	Decoder iters 149.0 (149.0)	Tok/s 21904 (21904)
0: Running moses detokenizer
0: BLEU(score=21.954287298414023, counts=[36162, 17421, 9633, 5558], totals=[66323, 63320, 60317, 57317], precisions=[54.52407158904151, 27.512634238787115, 15.970621881061724, 9.696948549296021], bp=1.0, sys_len=66323, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019735399, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2195, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019735399, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3965	Test BLEU: 21.95
0: Performance: Epoch: 1	Training: 811488 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019735400, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019735400, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019735400, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1294775964
0: TRAIN [2][0/1938]	Time 0.436 (0.436)	Data 2.65e-01 (2.65e-01)	Tok/s 38955 (38955)	Loss/tok 3.3723 (3.3723)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.105 (0.146)	Data 1.68e-04 (2.43e-02)	Tok/s 96839 (93811)	Loss/tok 2.9950 (3.1479)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.105 (0.132)	Data 2.18e-04 (1.28e-02)	Tok/s 98938 (96477)	Loss/tok 3.1201 (3.1485)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.107 (0.131)	Data 1.51e-04 (8.73e-03)	Tok/s 96696 (97580)	Loss/tok 3.0846 (3.1616)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.107 (0.129)	Data 1.86e-04 (6.65e-03)	Tok/s 96404 (98159)	Loss/tok 3.0267 (3.1491)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.107 (0.132)	Data 2.35e-04 (5.38e-03)	Tok/s 95681 (98805)	Loss/tok 3.0371 (3.1797)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.112 (0.136)	Data 1.71e-04 (4.53e-03)	Tok/s 92506 (99470)	Loss/tok 3.1243 (3.2079)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.107 (0.134)	Data 1.94e-04 (3.92e-03)	Tok/s 96205 (99491)	Loss/tok 3.0481 (3.2047)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][80/1938]	Time 0.270 (0.134)	Data 1.95e-04 (3.45e-03)	Tok/s 108989 (99466)	Loss/tok 3.6727 (3.2134)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.210 (0.138)	Data 1.67e-04 (3.09e-03)	Tok/s 110120 (100151)	Loss/tok 3.5242 (3.2283)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.107 (0.138)	Data 1.94e-04 (2.81e-03)	Tok/s 96704 (100371)	Loss/tok 3.0501 (3.2321)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.209 (0.135)	Data 1.39e-04 (2.57e-03)	Tok/s 111257 (100084)	Loss/tok 3.4430 (3.2238)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.106 (0.134)	Data 1.73e-04 (2.37e-03)	Tok/s 96864 (100066)	Loss/tok 3.2623 (3.2186)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.158 (0.133)	Data 1.57e-04 (2.20e-03)	Tok/s 107657 (100112)	Loss/tok 3.2355 (3.2175)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.157 (0.135)	Data 1.64e-04 (2.06e-03)	Tok/s 107093 (100407)	Loss/tok 3.1969 (3.2240)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.158 (0.136)	Data 1.60e-04 (1.94e-03)	Tok/s 106411 (100572)	Loss/tok 3.1534 (3.2254)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.106 (0.135)	Data 1.59e-04 (1.83e-03)	Tok/s 96979 (100609)	Loss/tok 2.9696 (3.2208)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.106 (0.134)	Data 2.82e-04 (1.73e-03)	Tok/s 96287 (100480)	Loss/tok 2.9733 (3.2208)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.107 (0.135)	Data 1.65e-04 (1.64e-03)	Tok/s 95239 (100536)	Loss/tok 3.0602 (3.2256)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.158 (0.135)	Data 1.44e-04 (1.56e-03)	Tok/s 106910 (100572)	Loss/tok 3.2104 (3.2280)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.158 (0.137)	Data 2.33e-04 (1.50e-03)	Tok/s 105972 (100762)	Loss/tok 3.1628 (3.2326)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][210/1938]	Time 0.157 (0.137)	Data 2.05e-04 (1.43e-03)	Tok/s 106888 (100793)	Loss/tok 3.3505 (3.2332)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.107 (0.137)	Data 2.07e-04 (1.38e-03)	Tok/s 96705 (100915)	Loss/tok 3.0104 (3.2345)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.210 (0.137)	Data 1.73e-04 (1.32e-03)	Tok/s 111326 (100940)	Loss/tok 3.4202 (3.2361)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.106 (0.137)	Data 1.37e-04 (1.28e-03)	Tok/s 97137 (100942)	Loss/tok 3.1763 (3.2360)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.158 (0.138)	Data 1.24e-04 (1.23e-03)	Tok/s 105996 (101028)	Loss/tok 3.0950 (3.2387)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.158 (0.137)	Data 1.25e-04 (1.19e-03)	Tok/s 107195 (101068)	Loss/tok 3.1854 (3.2374)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.211 (0.138)	Data 1.32e-04 (1.15e-03)	Tok/s 111533 (101219)	Loss/tok 3.4443 (3.2393)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.106 (0.138)	Data 1.31e-04 (1.11e-03)	Tok/s 97670 (101250)	Loss/tok 3.0342 (3.2395)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.158 (0.138)	Data 1.41e-04 (1.08e-03)	Tok/s 105821 (101250)	Loss/tok 3.2616 (3.2375)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.107 (0.138)	Data 1.42e-04 (1.05e-03)	Tok/s 96183 (101277)	Loss/tok 3.0839 (3.2425)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.106 (0.138)	Data 1.40e-04 (1.02e-03)	Tok/s 96966 (101190)	Loss/tok 3.0959 (3.2412)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.106 (0.139)	Data 1.66e-04 (9.94e-04)	Tok/s 95187 (101260)	Loss/tok 3.1668 (3.2452)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.106 (0.139)	Data 1.23e-04 (9.68e-04)	Tok/s 96012 (101273)	Loss/tok 2.9671 (3.2450)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][340/1938]	Time 0.270 (0.140)	Data 1.29e-04 (9.43e-04)	Tok/s 109289 (101379)	Loss/tok 3.6474 (3.2505)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.107 (0.139)	Data 1.50e-04 (9.21e-04)	Tok/s 95910 (101362)	Loss/tok 3.0958 (3.2486)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.107 (0.139)	Data 1.83e-04 (8.99e-04)	Tok/s 98000 (101340)	Loss/tok 3.0377 (3.2475)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.106 (0.138)	Data 1.34e-04 (8.78e-04)	Tok/s 98267 (101285)	Loss/tok 3.0705 (3.2454)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1938]	Time 0.105 (0.139)	Data 1.36e-04 (8.59e-04)	Tok/s 98884 (101305)	Loss/tok 3.1208 (3.2480)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.058 (0.138)	Data 1.20e-04 (8.41e-04)	Tok/s 92500 (101294)	Loss/tok 2.7721 (3.2479)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.211 (0.138)	Data 1.62e-04 (8.23e-04)	Tok/s 110451 (101291)	Loss/tok 3.4670 (3.2473)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.158 (0.138)	Data 1.31e-04 (8.06e-04)	Tok/s 106568 (101315)	Loss/tok 3.1893 (3.2463)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.107 (0.138)	Data 1.49e-04 (7.91e-04)	Tok/s 97755 (101263)	Loss/tok 3.0707 (3.2439)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.269 (0.138)	Data 1.51e-04 (7.76e-04)	Tok/s 110380 (101295)	Loss/tok 3.6992 (3.2448)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.106 (0.138)	Data 1.43e-04 (7.61e-04)	Tok/s 94260 (101271)	Loss/tok 2.9703 (3.2427)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.107 (0.137)	Data 1.40e-04 (7.47e-04)	Tok/s 96239 (101227)	Loss/tok 2.9826 (3.2431)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.209 (0.138)	Data 1.85e-04 (7.34e-04)	Tok/s 112947 (101262)	Loss/tok 3.3433 (3.2434)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.058 (0.138)	Data 1.41e-04 (7.22e-04)	Tok/s 90105 (101292)	Loss/tok 2.7268 (3.2446)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.158 (0.138)	Data 1.33e-04 (7.10e-04)	Tok/s 106105 (101292)	Loss/tok 3.2873 (3.2472)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.106 (0.138)	Data 1.33e-04 (6.98e-04)	Tok/s 95298 (101276)	Loss/tok 3.1206 (3.2464)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.107 (0.138)	Data 1.24e-04 (6.87e-04)	Tok/s 97602 (101321)	Loss/tok 3.1962 (3.2469)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][510/1938]	Time 0.106 (0.138)	Data 1.47e-04 (6.77e-04)	Tok/s 95832 (101324)	Loss/tok 3.0837 (3.2477)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.210 (0.138)	Data 1.57e-04 (6.66e-04)	Tok/s 110265 (101326)	Loss/tok 3.4348 (3.2494)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.159 (0.138)	Data 1.30e-04 (6.56e-04)	Tok/s 105951 (101361)	Loss/tok 3.2849 (3.2487)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.159 (0.138)	Data 1.22e-04 (6.47e-04)	Tok/s 104840 (101379)	Loss/tok 3.3545 (3.2483)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.107 (0.138)	Data 1.36e-04 (6.38e-04)	Tok/s 96141 (101398)	Loss/tok 3.0371 (3.2488)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][560/1938]	Time 0.106 (0.138)	Data 1.62e-04 (6.29e-04)	Tok/s 96624 (101423)	Loss/tok 3.1994 (3.2501)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.106 (0.138)	Data 1.09e-04 (6.20e-04)	Tok/s 96191 (101407)	Loss/tok 2.9968 (3.2504)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.107 (0.138)	Data 1.22e-04 (6.12e-04)	Tok/s 96022 (101321)	Loss/tok 2.9674 (3.2483)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.211 (0.138)	Data 1.51e-04 (6.04e-04)	Tok/s 109552 (101305)	Loss/tok 3.4507 (3.2478)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.158 (0.137)	Data 1.26e-04 (5.96e-04)	Tok/s 106162 (101270)	Loss/tok 3.1944 (3.2463)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.159 (0.137)	Data 1.40e-04 (5.89e-04)	Tok/s 105600 (101282)	Loss/tok 3.1417 (3.2456)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.210 (0.137)	Data 1.38e-04 (5.81e-04)	Tok/s 111965 (101309)	Loss/tok 3.3692 (3.2466)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.106 (0.138)	Data 1.42e-04 (5.74e-04)	Tok/s 98545 (101331)	Loss/tok 3.1827 (3.2486)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.158 (0.138)	Data 1.22e-04 (5.67e-04)	Tok/s 105863 (101365)	Loss/tok 3.3116 (3.2502)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.106 (0.138)	Data 1.56e-04 (5.61e-04)	Tok/s 97387 (101391)	Loss/tok 3.0255 (3.2511)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.106 (0.138)	Data 1.41e-04 (5.54e-04)	Tok/s 98025 (101407)	Loss/tok 3.0330 (3.2515)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.106 (0.138)	Data 1.41e-04 (5.48e-04)	Tok/s 96691 (101379)	Loss/tok 3.0021 (3.2514)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][680/1938]	Time 0.210 (0.138)	Data 1.22e-04 (5.42e-04)	Tok/s 110133 (101391)	Loss/tok 3.4264 (3.2511)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.158 (0.138)	Data 1.29e-04 (5.36e-04)	Tok/s 107504 (101422)	Loss/tok 3.1543 (3.2509)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.158 (0.139)	Data 1.33e-04 (5.31e-04)	Tok/s 107293 (101500)	Loss/tok 3.2591 (3.2533)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.107 (0.139)	Data 1.08e-04 (5.25e-04)	Tok/s 97629 (101541)	Loss/tok 3.0673 (3.2538)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.211 (0.139)	Data 1.23e-04 (5.19e-04)	Tok/s 109986 (101531)	Loss/tok 3.4903 (3.2536)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.158 (0.139)	Data 1.39e-04 (5.14e-04)	Tok/s 107106 (101513)	Loss/tok 3.3532 (3.2540)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.158 (0.139)	Data 1.51e-04 (5.09e-04)	Tok/s 105144 (101531)	Loss/tok 3.3024 (3.2558)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.059 (0.139)	Data 1.66e-04 (5.04e-04)	Tok/s 89670 (101498)	Loss/tok 2.7058 (3.2549)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.106 (0.139)	Data 1.05e-04 (4.99e-04)	Tok/s 96268 (101527)	Loss/tok 3.1383 (3.2558)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.158 (0.140)	Data 1.22e-04 (4.95e-04)	Tok/s 105464 (101561)	Loss/tok 3.2473 (3.2572)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.106 (0.140)	Data 1.26e-04 (4.90e-04)	Tok/s 98319 (101610)	Loss/tok 3.1655 (3.2591)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.270 (0.140)	Data 1.24e-04 (4.86e-04)	Tok/s 110886 (101567)	Loss/tok 3.5950 (3.2588)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.211 (0.140)	Data 1.10e-04 (4.81e-04)	Tok/s 110730 (101594)	Loss/tok 3.4484 (3.2599)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][810/1938]	Time 0.106 (0.140)	Data 1.22e-04 (4.77e-04)	Tok/s 95798 (101559)	Loss/tok 3.1250 (3.2602)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.107 (0.140)	Data 1.48e-04 (4.73e-04)	Tok/s 96746 (101560)	Loss/tok 2.9388 (3.2603)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.107 (0.140)	Data 1.31e-04 (4.69e-04)	Tok/s 94730 (101556)	Loss/tok 3.1112 (3.2603)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.107 (0.140)	Data 1.39e-04 (4.65e-04)	Tok/s 97134 (101544)	Loss/tok 3.1203 (3.2605)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.107 (0.140)	Data 1.85e-04 (4.62e-04)	Tok/s 95834 (101535)	Loss/tok 2.9655 (3.2605)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.271 (0.140)	Data 1.34e-04 (4.58e-04)	Tok/s 110793 (101556)	Loss/tok 3.5747 (3.2615)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.059 (0.140)	Data 1.27e-04 (4.54e-04)	Tok/s 90470 (101539)	Loss/tok 2.7133 (3.2616)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][880/1938]	Time 0.158 (0.140)	Data 1.33e-04 (4.50e-04)	Tok/s 106051 (101539)	Loss/tok 3.1933 (3.2618)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.273 (0.140)	Data 1.25e-04 (4.47e-04)	Tok/s 108953 (101523)	Loss/tok 3.5484 (3.2613)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.158 (0.140)	Data 1.24e-04 (4.43e-04)	Tok/s 106783 (101536)	Loss/tok 3.3417 (3.2622)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.107 (0.140)	Data 1.72e-04 (4.40e-04)	Tok/s 97586 (101503)	Loss/tok 3.0822 (3.2615)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.106 (0.140)	Data 1.22e-04 (4.37e-04)	Tok/s 98006 (101530)	Loss/tok 3.1715 (3.2629)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.059 (0.140)	Data 1.28e-04 (4.34e-04)	Tok/s 91283 (101512)	Loss/tok 2.7314 (3.2623)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.107 (0.140)	Data 1.22e-04 (4.30e-04)	Tok/s 95243 (101518)	Loss/tok 3.0229 (3.2620)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.158 (0.140)	Data 1.20e-04 (4.27e-04)	Tok/s 106169 (101523)	Loss/tok 3.1313 (3.2612)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.269 (0.140)	Data 1.42e-04 (4.24e-04)	Tok/s 110651 (101525)	Loss/tok 3.6778 (3.2614)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.210 (0.140)	Data 1.24e-04 (4.22e-04)	Tok/s 112438 (101558)	Loss/tok 3.3970 (3.2619)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.107 (0.140)	Data 1.12e-04 (4.19e-04)	Tok/s 98058 (101596)	Loss/tok 3.0303 (3.2627)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.106 (0.140)	Data 1.22e-04 (4.16e-04)	Tok/s 97720 (101588)	Loss/tok 3.2059 (3.2618)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1000/1938]	Time 0.210 (0.140)	Data 1.68e-04 (4.13e-04)	Tok/s 111563 (101568)	Loss/tok 3.5202 (3.2613)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1010/1938]	Time 0.107 (0.140)	Data 1.22e-04 (4.10e-04)	Tok/s 96591 (101568)	Loss/tok 3.0646 (3.2618)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.157 (0.140)	Data 1.21e-04 (4.08e-04)	Tok/s 108529 (101552)	Loss/tok 3.2160 (3.2607)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.158 (0.140)	Data 1.29e-04 (4.05e-04)	Tok/s 107031 (101578)	Loss/tok 3.2547 (3.2615)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.059 (0.140)	Data 1.20e-04 (4.03e-04)	Tok/s 90081 (101588)	Loss/tok 2.6033 (3.2622)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.269 (0.140)	Data 1.33e-04 (4.00e-04)	Tok/s 110014 (101587)	Loss/tok 3.5957 (3.2631)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.059 (0.140)	Data 1.48e-04 (3.98e-04)	Tok/s 89921 (101574)	Loss/tok 2.7375 (3.2622)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.157 (0.140)	Data 1.19e-04 (3.95e-04)	Tok/s 106246 (101553)	Loss/tok 3.2167 (3.2619)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.159 (0.140)	Data 1.26e-04 (3.93e-04)	Tok/s 106073 (101514)	Loss/tok 3.2991 (3.2611)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.107 (0.140)	Data 1.43e-04 (3.90e-04)	Tok/s 98255 (101499)	Loss/tok 2.9877 (3.2605)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.107 (0.140)	Data 1.49e-04 (3.88e-04)	Tok/s 98279 (101482)	Loss/tok 3.1070 (3.2597)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.211 (0.140)	Data 1.24e-04 (3.86e-04)	Tok/s 112164 (101517)	Loss/tok 3.3697 (3.2598)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.157 (0.140)	Data 1.31e-04 (3.84e-04)	Tok/s 106284 (101540)	Loss/tok 3.2043 (3.2599)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.058 (0.140)	Data 1.42e-04 (3.81e-04)	Tok/s 91508 (101531)	Loss/tok 2.6297 (3.2595)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1140/1938]	Time 0.106 (0.140)	Data 1.28e-04 (3.79e-04)	Tok/s 95965 (101561)	Loss/tok 3.0902 (3.2596)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.106 (0.140)	Data 1.20e-04 (3.77e-04)	Tok/s 96151 (101549)	Loss/tok 3.1563 (3.2592)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.158 (0.140)	Data 2.37e-04 (3.75e-04)	Tok/s 107107 (101584)	Loss/tok 3.2996 (3.2611)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.107 (0.140)	Data 1.48e-04 (3.73e-04)	Tok/s 94256 (101565)	Loss/tok 2.9964 (3.2601)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.107 (0.140)	Data 1.90e-04 (3.71e-04)	Tok/s 94683 (101540)	Loss/tok 2.9394 (3.2594)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.106 (0.140)	Data 1.34e-04 (3.69e-04)	Tok/s 97350 (101537)	Loss/tok 3.0634 (3.2595)	LR 2.000e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1200/1938]	Time 0.106 (0.140)	Data 1.20e-04 (3.67e-04)	Tok/s 96004 (101553)	Loss/tok 3.0933 (3.2606)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.106 (0.140)	Data 1.20e-04 (3.65e-04)	Tok/s 96380 (101562)	Loss/tok 3.0838 (3.2610)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.106 (0.140)	Data 1.35e-04 (3.64e-04)	Tok/s 97546 (101594)	Loss/tok 3.0782 (3.2616)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.058 (0.141)	Data 1.69e-04 (3.62e-04)	Tok/s 90023 (101612)	Loss/tok 2.6610 (3.2617)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.158 (0.140)	Data 1.96e-04 (3.60e-04)	Tok/s 106354 (101600)	Loss/tok 3.1702 (3.2616)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1250/1938]	Time 0.158 (0.141)	Data 1.05e-04 (3.58e-04)	Tok/s 105153 (101611)	Loss/tok 3.3791 (3.2636)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.57e-04)	Tok/s 98726 (101613)	Loss/tok 3.0538 (3.2633)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.105 (0.141)	Data 1.25e-04 (3.55e-04)	Tok/s 96697 (101635)	Loss/tok 3.1607 (3.2639)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.106 (0.141)	Data 1.48e-04 (3.53e-04)	Tok/s 96677 (101619)	Loss/tok 3.1324 (3.2635)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.270 (0.141)	Data 1.45e-04 (3.52e-04)	Tok/s 110221 (101616)	Loss/tok 3.6524 (3.2638)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.210 (0.141)	Data 1.55e-04 (3.50e-04)	Tok/s 110172 (101637)	Loss/tok 3.3737 (3.2641)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.106 (0.141)	Data 1.37e-04 (3.48e-04)	Tok/s 96304 (101635)	Loss/tok 3.0528 (3.2639)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.158 (0.141)	Data 1.19e-04 (3.47e-04)	Tok/s 104741 (101640)	Loss/tok 3.2752 (3.2635)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.158 (0.141)	Data 1.27e-04 (3.45e-04)	Tok/s 106506 (101617)	Loss/tok 3.2520 (3.2624)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.059 (0.140)	Data 1.23e-04 (3.43e-04)	Tok/s 90044 (101597)	Loss/tok 2.6781 (3.2617)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.106 (0.141)	Data 1.76e-04 (3.42e-04)	Tok/s 98087 (101606)	Loss/tok 3.0451 (3.2615)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.269 (0.141)	Data 1.25e-04 (3.40e-04)	Tok/s 110714 (101614)	Loss/tok 3.6347 (3.2621)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1370/1938]	Time 0.107 (0.140)	Data 1.57e-04 (3.39e-04)	Tok/s 98034 (101574)	Loss/tok 2.9827 (3.2609)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.107 (0.140)	Data 1.97e-04 (3.38e-04)	Tok/s 97007 (101593)	Loss/tok 3.0323 (3.2616)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.210 (0.140)	Data 1.53e-04 (3.36e-04)	Tok/s 110069 (101604)	Loss/tok 3.4939 (3.2618)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.058 (0.141)	Data 1.42e-04 (3.35e-04)	Tok/s 90504 (101625)	Loss/tok 2.7001 (3.2630)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.106 (0.141)	Data 1.28e-04 (3.33e-04)	Tok/s 96528 (101620)	Loss/tok 2.9214 (3.2623)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.158 (0.141)	Data 1.38e-04 (3.32e-04)	Tok/s 105684 (101639)	Loss/tok 3.2943 (3.2621)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.158 (0.141)	Data 1.23e-04 (3.31e-04)	Tok/s 106091 (101659)	Loss/tok 3.3331 (3.2625)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.159 (0.141)	Data 1.40e-04 (3.29e-04)	Tok/s 106492 (101668)	Loss/tok 3.2043 (3.2623)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.159 (0.141)	Data 1.52e-04 (3.28e-04)	Tok/s 105912 (101681)	Loss/tok 3.2330 (3.2620)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.106 (0.141)	Data 1.22e-04 (3.27e-04)	Tok/s 98144 (101681)	Loss/tok 2.9959 (3.2620)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.158 (0.141)	Data 1.26e-04 (3.26e-04)	Tok/s 106301 (101680)	Loss/tok 3.2958 (3.2620)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.158 (0.141)	Data 1.21e-04 (3.24e-04)	Tok/s 105759 (101672)	Loss/tok 3.2454 (3.2618)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.058 (0.141)	Data 1.30e-04 (3.23e-04)	Tok/s 91003 (101671)	Loss/tok 2.5631 (3.2619)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1500/1938]	Time 0.210 (0.141)	Data 1.29e-04 (3.22e-04)	Tok/s 110472 (101677)	Loss/tok 3.3408 (3.2614)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.210 (0.141)	Data 1.79e-04 (3.21e-04)	Tok/s 111385 (101675)	Loss/tok 3.4719 (3.2611)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.106 (0.141)	Data 1.95e-04 (3.20e-04)	Tok/s 96341 (101672)	Loss/tok 3.0022 (3.2610)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.107 (0.141)	Data 1.26e-04 (3.18e-04)	Tok/s 96424 (101667)	Loss/tok 3.0457 (3.2609)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1540/1938]	Time 0.211 (0.141)	Data 1.24e-04 (3.17e-04)	Tok/s 112108 (101660)	Loss/tok 3.3509 (3.2610)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.211 (0.141)	Data 1.22e-04 (3.16e-04)	Tok/s 110368 (101686)	Loss/tok 3.4356 (3.2618)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.106 (0.141)	Data 1.48e-04 (3.15e-04)	Tok/s 97893 (101677)	Loss/tok 3.0884 (3.2615)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.269 (0.141)	Data 1.22e-04 (3.14e-04)	Tok/s 108801 (101670)	Loss/tok 3.6567 (3.2618)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.159 (0.141)	Data 1.48e-04 (3.12e-04)	Tok/s 105794 (101660)	Loss/tok 3.2921 (3.2613)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.158 (0.141)	Data 1.65e-04 (3.11e-04)	Tok/s 107059 (101643)	Loss/tok 3.3086 (3.2605)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.269 (0.141)	Data 1.24e-04 (3.11e-04)	Tok/s 110109 (101648)	Loss/tok 3.6585 (3.2607)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.158 (0.141)	Data 1.28e-04 (3.10e-04)	Tok/s 105731 (101651)	Loss/tok 3.1962 (3.2611)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.106 (0.141)	Data 1.38e-04 (3.09e-04)	Tok/s 97459 (101651)	Loss/tok 3.0724 (3.2608)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.109 (0.141)	Data 1.17e-04 (3.07e-04)	Tok/s 96280 (101651)	Loss/tok 3.0225 (3.2602)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.210 (0.141)	Data 1.22e-04 (3.06e-04)	Tok/s 110740 (101650)	Loss/tok 3.5421 (3.2603)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.158 (0.141)	Data 1.25e-04 (3.05e-04)	Tok/s 107434 (101674)	Loss/tok 3.3056 (3.2606)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.210 (0.141)	Data 1.21e-04 (3.04e-04)	Tok/s 111161 (101696)	Loss/tok 3.3093 (3.2608)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1670/1938]	Time 0.106 (0.141)	Data 1.72e-04 (3.04e-04)	Tok/s 97975 (101703)	Loss/tok 3.0991 (3.2605)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.106 (0.141)	Data 1.24e-04 (3.03e-04)	Tok/s 96761 (101709)	Loss/tok 3.0158 (3.2605)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.106 (0.141)	Data 1.21e-04 (3.02e-04)	Tok/s 97288 (101706)	Loss/tok 2.9962 (3.2603)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.158 (0.141)	Data 1.41e-04 (3.01e-04)	Tok/s 104780 (101718)	Loss/tok 3.2806 (3.2607)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.106 (0.141)	Data 1.28e-04 (3.00e-04)	Tok/s 96195 (101700)	Loss/tok 2.9571 (3.2600)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.106 (0.141)	Data 1.36e-04 (2.99e-04)	Tok/s 96583 (101695)	Loss/tok 2.9953 (3.2598)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.059 (0.141)	Data 1.22e-04 (2.98e-04)	Tok/s 89705 (101684)	Loss/tok 2.5245 (3.2598)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.059 (0.141)	Data 1.20e-04 (2.97e-04)	Tok/s 89418 (101690)	Loss/tok 2.6136 (3.2598)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.272 (0.141)	Data 2.07e-04 (2.96e-04)	Tok/s 108941 (101703)	Loss/tok 3.6550 (3.2600)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.058 (0.141)	Data 1.31e-04 (2.95e-04)	Tok/s 89969 (101690)	Loss/tok 2.6456 (3.2594)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.107 (0.141)	Data 1.38e-04 (2.95e-04)	Tok/s 97688 (101681)	Loss/tok 2.9950 (3.2590)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.158 (0.141)	Data 1.10e-04 (2.94e-04)	Tok/s 108098 (101684)	Loss/tok 3.2225 (3.2586)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.159 (0.140)	Data 1.42e-04 (2.93e-04)	Tok/s 105918 (101670)	Loss/tok 3.1767 (3.2578)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1800/1938]	Time 0.106 (0.140)	Data 1.24e-04 (2.92e-04)	Tok/s 97162 (101656)	Loss/tok 3.0028 (3.2572)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1810/1938]	Time 0.159 (0.140)	Data 1.90e-04 (2.91e-04)	Tok/s 107196 (101659)	Loss/tok 3.2082 (3.2571)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.210 (0.141)	Data 1.21e-04 (2.91e-04)	Tok/s 111197 (101683)	Loss/tok 3.2595 (3.2576)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.158 (0.141)	Data 1.28e-04 (2.90e-04)	Tok/s 107591 (101708)	Loss/tok 3.1933 (3.2579)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.106 (0.141)	Data 1.20e-04 (2.89e-04)	Tok/s 96749 (101706)	Loss/tok 3.0200 (3.2575)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.159 (0.141)	Data 1.41e-04 (2.88e-04)	Tok/s 104683 (101715)	Loss/tok 3.2276 (3.2577)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.158 (0.141)	Data 1.30e-04 (2.87e-04)	Tok/s 105887 (101719)	Loss/tok 3.1903 (3.2579)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.107 (0.141)	Data 1.48e-04 (2.87e-04)	Tok/s 96497 (101714)	Loss/tok 3.0778 (3.2577)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.106 (0.141)	Data 1.26e-04 (2.86e-04)	Tok/s 98491 (101704)	Loss/tok 3.0848 (3.2571)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.106 (0.141)	Data 1.37e-04 (2.85e-04)	Tok/s 97253 (101699)	Loss/tok 3.1040 (3.2565)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.210 (0.141)	Data 1.46e-04 (2.84e-04)	Tok/s 111321 (101699)	Loss/tok 3.3484 (3.2563)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.106 (0.140)	Data 1.29e-04 (2.84e-04)	Tok/s 97256 (101699)	Loss/tok 3.0663 (3.2560)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.158 (0.141)	Data 1.22e-04 (2.83e-04)	Tok/s 106336 (101714)	Loss/tok 3.3385 (3.2567)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.107 (0.141)	Data 1.30e-04 (2.82e-04)	Tok/s 98675 (101704)	Loss/tok 3.0346 (3.2562)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
:::MLLOG {"namespace": "", "time_ms": 1593020008463, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020008463, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.734 (0.734)	Decoder iters 141.0 (141.0)	Tok/s 22108 (22108)
0: Running moses detokenizer
0: BLEU(score=23.358633341123237, counts=[36249, 17934, 10089, 5936], totals=[64733, 61730, 58727, 55727], precisions=[55.99771368544637, 29.052324639559373, 17.179491545626373, 10.651928149729933], bp=1.0, sys_len=64733, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020010385, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2336, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020010386, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2561	Test BLEU: 23.36
0: Performance: Epoch: 2	Training: 813598 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593020010386, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020010386, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593020010386, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3304058937
0: TRAIN [3][0/1938]	Time 0.470 (0.470)	Data 2.62e-01 (2.62e-01)	Tok/s 36072 (36072)	Loss/tok 3.1742 (3.1742)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.158 (0.164)	Data 1.64e-04 (2.40e-02)	Tok/s 105345 (95207)	Loss/tok 3.1084 (3.1329)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.271 (0.164)	Data 1.84e-04 (1.27e-02)	Tok/s 109400 (99879)	Loss/tok 3.5267 (3.1763)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.210 (0.158)	Data 1.83e-04 (8.63e-03)	Tok/s 110910 (100594)	Loss/tok 3.3610 (3.1774)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.108 (0.154)	Data 1.93e-04 (6.57e-03)	Tok/s 95609 (101099)	Loss/tok 3.0082 (3.1641)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.212 (0.152)	Data 1.39e-04 (5.32e-03)	Tok/s 109252 (101398)	Loss/tok 3.3682 (3.1615)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.162 (0.149)	Data 1.70e-04 (4.47e-03)	Tok/s 103296 (101314)	Loss/tok 3.0222 (3.1471)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.107 (0.145)	Data 1.62e-04 (3.87e-03)	Tok/s 96445 (101009)	Loss/tok 3.0382 (3.1392)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][80/1938]	Time 0.158 (0.146)	Data 1.44e-04 (3.41e-03)	Tok/s 105519 (101079)	Loss/tok 3.1677 (3.1600)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.158 (0.147)	Data 1.57e-04 (3.06e-03)	Tok/s 105992 (101407)	Loss/tok 3.3157 (3.1668)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.106 (0.144)	Data 1.82e-04 (2.77e-03)	Tok/s 95389 (101115)	Loss/tok 2.9636 (3.1564)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.211 (0.145)	Data 1.65e-04 (2.53e-03)	Tok/s 111899 (101496)	Loss/tok 3.1743 (3.1657)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.160 (0.144)	Data 2.00e-04 (2.34e-03)	Tok/s 104956 (101250)	Loss/tok 3.2017 (3.1611)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.157 (0.145)	Data 1.61e-04 (2.18e-03)	Tok/s 107613 (101613)	Loss/tok 3.1314 (3.1636)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.158 (0.144)	Data 1.70e-04 (2.03e-03)	Tok/s 106060 (101621)	Loss/tok 3.2385 (3.1597)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.106 (0.144)	Data 2.69e-04 (1.91e-03)	Tok/s 98682 (101724)	Loss/tok 2.9401 (3.1569)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.158 (0.145)	Data 2.11e-04 (1.80e-03)	Tok/s 108516 (101918)	Loss/tok 3.1936 (3.1652)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.059 (0.143)	Data 2.07e-04 (1.71e-03)	Tok/s 90367 (101711)	Loss/tok 2.6629 (3.1608)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.209 (0.143)	Data 1.65e-04 (1.62e-03)	Tok/s 111496 (101731)	Loss/tok 3.2334 (3.1626)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.106 (0.145)	Data 1.40e-04 (1.55e-03)	Tok/s 97248 (101826)	Loss/tok 3.0027 (3.1773)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.159 (0.145)	Data 1.59e-04 (1.48e-03)	Tok/s 104984 (101987)	Loss/tok 3.2539 (3.1790)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][210/1938]	Time 0.106 (0.145)	Data 1.94e-04 (1.42e-03)	Tok/s 97625 (102038)	Loss/tok 2.8192 (3.1769)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.107 (0.146)	Data 1.69e-04 (1.36e-03)	Tok/s 99049 (102122)	Loss/tok 3.1151 (3.1828)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.157 (0.146)	Data 1.59e-04 (1.31e-03)	Tok/s 106042 (102162)	Loss/tok 3.1180 (3.1835)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.158 (0.146)	Data 1.39e-04 (1.26e-03)	Tok/s 106550 (102177)	Loss/tok 3.0568 (3.1826)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.106 (0.147)	Data 1.44e-04 (1.22e-03)	Tok/s 97886 (102207)	Loss/tok 2.9904 (3.1868)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.158 (0.146)	Data 1.62e-04 (1.18e-03)	Tok/s 104594 (102175)	Loss/tok 3.2521 (3.1848)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.106 (0.145)	Data 1.92e-04 (1.14e-03)	Tok/s 97355 (102029)	Loss/tok 3.0558 (3.1834)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.106 (0.145)	Data 1.59e-04 (1.11e-03)	Tok/s 98830 (101953)	Loss/tok 2.9550 (3.1801)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.211 (0.144)	Data 1.69e-04 (1.07e-03)	Tok/s 110826 (101921)	Loss/tok 3.4207 (3.1798)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.211 (0.144)	Data 1.41e-04 (1.04e-03)	Tok/s 111387 (101966)	Loss/tok 3.4003 (3.1800)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.106 (0.144)	Data 2.54e-04 (1.02e-03)	Tok/s 95273 (101899)	Loss/tok 3.1423 (3.1782)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.270 (0.144)	Data 1.81e-04 (9.90e-04)	Tok/s 110571 (101887)	Loss/tok 3.5815 (3.1810)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][330/1938]	Time 0.210 (0.144)	Data 1.46e-04 (9.65e-04)	Tok/s 111275 (101899)	Loss/tok 3.3408 (3.1823)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.108 (0.144)	Data 1.90e-04 (9.42e-04)	Tok/s 94777 (101916)	Loss/tok 2.9786 (3.1803)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.058 (0.144)	Data 2.13e-04 (9.20e-04)	Tok/s 92499 (101906)	Loss/tok 2.5886 (3.1818)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.213 (0.146)	Data 1.67e-04 (8.99e-04)	Tok/s 111128 (102040)	Loss/tok 3.1479 (3.1865)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.158 (0.146)	Data 2.01e-04 (8.79e-04)	Tok/s 105954 (102068)	Loss/tok 3.2568 (3.1874)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.159 (0.146)	Data 1.67e-04 (8.61e-04)	Tok/s 105447 (102031)	Loss/tok 3.1044 (3.1886)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.211 (0.145)	Data 1.62e-04 (8.44e-04)	Tok/s 109973 (101984)	Loss/tok 3.3658 (3.1872)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.107 (0.145)	Data 1.65e-04 (8.27e-04)	Tok/s 96772 (101851)	Loss/tok 2.8872 (3.1845)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.211 (0.145)	Data 1.79e-04 (8.11e-04)	Tok/s 111697 (101835)	Loss/tok 3.3455 (3.1838)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.210 (0.145)	Data 1.81e-04 (7.96e-04)	Tok/s 110765 (101837)	Loss/tok 3.2787 (3.1849)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.108 (0.145)	Data 1.46e-04 (7.82e-04)	Tok/s 97193 (101811)	Loss/tok 3.0790 (3.1876)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.059 (0.144)	Data 1.30e-04 (7.68e-04)	Tok/s 90609 (101760)	Loss/tok 2.6173 (3.1868)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.108 (0.144)	Data 1.79e-04 (7.55e-04)	Tok/s 95518 (101742)	Loss/tok 3.0437 (3.1858)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][460/1938]	Time 0.270 (0.144)	Data 1.44e-04 (7.42e-04)	Tok/s 111158 (101740)	Loss/tok 3.4647 (3.1867)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.159 (0.144)	Data 1.65e-04 (7.30e-04)	Tok/s 107651 (101674)	Loss/tok 3.1367 (3.1851)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.158 (0.144)	Data 2.52e-04 (7.19e-04)	Tok/s 106979 (101706)	Loss/tok 3.0997 (3.1842)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.158 (0.144)	Data 1.72e-04 (7.08e-04)	Tok/s 105463 (101696)	Loss/tok 3.1223 (3.1846)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.157 (0.144)	Data 1.29e-04 (6.97e-04)	Tok/s 105460 (101653)	Loss/tok 3.2091 (3.1846)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.107 (0.144)	Data 1.77e-04 (6.87e-04)	Tok/s 97495 (101636)	Loss/tok 3.0344 (3.1859)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.106 (0.144)	Data 1.61e-04 (6.77e-04)	Tok/s 95741 (101629)	Loss/tok 3.0747 (3.1850)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.270 (0.143)	Data 1.70e-04 (6.67e-04)	Tok/s 109438 (101585)	Loss/tok 3.5713 (3.1845)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.159 (0.143)	Data 1.99e-04 (6.58e-04)	Tok/s 105870 (101583)	Loss/tok 3.1612 (3.1831)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.157 (0.143)	Data 1.80e-04 (6.50e-04)	Tok/s 106802 (101535)	Loss/tok 3.2496 (3.1831)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.105 (0.142)	Data 1.27e-04 (6.41e-04)	Tok/s 97790 (101471)	Loss/tok 3.1164 (3.1817)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][570/1938]	Time 0.158 (0.143)	Data 1.59e-04 (6.33e-04)	Tok/s 106565 (101529)	Loss/tok 3.1397 (3.1845)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.158 (0.143)	Data 1.79e-04 (6.25e-04)	Tok/s 106150 (101533)	Loss/tok 3.2203 (3.1849)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.105 (0.143)	Data 1.62e-04 (6.18e-04)	Tok/s 98457 (101534)	Loss/tok 2.9441 (3.1852)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.107 (0.142)	Data 1.59e-04 (6.10e-04)	Tok/s 95194 (101468)	Loss/tok 2.9301 (3.1831)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.269 (0.143)	Data 2.81e-04 (6.03e-04)	Tok/s 110669 (101543)	Loss/tok 3.4461 (3.1847)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.158 (0.143)	Data 2.17e-04 (5.97e-04)	Tok/s 105577 (101558)	Loss/tok 3.1093 (3.1845)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.107 (0.142)	Data 1.80e-04 (5.90e-04)	Tok/s 96940 (101535)	Loss/tok 3.0103 (3.1829)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.159 (0.142)	Data 2.11e-04 (5.84e-04)	Tok/s 106799 (101520)	Loss/tok 3.1795 (3.1826)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.213 (0.142)	Data 1.42e-04 (5.77e-04)	Tok/s 108604 (101514)	Loss/tok 3.4050 (3.1819)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.107 (0.142)	Data 1.75e-04 (5.71e-04)	Tok/s 95057 (101538)	Loss/tok 3.0065 (3.1826)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.158 (0.142)	Data 1.78e-04 (5.65e-04)	Tok/s 106219 (101561)	Loss/tok 3.1775 (3.1830)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.269 (0.143)	Data 1.64e-04 (5.60e-04)	Tok/s 110784 (101628)	Loss/tok 3.5390 (3.1850)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.157 (0.143)	Data 1.41e-04 (5.54e-04)	Tok/s 106840 (101608)	Loss/tok 3.0653 (3.1838)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][700/1938]	Time 0.158 (0.143)	Data 1.59e-04 (5.49e-04)	Tok/s 106150 (101597)	Loss/tok 3.2379 (3.1837)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.107 (0.143)	Data 1.75e-04 (5.44e-04)	Tok/s 95796 (101596)	Loss/tok 2.9733 (3.1827)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.210 (0.142)	Data 1.70e-04 (5.39e-04)	Tok/s 110532 (101562)	Loss/tok 3.3087 (3.1824)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.159 (0.142)	Data 1.78e-04 (5.33e-04)	Tok/s 104954 (101492)	Loss/tok 3.1316 (3.1806)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.157 (0.142)	Data 2.04e-04 (5.29e-04)	Tok/s 105994 (101568)	Loss/tok 3.1423 (3.1817)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.210 (0.142)	Data 1.25e-04 (5.24e-04)	Tok/s 111882 (101579)	Loss/tok 3.3926 (3.1810)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.158 (0.142)	Data 2.50e-04 (5.19e-04)	Tok/s 107197 (101599)	Loss/tok 3.0654 (3.1806)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.106 (0.142)	Data 1.54e-04 (5.15e-04)	Tok/s 94341 (101590)	Loss/tok 3.0008 (3.1803)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.158 (0.142)	Data 1.70e-04 (5.11e-04)	Tok/s 107940 (101563)	Loss/tok 3.1113 (3.1793)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.158 (0.142)	Data 1.59e-04 (5.06e-04)	Tok/s 107238 (101528)	Loss/tok 3.1738 (3.1779)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.058 (0.142)	Data 1.69e-04 (5.02e-04)	Tok/s 88732 (101512)	Loss/tok 2.6183 (3.1772)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.106 (0.142)	Data 1.74e-04 (4.98e-04)	Tok/s 97309 (101510)	Loss/tok 2.9546 (3.1770)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][820/1938]	Time 0.106 (0.141)	Data 1.91e-04 (4.95e-04)	Tok/s 98492 (101460)	Loss/tok 3.0158 (3.1756)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.158 (0.142)	Data 1.72e-04 (4.91e-04)	Tok/s 106219 (101473)	Loss/tok 3.2055 (3.1765)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.106 (0.141)	Data 1.66e-04 (4.87e-04)	Tok/s 96850 (101445)	Loss/tok 2.9010 (3.1746)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.157 (0.141)	Data 1.80e-04 (4.84e-04)	Tok/s 107423 (101420)	Loss/tok 3.1516 (3.1737)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.158 (0.141)	Data 1.96e-04 (4.80e-04)	Tok/s 107323 (101415)	Loss/tok 3.1228 (3.1726)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.210 (0.141)	Data 1.58e-04 (4.77e-04)	Tok/s 110911 (101433)	Loss/tok 3.2612 (3.1722)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.270 (0.141)	Data 1.43e-04 (4.73e-04)	Tok/s 110181 (101454)	Loss/tok 3.4540 (3.1723)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.159 (0.141)	Data 1.54e-04 (4.70e-04)	Tok/s 105745 (101448)	Loss/tok 3.1464 (3.1719)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.106 (0.141)	Data 2.52e-04 (4.67e-04)	Tok/s 95640 (101436)	Loss/tok 2.9443 (3.1715)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.107 (0.141)	Data 1.77e-04 (4.64e-04)	Tok/s 97492 (101452)	Loss/tok 2.9103 (3.1715)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.107 (0.141)	Data 1.90e-04 (4.61e-04)	Tok/s 96806 (101438)	Loss/tok 2.9148 (3.1705)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.107 (0.141)	Data 1.78e-04 (4.58e-04)	Tok/s 95887 (101401)	Loss/tok 3.0257 (3.1687)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.159 (0.141)	Data 1.83e-04 (4.55e-04)	Tok/s 106135 (101424)	Loss/tok 3.0683 (3.1689)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][950/1938]	Time 0.159 (0.141)	Data 1.86e-04 (4.52e-04)	Tok/s 104538 (101426)	Loss/tok 3.1923 (3.1692)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.108 (0.141)	Data 1.54e-04 (4.49e-04)	Tok/s 94670 (101422)	Loss/tok 2.8650 (3.1688)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.271 (0.141)	Data 2.32e-04 (4.46e-04)	Tok/s 109946 (101456)	Loss/tok 3.3996 (3.1694)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.158 (0.141)	Data 1.69e-04 (4.44e-04)	Tok/s 104791 (101445)	Loss/tok 3.0452 (3.1687)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.210 (0.141)	Data 1.26e-04 (4.41e-04)	Tok/s 111298 (101447)	Loss/tok 3.2894 (3.1695)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.107 (0.141)	Data 2.14e-04 (4.38e-04)	Tok/s 96062 (101417)	Loss/tok 2.9157 (3.1687)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.212 (0.141)	Data 1.51e-04 (4.36e-04)	Tok/s 108676 (101420)	Loss/tok 3.4055 (3.1691)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.158 (0.141)	Data 1.80e-04 (4.34e-04)	Tok/s 103728 (101428)	Loss/tok 3.1628 (3.1686)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.211 (0.141)	Data 1.61e-04 (4.31e-04)	Tok/s 110570 (101443)	Loss/tok 3.3711 (3.1683)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.210 (0.141)	Data 1.75e-04 (4.29e-04)	Tok/s 112129 (101466)	Loss/tok 3.3533 (3.1684)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.159 (0.141)	Data 2.24e-04 (4.27e-04)	Tok/s 104494 (101479)	Loss/tok 3.0003 (3.1684)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.158 (0.141)	Data 1.47e-04 (4.24e-04)	Tok/s 108668 (101481)	Loss/tok 3.0971 (3.1682)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.159 (0.141)	Data 1.76e-04 (4.22e-04)	Tok/s 105670 (101480)	Loss/tok 3.1803 (3.1679)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1080/1938]	Time 0.107 (0.141)	Data 1.65e-04 (4.20e-04)	Tok/s 95187 (101472)	Loss/tok 2.8451 (3.1677)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.059 (0.141)	Data 2.14e-04 (4.17e-04)	Tok/s 89760 (101466)	Loss/tok 2.5499 (3.1668)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.159 (0.141)	Data 1.63e-04 (4.15e-04)	Tok/s 105376 (101468)	Loss/tok 3.1399 (3.1664)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.108 (0.141)	Data 1.46e-04 (4.13e-04)	Tok/s 98052 (101416)	Loss/tok 2.9050 (3.1648)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.212 (0.141)	Data 1.22e-04 (4.11e-04)	Tok/s 109704 (101405)	Loss/tok 3.3613 (3.1644)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.212 (0.141)	Data 2.06e-04 (4.09e-04)	Tok/s 109112 (101384)	Loss/tok 3.3634 (3.1644)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.212 (0.141)	Data 1.78e-04 (4.07e-04)	Tok/s 110382 (101425)	Loss/tok 3.2251 (3.1650)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.108 (0.141)	Data 1.52e-04 (4.05e-04)	Tok/s 94191 (101391)	Loss/tok 2.9750 (3.1640)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.212 (0.141)	Data 1.78e-04 (4.03e-04)	Tok/s 110052 (101394)	Loss/tok 3.2628 (3.1639)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.107 (0.141)	Data 2.09e-04 (4.01e-04)	Tok/s 97674 (101365)	Loss/tok 2.9573 (3.1631)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.158 (0.141)	Data 1.96e-04 (3.99e-04)	Tok/s 106802 (101391)	Loss/tok 3.1840 (3.1639)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.107 (0.141)	Data 1.51e-04 (3.97e-04)	Tok/s 97527 (101408)	Loss/tok 3.0658 (3.1642)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.109 (0.141)	Data 1.73e-04 (3.96e-04)	Tok/s 92938 (101404)	Loss/tok 2.8622 (3.1640)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1210/1938]	Time 0.159 (0.141)	Data 1.98e-04 (3.94e-04)	Tok/s 106362 (101421)	Loss/tok 3.2218 (3.1644)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.212 (0.141)	Data 2.06e-04 (3.92e-04)	Tok/s 109159 (101410)	Loss/tok 3.2920 (3.1637)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.159 (0.141)	Data 1.70e-04 (3.90e-04)	Tok/s 104625 (101414)	Loss/tok 3.1836 (3.1633)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.211 (0.141)	Data 1.95e-04 (3.89e-04)	Tok/s 111297 (101417)	Loss/tok 3.2402 (3.1630)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.107 (0.141)	Data 1.86e-04 (3.87e-04)	Tok/s 95514 (101394)	Loss/tok 2.9311 (3.1621)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.108 (0.141)	Data 1.87e-04 (3.86e-04)	Tok/s 95516 (101385)	Loss/tok 3.0251 (3.1616)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.159 (0.141)	Data 1.58e-04 (3.84e-04)	Tok/s 107147 (101417)	Loss/tok 3.1579 (3.1622)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.107 (0.141)	Data 1.81e-04 (3.83e-04)	Tok/s 96222 (101379)	Loss/tok 2.9792 (3.1613)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.211 (0.141)	Data 1.97e-04 (3.81e-04)	Tok/s 111135 (101374)	Loss/tok 3.4146 (3.1609)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.107 (0.141)	Data 2.23e-04 (3.80e-04)	Tok/s 98327 (101368)	Loss/tok 2.9020 (3.1604)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.108 (0.141)	Data 1.61e-04 (3.78e-04)	Tok/s 95569 (101372)	Loss/tok 2.9745 (3.1601)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.158 (0.141)	Data 2.15e-04 (3.76e-04)	Tok/s 108018 (101361)	Loss/tok 3.0021 (3.1591)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.107 (0.141)	Data 1.43e-04 (3.75e-04)	Tok/s 97832 (101366)	Loss/tok 2.9525 (3.1586)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1340/1938]	Time 0.107 (0.141)	Data 1.70e-04 (3.73e-04)	Tok/s 99029 (101352)	Loss/tok 2.9199 (3.1579)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.164 (0.141)	Data 2.09e-04 (3.72e-04)	Tok/s 102612 (101341)	Loss/tok 3.0769 (3.1576)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.159 (0.141)	Data 1.49e-04 (3.71e-04)	Tok/s 107117 (101330)	Loss/tok 3.2245 (3.1572)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.69e-04)	Tok/s 96186 (101334)	Loss/tok 2.9542 (3.1568)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.107 (0.141)	Data 1.72e-04 (3.68e-04)	Tok/s 97049 (101361)	Loss/tok 2.9240 (3.1574)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.211 (0.141)	Data 1.66e-04 (3.66e-04)	Tok/s 110845 (101376)	Loss/tok 3.3998 (3.1576)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.212 (0.141)	Data 1.67e-04 (3.65e-04)	Tok/s 109878 (101371)	Loss/tok 3.2292 (3.1577)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.107 (0.141)	Data 2.09e-04 (3.64e-04)	Tok/s 95777 (101378)	Loss/tok 2.8737 (3.1577)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.108 (0.141)	Data 1.54e-04 (3.62e-04)	Tok/s 96167 (101354)	Loss/tok 2.9208 (3.1568)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.160 (0.141)	Data 1.63e-04 (3.61e-04)	Tok/s 105558 (101344)	Loss/tok 3.1487 (3.1562)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.107 (0.141)	Data 2.48e-04 (3.60e-04)	Tok/s 97466 (101333)	Loss/tok 2.9859 (3.1560)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.059 (0.141)	Data 1.96e-04 (3.59e-04)	Tok/s 90235 (101320)	Loss/tok 2.6229 (3.1557)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1460/1938]	Time 0.211 (0.141)	Data 1.60e-04 (3.57e-04)	Tok/s 111144 (101328)	Loss/tok 3.1980 (3.1556)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.107 (0.141)	Data 1.99e-04 (3.56e-04)	Tok/s 95858 (101319)	Loss/tok 2.9251 (3.1549)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.159 (0.141)	Data 1.94e-04 (3.55e-04)	Tok/s 106192 (101310)	Loss/tok 3.0390 (3.1543)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.059 (0.141)	Data 2.41e-04 (3.54e-04)	Tok/s 88904 (101303)	Loss/tok 2.5841 (3.1537)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.158 (0.141)	Data 2.01e-04 (3.53e-04)	Tok/s 106695 (101318)	Loss/tok 3.1347 (3.1532)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.159 (0.141)	Data 1.73e-04 (3.52e-04)	Tok/s 105380 (101331)	Loss/tok 3.1502 (3.1534)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.159 (0.141)	Data 2.03e-04 (3.50e-04)	Tok/s 105217 (101344)	Loss/tok 3.1213 (3.1533)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.272 (0.141)	Data 1.83e-04 (3.49e-04)	Tok/s 109480 (101349)	Loss/tok 3.3431 (3.1534)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.211 (0.141)	Data 1.90e-04 (3.48e-04)	Tok/s 110422 (101365)	Loss/tok 3.2807 (3.1532)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.159 (0.141)	Data 1.54e-04 (3.47e-04)	Tok/s 106002 (101364)	Loss/tok 3.1439 (3.1531)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.272 (0.141)	Data 1.73e-04 (3.46e-04)	Tok/s 109203 (101367)	Loss/tok 3.4249 (3.1532)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.107 (0.141)	Data 1.94e-04 (3.45e-04)	Tok/s 97185 (101351)	Loss/tok 2.9700 (3.1524)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.107 (0.141)	Data 1.89e-04 (3.44e-04)	Tok/s 96483 (101353)	Loss/tok 3.0036 (3.1523)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1590/1938]	Time 0.107 (0.141)	Data 1.42e-04 (3.43e-04)	Tok/s 96101 (101371)	Loss/tok 2.8592 (3.1529)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.110 (0.141)	Data 1.21e-04 (3.41e-04)	Tok/s 93834 (101383)	Loss/tok 2.9923 (3.1531)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.109 (0.141)	Data 1.27e-04 (3.40e-04)	Tok/s 95304 (101379)	Loss/tok 2.8782 (3.1530)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.271 (0.141)	Data 1.66e-04 (3.39e-04)	Tok/s 109332 (101386)	Loss/tok 3.4392 (3.1531)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.211 (0.141)	Data 1.49e-04 (3.38e-04)	Tok/s 109422 (101380)	Loss/tok 3.2111 (3.1529)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.106 (0.141)	Data 1.42e-04 (3.37e-04)	Tok/s 96385 (101386)	Loss/tok 2.8735 (3.1525)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.108 (0.141)	Data 1.30e-04 (3.36e-04)	Tok/s 94757 (101389)	Loss/tok 2.8772 (3.1526)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.158 (0.141)	Data 1.66e-04 (3.35e-04)	Tok/s 106743 (101387)	Loss/tok 3.1374 (3.1523)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.210 (0.141)	Data 1.48e-04 (3.34e-04)	Tok/s 111035 (101395)	Loss/tok 3.2124 (3.1520)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.158 (0.141)	Data 1.84e-04 (3.33e-04)	Tok/s 106760 (101400)	Loss/tok 3.0938 (3.1523)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.106 (0.141)	Data 1.24e-04 (3.32e-04)	Tok/s 98992 (101400)	Loss/tok 2.9740 (3.1518)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.106 (0.141)	Data 1.25e-04 (3.31e-04)	Tok/s 96418 (101384)	Loss/tok 2.8289 (3.1514)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.107 (0.141)	Data 1.90e-04 (3.30e-04)	Tok/s 98777 (101384)	Loss/tok 2.9615 (3.1513)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1720/1938]	Time 0.271 (0.141)	Data 1.19e-04 (3.28e-04)	Tok/s 110512 (101413)	Loss/tok 3.3801 (3.1513)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.059 (0.141)	Data 1.06e-04 (3.27e-04)	Tok/s 87895 (101412)	Loss/tok 2.6177 (3.1516)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.159 (0.141)	Data 1.24e-04 (3.26e-04)	Tok/s 105961 (101421)	Loss/tok 3.2355 (3.1514)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.158 (0.141)	Data 1.21e-04 (3.25e-04)	Tok/s 105621 (101433)	Loss/tok 3.1540 (3.1513)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.158 (0.141)	Data 1.23e-04 (3.24e-04)	Tok/s 107730 (101430)	Loss/tok 3.0706 (3.1506)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.158 (0.141)	Data 1.36e-04 (3.23e-04)	Tok/s 105537 (101426)	Loss/tok 3.0274 (3.1504)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.106 (0.141)	Data 1.47e-04 (3.22e-04)	Tok/s 98770 (101438)	Loss/tok 2.9514 (3.1501)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.271 (0.141)	Data 1.21e-04 (3.21e-04)	Tok/s 110766 (101425)	Loss/tok 3.3843 (3.1496)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.210 (0.141)	Data 1.25e-04 (3.20e-04)	Tok/s 110640 (101428)	Loss/tok 3.3561 (3.1495)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.106 (0.141)	Data 1.34e-04 (3.20e-04)	Tok/s 94859 (101422)	Loss/tok 2.8167 (3.1493)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.157 (0.141)	Data 1.45e-04 (3.19e-04)	Tok/s 106967 (101438)	Loss/tok 3.0379 (3.1497)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.106 (0.141)	Data 1.87e-04 (3.18e-04)	Tok/s 96120 (101442)	Loss/tok 2.9013 (3.1494)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.108 (0.141)	Data 1.98e-04 (3.17e-04)	Tok/s 95676 (101420)	Loss/tok 2.9418 (3.1491)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1850/1938]	Time 0.157 (0.141)	Data 2.36e-04 (3.17e-04)	Tok/s 107017 (101410)	Loss/tok 3.1423 (3.1485)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.158 (0.141)	Data 1.85e-04 (3.16e-04)	Tok/s 105171 (101426)	Loss/tok 3.0514 (3.1486)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.106 (0.141)	Data 1.23e-04 (3.15e-04)	Tok/s 97468 (101426)	Loss/tok 2.9074 (3.1483)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.058 (0.141)	Data 1.38e-04 (3.14e-04)	Tok/s 90318 (101407)	Loss/tok 2.5971 (3.1474)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.059 (0.141)	Data 1.19e-04 (3.13e-04)	Tok/s 89322 (101411)	Loss/tok 2.5101 (3.1473)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.058 (0.141)	Data 1.23e-04 (3.12e-04)	Tok/s 90735 (101415)	Loss/tok 2.4681 (3.1471)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.106 (0.141)	Data 1.27e-04 (3.11e-04)	Tok/s 95602 (101394)	Loss/tok 2.8832 (3.1466)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.106 (0.141)	Data 1.64e-04 (3.10e-04)	Tok/s 99270 (101400)	Loss/tok 2.8157 (3.1462)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.158 (0.141)	Data 1.51e-04 (3.09e-04)	Tok/s 106374 (101392)	Loss/tok 3.1353 (3.1457)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020284297, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020284298, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.613 (0.613)	Decoder iters 101.0 (101.0)	Tok/s 26774 (26774)
0: Running moses detokenizer
0: BLEU(score=24.075710241452697, counts=[37124, 18623, 10612, 6308], totals=[65517, 62514, 59511, 56513], precisions=[56.66315612741731, 29.790127011549412, 17.831997445850348, 11.162033514412613], bp=1.0, sys_len=65517, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020286085, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2408, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020286086, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1468	Test BLEU: 24.08
0: Performance: Epoch: 3	Training: 810954 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020286086, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020286087, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1122,nvidia,2020-06-24 10:19:29 AM
