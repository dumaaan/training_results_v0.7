+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
srun: Job 467824 step creation temporarily disabled, retrying
srun: Step created for job 467824
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019162166, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019162203, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019162203, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019162203, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019162203, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: Job 467824 step creation temporarily disabled, retrying
srun: Step created for job 467824
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-527
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019167980, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177098/results:/results ./run_and_time.sh
srun: Job 467824 step creation temporarily disabled, retrying
srun: Step created for job 467824
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=256
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=8
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019172210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019172223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2009424748
:::MLLOG {"namespace": "", "time_ms": 1593019177984, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2009424748, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1259586470
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019185954, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019185955, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019185955, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019185955, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019185955, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019187597, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019187597, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019187597, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019187861, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019187863, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019187863, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019187864, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019187864, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019187864, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019187864, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019187864, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019187864, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019187865, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019187865, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019187865, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2488743316
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.399 (0.399)	Data 2.91e-01 (2.91e-01)	Tok/s 25209 (25209)	Loss/tok 10.5602 (10.5602)	LR 2.047e-05
0: TRAIN [0][10/1938]	Time 0.104 (0.147)	Data 1.44e-04 (2.66e-02)	Tok/s 97733 (93741)	Loss/tok 9.5728 (10.0117)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.106 (0.156)	Data 1.44e-04 (1.40e-02)	Tok/s 99596 (100388)	Loss/tok 9.1827 (9.6892)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.061 (0.150)	Data 1.20e-04 (9.54e-03)	Tok/s 87271 (101243)	Loss/tok 8.7605 (9.5184)	LR 4.083e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1938]	Time 0.104 (0.153)	Data 1.49e-04 (7.25e-03)	Tok/s 98012 (102539)	Loss/tok 8.8132 (9.4221)	LR 4.909e-05
0: TRAIN [0][50/1938]	Time 0.059 (0.145)	Data 1.33e-04 (5.86e-03)	Tok/s 87532 (102020)	Loss/tok 8.3454 (9.3106)	LR 6.181e-05
0: TRAIN [0][60/1938]	Time 0.106 (0.142)	Data 1.29e-04 (4.92e-03)	Tok/s 98455 (101791)	Loss/tok 8.3566 (9.1951)	LR 7.781e-05
0: TRAIN [0][70/1938]	Time 0.106 (0.141)	Data 1.31e-04 (4.25e-03)	Tok/s 97115 (101913)	Loss/tok 8.1154 (9.0807)	LR 9.796e-05
0: TRAIN [0][80/1938]	Time 0.157 (0.142)	Data 1.23e-04 (3.74e-03)	Tok/s 105740 (102050)	Loss/tok 8.1948 (8.9724)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.106 (0.142)	Data 1.68e-04 (3.35e-03)	Tok/s 97859 (102111)	Loss/tok 7.8111 (8.8711)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.156 (0.141)	Data 1.35e-04 (3.03e-03)	Tok/s 109146 (102109)	Loss/tok 8.0176 (8.7825)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.268 (0.144)	Data 1.46e-04 (2.77e-03)	Tok/s 111406 (102447)	Loss/tok 8.1520 (8.6976)	LR 2.461e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][120/1938]	Time 0.058 (0.142)	Data 1.25e-04 (2.55e-03)	Tok/s 90312 (102197)	Loss/tok 7.2004 (8.6391)	LR 3.027e-04
0: TRAIN [0][130/1938]	Time 0.105 (0.142)	Data 1.76e-04 (2.37e-03)	Tok/s 97227 (102325)	Loss/tok 7.6865 (8.5784)	LR 3.811e-04
0: TRAIN [0][140/1938]	Time 0.156 (0.145)	Data 1.50e-04 (2.21e-03)	Tok/s 108817 (102737)	Loss/tok 7.8029 (8.5164)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.105 (0.146)	Data 1.32e-04 (2.08e-03)	Tok/s 97411 (102807)	Loss/tok 7.5822 (8.4649)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.106 (0.145)	Data 1.45e-04 (1.96e-03)	Tok/s 97642 (102634)	Loss/tok 7.4358 (8.4198)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.157 (0.145)	Data 1.30e-04 (1.85e-03)	Tok/s 108159 (102711)	Loss/tok 7.4834 (8.3661)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.106 (0.144)	Data 1.41e-04 (1.75e-03)	Tok/s 97650 (102664)	Loss/tok 7.2202 (8.3169)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.105 (0.145)	Data 1.33e-04 (1.67e-03)	Tok/s 99529 (102840)	Loss/tok 6.9089 (8.2545)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.158 (0.145)	Data 1.32e-04 (1.59e-03)	Tok/s 107250 (102836)	Loss/tok 7.0228 (8.1966)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.106 (0.143)	Data 1.33e-04 (1.53e-03)	Tok/s 98806 (102623)	Loss/tok 6.6617 (8.1439)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.158 (0.142)	Data 1.30e-04 (1.46e-03)	Tok/s 108242 (102529)	Loss/tok 6.6869 (8.0866)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.106 (0.143)	Data 1.25e-04 (1.41e-03)	Tok/s 95180 (102611)	Loss/tok 6.4091 (8.0172)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.106 (0.143)	Data 1.33e-04 (1.35e-03)	Tok/s 96189 (102601)	Loss/tok 6.1953 (7.9569)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][250/1938]	Time 0.157 (0.143)	Data 1.38e-04 (1.31e-03)	Tok/s 107268 (102634)	Loss/tok 6.6634 (7.8955)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.158 (0.144)	Data 1.45e-04 (1.26e-03)	Tok/s 106404 (102755)	Loss/tok 6.2460 (7.8299)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.108 (0.144)	Data 1.36e-04 (1.22e-03)	Tok/s 94892 (102813)	Loss/tok 5.9690 (7.7644)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.210 (0.144)	Data 1.33e-04 (1.18e-03)	Tok/s 110342 (102852)	Loss/tok 6.2201 (7.7011)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.106 (0.144)	Data 1.55e-04 (1.15e-03)	Tok/s 97121 (102769)	Loss/tok 5.6942 (7.6490)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.106 (0.144)	Data 1.30e-04 (1.11e-03)	Tok/s 98089 (102849)	Loss/tok 5.7026 (7.5859)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.158 (0.144)	Data 1.22e-04 (1.08e-03)	Tok/s 108124 (102849)	Loss/tok 5.8356 (7.5279)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.106 (0.144)	Data 1.45e-04 (1.05e-03)	Tok/s 99680 (102778)	Loss/tok 5.4749 (7.4734)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.157 (0.144)	Data 1.37e-04 (1.03e-03)	Tok/s 105248 (102804)	Loss/tok 5.6052 (7.4121)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.107 (0.144)	Data 1.26e-04 (9.99e-04)	Tok/s 96250 (102719)	Loss/tok 5.2000 (7.3596)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.106 (0.144)	Data 1.70e-04 (9.75e-04)	Tok/s 96273 (102689)	Loss/tok 4.9519 (7.3059)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.107 (0.143)	Data 1.26e-04 (9.52e-04)	Tok/s 96139 (102492)	Loss/tok 5.0263 (7.2638)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.107 (0.143)	Data 1.35e-04 (9.30e-04)	Tok/s 96167 (102479)	Loss/tok 4.9701 (7.2094)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][380/1938]	Time 0.271 (0.143)	Data 1.36e-04 (9.10e-04)	Tok/s 110094 (102444)	Loss/tok 5.5096 (7.1584)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.159 (0.142)	Data 1.68e-04 (8.90e-04)	Tok/s 106042 (102390)	Loss/tok 5.1367 (7.1103)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.061 (0.142)	Data 1.68e-04 (8.72e-04)	Tok/s 86930 (102261)	Loss/tok 4.0371 (7.0654)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.107 (0.142)	Data 1.51e-04 (8.55e-04)	Tok/s 95244 (102282)	Loss/tok 4.5552 (7.0127)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.270 (0.142)	Data 1.39e-04 (8.38e-04)	Tok/s 109839 (102235)	Loss/tok 5.3412 (6.9641)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.210 (0.142)	Data 1.85e-04 (8.22e-04)	Tok/s 109883 (102197)	Loss/tok 5.1194 (6.9175)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.106 (0.142)	Data 1.42e-04 (8.06e-04)	Tok/s 97261 (102213)	Loss/tok 4.4374 (6.8666)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.210 (0.142)	Data 1.33e-04 (7.91e-04)	Tok/s 111734 (102155)	Loss/tok 4.9243 (6.8226)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.210 (0.141)	Data 1.48e-04 (7.77e-04)	Tok/s 111585 (102118)	Loss/tok 4.9892 (6.7791)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.159 (0.141)	Data 1.22e-04 (7.64e-04)	Tok/s 105582 (102014)	Loss/tok 4.7224 (6.7406)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.106 (0.141)	Data 1.72e-04 (7.51e-04)	Tok/s 97420 (102072)	Loss/tok 4.2730 (6.6909)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.106 (0.142)	Data 1.27e-04 (7.39e-04)	Tok/s 96184 (102087)	Loss/tok 4.2698 (6.6452)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][500/1938]	Time 0.158 (0.142)	Data 1.53e-04 (7.27e-04)	Tok/s 106424 (102143)	Loss/tok 4.5353 (6.5974)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.159 (0.142)	Data 1.30e-04 (7.15e-04)	Tok/s 103496 (102078)	Loss/tok 4.4874 (6.5608)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.106 (0.142)	Data 1.27e-04 (7.04e-04)	Tok/s 97128 (102045)	Loss/tok 4.1368 (6.5214)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][530/1938]	Time 0.107 (0.141)	Data 2.33e-04 (6.94e-04)	Tok/s 95030 (102005)	Loss/tok 4.1839 (6.4835)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.059 (0.141)	Data 1.45e-04 (6.84e-04)	Tok/s 91220 (102000)	Loss/tok 3.5125 (6.4455)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.158 (0.142)	Data 1.41e-04 (6.75e-04)	Tok/s 104632 (102053)	Loss/tok 4.4295 (6.4017)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.158 (0.142)	Data 1.71e-04 (6.65e-04)	Tok/s 105411 (102024)	Loss/tok 4.3337 (6.3667)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.060 (0.142)	Data 1.50e-04 (6.56e-04)	Tok/s 86566 (102022)	Loss/tok 3.3390 (6.3299)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.106 (0.141)	Data 1.52e-04 (6.47e-04)	Tok/s 98277 (101974)	Loss/tok 3.9474 (6.2971)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.159 (0.142)	Data 1.34e-04 (6.39e-04)	Tok/s 104991 (102021)	Loss/tok 4.3345 (6.2573)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.159 (0.142)	Data 1.79e-04 (6.31e-04)	Tok/s 104647 (102002)	Loss/tok 4.3427 (6.2255)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.160 (0.142)	Data 1.52e-04 (6.23e-04)	Tok/s 105773 (102066)	Loss/tok 4.1993 (6.1877)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.108 (0.142)	Data 1.62e-04 (6.15e-04)	Tok/s 96602 (102062)	Loss/tok 3.9583 (6.1551)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.107 (0.142)	Data 1.32e-04 (6.08e-04)	Tok/s 95338 (102101)	Loss/tok 3.8751 (6.1192)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.159 (0.143)	Data 1.70e-04 (6.01e-04)	Tok/s 106203 (102156)	Loss/tok 4.2756 (6.0824)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1938]	Time 0.107 (0.143)	Data 1.81e-04 (5.94e-04)	Tok/s 97859 (102160)	Loss/tok 3.8625 (6.0516)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.058 (0.143)	Data 1.38e-04 (5.87e-04)	Tok/s 90600 (102137)	Loss/tok 3.2474 (6.0241)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][670/1938]	Time 0.158 (0.143)	Data 1.60e-04 (5.81e-04)	Tok/s 106628 (102130)	Loss/tok 4.0649 (5.9946)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.108 (0.143)	Data 2.00e-04 (5.74e-04)	Tok/s 94748 (102136)	Loss/tok 3.7170 (5.9660)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.159 (0.143)	Data 1.48e-04 (5.68e-04)	Tok/s 104651 (102083)	Loss/tok 4.0181 (5.9420)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.269 (0.143)	Data 1.21e-04 (5.62e-04)	Tok/s 108752 (102046)	Loss/tok 4.5176 (5.9170)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.107 (0.143)	Data 1.45e-04 (5.56e-04)	Tok/s 96592 (102046)	Loss/tok 3.6366 (5.8906)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.271 (0.143)	Data 1.45e-04 (5.51e-04)	Tok/s 110047 (102035)	Loss/tok 4.4874 (5.8640)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.059 (0.143)	Data 1.22e-04 (5.45e-04)	Tok/s 88508 (102018)	Loss/tok 3.0793 (5.8400)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.107 (0.143)	Data 1.54e-04 (5.40e-04)	Tok/s 94377 (101997)	Loss/tok 3.7165 (5.8150)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.107 (0.143)	Data 1.47e-04 (5.35e-04)	Tok/s 95802 (102005)	Loss/tok 3.8122 (5.7893)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.108 (0.143)	Data 1.26e-04 (5.29e-04)	Tok/s 96511 (101994)	Loss/tok 3.7798 (5.7672)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.158 (0.143)	Data 1.58e-04 (5.24e-04)	Tok/s 105368 (101960)	Loss/tok 3.9365 (5.7463)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.107 (0.143)	Data 1.45e-04 (5.20e-04)	Tok/s 95198 (101917)	Loss/tok 3.7212 (5.7258)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.107 (0.143)	Data 1.55e-04 (5.15e-04)	Tok/s 96228 (101932)	Loss/tok 3.6558 (5.7015)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][800/1938]	Time 0.158 (0.143)	Data 1.64e-04 (5.10e-04)	Tok/s 104668 (101946)	Loss/tok 3.9862 (5.6780)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.270 (0.143)	Data 1.24e-04 (5.06e-04)	Tok/s 110658 (101907)	Loss/tok 4.4761 (5.6588)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.159 (0.143)	Data 1.44e-04 (5.02e-04)	Tok/s 106237 (101924)	Loss/tok 4.0028 (5.6368)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.159 (0.143)	Data 1.22e-04 (4.97e-04)	Tok/s 105510 (101899)	Loss/tok 3.9067 (5.6181)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.106 (0.143)	Data 1.58e-04 (4.93e-04)	Tok/s 96421 (101871)	Loss/tok 3.6641 (5.5986)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.108 (0.143)	Data 1.48e-04 (4.89e-04)	Tok/s 94545 (101900)	Loss/tok 3.7149 (5.5767)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.159 (0.144)	Data 1.60e-04 (4.85e-04)	Tok/s 107038 (101937)	Loss/tok 3.8669 (5.5546)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.107 (0.144)	Data 1.81e-04 (4.82e-04)	Tok/s 97276 (101946)	Loss/tok 3.5858 (5.5352)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.272 (0.144)	Data 1.40e-04 (4.78e-04)	Tok/s 111002 (101968)	Loss/tok 4.2412 (5.5149)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.108 (0.144)	Data 1.49e-04 (4.74e-04)	Tok/s 95269 (102000)	Loss/tok 3.6831 (5.4935)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.212 (0.144)	Data 1.67e-04 (4.71e-04)	Tok/s 111596 (101997)	Loss/tok 3.9694 (5.4755)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.212 (0.144)	Data 1.51e-04 (4.67e-04)	Tok/s 108339 (101996)	Loss/tok 4.0591 (5.4570)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][920/1938]	Time 0.159 (0.144)	Data 1.81e-04 (4.64e-04)	Tok/s 107091 (101985)	Loss/tok 4.0245 (5.4400)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.159 (0.144)	Data 1.74e-04 (4.60e-04)	Tok/s 104950 (101961)	Loss/tok 3.9797 (5.4240)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.210 (0.144)	Data 1.32e-04 (4.57e-04)	Tok/s 111607 (101956)	Loss/tok 3.9324 (5.4075)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.159 (0.144)	Data 1.46e-04 (4.54e-04)	Tok/s 104269 (101923)	Loss/tok 3.8736 (5.3925)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][960/1938]	Time 0.160 (0.144)	Data 1.93e-04 (4.50e-04)	Tok/s 106388 (101910)	Loss/tok 3.8787 (5.3760)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.108 (0.144)	Data 1.65e-04 (4.47e-04)	Tok/s 94330 (101863)	Loss/tok 3.6014 (5.3623)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.107 (0.144)	Data 1.76e-04 (4.44e-04)	Tok/s 95360 (101864)	Loss/tok 3.5268 (5.3465)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.159 (0.143)	Data 1.29e-04 (4.41e-04)	Tok/s 106335 (101796)	Loss/tok 3.8968 (5.3346)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.213 (0.144)	Data 1.34e-04 (4.39e-04)	Tok/s 109677 (101804)	Loss/tok 4.0972 (5.3184)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.159 (0.143)	Data 1.48e-04 (4.36e-04)	Tok/s 104298 (101763)	Loss/tok 3.7953 (5.3058)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.107 (0.143)	Data 2.17e-04 (4.33e-04)	Tok/s 99096 (101770)	Loss/tok 3.4491 (5.2904)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.213 (0.143)	Data 1.86e-04 (4.30e-04)	Tok/s 109272 (101771)	Loss/tok 3.9917 (5.2757)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.107 (0.143)	Data 1.89e-04 (4.28e-04)	Tok/s 96818 (101752)	Loss/tok 3.5283 (5.2624)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.270 (0.143)	Data 1.46e-04 (4.25e-04)	Tok/s 109250 (101738)	Loss/tok 4.2167 (5.2481)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.160 (0.144)	Data 1.44e-04 (4.22e-04)	Tok/s 104715 (101736)	Loss/tok 3.8563 (5.2339)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.108 (0.143)	Data 1.63e-04 (4.20e-04)	Tok/s 96550 (101724)	Loss/tok 3.5833 (5.2207)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.108 (0.143)	Data 1.29e-04 (4.17e-04)	Tok/s 95601 (101699)	Loss/tok 3.5893 (5.2085)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1090/1938]	Time 0.107 (0.143)	Data 1.58e-04 (4.15e-04)	Tok/s 95416 (101708)	Loss/tok 3.5331 (5.1950)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.061 (0.143)	Data 1.63e-04 (4.13e-04)	Tok/s 87738 (101667)	Loss/tok 2.9257 (5.1839)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.107 (0.143)	Data 1.19e-04 (4.10e-04)	Tok/s 97163 (101658)	Loss/tok 3.4735 (5.1714)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.060 (0.143)	Data 1.68e-04 (4.08e-04)	Tok/s 86896 (101610)	Loss/tok 3.0655 (5.1609)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.059 (0.143)	Data 1.60e-04 (4.06e-04)	Tok/s 90230 (101561)	Loss/tok 3.0963 (5.1513)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.160 (0.142)	Data 1.54e-04 (4.03e-04)	Tok/s 103451 (101531)	Loss/tok 3.7633 (5.1400)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.107 (0.142)	Data 1.26e-04 (4.01e-04)	Tok/s 95162 (101531)	Loss/tok 3.4170 (5.1275)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.107 (0.142)	Data 1.23e-04 (3.99e-04)	Tok/s 96164 (101497)	Loss/tok 3.3942 (5.1170)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.108 (0.142)	Data 1.87e-04 (3.97e-04)	Tok/s 97273 (101468)	Loss/tok 3.3746 (5.1061)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.107 (0.142)	Data 1.60e-04 (3.95e-04)	Tok/s 96092 (101469)	Loss/tok 3.3599 (5.0944)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.159 (0.142)	Data 1.51e-04 (3.93e-04)	Tok/s 107015 (101463)	Loss/tok 3.7476 (5.0827)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.159 (0.142)	Data 1.44e-04 (3.91e-04)	Tok/s 105478 (101445)	Loss/tok 3.7716 (5.0721)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.159 (0.142)	Data 1.41e-04 (3.89e-04)	Tok/s 107401 (101433)	Loss/tok 3.6532 (5.0609)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1220/1938]	Time 0.108 (0.142)	Data 1.28e-04 (3.87e-04)	Tok/s 93333 (101425)	Loss/tok 3.5285 (5.0502)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.273 (0.142)	Data 1.30e-04 (3.85e-04)	Tok/s 109073 (101440)	Loss/tok 4.1110 (5.0386)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1240/1938]	Time 0.271 (0.143)	Data 1.49e-04 (3.83e-04)	Tok/s 111056 (101441)	Loss/tok 4.1213 (5.0278)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.161 (0.143)	Data 1.27e-04 (3.81e-04)	Tok/s 104921 (101450)	Loss/tok 3.8083 (5.0168)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.108 (0.143)	Data 1.52e-04 (3.79e-04)	Tok/s 97558 (101447)	Loss/tok 3.4341 (5.0064)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.271 (0.143)	Data 1.38e-04 (3.77e-04)	Tok/s 109533 (101436)	Loss/tok 4.1063 (4.9962)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.158 (0.142)	Data 1.48e-04 (3.75e-04)	Tok/s 106723 (101413)	Loss/tok 3.5977 (4.9869)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.108 (0.142)	Data 1.24e-04 (3.74e-04)	Tok/s 95969 (101406)	Loss/tok 3.6110 (4.9771)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.109 (0.143)	Data 1.88e-04 (3.72e-04)	Tok/s 95723 (101410)	Loss/tok 3.4384 (4.9668)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.060 (0.143)	Data 1.37e-04 (3.70e-04)	Tok/s 87155 (101412)	Loss/tok 2.8917 (4.9564)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.160 (0.143)	Data 1.22e-04 (3.69e-04)	Tok/s 104785 (101410)	Loss/tok 3.7190 (4.9468)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.158 (0.143)	Data 1.62e-04 (3.67e-04)	Tok/s 103584 (101392)	Loss/tok 3.6359 (4.9379)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.061 (0.142)	Data 1.43e-04 (3.66e-04)	Tok/s 88422 (101362)	Loss/tok 2.9312 (4.9299)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.107 (0.142)	Data 1.64e-04 (3.64e-04)	Tok/s 99041 (101339)	Loss/tok 3.5760 (4.9216)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.159 (0.142)	Data 1.29e-04 (3.62e-04)	Tok/s 106393 (101315)	Loss/tok 3.6564 (4.9132)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1370/1938]	Time 0.108 (0.142)	Data 1.43e-04 (3.61e-04)	Tok/s 94025 (101313)	Loss/tok 3.5164 (4.9040)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.273 (0.142)	Data 1.76e-04 (3.59e-04)	Tok/s 107575 (101348)	Loss/tok 4.1048 (4.8931)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.160 (0.142)	Data 1.59e-04 (3.58e-04)	Tok/s 105462 (101345)	Loss/tok 3.6232 (4.8843)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.159 (0.142)	Data 1.32e-04 (3.56e-04)	Tok/s 105373 (101335)	Loss/tok 3.6297 (4.8758)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.108 (0.142)	Data 1.96e-04 (3.55e-04)	Tok/s 95060 (101341)	Loss/tok 3.3533 (4.8667)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.160 (0.142)	Data 1.43e-04 (3.53e-04)	Tok/s 104589 (101326)	Loss/tok 3.5753 (4.8588)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.107 (0.142)	Data 1.51e-04 (3.52e-04)	Tok/s 98375 (101307)	Loss/tok 3.3764 (4.8510)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.158 (0.142)	Data 1.86e-04 (3.51e-04)	Tok/s 104614 (101312)	Loss/tok 3.5821 (4.8422)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.49e-04)	Tok/s 95382 (101321)	Loss/tok 3.4916 (4.8333)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.107 (0.142)	Data 1.51e-04 (3.48e-04)	Tok/s 97554 (101291)	Loss/tok 3.4208 (4.8263)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.159 (0.142)	Data 1.24e-04 (3.47e-04)	Tok/s 105337 (101268)	Loss/tok 3.6360 (4.8191)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.107 (0.142)	Data 1.37e-04 (3.45e-04)	Tok/s 96679 (101265)	Loss/tok 3.4317 (4.8111)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.107 (0.142)	Data 1.40e-04 (3.44e-04)	Tok/s 96949 (101248)	Loss/tok 3.4230 (4.8038)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1500/1938]	Time 0.272 (0.142)	Data 1.65e-04 (3.43e-04)	Tok/s 109807 (101248)	Loss/tok 3.9389 (4.7956)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.107 (0.142)	Data 1.40e-04 (3.41e-04)	Tok/s 96801 (101232)	Loss/tok 3.3275 (4.7883)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.060 (0.142)	Data 1.28e-04 (3.40e-04)	Tok/s 88642 (101212)	Loss/tok 2.8372 (4.7811)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.107 (0.142)	Data 1.42e-04 (3.39e-04)	Tok/s 97469 (101207)	Loss/tok 3.4456 (4.7735)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.107 (0.142)	Data 1.37e-04 (3.38e-04)	Tok/s 97091 (101194)	Loss/tok 3.4509 (4.7662)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.107 (0.141)	Data 1.49e-04 (3.37e-04)	Tok/s 96015 (101176)	Loss/tok 3.3258 (4.7595)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.107 (0.142)	Data 1.57e-04 (3.35e-04)	Tok/s 96689 (101195)	Loss/tok 3.3450 (4.7512)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.159 (0.142)	Data 1.27e-04 (3.34e-04)	Tok/s 106987 (101188)	Loss/tok 3.6413 (4.7443)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.211 (0.142)	Data 1.49e-04 (3.33e-04)	Tok/s 111036 (101197)	Loss/tok 3.8333 (4.7367)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.106 (0.142)	Data 1.81e-04 (3.32e-04)	Tok/s 99321 (101206)	Loss/tok 3.3196 (4.7294)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.159 (0.142)	Data 1.70e-04 (3.31e-04)	Tok/s 105490 (101202)	Loss/tok 3.5803 (4.7227)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.159 (0.142)	Data 1.92e-04 (3.30e-04)	Tok/s 106573 (101188)	Loss/tok 3.6002 (4.7161)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.213 (0.141)	Data 1.46e-04 (3.29e-04)	Tok/s 111810 (101170)	Loss/tok 3.6697 (4.7096)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1630/1938]	Time 0.212 (0.141)	Data 1.28e-04 (3.28e-04)	Tok/s 108258 (101178)	Loss/tok 3.9375 (4.7022)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.159 (0.142)	Data 2.15e-04 (3.27e-04)	Tok/s 106257 (101192)	Loss/tok 3.6260 (4.6947)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.107 (0.142)	Data 1.67e-04 (3.26e-04)	Tok/s 96870 (101209)	Loss/tok 3.3112 (4.6870)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.213 (0.142)	Data 1.62e-04 (3.25e-04)	Tok/s 110296 (101216)	Loss/tok 3.7426 (4.6802)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.107 (0.142)	Data 1.45e-04 (3.24e-04)	Tok/s 95964 (101207)	Loss/tok 3.3144 (4.6740)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.107 (0.142)	Data 1.47e-04 (3.23e-04)	Tok/s 98740 (101208)	Loss/tok 3.4412 (4.6674)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.211 (0.142)	Data 1.29e-04 (3.22e-04)	Tok/s 111452 (101202)	Loss/tok 3.6836 (4.6610)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.159 (0.142)	Data 1.77e-04 (3.21e-04)	Tok/s 105250 (101192)	Loss/tok 3.7246 (4.6552)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.107 (0.142)	Data 1.52e-04 (3.20e-04)	Tok/s 96480 (101207)	Loss/tok 3.3256 (4.6482)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.212 (0.142)	Data 1.64e-04 (3.19e-04)	Tok/s 109614 (101217)	Loss/tok 3.8367 (4.6416)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.106 (0.142)	Data 1.65e-04 (3.18e-04)	Tok/s 94733 (101195)	Loss/tok 3.4932 (4.6363)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.159 (0.141)	Data 1.85e-04 (3.17e-04)	Tok/s 104897 (101180)	Loss/tok 3.5749 (4.6306)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1750/1938]	Time 0.108 (0.141)	Data 2.03e-04 (3.16e-04)	Tok/s 95144 (101181)	Loss/tok 3.3294 (4.6244)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.213 (0.142)	Data 1.41e-04 (3.16e-04)	Tok/s 109111 (101186)	Loss/tok 3.6941 (4.6182)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.213 (0.142)	Data 1.44e-04 (3.15e-04)	Tok/s 109906 (101185)	Loss/tok 3.7878 (4.6122)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.159 (0.142)	Data 1.49e-04 (3.14e-04)	Tok/s 105143 (101190)	Loss/tok 3.6122 (4.6060)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.160 (0.142)	Data 1.84e-04 (3.13e-04)	Tok/s 105546 (101193)	Loss/tok 3.6030 (4.6001)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.107 (0.142)	Data 1.69e-04 (3.12e-04)	Tok/s 96083 (101185)	Loss/tok 3.4373 (4.5946)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.159 (0.142)	Data 1.46e-04 (3.11e-04)	Tok/s 106239 (101189)	Loss/tok 3.5751 (4.5884)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.108 (0.142)	Data 1.30e-04 (3.10e-04)	Tok/s 94929 (101200)	Loss/tok 3.3493 (4.5823)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.158 (0.142)	Data 1.66e-04 (3.10e-04)	Tok/s 106009 (101197)	Loss/tok 3.4964 (4.5768)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.107 (0.142)	Data 1.33e-04 (3.09e-04)	Tok/s 96980 (101189)	Loss/tok 3.2450 (4.5714)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.107 (0.142)	Data 1.48e-04 (3.08e-04)	Tok/s 97766 (101185)	Loss/tok 3.2541 (4.5659)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.107 (0.142)	Data 1.39e-04 (3.07e-04)	Tok/s 98745 (101183)	Loss/tok 3.4536 (4.5605)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1870/1938]	Time 0.107 (0.142)	Data 1.76e-04 (3.07e-04)	Tok/s 95780 (101182)	Loss/tok 3.3036 (4.5553)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.059 (0.141)	Data 1.62e-04 (3.06e-04)	Tok/s 88387 (101157)	Loss/tok 2.7239 (4.5507)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.158 (0.141)	Data 1.65e-04 (3.05e-04)	Tok/s 105226 (101168)	Loss/tok 3.4179 (4.5450)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.160 (0.141)	Data 1.86e-04 (3.04e-04)	Tok/s 104781 (101171)	Loss/tok 3.4233 (4.5394)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.212 (0.141)	Data 1.52e-04 (3.03e-04)	Tok/s 111749 (101173)	Loss/tok 3.6057 (4.5341)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.159 (0.141)	Data 1.89e-04 (3.03e-04)	Tok/s 104080 (101182)	Loss/tok 3.5152 (4.5285)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.159 (0.141)	Data 1.38e-04 (3.02e-04)	Tok/s 104852 (101162)	Loss/tok 3.5351 (4.5240)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019462040, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019462041, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.574 (0.574)	Decoder iters 95.0 (95.0)	Tok/s 27306 (27306)
0: Running moses detokenizer
0: BLEU(score=20.142027071675304, counts=[34411, 15809, 8415, 4668], totals=[63908, 60905, 57902, 54903], precisions=[53.84458909682669, 25.956817995238485, 14.533176746917205, 8.502267635648325], bp=0.9880546442359512, sys_len=63908, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019463749, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2014, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019463749, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.5213	Test BLEU: 20.14
0: Performance: Epoch: 0	Training: 809297 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019463749, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019463750, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019463750, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3616108634
0: TRAIN [1][0/1938]	Time 0.414 (0.414)	Data 2.52e-01 (2.52e-01)	Tok/s 41322 (41322)	Loss/tok 3.3984 (3.3984)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.161 (0.174)	Data 1.80e-04 (2.31e-02)	Tok/s 104133 (95844)	Loss/tok 3.4460 (3.4447)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.160 (0.155)	Data 1.44e-04 (1.22e-02)	Tok/s 106487 (98388)	Loss/tok 3.4773 (3.4125)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.107 (0.150)	Data 1.44e-04 (8.30e-03)	Tok/s 98536 (99377)	Loss/tok 3.1162 (3.4330)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.107 (0.143)	Data 1.66e-04 (6.31e-03)	Tok/s 97768 (99365)	Loss/tok 3.1730 (3.4104)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.160 (0.145)	Data 2.04e-04 (5.11e-03)	Tok/s 106984 (99672)	Loss/tok 3.4622 (3.4266)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][60/1938]	Time 0.108 (0.146)	Data 1.34e-04 (4.30e-03)	Tok/s 96532 (99884)	Loss/tok 3.2342 (3.4496)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][70/1938]	Time 0.213 (0.145)	Data 1.66e-04 (3.71e-03)	Tok/s 109839 (99866)	Loss/tok 3.7632 (3.4613)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.160 (0.141)	Data 1.62e-04 (3.28e-03)	Tok/s 104765 (99541)	Loss/tok 3.4223 (3.4493)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.160 (0.141)	Data 1.48e-04 (2.93e-03)	Tok/s 105405 (99778)	Loss/tok 3.4266 (3.4443)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.213 (0.141)	Data 1.85e-04 (2.66e-03)	Tok/s 109174 (99848)	Loss/tok 3.6159 (3.4407)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.214 (0.142)	Data 1.55e-04 (2.43e-03)	Tok/s 109941 (100041)	Loss/tok 3.6138 (3.4460)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.213 (0.143)	Data 1.61e-04 (2.25e-03)	Tok/s 109264 (100223)	Loss/tok 3.5626 (3.4520)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.213 (0.143)	Data 1.45e-04 (2.09e-03)	Tok/s 109153 (100302)	Loss/tok 3.6997 (3.4508)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.106 (0.143)	Data 1.69e-04 (1.95e-03)	Tok/s 98105 (100400)	Loss/tok 3.2155 (3.4574)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.160 (0.143)	Data 1.47e-04 (1.83e-03)	Tok/s 106296 (100474)	Loss/tok 3.3871 (3.4582)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.107 (0.144)	Data 2.07e-04 (1.73e-03)	Tok/s 98057 (100582)	Loss/tok 3.1931 (3.4610)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][170/1938]	Time 0.274 (0.144)	Data 1.77e-04 (1.64e-03)	Tok/s 108814 (100647)	Loss/tok 3.8923 (3.4673)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.059 (0.143)	Data 1.70e-04 (1.56e-03)	Tok/s 90509 (100541)	Loss/tok 2.9011 (3.4614)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.107 (0.143)	Data 1.44e-04 (1.48e-03)	Tok/s 95303 (100600)	Loss/tok 3.1600 (3.4596)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][200/1938]	Time 0.107 (0.143)	Data 1.48e-04 (1.42e-03)	Tok/s 98021 (100572)	Loss/tok 3.1007 (3.4638)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.108 (0.143)	Data 1.32e-04 (1.36e-03)	Tok/s 96570 (100690)	Loss/tok 3.2012 (3.4647)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.160 (0.144)	Data 1.91e-04 (1.30e-03)	Tok/s 106210 (100801)	Loss/tok 3.4839 (3.4670)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.107 (0.143)	Data 1.78e-04 (1.25e-03)	Tok/s 95529 (100773)	Loss/tok 3.1232 (3.4615)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.107 (0.143)	Data 1.64e-04 (1.21e-03)	Tok/s 99559 (100835)	Loss/tok 3.2481 (3.4621)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.107 (0.142)	Data 1.70e-04 (1.17e-03)	Tok/s 98092 (100757)	Loss/tok 3.2550 (3.4568)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.107 (0.143)	Data 1.66e-04 (1.13e-03)	Tok/s 95966 (100876)	Loss/tok 3.3752 (3.4619)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.107 (0.143)	Data 2.02e-04 (1.09e-03)	Tok/s 97405 (100918)	Loss/tok 3.2540 (3.4594)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.159 (0.143)	Data 1.85e-04 (1.06e-03)	Tok/s 106331 (100906)	Loss/tok 3.5555 (3.4588)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.107 (0.144)	Data 1.87e-04 (1.03e-03)	Tok/s 95756 (101037)	Loss/tok 3.1178 (3.4615)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.107 (0.144)	Data 1.48e-04 (1.00e-03)	Tok/s 96132 (101118)	Loss/tok 3.2372 (3.4591)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.212 (0.144)	Data 1.79e-04 (9.74e-04)	Tok/s 110464 (101138)	Loss/tok 3.6029 (3.4569)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.158 (0.144)	Data 1.48e-04 (9.48e-04)	Tok/s 105391 (101162)	Loss/tok 3.3976 (3.4571)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][330/1938]	Time 0.059 (0.143)	Data 1.45e-04 (9.24e-04)	Tok/s 88978 (101061)	Loss/tok 2.7792 (3.4568)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.213 (0.144)	Data 1.52e-04 (9.02e-04)	Tok/s 109390 (101150)	Loss/tok 3.7360 (3.4614)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.159 (0.144)	Data 1.49e-04 (8.81e-04)	Tok/s 105503 (101128)	Loss/tok 3.5488 (3.4612)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.160 (0.144)	Data 1.72e-04 (8.61e-04)	Tok/s 105581 (101213)	Loss/tok 3.4107 (3.4622)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.107 (0.144)	Data 1.90e-04 (8.42e-04)	Tok/s 95484 (101207)	Loss/tok 3.2425 (3.4604)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.160 (0.144)	Data 1.51e-04 (8.24e-04)	Tok/s 106140 (101247)	Loss/tok 3.4396 (3.4601)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.107 (0.144)	Data 1.82e-04 (8.07e-04)	Tok/s 95731 (101255)	Loss/tok 3.1830 (3.4590)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.159 (0.144)	Data 1.38e-04 (7.91e-04)	Tok/s 106714 (101341)	Loss/tok 3.2908 (3.4586)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.107 (0.144)	Data 1.34e-04 (7.76e-04)	Tok/s 97165 (101295)	Loss/tok 3.1820 (3.4578)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.107 (0.144)	Data 1.87e-04 (7.61e-04)	Tok/s 97930 (101275)	Loss/tok 3.2021 (3.4557)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.109 (0.143)	Data 2.22e-04 (7.47e-04)	Tok/s 94516 (101175)	Loss/tok 3.2217 (3.4538)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.107 (0.143)	Data 2.10e-04 (7.34e-04)	Tok/s 96974 (101201)	Loss/tok 3.1498 (3.4529)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.108 (0.143)	Data 1.62e-04 (7.21e-04)	Tok/s 96126 (101108)	Loss/tok 3.1524 (3.4497)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][460/1938]	Time 0.108 (0.143)	Data 1.76e-04 (7.09e-04)	Tok/s 95617 (101127)	Loss/tok 3.2054 (3.4488)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.107 (0.143)	Data 1.45e-04 (6.97e-04)	Tok/s 95946 (101099)	Loss/tok 3.3031 (3.4491)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.213 (0.143)	Data 1.38e-04 (6.85e-04)	Tok/s 110069 (101094)	Loss/tok 3.5902 (3.4496)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.107 (0.142)	Data 1.44e-04 (6.74e-04)	Tok/s 94187 (101052)	Loss/tok 3.2494 (3.4488)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.212 (0.142)	Data 1.43e-04 (6.64e-04)	Tok/s 110791 (101021)	Loss/tok 3.5673 (3.4474)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.107 (0.142)	Data 1.34e-04 (6.54e-04)	Tok/s 96293 (100971)	Loss/tok 3.2340 (3.4459)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.213 (0.141)	Data 1.75e-04 (6.45e-04)	Tok/s 107847 (100924)	Loss/tok 3.6118 (3.4438)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.273 (0.141)	Data 1.96e-04 (6.35e-04)	Tok/s 108962 (100895)	Loss/tok 3.8029 (3.4436)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.107 (0.141)	Data 1.98e-04 (6.27e-04)	Tok/s 97744 (100825)	Loss/tok 3.2146 (3.4416)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.158 (0.141)	Data 1.29e-04 (6.18e-04)	Tok/s 104932 (100852)	Loss/tok 3.4803 (3.4420)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.159 (0.141)	Data 1.73e-04 (6.10e-04)	Tok/s 105963 (100844)	Loss/tok 3.4593 (3.4422)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.159 (0.141)	Data 1.74e-04 (6.02e-04)	Tok/s 104473 (100834)	Loss/tok 3.4202 (3.4406)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.107 (0.141)	Data 1.62e-04 (5.95e-04)	Tok/s 95794 (100852)	Loss/tok 3.2640 (3.4418)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][590/1938]	Time 0.107 (0.141)	Data 1.44e-04 (5.88e-04)	Tok/s 95310 (100807)	Loss/tok 3.2617 (3.4398)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.160 (0.141)	Data 1.76e-04 (5.80e-04)	Tok/s 106485 (100858)	Loss/tok 3.4139 (3.4403)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.107 (0.141)	Data 1.65e-04 (5.73e-04)	Tok/s 96803 (100897)	Loss/tok 3.0750 (3.4408)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.107 (0.141)	Data 1.48e-04 (5.67e-04)	Tok/s 95104 (100878)	Loss/tok 3.1767 (3.4407)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][630/1938]	Time 0.107 (0.141)	Data 1.75e-04 (5.60e-04)	Tok/s 95339 (100838)	Loss/tok 3.0835 (3.4388)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.213 (0.141)	Data 1.72e-04 (5.54e-04)	Tok/s 110009 (100856)	Loss/tok 3.6040 (3.4381)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.159 (0.141)	Data 1.85e-04 (5.48e-04)	Tok/s 105565 (100873)	Loss/tok 3.4394 (3.4391)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.108 (0.141)	Data 1.64e-04 (5.42e-04)	Tok/s 94434 (100898)	Loss/tok 3.2083 (3.4385)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.271 (0.141)	Data 1.29e-04 (5.36e-04)	Tok/s 109850 (100896)	Loss/tok 3.8321 (3.4385)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.108 (0.141)	Data 2.02e-04 (5.31e-04)	Tok/s 95393 (100868)	Loss/tok 3.2218 (3.4372)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.059 (0.141)	Data 1.44e-04 (5.26e-04)	Tok/s 89544 (100831)	Loss/tok 2.7464 (3.4357)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.160 (0.141)	Data 1.25e-04 (5.20e-04)	Tok/s 105972 (100855)	Loss/tok 3.4100 (3.4354)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.107 (0.141)	Data 1.68e-04 (5.15e-04)	Tok/s 95846 (100862)	Loss/tok 3.1382 (3.4346)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.159 (0.141)	Data 1.28e-04 (5.10e-04)	Tok/s 106182 (100852)	Loss/tok 3.3545 (3.4328)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.273 (0.141)	Data 1.96e-04 (5.05e-04)	Tok/s 109819 (100871)	Loss/tok 3.7782 (3.4342)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.159 (0.141)	Data 1.77e-04 (5.01e-04)	Tok/s 107595 (100866)	Loss/tok 3.3407 (3.4344)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.159 (0.141)	Data 1.46e-04 (4.96e-04)	Tok/s 104136 (100869)	Loss/tok 3.3572 (3.4341)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][760/1938]	Time 0.274 (0.141)	Data 1.49e-04 (4.92e-04)	Tok/s 109490 (100857)	Loss/tok 3.7458 (3.4337)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.106 (0.141)	Data 1.99e-04 (4.88e-04)	Tok/s 98197 (100867)	Loss/tok 3.0551 (3.4347)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][780/1938]	Time 0.059 (0.141)	Data 1.58e-04 (4.84e-04)	Tok/s 89691 (100860)	Loss/tok 2.8633 (3.4344)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.107 (0.141)	Data 1.23e-04 (4.79e-04)	Tok/s 97934 (100847)	Loss/tok 3.0026 (3.4330)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.159 (0.141)	Data 1.57e-04 (4.75e-04)	Tok/s 106240 (100858)	Loss/tok 3.3766 (3.4329)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.160 (0.141)	Data 1.27e-04 (4.71e-04)	Tok/s 105587 (100845)	Loss/tok 3.3258 (3.4317)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.158 (0.141)	Data 1.44e-04 (4.67e-04)	Tok/s 104060 (100869)	Loss/tok 3.4253 (3.4313)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.107 (0.141)	Data 2.10e-04 (4.64e-04)	Tok/s 96209 (100876)	Loss/tok 3.1938 (3.4311)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.271 (0.141)	Data 1.46e-04 (4.60e-04)	Tok/s 109264 (100897)	Loss/tok 3.7350 (3.4329)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.059 (0.141)	Data 1.43e-04 (4.57e-04)	Tok/s 88875 (100882)	Loss/tok 2.7948 (3.4322)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.108 (0.141)	Data 1.30e-04 (4.53e-04)	Tok/s 96031 (100833)	Loss/tok 3.2020 (3.4314)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.107 (0.141)	Data 1.64e-04 (4.50e-04)	Tok/s 96325 (100842)	Loss/tok 3.2505 (3.4309)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.059 (0.141)	Data 1.75e-04 (4.47e-04)	Tok/s 88848 (100844)	Loss/tok 2.7614 (3.4305)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.159 (0.141)	Data 1.49e-04 (4.43e-04)	Tok/s 107830 (100877)	Loss/tok 3.3088 (3.4314)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][900/1938]	Time 0.158 (0.141)	Data 1.36e-04 (4.40e-04)	Tok/s 105429 (100897)	Loss/tok 3.4263 (3.4308)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.107 (0.141)	Data 1.62e-04 (4.37e-04)	Tok/s 96258 (100874)	Loss/tok 3.1895 (3.4297)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.272 (0.141)	Data 1.32e-04 (4.34e-04)	Tok/s 107760 (100872)	Loss/tok 3.9509 (3.4309)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.106 (0.141)	Data 1.65e-04 (4.31e-04)	Tok/s 96632 (100845)	Loss/tok 3.1303 (3.4293)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.107 (0.141)	Data 2.27e-04 (4.29e-04)	Tok/s 95199 (100827)	Loss/tok 3.1993 (3.4282)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.160 (0.141)	Data 1.66e-04 (4.26e-04)	Tok/s 104690 (100851)	Loss/tok 3.3992 (3.4274)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][960/1938]	Time 0.159 (0.141)	Data 1.49e-04 (4.23e-04)	Tok/s 107398 (100875)	Loss/tok 3.4641 (3.4278)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.107 (0.141)	Data 1.36e-04 (4.20e-04)	Tok/s 94340 (100870)	Loss/tok 3.1425 (3.4276)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.160 (0.141)	Data 1.45e-04 (4.18e-04)	Tok/s 103763 (100895)	Loss/tok 3.3893 (3.4275)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.212 (0.141)	Data 1.82e-04 (4.15e-04)	Tok/s 109958 (100902)	Loss/tok 3.5817 (3.4269)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.160 (0.141)	Data 1.62e-04 (4.13e-04)	Tok/s 105901 (100894)	Loss/tok 3.3148 (3.4271)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.160 (0.141)	Data 1.45e-04 (4.10e-04)	Tok/s 104741 (100870)	Loss/tok 3.3455 (3.4257)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.159 (0.141)	Data 1.46e-04 (4.08e-04)	Tok/s 105554 (100869)	Loss/tok 3.3969 (3.4247)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.107 (0.141)	Data 1.46e-04 (4.05e-04)	Tok/s 96850 (100889)	Loss/tok 3.2085 (3.4242)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.107 (0.141)	Data 1.64e-04 (4.03e-04)	Tok/s 95433 (100891)	Loss/tok 3.1739 (3.4244)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.160 (0.141)	Data 1.56e-04 (4.01e-04)	Tok/s 106039 (100910)	Loss/tok 3.4534 (3.4247)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.160 (0.141)	Data 1.50e-04 (3.98e-04)	Tok/s 104617 (100910)	Loss/tok 3.4533 (3.4247)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.107 (0.141)	Data 1.70e-04 (3.96e-04)	Tok/s 97032 (100915)	Loss/tok 3.0690 (3.4241)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.107 (0.141)	Data 1.70e-04 (3.94e-04)	Tok/s 97215 (100953)	Loss/tok 3.1315 (3.4243)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1090/1938]	Time 0.160 (0.141)	Data 1.54e-04 (3.92e-04)	Tok/s 105207 (100961)	Loss/tok 3.4048 (3.4237)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.107 (0.141)	Data 1.49e-04 (3.90e-04)	Tok/s 97307 (100957)	Loss/tok 3.0918 (3.4230)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1110/1938]	Time 0.106 (0.142)	Data 1.78e-04 (3.88e-04)	Tok/s 97576 (100982)	Loss/tok 3.1318 (3.4244)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.107 (0.142)	Data 1.81e-04 (3.86e-04)	Tok/s 95909 (101006)	Loss/tok 3.1269 (3.4249)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.059 (0.142)	Data 1.50e-04 (3.84e-04)	Tok/s 90084 (100998)	Loss/tok 2.7402 (3.4238)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.158 (0.142)	Data 1.53e-04 (3.82e-04)	Tok/s 106348 (101004)	Loss/tok 3.3688 (3.4227)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.273 (0.142)	Data 1.53e-04 (3.80e-04)	Tok/s 108688 (101006)	Loss/tok 3.7386 (3.4222)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.159 (0.142)	Data 1.44e-04 (3.78e-04)	Tok/s 107617 (101059)	Loss/tok 3.3725 (3.4235)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.159 (0.142)	Data 1.79e-04 (3.76e-04)	Tok/s 106140 (101056)	Loss/tok 3.3697 (3.4226)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.159 (0.142)	Data 1.27e-04 (3.74e-04)	Tok/s 105767 (101046)	Loss/tok 3.5521 (3.4219)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1190/1938]	Time 0.059 (0.142)	Data 1.31e-04 (3.72e-04)	Tok/s 89949 (101051)	Loss/tok 2.7242 (3.4217)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.059 (0.142)	Data 1.88e-04 (3.70e-04)	Tok/s 89384 (101046)	Loss/tok 2.7655 (3.4209)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.107 (0.142)	Data 1.36e-04 (3.68e-04)	Tok/s 97068 (101060)	Loss/tok 3.1924 (3.4209)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.212 (0.142)	Data 1.62e-04 (3.67e-04)	Tok/s 110830 (101078)	Loss/tok 3.6526 (3.4220)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.158 (0.142)	Data 1.52e-04 (3.65e-04)	Tok/s 106721 (101057)	Loss/tok 3.4067 (3.4212)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.211 (0.142)	Data 1.71e-04 (3.63e-04)	Tok/s 110747 (101058)	Loss/tok 3.4227 (3.4201)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.159 (0.142)	Data 1.59e-04 (3.61e-04)	Tok/s 104723 (101057)	Loss/tok 3.4997 (3.4200)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.107 (0.142)	Data 1.47e-04 (3.60e-04)	Tok/s 97817 (101072)	Loss/tok 3.2404 (3.4197)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.159 (0.142)	Data 1.46e-04 (3.58e-04)	Tok/s 105292 (101076)	Loss/tok 3.3949 (3.4193)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.107 (0.142)	Data 1.32e-04 (3.56e-04)	Tok/s 95734 (101070)	Loss/tok 3.1046 (3.4189)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.107 (0.142)	Data 1.25e-04 (3.55e-04)	Tok/s 96888 (101056)	Loss/tok 3.0865 (3.4178)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.159 (0.142)	Data 1.29e-04 (3.53e-04)	Tok/s 106897 (101048)	Loss/tok 3.3843 (3.4169)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.107 (0.142)	Data 1.32e-04 (3.52e-04)	Tok/s 97550 (101047)	Loss/tok 3.1026 (3.4167)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1320/1938]	Time 0.107 (0.142)	Data 1.36e-04 (3.50e-04)	Tok/s 96907 (101074)	Loss/tok 3.0472 (3.4174)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.059 (0.142)	Data 1.34e-04 (3.49e-04)	Tok/s 90822 (101067)	Loss/tok 2.7244 (3.4175)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.213 (0.142)	Data 1.61e-04 (3.47e-04)	Tok/s 109401 (101099)	Loss/tok 3.5193 (3.4177)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.271 (0.142)	Data 1.40e-04 (3.46e-04)	Tok/s 109831 (101100)	Loss/tok 3.7031 (3.4174)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.211 (0.142)	Data 1.26e-04 (3.44e-04)	Tok/s 111181 (101079)	Loss/tok 3.5374 (3.4166)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.159 (0.142)	Data 1.54e-04 (3.43e-04)	Tok/s 105472 (101074)	Loss/tok 3.3856 (3.4163)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.107 (0.142)	Data 1.42e-04 (3.42e-04)	Tok/s 95244 (101095)	Loss/tok 3.1906 (3.4166)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.107 (0.142)	Data 1.35e-04 (3.40e-04)	Tok/s 99051 (101094)	Loss/tok 3.1272 (3.4154)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.159 (0.142)	Data 1.38e-04 (3.39e-04)	Tok/s 106627 (101083)	Loss/tok 3.4269 (3.4142)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.160 (0.142)	Data 1.44e-04 (3.37e-04)	Tok/s 104902 (101072)	Loss/tok 3.3228 (3.4139)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.212 (0.142)	Data 1.44e-04 (3.36e-04)	Tok/s 109090 (101073)	Loss/tok 3.6006 (3.4139)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.35e-04)	Tok/s 95417 (101070)	Loss/tok 3.1609 (3.4135)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1440/1938]	Time 0.106 (0.142)	Data 1.49e-04 (3.33e-04)	Tok/s 97303 (101049)	Loss/tok 3.1232 (3.4126)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.159 (0.142)	Data 1.59e-04 (3.32e-04)	Tok/s 107799 (101062)	Loss/tok 3.3780 (3.4132)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.272 (0.142)	Data 1.46e-04 (3.31e-04)	Tok/s 108839 (101056)	Loss/tok 3.7639 (3.4130)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1470/1938]	Time 0.107 (0.142)	Data 1.52e-04 (3.30e-04)	Tok/s 95443 (101043)	Loss/tok 3.2301 (3.4125)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.108 (0.142)	Data 1.27e-04 (3.28e-04)	Tok/s 96746 (101030)	Loss/tok 3.2523 (3.4117)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.213 (0.142)	Data 1.50e-04 (3.27e-04)	Tok/s 109572 (101028)	Loss/tok 3.4853 (3.4115)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.213 (0.142)	Data 1.59e-04 (3.26e-04)	Tok/s 109058 (101038)	Loss/tok 3.5122 (3.4115)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.272 (0.142)	Data 1.42e-04 (3.25e-04)	Tok/s 109617 (101063)	Loss/tok 3.7300 (3.4119)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.160 (0.142)	Data 1.45e-04 (3.24e-04)	Tok/s 104226 (101044)	Loss/tok 3.4473 (3.4110)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.160 (0.142)	Data 1.66e-04 (3.23e-04)	Tok/s 104967 (100992)	Loss/tok 3.3889 (3.4096)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.107 (0.142)	Data 2.12e-04 (3.21e-04)	Tok/s 95260 (100990)	Loss/tok 3.1739 (3.4091)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.059 (0.142)	Data 1.62e-04 (3.20e-04)	Tok/s 88317 (100994)	Loss/tok 2.7123 (3.4086)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.158 (0.142)	Data 1.46e-04 (3.19e-04)	Tok/s 107348 (100995)	Loss/tok 3.4317 (3.4081)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.107 (0.142)	Data 1.25e-04 (3.18e-04)	Tok/s 97619 (100995)	Loss/tok 3.2098 (3.4077)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1580/1938]	Time 0.107 (0.142)	Data 1.54e-04 (3.17e-04)	Tok/s 94403 (100996)	Loss/tok 3.1722 (3.4077)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.160 (0.142)	Data 1.59e-04 (3.16e-04)	Tok/s 104409 (101006)	Loss/tok 3.4670 (3.4073)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.160 (0.142)	Data 1.55e-04 (3.15e-04)	Tok/s 105409 (101023)	Loss/tok 3.3025 (3.4072)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.107 (0.142)	Data 1.54e-04 (3.14e-04)	Tok/s 95969 (101021)	Loss/tok 3.1099 (3.4072)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.106 (0.142)	Data 1.37e-04 (3.13e-04)	Tok/s 100040 (101006)	Loss/tok 3.3143 (3.4072)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.159 (0.142)	Data 1.24e-04 (3.12e-04)	Tok/s 107726 (101002)	Loss/tok 3.3797 (3.4066)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.107 (0.142)	Data 1.42e-04 (3.11e-04)	Tok/s 98717 (101004)	Loss/tok 3.1398 (3.4062)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.212 (0.142)	Data 1.43e-04 (3.10e-04)	Tok/s 109428 (101010)	Loss/tok 3.6506 (3.4057)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.212 (0.142)	Data 1.55e-04 (3.09e-04)	Tok/s 110041 (101021)	Loss/tok 3.4610 (3.4057)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.058 (0.142)	Data 1.60e-04 (3.08e-04)	Tok/s 89496 (101011)	Loss/tok 2.6802 (3.4049)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.160 (0.142)	Data 1.25e-04 (3.07e-04)	Tok/s 105468 (101012)	Loss/tok 3.2677 (3.4043)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.107 (0.142)	Data 1.59e-04 (3.06e-04)	Tok/s 96158 (101005)	Loss/tok 3.2295 (3.4037)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.160 (0.142)	Data 1.46e-04 (3.05e-04)	Tok/s 105049 (101000)	Loss/tok 3.2956 (3.4037)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1710/1938]	Time 0.159 (0.142)	Data 1.93e-04 (3.05e-04)	Tok/s 105183 (100981)	Loss/tok 3.4453 (3.4030)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.159 (0.141)	Data 1.44e-04 (3.04e-04)	Tok/s 103887 (100975)	Loss/tok 3.3467 (3.4023)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.272 (0.141)	Data 2.06e-04 (3.03e-04)	Tok/s 109859 (100978)	Loss/tok 3.5973 (3.4018)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.159 (0.141)	Data 1.51e-04 (3.02e-04)	Tok/s 105518 (100990)	Loss/tok 3.2123 (3.4014)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.213 (0.142)	Data 1.30e-04 (3.01e-04)	Tok/s 109702 (100997)	Loss/tok 3.5784 (3.4015)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.213 (0.141)	Data 1.41e-04 (3.00e-04)	Tok/s 110384 (100994)	Loss/tok 3.3682 (3.4009)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.274 (0.142)	Data 1.72e-04 (2.99e-04)	Tok/s 109305 (101026)	Loss/tok 3.7476 (3.4016)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.107 (0.142)	Data 1.44e-04 (2.98e-04)	Tok/s 96223 (101040)	Loss/tok 3.2232 (3.4018)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.159 (0.142)	Data 1.56e-04 (2.98e-04)	Tok/s 104969 (101043)	Loss/tok 3.4144 (3.4018)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.159 (0.142)	Data 1.40e-04 (2.97e-04)	Tok/s 104390 (101030)	Loss/tok 3.4350 (3.4010)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.212 (0.142)	Data 1.39e-04 (2.96e-04)	Tok/s 109662 (101024)	Loss/tok 3.4518 (3.4005)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.159 (0.142)	Data 1.37e-04 (2.95e-04)	Tok/s 106700 (101037)	Loss/tok 3.1368 (3.4001)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1830/1938]	Time 0.160 (0.142)	Data 1.96e-04 (2.94e-04)	Tok/s 106002 (101026)	Loss/tok 3.2335 (3.3994)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.107 (0.142)	Data 1.59e-04 (2.94e-04)	Tok/s 95681 (101016)	Loss/tok 3.2318 (3.3990)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.160 (0.142)	Data 1.44e-04 (2.93e-04)	Tok/s 104034 (101004)	Loss/tok 3.3755 (3.3988)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.108 (0.142)	Data 1.46e-04 (2.92e-04)	Tok/s 94663 (101014)	Loss/tok 3.0962 (3.3987)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.059 (0.142)	Data 1.70e-04 (2.91e-04)	Tok/s 88383 (101009)	Loss/tok 2.7258 (3.3981)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1880/1938]	Time 0.107 (0.142)	Data 1.76e-04 (2.91e-04)	Tok/s 96401 (101007)	Loss/tok 3.1480 (3.3978)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.159 (0.142)	Data 1.23e-04 (2.90e-04)	Tok/s 105561 (101009)	Loss/tok 3.3684 (3.3975)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.214 (0.142)	Data 1.42e-04 (2.89e-04)	Tok/s 108994 (101007)	Loss/tok 3.4740 (3.3974)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.213 (0.142)	Data 1.69e-04 (2.89e-04)	Tok/s 109440 (101018)	Loss/tok 3.3993 (3.3972)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.212 (0.142)	Data 1.75e-04 (2.88e-04)	Tok/s 110724 (101009)	Loss/tok 3.4935 (3.3965)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.106 (0.142)	Data 1.44e-04 (2.87e-04)	Tok/s 96156 (101007)	Loss/tok 3.1251 (3.3960)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019738765, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019738766, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.614 (0.614)	Decoder iters 108.0 (108.0)	Tok/s 26293 (26293)
0: Running moses detokenizer
0: BLEU(score=22.380222474967372, counts=[35880, 17313, 9610, 5544], totals=[64863, 61860, 58857, 55861], precisions=[55.31659035197262, 27.987390882638216, 16.327709533275566, 9.924634360287142], bp=1.0, sys_len=64863, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019740588, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2238, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019740589, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3956	Test BLEU: 22.38
0: Performance: Epoch: 1	Training: 807953 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019740589, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019740590, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019740590, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3803618023
0: TRAIN [2][0/1938]	Time 0.363 (0.363)	Data 2.57e-01 (2.57e-01)	Tok/s 28725 (28725)	Loss/tok 3.2533 (3.2533)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.161 (0.165)	Data 1.24e-04 (2.35e-02)	Tok/s 104192 (94315)	Loss/tok 3.0739 (3.2637)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.108 (0.147)	Data 1.24e-04 (1.24e-02)	Tok/s 95256 (96832)	Loss/tok 2.9826 (3.2259)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.159 (0.155)	Data 1.75e-04 (8.43e-03)	Tok/s 105238 (99381)	Loss/tok 3.3335 (3.2749)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.106 (0.150)	Data 1.98e-04 (6.41e-03)	Tok/s 99020 (99409)	Loss/tok 2.9853 (3.2765)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.160 (0.145)	Data 1.49e-04 (5.18e-03)	Tok/s 104881 (99289)	Loss/tok 3.2765 (3.2599)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.111 (0.148)	Data 1.44e-04 (4.36e-03)	Tok/s 91843 (100003)	Loss/tok 3.1048 (3.2706)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][70/1938]	Time 0.107 (0.148)	Data 1.64e-04 (3.77e-03)	Tok/s 95881 (100155)	Loss/tok 3.0064 (3.2765)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.107 (0.146)	Data 1.62e-04 (3.32e-03)	Tok/s 94309 (100209)	Loss/tok 3.0557 (3.2707)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.159 (0.147)	Data 2.85e-04 (2.98e-03)	Tok/s 106970 (100531)	Loss/tok 3.2095 (3.2777)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][100/1938]	Time 0.272 (0.150)	Data 1.45e-04 (2.70e-03)	Tok/s 109246 (100692)	Loss/tok 3.6830 (3.2972)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][110/1938]	Time 0.159 (0.150)	Data 1.29e-04 (2.47e-03)	Tok/s 105333 (100794)	Loss/tok 3.1656 (3.2956)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.107 (0.149)	Data 1.57e-04 (2.28e-03)	Tok/s 97591 (100671)	Loss/tok 3.0962 (3.2933)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.059 (0.147)	Data 1.19e-04 (2.11e-03)	Tok/s 87954 (100636)	Loss/tok 2.6824 (3.2867)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.159 (0.147)	Data 1.22e-04 (1.97e-03)	Tok/s 104918 (100661)	Loss/tok 3.2601 (3.2811)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.212 (0.148)	Data 1.32e-04 (1.85e-03)	Tok/s 110476 (100886)	Loss/tok 3.4816 (3.2851)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.107 (0.148)	Data 1.57e-04 (1.75e-03)	Tok/s 98175 (100905)	Loss/tok 3.1030 (3.2854)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.107 (0.146)	Data 1.58e-04 (1.65e-03)	Tok/s 97598 (100832)	Loss/tok 3.0974 (3.2799)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.107 (0.145)	Data 1.47e-04 (1.57e-03)	Tok/s 96337 (100761)	Loss/tok 3.0695 (3.2740)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.107 (0.144)	Data 1.41e-04 (1.50e-03)	Tok/s 95983 (100759)	Loss/tok 3.1202 (3.2692)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.158 (0.144)	Data 1.25e-04 (1.43e-03)	Tok/s 105860 (100814)	Loss/tok 3.2399 (3.2682)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.160 (0.144)	Data 1.47e-04 (1.37e-03)	Tok/s 104938 (100797)	Loss/tok 3.3297 (3.2660)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.212 (0.144)	Data 1.38e-04 (1.31e-03)	Tok/s 111838 (100984)	Loss/tok 3.3386 (3.2665)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.159 (0.144)	Data 1.18e-04 (1.26e-03)	Tok/s 106130 (100929)	Loss/tok 3.2836 (3.2665)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][240/1938]	Time 0.059 (0.143)	Data 1.44e-04 (1.22e-03)	Tok/s 90767 (100795)	Loss/tok 2.6325 (3.2636)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.059 (0.143)	Data 1.31e-04 (1.17e-03)	Tok/s 90059 (100767)	Loss/tok 2.6720 (3.2638)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.160 (0.142)	Data 1.32e-04 (1.13e-03)	Tok/s 105273 (100707)	Loss/tok 3.2682 (3.2616)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.107 (0.141)	Data 1.60e-04 (1.10e-03)	Tok/s 96581 (100548)	Loss/tok 3.0808 (3.2577)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.107 (0.142)	Data 1.60e-04 (1.06e-03)	Tok/s 96221 (100608)	Loss/tok 2.9435 (3.2609)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.107 (0.141)	Data 1.32e-04 (1.03e-03)	Tok/s 98408 (100644)	Loss/tok 3.0111 (3.2578)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.211 (0.141)	Data 1.41e-04 (1.00e-03)	Tok/s 110544 (100658)	Loss/tok 3.5317 (3.2577)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.272 (0.142)	Data 1.48e-04 (9.76e-04)	Tok/s 108126 (100783)	Loss/tok 3.6685 (3.2618)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.108 (0.141)	Data 1.39e-04 (9.50e-04)	Tok/s 96753 (100637)	Loss/tok 3.0552 (3.2574)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.107 (0.140)	Data 1.40e-04 (9.26e-04)	Tok/s 95871 (100568)	Loss/tok 2.9927 (3.2545)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.059 (0.140)	Data 1.48e-04 (9.03e-04)	Tok/s 89318 (100577)	Loss/tok 2.6434 (3.2564)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.159 (0.140)	Data 1.40e-04 (8.81e-04)	Tok/s 105391 (100500)	Loss/tok 3.3402 (3.2539)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][360/1938]	Time 0.160 (0.139)	Data 2.21e-04 (8.61e-04)	Tok/s 104235 (100438)	Loss/tok 3.2378 (3.2509)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.107 (0.140)	Data 1.60e-04 (8.42e-04)	Tok/s 96880 (100547)	Loss/tok 3.1125 (3.2577)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.106 (0.140)	Data 1.25e-04 (8.23e-04)	Tok/s 94250 (100590)	Loss/tok 3.1067 (3.2574)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.107 (0.139)	Data 1.42e-04 (8.06e-04)	Tok/s 96449 (100484)	Loss/tok 3.1200 (3.2547)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.159 (0.139)	Data 1.47e-04 (7.90e-04)	Tok/s 104750 (100476)	Loss/tok 3.2850 (3.2545)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.108 (0.139)	Data 1.34e-04 (7.74e-04)	Tok/s 97337 (100511)	Loss/tok 3.0743 (3.2543)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][420/1938]	Time 0.107 (0.139)	Data 1.14e-04 (7.59e-04)	Tok/s 93851 (100416)	Loss/tok 3.0702 (3.2541)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.274 (0.140)	Data 1.72e-04 (7.45e-04)	Tok/s 108058 (100504)	Loss/tok 3.6311 (3.2603)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.107 (0.140)	Data 1.15e-04 (7.32e-04)	Tok/s 96581 (100510)	Loss/tok 3.0182 (3.2621)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.107 (0.139)	Data 1.34e-04 (7.19e-04)	Tok/s 95139 (100431)	Loss/tok 3.0293 (3.2589)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.212 (0.140)	Data 1.90e-04 (7.06e-04)	Tok/s 111273 (100487)	Loss/tok 3.4076 (3.2620)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.106 (0.140)	Data 1.25e-04 (6.94e-04)	Tok/s 94610 (100442)	Loss/tok 3.0435 (3.2607)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.107 (0.140)	Data 1.08e-04 (6.83e-04)	Tok/s 97394 (100449)	Loss/tok 3.0286 (3.2600)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.107 (0.140)	Data 1.31e-04 (6.72e-04)	Tok/s 96819 (100486)	Loss/tok 3.0341 (3.2609)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.160 (0.139)	Data 1.36e-04 (6.62e-04)	Tok/s 106117 (100413)	Loss/tok 3.2319 (3.2587)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.107 (0.139)	Data 1.39e-04 (6.51e-04)	Tok/s 98001 (100379)	Loss/tok 3.0764 (3.2567)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.274 (0.140)	Data 1.71e-04 (6.42e-04)	Tok/s 108807 (100383)	Loss/tok 3.7383 (3.2601)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.160 (0.139)	Data 1.30e-04 (6.32e-04)	Tok/s 104769 (100371)	Loss/tok 3.2864 (3.2593)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.108 (0.139)	Data 1.67e-04 (6.23e-04)	Tok/s 95570 (100384)	Loss/tok 3.0149 (3.2587)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][550/1938]	Time 0.106 (0.140)	Data 1.54e-04 (6.15e-04)	Tok/s 96683 (100432)	Loss/tok 3.0444 (3.2611)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.160 (0.140)	Data 1.51e-04 (6.07e-04)	Tok/s 106236 (100448)	Loss/tok 3.1852 (3.2607)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.108 (0.140)	Data 1.24e-04 (5.99e-04)	Tok/s 96240 (100508)	Loss/tok 3.0112 (3.2625)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.160 (0.141)	Data 1.13e-04 (5.91e-04)	Tok/s 105344 (100571)	Loss/tok 3.1883 (3.2637)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.159 (0.141)	Data 1.30e-04 (5.84e-04)	Tok/s 106492 (100568)	Loss/tok 3.3220 (3.2632)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.059 (0.141)	Data 1.39e-04 (5.76e-04)	Tok/s 89738 (100591)	Loss/tok 2.8281 (3.2637)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.059 (0.140)	Data 1.30e-04 (5.69e-04)	Tok/s 89622 (100554)	Loss/tok 2.5880 (3.2624)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][620/1938]	Time 0.108 (0.140)	Data 1.28e-04 (5.62e-04)	Tok/s 95559 (100548)	Loss/tok 3.1148 (3.2620)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.272 (0.141)	Data 1.31e-04 (5.56e-04)	Tok/s 109069 (100589)	Loss/tok 3.5770 (3.2623)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.274 (0.141)	Data 1.53e-04 (5.49e-04)	Tok/s 110593 (100597)	Loss/tok 3.6310 (3.2629)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.160 (0.141)	Data 1.35e-04 (5.43e-04)	Tok/s 104065 (100658)	Loss/tok 3.2699 (3.2635)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.106 (0.141)	Data 1.34e-04 (5.37e-04)	Tok/s 95991 (100617)	Loss/tok 2.9073 (3.2612)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.058 (0.140)	Data 1.90e-04 (5.32e-04)	Tok/s 91255 (100617)	Loss/tok 2.6822 (3.2604)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.107 (0.140)	Data 1.29e-04 (5.26e-04)	Tok/s 96401 (100629)	Loss/tok 3.0077 (3.2604)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.107 (0.140)	Data 1.26e-04 (5.20e-04)	Tok/s 97848 (100650)	Loss/tok 3.0976 (3.2598)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.107 (0.140)	Data 1.10e-04 (5.15e-04)	Tok/s 95409 (100640)	Loss/tok 3.1731 (3.2595)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.159 (0.141)	Data 1.45e-04 (5.10e-04)	Tok/s 105161 (100705)	Loss/tok 3.1649 (3.2618)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.106 (0.141)	Data 1.39e-04 (5.05e-04)	Tok/s 97457 (100698)	Loss/tok 3.0115 (3.2631)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.107 (0.141)	Data 1.41e-04 (5.00e-04)	Tok/s 94923 (100719)	Loss/tok 3.1775 (3.2646)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.107 (0.141)	Data 1.68e-04 (4.95e-04)	Tok/s 97762 (100731)	Loss/tok 3.1357 (3.2656)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][750/1938]	Time 0.160 (0.141)	Data 1.28e-04 (4.91e-04)	Tok/s 106123 (100750)	Loss/tok 3.2322 (3.2652)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.213 (0.141)	Data 1.31e-04 (4.86e-04)	Tok/s 108989 (100746)	Loss/tok 3.3867 (3.2647)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.107 (0.141)	Data 1.32e-04 (4.82e-04)	Tok/s 96230 (100731)	Loss/tok 3.1394 (3.2642)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.159 (0.141)	Data 1.32e-04 (4.77e-04)	Tok/s 106654 (100730)	Loss/tok 3.1851 (3.2650)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.274 (0.141)	Data 1.41e-04 (4.73e-04)	Tok/s 110057 (100747)	Loss/tok 3.6220 (3.2654)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.107 (0.141)	Data 1.55e-04 (4.69e-04)	Tok/s 94117 (100736)	Loss/tok 3.0001 (3.2647)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.160 (0.141)	Data 1.42e-04 (4.65e-04)	Tok/s 104676 (100732)	Loss/tok 3.2395 (3.2636)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.212 (0.141)	Data 1.35e-04 (4.61e-04)	Tok/s 112881 (100738)	Loss/tok 3.3106 (3.2634)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.107 (0.141)	Data 1.67e-04 (4.58e-04)	Tok/s 94822 (100735)	Loss/tok 3.0998 (3.2628)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.059 (0.141)	Data 1.93e-04 (4.54e-04)	Tok/s 90860 (100735)	Loss/tok 2.5455 (3.2616)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.107 (0.141)	Data 1.33e-04 (4.51e-04)	Tok/s 98213 (100744)	Loss/tok 3.0832 (3.2616)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.108 (0.141)	Data 1.78e-04 (4.47e-04)	Tok/s 94883 (100734)	Loss/tok 3.0396 (3.2606)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][870/1938]	Time 0.160 (0.141)	Data 2.08e-04 (4.44e-04)	Tok/s 104938 (100743)	Loss/tok 3.3156 (3.2601)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.107 (0.141)	Data 1.34e-04 (4.40e-04)	Tok/s 95437 (100750)	Loss/tok 3.0026 (3.2598)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][890/1938]	Time 0.160 (0.141)	Data 1.36e-04 (4.37e-04)	Tok/s 106312 (100761)	Loss/tok 3.1926 (3.2599)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.107 (0.141)	Data 1.44e-04 (4.34e-04)	Tok/s 96934 (100762)	Loss/tok 2.9855 (3.2592)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.106 (0.141)	Data 1.20e-04 (4.31e-04)	Tok/s 96425 (100754)	Loss/tok 3.0023 (3.2595)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.160 (0.140)	Data 1.41e-04 (4.27e-04)	Tok/s 106206 (100740)	Loss/tok 3.2600 (3.2588)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.160 (0.140)	Data 1.77e-04 (4.24e-04)	Tok/s 105550 (100735)	Loss/tok 3.1992 (3.2581)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.212 (0.140)	Data 1.28e-04 (4.21e-04)	Tok/s 109162 (100740)	Loss/tok 3.4782 (3.2580)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.159 (0.141)	Data 1.37e-04 (4.18e-04)	Tok/s 105444 (100783)	Loss/tok 3.2282 (3.2596)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.160 (0.141)	Data 1.45e-04 (4.16e-04)	Tok/s 105436 (100769)	Loss/tok 3.2620 (3.2588)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.272 (0.141)	Data 1.77e-04 (4.13e-04)	Tok/s 110035 (100801)	Loss/tok 3.5440 (3.2600)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.107 (0.141)	Data 1.98e-04 (4.10e-04)	Tok/s 94684 (100813)	Loss/tok 3.0302 (3.2598)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.161 (0.141)	Data 1.28e-04 (4.07e-04)	Tok/s 104161 (100809)	Loss/tok 3.2840 (3.2599)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.059 (0.141)	Data 1.83e-04 (4.05e-04)	Tok/s 90629 (100779)	Loss/tok 2.6823 (3.2593)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.059 (0.141)	Data 1.99e-04 (4.03e-04)	Tok/s 90169 (100798)	Loss/tok 2.7322 (3.2602)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1020/1938]	Time 0.213 (0.141)	Data 1.81e-04 (4.01e-04)	Tok/s 109043 (100818)	Loss/tok 3.5526 (3.2604)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.160 (0.141)	Data 1.68e-04 (3.98e-04)	Tok/s 106252 (100841)	Loss/tok 3.3303 (3.2615)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.107 (0.141)	Data 1.82e-04 (3.96e-04)	Tok/s 95979 (100869)	Loss/tok 3.0549 (3.2622)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.107 (0.141)	Data 1.70e-04 (3.94e-04)	Tok/s 94752 (100858)	Loss/tok 3.0753 (3.2618)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.108 (0.141)	Data 1.69e-04 (3.92e-04)	Tok/s 95802 (100856)	Loss/tok 3.0205 (3.2615)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.108 (0.141)	Data 2.51e-04 (3.90e-04)	Tok/s 96592 (100836)	Loss/tok 2.9315 (3.2614)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.059 (0.141)	Data 1.83e-04 (3.88e-04)	Tok/s 89745 (100819)	Loss/tok 2.5729 (3.2607)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.160 (0.141)	Data 1.86e-04 (3.86e-04)	Tok/s 104947 (100813)	Loss/tok 3.3650 (3.2613)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.107 (0.141)	Data 1.88e-04 (3.84e-04)	Tok/s 95926 (100802)	Loss/tok 3.0460 (3.2608)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.159 (0.141)	Data 2.11e-04 (3.82e-04)	Tok/s 105653 (100809)	Loss/tok 3.3427 (3.2605)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.107 (0.141)	Data 1.72e-04 (3.80e-04)	Tok/s 95618 (100816)	Loss/tok 3.0201 (3.2606)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1130/1938]	Time 0.107 (0.141)	Data 2.05e-04 (3.78e-04)	Tok/s 96390 (100816)	Loss/tok 3.0368 (3.2610)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.107 (0.141)	Data 1.64e-04 (3.76e-04)	Tok/s 95506 (100799)	Loss/tok 3.0746 (3.2603)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.107 (0.141)	Data 1.41e-04 (3.75e-04)	Tok/s 95296 (100778)	Loss/tok 2.9016 (3.2596)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.107 (0.141)	Data 1.81e-04 (3.73e-04)	Tok/s 96384 (100761)	Loss/tok 2.9406 (3.2587)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.160 (0.141)	Data 1.49e-04 (3.71e-04)	Tok/s 103896 (100776)	Loss/tok 3.1236 (3.2586)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.159 (0.141)	Data 1.36e-04 (3.69e-04)	Tok/s 105693 (100791)	Loss/tok 3.2404 (3.2598)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.059 (0.141)	Data 1.51e-04 (3.68e-04)	Tok/s 90172 (100786)	Loss/tok 2.5333 (3.2588)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.107 (0.141)	Data 1.67e-04 (3.66e-04)	Tok/s 93852 (100759)	Loss/tok 3.0257 (3.2583)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.107 (0.140)	Data 2.07e-04 (3.64e-04)	Tok/s 94710 (100735)	Loss/tok 3.0028 (3.2571)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.212 (0.141)	Data 1.67e-04 (3.63e-04)	Tok/s 110046 (100752)	Loss/tok 3.4808 (3.2581)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.107 (0.141)	Data 1.97e-04 (3.61e-04)	Tok/s 96340 (100759)	Loss/tok 3.0013 (3.2582)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.213 (0.141)	Data 1.57e-04 (3.60e-04)	Tok/s 109142 (100769)	Loss/tok 3.3355 (3.2582)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1250/1938]	Time 0.107 (0.141)	Data 1.69e-04 (3.58e-04)	Tok/s 95944 (100765)	Loss/tok 3.0216 (3.2586)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.107 (0.141)	Data 1.45e-04 (3.57e-04)	Tok/s 98821 (100795)	Loss/tok 2.8922 (3.2597)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1270/1938]	Time 0.107 (0.141)	Data 1.77e-04 (3.55e-04)	Tok/s 97057 (100768)	Loss/tok 3.1477 (3.2590)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.211 (0.141)	Data 1.71e-04 (3.54e-04)	Tok/s 111554 (100777)	Loss/tok 3.2968 (3.2595)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.061 (0.141)	Data 2.22e-04 (3.53e-04)	Tok/s 86962 (100791)	Loss/tok 2.6880 (3.2607)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.059 (0.141)	Data 1.51e-04 (3.51e-04)	Tok/s 88625 (100803)	Loss/tok 2.6609 (3.2613)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.212 (0.141)	Data 1.62e-04 (3.50e-04)	Tok/s 109074 (100801)	Loss/tok 3.4343 (3.2610)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.160 (0.141)	Data 1.46e-04 (3.48e-04)	Tok/s 105263 (100816)	Loss/tok 3.1938 (3.2608)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.160 (0.141)	Data 1.70e-04 (3.47e-04)	Tok/s 104718 (100839)	Loss/tok 3.2609 (3.2609)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.160 (0.142)	Data 1.26e-04 (3.46e-04)	Tok/s 104257 (100873)	Loss/tok 3.3710 (3.2619)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.161 (0.142)	Data 1.58e-04 (3.44e-04)	Tok/s 102927 (100862)	Loss/tok 3.2821 (3.2613)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.107 (0.142)	Data 1.53e-04 (3.43e-04)	Tok/s 93399 (100874)	Loss/tok 3.1985 (3.2611)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.108 (0.142)	Data 1.66e-04 (3.42e-04)	Tok/s 95312 (100851)	Loss/tok 3.1704 (3.2606)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.213 (0.142)	Data 1.64e-04 (3.41e-04)	Tok/s 108886 (100852)	Loss/tok 3.5001 (3.2603)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.106 (0.142)	Data 1.82e-04 (3.39e-04)	Tok/s 96384 (100852)	Loss/tok 2.9814 (3.2604)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1400/1938]	Time 0.107 (0.142)	Data 1.43e-04 (3.38e-04)	Tok/s 96562 (100865)	Loss/tok 3.0729 (3.2600)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.107 (0.142)	Data 1.24e-04 (3.37e-04)	Tok/s 95726 (100868)	Loss/tok 3.0374 (3.2603)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.108 (0.142)	Data 2.04e-04 (3.36e-04)	Tok/s 94340 (100860)	Loss/tok 3.0284 (3.2602)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.059 (0.142)	Data 1.47e-04 (3.35e-04)	Tok/s 89108 (100853)	Loss/tok 2.6274 (3.2603)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.160 (0.142)	Data 1.47e-04 (3.34e-04)	Tok/s 104322 (100855)	Loss/tok 3.2100 (3.2602)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.160 (0.142)	Data 1.47e-04 (3.32e-04)	Tok/s 106565 (100844)	Loss/tok 3.3271 (3.2599)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1460/1938]	Time 0.107 (0.142)	Data 1.92e-04 (3.31e-04)	Tok/s 97719 (100850)	Loss/tok 3.0424 (3.2601)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.059 (0.141)	Data 1.67e-04 (3.30e-04)	Tok/s 89921 (100825)	Loss/tok 2.6319 (3.2594)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.161 (0.141)	Data 2.34e-04 (3.29e-04)	Tok/s 104109 (100821)	Loss/tok 3.2130 (3.2589)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.107 (0.141)	Data 1.48e-04 (3.28e-04)	Tok/s 96553 (100802)	Loss/tok 3.0542 (3.2583)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.106 (0.141)	Data 1.66e-04 (3.27e-04)	Tok/s 96810 (100779)	Loss/tok 3.0722 (3.2574)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.159 (0.141)	Data 1.57e-04 (3.26e-04)	Tok/s 107439 (100787)	Loss/tok 3.2339 (3.2573)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.107 (0.141)	Data 1.46e-04 (3.25e-04)	Tok/s 97233 (100785)	Loss/tok 3.0382 (3.2574)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.107 (0.141)	Data 1.71e-04 (3.24e-04)	Tok/s 97771 (100786)	Loss/tok 2.9106 (3.2572)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.159 (0.141)	Data 2.01e-04 (3.23e-04)	Tok/s 104585 (100801)	Loss/tok 3.2075 (3.2571)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.107 (0.141)	Data 1.78e-04 (3.22e-04)	Tok/s 97998 (100787)	Loss/tok 3.1185 (3.2567)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.159 (0.141)	Data 1.73e-04 (3.21e-04)	Tok/s 106655 (100804)	Loss/tok 3.2403 (3.2565)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.108 (0.141)	Data 1.43e-04 (3.20e-04)	Tok/s 93515 (100792)	Loss/tok 2.9801 (3.2564)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.160 (0.141)	Data 1.61e-04 (3.19e-04)	Tok/s 104788 (100793)	Loss/tok 3.2895 (3.2563)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1590/1938]	Time 0.160 (0.141)	Data 1.80e-04 (3.18e-04)	Tok/s 104985 (100796)	Loss/tok 3.2313 (3.2559)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.107 (0.141)	Data 1.50e-04 (3.17e-04)	Tok/s 95097 (100806)	Loss/tok 3.0191 (3.2559)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.160 (0.141)	Data 1.34e-04 (3.16e-04)	Tok/s 105314 (100822)	Loss/tok 3.2903 (3.2561)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1620/1938]	Time 0.213 (0.141)	Data 1.94e-04 (3.15e-04)	Tok/s 110213 (100830)	Loss/tok 3.4656 (3.2567)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.108 (0.141)	Data 1.25e-04 (3.15e-04)	Tok/s 95121 (100803)	Loss/tok 3.0097 (3.2562)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.160 (0.141)	Data 1.63e-04 (3.14e-04)	Tok/s 104161 (100805)	Loss/tok 3.1879 (3.2564)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.108 (0.141)	Data 2.14e-04 (3.13e-04)	Tok/s 97228 (100810)	Loss/tok 3.0941 (3.2563)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.161 (0.141)	Data 2.47e-04 (3.12e-04)	Tok/s 106682 (100831)	Loss/tok 3.1499 (3.2560)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.108 (0.141)	Data 1.98e-04 (3.11e-04)	Tok/s 95229 (100835)	Loss/tok 3.1031 (3.2560)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.161 (0.141)	Data 1.72e-04 (3.10e-04)	Tok/s 105703 (100828)	Loss/tok 3.1572 (3.2554)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.214 (0.141)	Data 1.66e-04 (3.10e-04)	Tok/s 108767 (100847)	Loss/tok 3.4504 (3.2556)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.060 (0.141)	Data 1.95e-04 (3.09e-04)	Tok/s 89068 (100831)	Loss/tok 2.6229 (3.2552)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.107 (0.141)	Data 1.45e-04 (3.08e-04)	Tok/s 96332 (100842)	Loss/tok 2.9994 (3.2559)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.159 (0.141)	Data 1.45e-04 (3.07e-04)	Tok/s 105211 (100843)	Loss/tok 3.3183 (3.2557)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.161 (0.141)	Data 1.77e-04 (3.06e-04)	Tok/s 103950 (100845)	Loss/tok 3.3149 (3.2559)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.161 (0.141)	Data 2.73e-04 (3.06e-04)	Tok/s 102911 (100846)	Loss/tok 3.2511 (3.2560)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1750/1938]	Time 0.214 (0.141)	Data 2.41e-04 (3.05e-04)	Tok/s 107964 (100841)	Loss/tok 3.4842 (3.2557)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.108 (0.141)	Data 2.46e-04 (3.04e-04)	Tok/s 94423 (100834)	Loss/tok 2.9545 (3.2557)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.211 (0.142)	Data 1.73e-04 (3.04e-04)	Tok/s 109894 (100876)	Loss/tok 3.4395 (3.2574)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.159 (0.142)	Data 1.95e-04 (3.03e-04)	Tok/s 106722 (100897)	Loss/tok 3.2461 (3.2578)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.160 (0.142)	Data 1.83e-04 (3.02e-04)	Tok/s 102104 (100905)	Loss/tok 3.3035 (3.2576)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.161 (0.142)	Data 2.61e-04 (3.01e-04)	Tok/s 103695 (100907)	Loss/tok 3.1797 (3.2577)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1810/1938]	Time 0.159 (0.142)	Data 1.70e-04 (3.01e-04)	Tok/s 106364 (100912)	Loss/tok 3.2450 (3.2584)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.107 (0.142)	Data 1.75e-04 (3.00e-04)	Tok/s 97316 (100911)	Loss/tok 3.0644 (3.2581)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.160 (0.142)	Data 1.76e-04 (3.00e-04)	Tok/s 104968 (100892)	Loss/tok 3.2329 (3.2577)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.107 (0.142)	Data 1.31e-04 (2.99e-04)	Tok/s 98357 (100898)	Loss/tok 3.1018 (3.2574)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.108 (0.142)	Data 1.54e-04 (2.98e-04)	Tok/s 95914 (100893)	Loss/tok 3.0057 (3.2575)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.272 (0.142)	Data 2.39e-04 (2.98e-04)	Tok/s 108876 (100876)	Loss/tok 3.7444 (3.2577)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.107 (0.142)	Data 2.18e-04 (2.97e-04)	Tok/s 95435 (100868)	Loss/tok 3.0731 (3.2578)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.160 (0.142)	Data 1.27e-04 (2.97e-04)	Tok/s 105030 (100859)	Loss/tok 3.1785 (3.2572)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.059 (0.142)	Data 1.62e-04 (2.96e-04)	Tok/s 86433 (100853)	Loss/tok 2.5866 (3.2574)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.059 (0.142)	Data 1.99e-04 (2.95e-04)	Tok/s 90025 (100845)	Loss/tok 2.7221 (3.2570)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.107 (0.142)	Data 1.92e-04 (2.95e-04)	Tok/s 94333 (100845)	Loss/tok 3.0037 (3.2565)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.059 (0.142)	Data 1.42e-04 (2.94e-04)	Tok/s 90297 (100855)	Loss/tok 2.6709 (3.2569)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.271 (0.142)	Data 1.58e-04 (2.93e-04)	Tok/s 110139 (100861)	Loss/tok 3.5057 (3.2570)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
:::MLLOG {"namespace": "", "time_ms": 1593020015981, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020015982, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.662 (0.662)	Decoder iters 117.0 (117.0)	Tok/s 24837 (24837)
0: Running moses detokenizer
0: BLEU(score=23.09710184944609, counts=[36412, 17991, 10046, 5801], totals=[65115, 62112, 59109, 56112], precisions=[55.919526990708746, 28.965417310664606, 16.995719771946742, 10.338252067293983], bp=1.0, sys_len=65115, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020017953, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.231, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020017953, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2556	Test BLEU: 23.10
0: Performance: Epoch: 2	Training: 806787 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593020017954, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020017954, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593020017954, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 840514017
0: TRAIN [3][0/1938]	Time 0.364 (0.364)	Data 2.58e-01 (2.58e-01)	Tok/s 28021 (28021)	Loss/tok 2.9765 (2.9765)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.108 (0.184)	Data 2.12e-04 (2.36e-02)	Tok/s 98443 (96576)	Loss/tok 3.0395 (3.2352)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.109 (0.160)	Data 1.39e-04 (1.24e-02)	Tok/s 95360 (98147)	Loss/tok 2.9659 (3.1670)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.214 (0.155)	Data 1.74e-04 (8.49e-03)	Tok/s 109629 (99314)	Loss/tok 3.2845 (3.1482)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][40/1938]	Time 0.213 (0.154)	Data 1.90e-04 (6.47e-03)	Tok/s 108713 (99556)	Loss/tok 3.2293 (3.1681)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.159 (0.156)	Data 1.65e-04 (5.23e-03)	Tok/s 106207 (100154)	Loss/tok 3.1337 (3.1848)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.059 (0.152)	Data 1.89e-04 (4.40e-03)	Tok/s 91064 (100240)	Loss/tok 2.6606 (3.1723)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.108 (0.148)	Data 1.50e-04 (3.81e-03)	Tok/s 95194 (99850)	Loss/tok 3.0332 (3.1545)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.108 (0.148)	Data 1.45e-04 (3.36e-03)	Tok/s 94491 (99977)	Loss/tok 3.0264 (3.1514)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.108 (0.147)	Data 1.40e-04 (3.01e-03)	Tok/s 97175 (100085)	Loss/tok 2.9692 (3.1487)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.160 (0.145)	Data 1.73e-04 (2.73e-03)	Tok/s 104946 (100028)	Loss/tok 3.1723 (3.1449)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.108 (0.146)	Data 1.62e-04 (2.50e-03)	Tok/s 95108 (100288)	Loss/tok 2.9492 (3.1470)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.273 (0.148)	Data 1.49e-04 (2.31e-03)	Tok/s 110433 (100535)	Loss/tok 3.4607 (3.1557)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.275 (0.147)	Data 1.69e-04 (2.14e-03)	Tok/s 107510 (100509)	Loss/tok 3.4573 (3.1558)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.160 (0.146)	Data 1.50e-04 (2.00e-03)	Tok/s 105682 (100433)	Loss/tok 3.2319 (3.1537)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.161 (0.145)	Data 1.22e-04 (1.88e-03)	Tok/s 103666 (100405)	Loss/tok 3.1310 (3.1506)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.107 (0.145)	Data 1.59e-04 (1.78e-03)	Tok/s 98803 (100459)	Loss/tok 2.9182 (3.1496)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][170/1938]	Time 0.212 (0.146)	Data 1.62e-04 (1.68e-03)	Tok/s 108558 (100550)	Loss/tok 3.4775 (3.1575)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.060 (0.144)	Data 1.63e-04 (1.60e-03)	Tok/s 87766 (100264)	Loss/tok 2.5544 (3.1502)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.059 (0.143)	Data 1.91e-04 (1.52e-03)	Tok/s 88684 (100211)	Loss/tok 2.6004 (3.1470)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.108 (0.142)	Data 1.84e-04 (1.46e-03)	Tok/s 97074 (100139)	Loss/tok 2.8632 (3.1480)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.109 (0.142)	Data 1.63e-04 (1.40e-03)	Tok/s 95929 (100107)	Loss/tok 2.9570 (3.1461)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.275 (0.143)	Data 1.77e-04 (1.34e-03)	Tok/s 107120 (100215)	Loss/tok 3.7323 (3.1532)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.059 (0.143)	Data 1.49e-04 (1.29e-03)	Tok/s 89383 (100191)	Loss/tok 2.5626 (3.1553)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.107 (0.143)	Data 2.08e-04 (1.24e-03)	Tok/s 96227 (100276)	Loss/tok 2.9008 (3.1594)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][250/1938]	Time 0.213 (0.144)	Data 1.69e-04 (1.20e-03)	Tok/s 109768 (100303)	Loss/tok 3.4126 (3.1633)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.059 (0.143)	Data 1.51e-04 (1.16e-03)	Tok/s 87997 (100344)	Loss/tok 2.6460 (3.1646)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.158 (0.144)	Data 2.09e-04 (1.13e-03)	Tok/s 106645 (100403)	Loss/tok 3.1859 (3.1675)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.059 (0.144)	Data 1.59e-04 (1.09e-03)	Tok/s 89977 (100465)	Loss/tok 2.5863 (3.1690)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.107 (0.144)	Data 1.79e-04 (1.06e-03)	Tok/s 96941 (100459)	Loss/tok 2.9435 (3.1694)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.109 (0.145)	Data 1.91e-04 (1.03e-03)	Tok/s 95545 (100562)	Loss/tok 2.9813 (3.1745)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.107 (0.145)	Data 1.44e-04 (1.00e-03)	Tok/s 95157 (100602)	Loss/tok 2.9952 (3.1744)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.160 (0.144)	Data 1.76e-04 (9.77e-04)	Tok/s 104822 (100570)	Loss/tok 3.2110 (3.1739)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.161 (0.144)	Data 1.82e-04 (9.53e-04)	Tok/s 104903 (100497)	Loss/tok 3.1115 (3.1715)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.108 (0.145)	Data 1.40e-04 (9.31e-04)	Tok/s 96901 (100587)	Loss/tok 3.1283 (3.1769)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.108 (0.144)	Data 1.85e-04 (9.10e-04)	Tok/s 95687 (100547)	Loss/tok 3.0595 (3.1769)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.108 (0.144)	Data 1.58e-04 (8.90e-04)	Tok/s 93562 (100455)	Loss/tok 2.9392 (3.1749)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.213 (0.144)	Data 1.61e-04 (8.71e-04)	Tok/s 109900 (100477)	Loss/tok 3.3595 (3.1760)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][380/1938]	Time 0.059 (0.144)	Data 1.57e-04 (8.52e-04)	Tok/s 88503 (100502)	Loss/tok 2.7282 (3.1779)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][390/1938]	Time 0.160 (0.144)	Data 2.21e-04 (8.35e-04)	Tok/s 103866 (100439)	Loss/tok 3.1915 (3.1763)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.107 (0.143)	Data 2.92e-04 (8.19e-04)	Tok/s 98948 (100385)	Loss/tok 3.0385 (3.1741)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.107 (0.144)	Data 1.75e-04 (8.04e-04)	Tok/s 97291 (100442)	Loss/tok 2.9598 (3.1783)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.213 (0.144)	Data 1.54e-04 (7.89e-04)	Tok/s 111815 (100486)	Loss/tok 3.2245 (3.1797)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.109 (0.144)	Data 1.27e-04 (7.74e-04)	Tok/s 97219 (100508)	Loss/tok 2.9185 (3.1802)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.059 (0.144)	Data 1.33e-04 (7.61e-04)	Tok/s 89090 (100517)	Loss/tok 2.5648 (3.1803)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.107 (0.144)	Data 1.70e-04 (7.48e-04)	Tok/s 95342 (100465)	Loss/tok 3.0083 (3.1785)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.106 (0.143)	Data 1.73e-04 (7.35e-04)	Tok/s 94903 (100470)	Loss/tok 3.0046 (3.1771)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.059 (0.144)	Data 1.83e-04 (7.23e-04)	Tok/s 88810 (100492)	Loss/tok 2.6101 (3.1785)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.214 (0.144)	Data 1.62e-04 (7.12e-04)	Tok/s 109792 (100582)	Loss/tok 3.3169 (3.1805)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.109 (0.144)	Data 1.64e-04 (7.01e-04)	Tok/s 94172 (100608)	Loss/tok 3.0639 (3.1809)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.108 (0.144)	Data 1.65e-04 (6.91e-04)	Tok/s 93566 (100554)	Loss/tok 2.9020 (3.1782)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.059 (0.144)	Data 1.78e-04 (6.81e-04)	Tok/s 89604 (100550)	Loss/tok 2.4706 (3.1793)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][520/1938]	Time 0.107 (0.144)	Data 2.74e-04 (6.71e-04)	Tok/s 96380 (100534)	Loss/tok 2.9393 (3.1787)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.159 (0.144)	Data 1.55e-04 (6.62e-04)	Tok/s 106298 (100576)	Loss/tok 3.1820 (3.1804)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.274 (0.144)	Data 2.12e-04 (6.53e-04)	Tok/s 109700 (100591)	Loss/tok 3.6422 (3.1817)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.159 (0.144)	Data 1.71e-04 (6.45e-04)	Tok/s 104637 (100596)	Loss/tok 3.2044 (3.1810)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.160 (0.144)	Data 1.83e-04 (6.36e-04)	Tok/s 105848 (100638)	Loss/tok 3.1476 (3.1817)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.107 (0.144)	Data 1.73e-04 (6.28e-04)	Tok/s 95079 (100635)	Loss/tok 2.8485 (3.1806)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.107 (0.144)	Data 1.56e-04 (6.21e-04)	Tok/s 98717 (100644)	Loss/tok 2.9219 (3.1803)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.159 (0.144)	Data 1.59e-04 (6.13e-04)	Tok/s 106682 (100637)	Loss/tok 3.0288 (3.1788)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.160 (0.144)	Data 1.41e-04 (6.06e-04)	Tok/s 106303 (100642)	Loss/tok 3.1796 (3.1794)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.108 (0.144)	Data 1.59e-04 (5.99e-04)	Tok/s 96947 (100612)	Loss/tok 2.9327 (3.1785)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.107 (0.144)	Data 1.70e-04 (5.92e-04)	Tok/s 96182 (100594)	Loss/tok 2.9501 (3.1781)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][630/1938]	Time 0.160 (0.143)	Data 1.72e-04 (5.85e-04)	Tok/s 104826 (100595)	Loss/tok 3.2245 (3.1774)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.108 (0.143)	Data 1.98e-04 (5.79e-04)	Tok/s 96707 (100608)	Loss/tok 2.9676 (3.1772)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.159 (0.143)	Data 2.33e-04 (5.72e-04)	Tok/s 106771 (100619)	Loss/tok 3.2578 (3.1776)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.160 (0.144)	Data 1.62e-04 (5.66e-04)	Tok/s 104429 (100641)	Loss/tok 3.0940 (3.1774)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.159 (0.144)	Data 1.60e-04 (5.60e-04)	Tok/s 105307 (100641)	Loss/tok 3.1299 (3.1776)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.159 (0.144)	Data 1.86e-04 (5.55e-04)	Tok/s 105863 (100676)	Loss/tok 3.2107 (3.1790)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.272 (0.144)	Data 1.64e-04 (5.49e-04)	Tok/s 108952 (100733)	Loss/tok 3.6450 (3.1814)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.108 (0.144)	Data 1.42e-04 (5.44e-04)	Tok/s 95549 (100734)	Loss/tok 2.9683 (3.1804)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.159 (0.144)	Data 1.62e-04 (5.39e-04)	Tok/s 106505 (100691)	Loss/tok 3.2102 (3.1791)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.160 (0.144)	Data 1.85e-04 (5.33e-04)	Tok/s 104566 (100729)	Loss/tok 3.1373 (3.1796)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.108 (0.144)	Data 1.59e-04 (5.29e-04)	Tok/s 94076 (100720)	Loss/tok 3.1125 (3.1791)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.107 (0.144)	Data 1.77e-04 (5.24e-04)	Tok/s 97184 (100734)	Loss/tok 3.0335 (3.1791)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.109 (0.144)	Data 1.65e-04 (5.19e-04)	Tok/s 93830 (100750)	Loss/tok 3.0167 (3.1792)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][760/1938]	Time 0.108 (0.144)	Data 1.64e-04 (5.15e-04)	Tok/s 94323 (100735)	Loss/tok 2.9907 (3.1798)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.159 (0.144)	Data 1.61e-04 (5.10e-04)	Tok/s 104809 (100733)	Loss/tok 3.2036 (3.1786)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.212 (0.144)	Data 1.96e-04 (5.06e-04)	Tok/s 108910 (100739)	Loss/tok 3.4663 (3.1783)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.213 (0.144)	Data 3.04e-04 (5.02e-04)	Tok/s 108015 (100729)	Loss/tok 3.3006 (3.1781)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.273 (0.144)	Data 1.41e-04 (4.98e-04)	Tok/s 109387 (100731)	Loss/tok 3.3928 (3.1779)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.108 (0.144)	Data 1.59e-04 (4.94e-04)	Tok/s 96358 (100715)	Loss/tok 2.9515 (3.1771)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.107 (0.144)	Data 1.60e-04 (4.91e-04)	Tok/s 94521 (100678)	Loss/tok 3.0446 (3.1759)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.160 (0.144)	Data 1.65e-04 (4.87e-04)	Tok/s 104022 (100660)	Loss/tok 3.1014 (3.1746)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.212 (0.144)	Data 1.46e-04 (4.83e-04)	Tok/s 109919 (100702)	Loss/tok 3.2474 (3.1741)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.273 (0.144)	Data 1.66e-04 (4.79e-04)	Tok/s 109860 (100686)	Loss/tok 3.4995 (3.1733)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.160 (0.144)	Data 1.86e-04 (4.76e-04)	Tok/s 106685 (100666)	Loss/tok 3.1178 (3.1732)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.159 (0.144)	Data 1.47e-04 (4.73e-04)	Tok/s 105358 (100673)	Loss/tok 3.1254 (3.1731)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][880/1938]	Time 0.107 (0.144)	Data 1.51e-04 (4.69e-04)	Tok/s 96591 (100656)	Loss/tok 2.9275 (3.1725)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.109 (0.143)	Data 2.28e-04 (4.66e-04)	Tok/s 95321 (100625)	Loss/tok 2.9118 (3.1721)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.109 (0.143)	Data 1.67e-04 (4.63e-04)	Tok/s 94680 (100580)	Loss/tok 2.9238 (3.1708)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.108 (0.143)	Data 1.46e-04 (4.60e-04)	Tok/s 93061 (100560)	Loss/tok 3.0185 (3.1700)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.109 (0.143)	Data 1.65e-04 (4.56e-04)	Tok/s 94398 (100534)	Loss/tok 2.9541 (3.1687)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.213 (0.143)	Data 2.24e-04 (4.54e-04)	Tok/s 109408 (100541)	Loss/tok 3.3428 (3.1686)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.160 (0.143)	Data 1.87e-04 (4.51e-04)	Tok/s 104609 (100522)	Loss/tok 3.2618 (3.1684)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.059 (0.142)	Data 1.59e-04 (4.48e-04)	Tok/s 88484 (100474)	Loss/tok 2.5587 (3.1670)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.108 (0.142)	Data 2.26e-04 (4.46e-04)	Tok/s 96064 (100463)	Loss/tok 2.9252 (3.1660)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.107 (0.142)	Data 2.27e-04 (4.43e-04)	Tok/s 99335 (100458)	Loss/tok 2.8299 (3.1653)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.273 (0.142)	Data 1.50e-04 (4.40e-04)	Tok/s 107259 (100471)	Loss/tok 3.5272 (3.1656)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.160 (0.142)	Data 1.48e-04 (4.37e-04)	Tok/s 106982 (100464)	Loss/tok 3.1886 (3.1650)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.060 (0.142)	Data 1.42e-04 (4.35e-04)	Tok/s 88307 (100442)	Loss/tok 2.5212 (3.1640)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1010/1938]	Time 0.160 (0.142)	Data 1.27e-04 (4.32e-04)	Tok/s 104477 (100447)	Loss/tok 3.1397 (3.1641)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.108 (0.142)	Data 1.29e-04 (4.29e-04)	Tok/s 95630 (100452)	Loss/tok 3.0003 (3.1633)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.108 (0.142)	Data 2.12e-04 (4.27e-04)	Tok/s 95015 (100459)	Loss/tok 2.9252 (3.1623)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.272 (0.142)	Data 1.54e-04 (4.25e-04)	Tok/s 110275 (100442)	Loss/tok 3.4643 (3.1621)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.108 (0.142)	Data 1.88e-04 (4.22e-04)	Tok/s 95900 (100443)	Loss/tok 3.1003 (3.1623)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.159 (0.142)	Data 1.51e-04 (4.20e-04)	Tok/s 105295 (100482)	Loss/tok 3.1899 (3.1623)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.059 (0.142)	Data 1.43e-04 (4.17e-04)	Tok/s 91199 (100493)	Loss/tok 2.5925 (3.1629)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.107 (0.142)	Data 1.51e-04 (4.15e-04)	Tok/s 98560 (100471)	Loss/tok 2.9469 (3.1618)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.159 (0.141)	Data 1.47e-04 (4.13e-04)	Tok/s 104440 (100439)	Loss/tok 3.1549 (3.1608)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.160 (0.141)	Data 3.12e-04 (4.11e-04)	Tok/s 105413 (100434)	Loss/tok 3.1396 (3.1607)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.212 (0.142)	Data 1.99e-04 (4.09e-04)	Tok/s 110391 (100454)	Loss/tok 3.2244 (3.1605)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.159 (0.141)	Data 1.90e-04 (4.06e-04)	Tok/s 104014 (100449)	Loss/tok 3.1599 (3.1603)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.213 (0.142)	Data 1.49e-04 (4.04e-04)	Tok/s 109231 (100474)	Loss/tok 3.2826 (3.1616)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1140/1938]	Time 0.107 (0.142)	Data 1.36e-04 (4.02e-04)	Tok/s 96742 (100472)	Loss/tok 3.0418 (3.1609)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.107 (0.142)	Data 1.56e-04 (4.00e-04)	Tok/s 97592 (100485)	Loss/tok 3.0152 (3.1610)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.108 (0.142)	Data 1.53e-04 (3.98e-04)	Tok/s 96055 (100486)	Loss/tok 3.0167 (3.1604)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.211 (0.142)	Data 1.84e-04 (3.96e-04)	Tok/s 109134 (100505)	Loss/tok 3.2453 (3.1603)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.107 (0.142)	Data 1.45e-04 (3.94e-04)	Tok/s 96116 (100480)	Loss/tok 2.9772 (3.1595)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.058 (0.142)	Data 1.49e-04 (3.93e-04)	Tok/s 92573 (100491)	Loss/tok 2.5637 (3.1602)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.158 (0.142)	Data 1.85e-04 (3.91e-04)	Tok/s 108151 (100516)	Loss/tok 3.1080 (3.1610)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.107 (0.142)	Data 1.75e-04 (3.89e-04)	Tok/s 97606 (100508)	Loss/tok 2.8253 (3.1605)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.107 (0.141)	Data 1.52e-04 (3.87e-04)	Tok/s 94477 (100472)	Loss/tok 3.0382 (3.1592)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.159 (0.141)	Data 1.50e-04 (3.85e-04)	Tok/s 105067 (100480)	Loss/tok 3.1546 (3.1587)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.159 (0.142)	Data 1.49e-04 (3.84e-04)	Tok/s 106766 (100518)	Loss/tok 3.0465 (3.1586)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.108 (0.141)	Data 1.45e-04 (3.82e-04)	Tok/s 96605 (100511)	Loss/tok 2.9219 (3.1578)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.160 (0.141)	Data 1.64e-04 (3.80e-04)	Tok/s 105537 (100514)	Loss/tok 3.0830 (3.1573)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1270/1938]	Time 0.272 (0.142)	Data 1.55e-04 (3.78e-04)	Tok/s 109027 (100521)	Loss/tok 3.3964 (3.1576)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.059 (0.141)	Data 1.70e-04 (3.77e-04)	Tok/s 90004 (100481)	Loss/tok 2.5425 (3.1566)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.108 (0.141)	Data 1.89e-04 (3.75e-04)	Tok/s 95282 (100490)	Loss/tok 3.0541 (3.1562)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.73e-04)	Tok/s 96771 (100465)	Loss/tok 2.9815 (3.1551)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.107 (0.141)	Data 1.77e-04 (3.72e-04)	Tok/s 96429 (100482)	Loss/tok 2.9215 (3.1549)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.160 (0.141)	Data 1.42e-04 (3.70e-04)	Tok/s 104421 (100501)	Loss/tok 3.0213 (3.1550)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.059 (0.141)	Data 1.30e-04 (3.68e-04)	Tok/s 89645 (100500)	Loss/tok 2.6457 (3.1551)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.108 (0.141)	Data 1.34e-04 (3.67e-04)	Tok/s 94735 (100501)	Loss/tok 2.8899 (3.1550)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.160 (0.141)	Data 1.61e-04 (3.65e-04)	Tok/s 105776 (100508)	Loss/tok 3.2152 (3.1546)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.160 (0.141)	Data 1.37e-04 (3.64e-04)	Tok/s 105086 (100516)	Loss/tok 3.1063 (3.1546)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.107 (0.141)	Data 1.31e-04 (3.62e-04)	Tok/s 95548 (100524)	Loss/tok 2.7780 (3.1545)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.215 (0.141)	Data 1.57e-04 (3.61e-04)	Tok/s 108669 (100518)	Loss/tok 3.3546 (3.1551)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.108 (0.141)	Data 1.85e-04 (3.59e-04)	Tok/s 94869 (100522)	Loss/tok 2.9438 (3.1553)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1400/1938]	Time 0.213 (0.142)	Data 2.34e-04 (3.58e-04)	Tok/s 109268 (100550)	Loss/tok 3.3366 (3.1555)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.160 (0.142)	Data 1.22e-04 (3.56e-04)	Tok/s 103150 (100572)	Loss/tok 3.2222 (3.1554)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.108 (0.142)	Data 1.28e-04 (3.55e-04)	Tok/s 96202 (100561)	Loss/tok 2.9632 (3.1547)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.107 (0.141)	Data 1.61e-04 (3.53e-04)	Tok/s 95542 (100553)	Loss/tok 2.8535 (3.1540)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.107 (0.141)	Data 1.30e-04 (3.52e-04)	Tok/s 99089 (100529)	Loss/tok 2.8848 (3.1529)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.159 (0.141)	Data 1.43e-04 (3.50e-04)	Tok/s 104982 (100545)	Loss/tok 3.1083 (3.1525)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.058 (0.141)	Data 1.32e-04 (3.49e-04)	Tok/s 89560 (100569)	Loss/tok 2.5617 (3.1531)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.059 (0.141)	Data 1.51e-04 (3.48e-04)	Tok/s 88638 (100546)	Loss/tok 2.5771 (3.1526)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.159 (0.141)	Data 1.45e-04 (3.46e-04)	Tok/s 103595 (100526)	Loss/tok 3.2011 (3.1518)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.107 (0.141)	Data 1.41e-04 (3.45e-04)	Tok/s 96545 (100541)	Loss/tok 2.9260 (3.1514)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.161 (0.141)	Data 1.53e-04 (3.44e-04)	Tok/s 104516 (100569)	Loss/tok 3.1499 (3.1519)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.107 (0.141)	Data 1.42e-04 (3.43e-04)	Tok/s 98125 (100557)	Loss/tok 2.9940 (3.1513)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1520/1938]	Time 0.106 (0.141)	Data 1.65e-04 (3.41e-04)	Tok/s 96604 (100548)	Loss/tok 2.8777 (3.1509)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.107 (0.141)	Data 1.30e-04 (3.40e-04)	Tok/s 95789 (100544)	Loss/tok 2.9372 (3.1509)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.213 (0.141)	Data 1.57e-04 (3.39e-04)	Tok/s 109943 (100560)	Loss/tok 3.3016 (3.1514)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.160 (0.141)	Data 1.70e-04 (3.38e-04)	Tok/s 106135 (100566)	Loss/tok 3.1601 (3.1511)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.059 (0.141)	Data 1.28e-04 (3.36e-04)	Tok/s 88039 (100582)	Loss/tok 2.5300 (3.1519)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.107 (0.141)	Data 1.23e-04 (3.35e-04)	Tok/s 96550 (100579)	Loss/tok 2.9767 (3.1522)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.107 (0.141)	Data 1.59e-04 (3.34e-04)	Tok/s 97779 (100591)	Loss/tok 2.9387 (3.1525)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.107 (0.141)	Data 1.44e-04 (3.33e-04)	Tok/s 96309 (100583)	Loss/tok 2.9997 (3.1520)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.159 (0.141)	Data 1.93e-04 (3.32e-04)	Tok/s 105521 (100570)	Loss/tok 3.1089 (3.1514)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.107 (0.141)	Data 1.42e-04 (3.31e-04)	Tok/s 94324 (100569)	Loss/tok 2.9723 (3.1511)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.160 (0.141)	Data 1.32e-04 (3.30e-04)	Tok/s 104983 (100594)	Loss/tok 3.0362 (3.1514)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.275 (0.141)	Data 1.69e-04 (3.28e-04)	Tok/s 108507 (100594)	Loss/tok 3.5010 (3.1516)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.107 (0.141)	Data 1.48e-04 (3.27e-04)	Tok/s 94779 (100593)	Loss/tok 2.9180 (3.1514)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1650/1938]	Time 0.107 (0.142)	Data 1.28e-04 (3.26e-04)	Tok/s 97685 (100608)	Loss/tok 3.0001 (3.1518)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.160 (0.142)	Data 1.58e-04 (3.25e-04)	Tok/s 102645 (100616)	Loss/tok 3.2446 (3.1521)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.107 (0.142)	Data 1.37e-04 (3.24e-04)	Tok/s 99158 (100625)	Loss/tok 2.9848 (3.1520)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.107 (0.142)	Data 1.67e-04 (3.23e-04)	Tok/s 93713 (100645)	Loss/tok 2.9308 (3.1519)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.159 (0.142)	Data 1.41e-04 (3.22e-04)	Tok/s 105458 (100661)	Loss/tok 3.0782 (3.1520)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.213 (0.142)	Data 1.32e-04 (3.21e-04)	Tok/s 111048 (100683)	Loss/tok 3.2360 (3.1522)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.212 (0.142)	Data 1.59e-04 (3.20e-04)	Tok/s 110529 (100682)	Loss/tok 3.3248 (3.1522)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.107 (0.142)	Data 1.51e-04 (3.19e-04)	Tok/s 98133 (100680)	Loss/tok 2.9348 (3.1517)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.107 (0.142)	Data 1.29e-04 (3.18e-04)	Tok/s 96284 (100664)	Loss/tok 2.8936 (3.1510)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.107 (0.142)	Data 1.20e-04 (3.17e-04)	Tok/s 96757 (100640)	Loss/tok 2.9041 (3.1502)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.107 (0.142)	Data 1.53e-04 (3.16e-04)	Tok/s 97170 (100642)	Loss/tok 2.9586 (3.1505)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.272 (0.142)	Data 1.35e-04 (3.15e-04)	Tok/s 109397 (100670)	Loss/tok 3.5805 (3.1512)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.108 (0.142)	Data 1.29e-04 (3.14e-04)	Tok/s 96904 (100667)	Loss/tok 2.8712 (3.1509)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1780/1938]	Time 0.059 (0.142)	Data 1.31e-04 (3.13e-04)	Tok/s 90230 (100661)	Loss/tok 2.6257 (3.1507)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.108 (0.142)	Data 1.34e-04 (3.13e-04)	Tok/s 93809 (100669)	Loss/tok 2.9829 (3.1505)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.107 (0.142)	Data 1.37e-04 (3.12e-04)	Tok/s 96318 (100679)	Loss/tok 2.9888 (3.1503)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.107 (0.142)	Data 1.29e-04 (3.11e-04)	Tok/s 96857 (100679)	Loss/tok 2.9408 (3.1498)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.107 (0.142)	Data 1.95e-04 (3.10e-04)	Tok/s 96924 (100669)	Loss/tok 2.8480 (3.1496)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.059 (0.142)	Data 1.65e-04 (3.09e-04)	Tok/s 88602 (100680)	Loss/tok 2.5418 (3.1499)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.107 (0.142)	Data 1.43e-04 (3.08e-04)	Tok/s 96484 (100670)	Loss/tok 2.8783 (3.1497)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.213 (0.142)	Data 1.32e-04 (3.07e-04)	Tok/s 108769 (100672)	Loss/tok 3.3418 (3.1494)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.108 (0.142)	Data 1.32e-04 (3.07e-04)	Tok/s 94972 (100666)	Loss/tok 2.9980 (3.1493)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.059 (0.142)	Data 1.29e-04 (3.06e-04)	Tok/s 87797 (100660)	Loss/tok 2.4841 (3.1487)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.107 (0.142)	Data 1.27e-04 (3.05e-04)	Tok/s 93586 (100656)	Loss/tok 3.0222 (3.1487)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.107 (0.142)	Data 1.40e-04 (3.04e-04)	Tok/s 96173 (100670)	Loss/tok 2.8198 (3.1486)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.159 (0.142)	Data 1.50e-04 (3.03e-04)	Tok/s 106035 (100665)	Loss/tok 3.0469 (3.1484)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1910/1938]	Time 0.108 (0.142)	Data 1.45e-04 (3.03e-04)	Tok/s 96412 (100669)	Loss/tok 2.8088 (3.1482)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.159 (0.142)	Data 2.02e-04 (3.02e-04)	Tok/s 105402 (100668)	Loss/tok 3.1938 (3.1480)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.159 (0.142)	Data 1.68e-04 (3.01e-04)	Tok/s 105605 (100672)	Loss/tok 3.0742 (3.1480)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020293714, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020293715, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.632 (0.632)	Decoder iters 107.0 (107.0)	Tok/s 26003 (26003)
0: Running moses detokenizer
0: BLEU(score=24.166995368407388, counts=[37288, 18734, 10704, 6356], totals=[65691, 62688, 59685, 56687], precisions=[56.76272244295261, 29.88450740173558, 17.93415431012817, 11.212447298322367], bp=1.0, sys_len=65691, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020295511, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24170000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020295511, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1456	Test BLEU: 24.17
0: Performance: Epoch: 3	Training: 805470 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020295512, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020295512, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:20 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:30 AM
ENDING TIMING RUN AT 2020-06-24 10:38:21 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:30 AM
