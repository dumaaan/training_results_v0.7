+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
srun: Job 467817 step creation temporarily disabled, retrying
srun: Step created for job 467817
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019137497, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019137534, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019137535, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019137535, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019137535, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-317
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019143504, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177109/results:/results ./run_and_time.sh
srun: Job 467817 step creation temporarily disabled, retrying
srun: Step created for job 467817
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-24 10:19:05 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019147779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019147802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3988843955
:::MLLOG {"namespace": "", "time_ms": 1593019153672, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3988843955, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2041531330
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019161735, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019161736, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019161736, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019161736, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019161736, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019163341, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019163341, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019163342, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019163615, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019163616, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019163616, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019163617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019163617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019163617, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019163618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019163618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019163618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019163618, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019163618, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019163619, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 292645429
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.363 (0.363)	Data 2.99e-01 (2.99e-01)	Tok/s 14742 (14742)	Loss/tok 10.5264 (10.5264)	LR 2.047e-05
0: TRAIN [0][10/1938]	Time 0.103 (0.144)	Data 1.40e-04 (2.79e-02)	Tok/s 99552 (91827)	Loss/tok 9.6408 (10.0827)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.104 (0.150)	Data 1.57e-04 (1.47e-02)	Tok/s 99318 (98932)	Loss/tok 9.2082 (9.7780)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.058 (0.137)	Data 2.19e-04 (1.00e-02)	Tok/s 88390 (99078)	Loss/tok 8.7010 (9.6032)	LR 4.083e-05
0: TRAIN [0][40/1938]	Time 0.105 (0.143)	Data 2.02e-04 (7.62e-03)	Tok/s 97135 (100958)	Loss/tok 8.7982 (9.4268)	LR 5.141e-05
0: TRAIN [0][50/1938]	Time 0.104 (0.142)	Data 1.79e-04 (6.17e-03)	Tok/s 99134 (101347)	Loss/tok 8.4748 (9.2812)	LR 6.472e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][60/1938]	Time 0.154 (0.140)	Data 2.84e-04 (5.19e-03)	Tok/s 109343 (101604)	Loss/tok 8.5284 (9.1529)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.105 (0.138)	Data 2.40e-04 (4.48e-03)	Tok/s 97761 (101823)	Loss/tok 8.1446 (9.0390)	LR 1.002e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][80/1938]	Time 0.104 (0.138)	Data 1.75e-04 (3.95e-03)	Tok/s 98243 (101881)	Loss/tok 7.9713 (8.9384)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.104 (0.136)	Data 1.80e-04 (3.54e-03)	Tok/s 100055 (101980)	Loss/tok 7.8162 (8.8431)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.105 (0.137)	Data 1.21e-04 (3.20e-03)	Tok/s 96385 (102195)	Loss/tok 7.7465 (8.7558)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.157 (0.137)	Data 1.75e-04 (2.93e-03)	Tok/s 106470 (102321)	Loss/tok 7.9850 (8.6826)	LR 2.461e-04
0: TRAIN [0][120/1938]	Time 0.106 (0.136)	Data 1.23e-04 (2.70e-03)	Tok/s 97585 (102143)	Loss/tok 7.7473 (8.6210)	LR 3.098e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][130/1938]	Time 0.105 (0.137)	Data 2.54e-04 (2.51e-03)	Tok/s 97861 (102225)	Loss/tok 7.6613 (8.5642)	LR 3.811e-04
0: TRAIN [0][140/1938]	Time 0.106 (0.136)	Data 2.01e-04 (2.35e-03)	Tok/s 98693 (102067)	Loss/tok 7.6014 (8.5138)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.105 (0.137)	Data 1.87e-04 (2.20e-03)	Tok/s 97419 (102197)	Loss/tok 7.5506 (8.4602)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.107 (0.137)	Data 1.39e-04 (2.08e-03)	Tok/s 97241 (102281)	Loss/tok 7.4886 (8.4095)	LR 7.604e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][170/1938]	Time 0.156 (0.136)	Data 1.82e-04 (1.97e-03)	Tok/s 108854 (102111)	Loss/tok 8.0482 (8.3665)	LR 9.355e-04
0: TRAIN [0][180/1938]	Time 0.106 (0.136)	Data 1.72e-04 (1.87e-03)	Tok/s 95977 (102077)	Loss/tok 7.2134 (8.3239)	LR 1.178e-03
0: TRAIN [0][190/1938]	Time 0.157 (0.137)	Data 1.70e-04 (1.78e-03)	Tok/s 105691 (102224)	Loss/tok 7.4522 (8.2730)	LR 1.483e-03
0: TRAIN [0][200/1938]	Time 0.208 (0.136)	Data 2.79e-04 (1.70e-03)	Tok/s 113026 (102166)	Loss/tok 7.4759 (8.2239)	LR 1.867e-03
0: TRAIN [0][210/1938]	Time 0.155 (0.136)	Data 2.49e-04 (1.63e-03)	Tok/s 107474 (102119)	Loss/tok 6.9703 (8.1684)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.107 (0.137)	Data 1.44e-04 (1.56e-03)	Tok/s 97113 (102205)	Loss/tok 6.6417 (8.1004)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.209 (0.140)	Data 1.80e-04 (1.50e-03)	Tok/s 110757 (102472)	Loss/tok 6.7740 (8.0201)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.106 (0.140)	Data 1.50e-04 (1.45e-03)	Tok/s 98030 (102457)	Loss/tok 6.2786 (7.9597)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.158 (0.140)	Data 1.42e-04 (1.40e-03)	Tok/s 105397 (102488)	Loss/tok 6.4830 (7.8985)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.158 (0.139)	Data 2.26e-04 (1.35e-03)	Tok/s 107050 (102356)	Loss/tok 6.4361 (7.8452)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.158 (0.140)	Data 1.40e-04 (1.31e-03)	Tok/s 104001 (102461)	Loss/tok 6.1456 (7.7781)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.158 (0.139)	Data 1.63e-04 (1.27e-03)	Tok/s 106826 (102375)	Loss/tok 6.1242 (7.7235)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.209 (0.138)	Data 1.57e-04 (1.23e-03)	Tok/s 112184 (102278)	Loss/tok 6.3033 (7.6722)	LR 2.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][300/1938]	Time 0.107 (0.138)	Data 2.20e-04 (1.20e-03)	Tok/s 98149 (102281)	Loss/tok 5.4522 (7.6133)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.107 (0.138)	Data 1.66e-04 (1.16e-03)	Tok/s 96266 (102207)	Loss/tok 5.4120 (7.5598)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.209 (0.138)	Data 1.43e-04 (1.13e-03)	Tok/s 111578 (102192)	Loss/tok 6.0769 (7.5011)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.270 (0.140)	Data 1.56e-04 (1.10e-03)	Tok/s 109117 (102336)	Loss/tok 5.9735 (7.4324)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.106 (0.140)	Data 1.83e-04 (1.08e-03)	Tok/s 98948 (102367)	Loss/tok 5.2424 (7.3733)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.159 (0.139)	Data 1.40e-04 (1.05e-03)	Tok/s 105463 (102304)	Loss/tok 5.5275 (7.3222)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.059 (0.139)	Data 1.50e-04 (1.03e-03)	Tok/s 89450 (102301)	Loss/tok 4.2478 (7.2674)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.059 (0.139)	Data 2.18e-04 (1.00e-03)	Tok/s 87659 (102307)	Loss/tok 4.0943 (7.2118)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.210 (0.140)	Data 1.89e-04 (9.82e-04)	Tok/s 111414 (102313)	Loss/tok 5.3466 (7.1544)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.158 (0.140)	Data 1.98e-04 (9.62e-04)	Tok/s 108390 (102388)	Loss/tok 5.0674 (7.0950)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.158 (0.140)	Data 1.93e-04 (9.43e-04)	Tok/s 107957 (102319)	Loss/tok 5.1728 (7.0484)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.212 (0.140)	Data 1.79e-04 (9.24e-04)	Tok/s 110120 (102334)	Loss/tok 5.1835 (6.9976)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.106 (0.140)	Data 2.05e-04 (9.06e-04)	Tok/s 98927 (102229)	Loss/tok 4.5615 (6.9554)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][430/1938]	Time 0.059 (0.139)	Data 1.39e-04 (8.89e-04)	Tok/s 89036 (102106)	Loss/tok 3.8226 (6.9162)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.159 (0.139)	Data 1.68e-04 (8.74e-04)	Tok/s 105663 (102070)	Loss/tok 4.7703 (6.8708)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.106 (0.139)	Data 2.53e-04 (8.59e-04)	Tok/s 94278 (102129)	Loss/tok 4.4806 (6.8179)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.107 (0.139)	Data 1.78e-04 (8.44e-04)	Tok/s 98719 (102080)	Loss/tok 4.3703 (6.7764)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.158 (0.139)	Data 1.48e-04 (8.30e-04)	Tok/s 107033 (102077)	Loss/tok 4.6409 (6.7313)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.159 (0.139)	Data 1.73e-04 (8.17e-04)	Tok/s 105662 (102041)	Loss/tok 4.6166 (6.6894)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.158 (0.139)	Data 2.21e-04 (8.04e-04)	Tok/s 106702 (102068)	Loss/tok 4.5563 (6.6425)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.107 (0.138)	Data 2.01e-04 (7.92e-04)	Tok/s 95064 (102010)	Loss/tok 4.1509 (6.6051)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.059 (0.138)	Data 1.63e-04 (7.80e-04)	Tok/s 89983 (101953)	Loss/tok 3.6089 (6.5665)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.157 (0.138)	Data 2.57e-04 (7.69e-04)	Tok/s 106185 (101941)	Loss/tok 4.5548 (6.5257)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.158 (0.138)	Data 2.15e-04 (7.58e-04)	Tok/s 107101 (101962)	Loss/tok 4.3315 (6.4847)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.157 (0.139)	Data 1.58e-04 (7.47e-04)	Tok/s 106885 (102039)	Loss/tok 4.4640 (6.4378)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.158 (0.139)	Data 2.17e-04 (7.37e-04)	Tok/s 106164 (102045)	Loss/tok 4.3646 (6.3998)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][560/1938]	Time 0.106 (0.139)	Data 1.64e-04 (7.26e-04)	Tok/s 96601 (101969)	Loss/tok 4.0763 (6.3680)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.157 (0.139)	Data 2.28e-04 (7.17e-04)	Tok/s 108631 (101960)	Loss/tok 4.3212 (6.3339)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.105 (0.139)	Data 1.60e-04 (7.07e-04)	Tok/s 97994 (101981)	Loss/tok 4.0358 (6.2973)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.157 (0.139)	Data 2.64e-04 (6.99e-04)	Tok/s 105979 (101941)	Loss/tok 4.3758 (6.2670)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.210 (0.139)	Data 2.23e-04 (6.91e-04)	Tok/s 112546 (101931)	Loss/tok 4.4630 (6.2330)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.106 (0.139)	Data 1.57e-04 (6.82e-04)	Tok/s 97034 (101896)	Loss/tok 3.9054 (6.2020)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.106 (0.138)	Data 1.64e-04 (6.74e-04)	Tok/s 94899 (101884)	Loss/tok 3.9076 (6.1708)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.059 (0.138)	Data 1.65e-04 (6.67e-04)	Tok/s 89830 (101860)	Loss/tok 3.2347 (6.1412)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.158 (0.138)	Data 1.79e-04 (6.59e-04)	Tok/s 107837 (101867)	Loss/tok 4.0689 (6.1096)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.157 (0.138)	Data 2.36e-04 (6.52e-04)	Tok/s 107346 (101876)	Loss/tok 4.0988 (6.0789)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.106 (0.138)	Data 1.98e-04 (6.45e-04)	Tok/s 96462 (101862)	Loss/tok 3.8155 (6.0511)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.106 (0.138)	Data 1.68e-04 (6.38e-04)	Tok/s 96255 (101883)	Loss/tok 3.8048 (6.0196)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.158 (0.138)	Data 1.68e-04 (6.31e-04)	Tok/s 106179 (101864)	Loss/tok 4.1639 (5.9928)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][690/1938]	Time 0.106 (0.138)	Data 1.87e-04 (6.25e-04)	Tok/s 98153 (101845)	Loss/tok 3.7377 (5.9663)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.106 (0.138)	Data 1.48e-04 (6.19e-04)	Tok/s 97842 (101777)	Loss/tok 3.9083 (5.9455)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.059 (0.137)	Data 1.74e-04 (6.12e-04)	Tok/s 90166 (101714)	Loss/tok 3.2901 (5.9234)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.108 (0.138)	Data 1.57e-04 (6.06e-04)	Tok/s 96280 (101725)	Loss/tok 3.8901 (5.8960)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.106 (0.138)	Data 1.53e-04 (6.00e-04)	Tok/s 96971 (101740)	Loss/tok 3.9993 (5.8702)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.106 (0.138)	Data 2.33e-04 (5.95e-04)	Tok/s 98776 (101722)	Loss/tok 3.7464 (5.8456)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.211 (0.138)	Data 1.48e-04 (5.89e-04)	Tok/s 110234 (101713)	Loss/tok 4.2203 (5.8213)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.106 (0.138)	Data 1.69e-04 (5.84e-04)	Tok/s 97701 (101766)	Loss/tok 3.8199 (5.7911)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.060 (0.138)	Data 1.72e-04 (5.79e-04)	Tok/s 89454 (101751)	Loss/tok 3.1469 (5.7683)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.158 (0.139)	Data 1.69e-04 (5.73e-04)	Tok/s 105964 (101785)	Loss/tok 4.0984 (5.7429)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.106 (0.139)	Data 1.57e-04 (5.69e-04)	Tok/s 96848 (101781)	Loss/tok 3.6686 (5.7208)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.106 (0.139)	Data 2.78e-04 (5.64e-04)	Tok/s 98695 (101751)	Loss/tok 3.6575 (5.7011)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][810/1938]	Time 0.210 (0.138)	Data 2.25e-04 (5.59e-04)	Tok/s 111619 (101736)	Loss/tok 4.2032 (5.6810)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.158 (0.138)	Data 1.34e-04 (5.54e-04)	Tok/s 106205 (101730)	Loss/tok 4.0790 (5.6607)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.058 (0.138)	Data 1.59e-04 (5.49e-04)	Tok/s 90258 (101715)	Loss/tok 3.1891 (5.6405)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.106 (0.139)	Data 1.96e-04 (5.45e-04)	Tok/s 96859 (101762)	Loss/tok 3.5364 (5.6172)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.211 (0.139)	Data 1.67e-04 (5.40e-04)	Tok/s 109289 (101808)	Loss/tok 4.2788 (5.5944)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.106 (0.139)	Data 1.30e-04 (5.36e-04)	Tok/s 99677 (101783)	Loss/tok 3.7633 (5.5768)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.105 (0.139)	Data 2.07e-04 (5.32e-04)	Tok/s 99898 (101747)	Loss/tok 3.6977 (5.5598)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][880/1938]	Time 0.106 (0.139)	Data 2.58e-04 (5.28e-04)	Tok/s 98161 (101770)	Loss/tok 3.6177 (5.5387)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.106 (0.139)	Data 1.39e-04 (5.24e-04)	Tok/s 97023 (101783)	Loss/tok 3.5980 (5.5185)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.159 (0.139)	Data 1.50e-04 (5.20e-04)	Tok/s 104676 (101781)	Loss/tok 3.9493 (5.5011)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.159 (0.139)	Data 1.49e-04 (5.16e-04)	Tok/s 104630 (101812)	Loss/tok 3.8889 (5.4811)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.108 (0.139)	Data 1.40e-04 (5.12e-04)	Tok/s 96745 (101774)	Loss/tok 3.5546 (5.4660)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.106 (0.139)	Data 1.56e-04 (5.09e-04)	Tok/s 99077 (101799)	Loss/tok 3.6199 (5.4469)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.158 (0.139)	Data 1.50e-04 (5.05e-04)	Tok/s 105172 (101765)	Loss/tok 3.8781 (5.4318)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.106 (0.139)	Data 1.60e-04 (5.02e-04)	Tok/s 98815 (101797)	Loss/tok 3.6010 (5.4126)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.210 (0.139)	Data 1.85e-04 (4.98e-04)	Tok/s 111464 (101801)	Loss/tok 3.9303 (5.3962)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.270 (0.139)	Data 1.85e-04 (4.95e-04)	Tok/s 109930 (101789)	Loss/tok 4.3820 (5.3808)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.106 (0.139)	Data 1.57e-04 (4.92e-04)	Tok/s 98494 (101791)	Loss/tok 3.6027 (5.3653)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.107 (0.139)	Data 1.27e-04 (4.88e-04)	Tok/s 96344 (101761)	Loss/tok 3.6802 (5.3514)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.159 (0.139)	Data 1.68e-04 (4.85e-04)	Tok/s 105336 (101760)	Loss/tok 3.7857 (5.3362)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1010/1938]	Time 0.107 (0.139)	Data 1.39e-04 (4.82e-04)	Tok/s 96658 (101741)	Loss/tok 3.6076 (5.3216)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.106 (0.139)	Data 1.34e-04 (4.79e-04)	Tok/s 98627 (101702)	Loss/tok 3.4943 (5.3087)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1030/1938]	Time 0.107 (0.139)	Data 1.40e-04 (4.76e-04)	Tok/s 95714 (101708)	Loss/tok 3.6427 (5.2937)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.106 (0.139)	Data 1.99e-04 (4.73e-04)	Tok/s 96512 (101712)	Loss/tok 3.5048 (5.2792)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.106 (0.139)	Data 2.74e-04 (4.70e-04)	Tok/s 97144 (101697)	Loss/tok 3.4795 (5.2657)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.160 (0.139)	Data 1.21e-04 (4.67e-04)	Tok/s 104561 (101729)	Loss/tok 3.7838 (5.2500)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.159 (0.139)	Data 1.94e-04 (4.64e-04)	Tok/s 108648 (101703)	Loss/tok 3.6334 (5.2375)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.106 (0.139)	Data 1.35e-04 (4.61e-04)	Tok/s 95873 (101684)	Loss/tok 3.5370 (5.2249)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.158 (0.139)	Data 1.27e-04 (4.59e-04)	Tok/s 105625 (101708)	Loss/tok 3.8541 (5.2096)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.107 (0.139)	Data 1.40e-04 (4.56e-04)	Tok/s 95450 (101703)	Loss/tok 3.4343 (5.1963)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.059 (0.139)	Data 1.44e-04 (4.54e-04)	Tok/s 88542 (101714)	Loss/tok 3.0414 (5.1827)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.107 (0.139)	Data 1.59e-04 (4.51e-04)	Tok/s 97719 (101719)	Loss/tok 3.5792 (5.1702)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.159 (0.139)	Data 1.21e-04 (4.48e-04)	Tok/s 105175 (101744)	Loss/tok 3.8179 (5.1566)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.107 (0.140)	Data 1.46e-04 (4.46e-04)	Tok/s 97085 (101775)	Loss/tok 3.5723 (5.1421)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1150/1938]	Time 0.212 (0.140)	Data 1.20e-04 (4.44e-04)	Tok/s 109233 (101771)	Loss/tok 4.0594 (5.1303)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.158 (0.140)	Data 1.47e-04 (4.41e-04)	Tok/s 105721 (101772)	Loss/tok 3.6916 (5.1184)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.272 (0.140)	Data 1.23e-04 (4.39e-04)	Tok/s 111907 (101786)	Loss/tok 3.9227 (5.1050)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.106 (0.140)	Data 1.22e-04 (4.37e-04)	Tok/s 99945 (101751)	Loss/tok 3.4244 (5.0951)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.158 (0.140)	Data 1.50e-04 (4.34e-04)	Tok/s 107201 (101720)	Loss/tok 3.7261 (5.0851)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.106 (0.140)	Data 2.32e-04 (4.32e-04)	Tok/s 96117 (101751)	Loss/tok 3.4369 (5.0718)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.107 (0.140)	Data 1.90e-04 (4.30e-04)	Tok/s 95525 (101742)	Loss/tok 3.5010 (5.0610)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.058 (0.140)	Data 1.53e-04 (4.28e-04)	Tok/s 91032 (101736)	Loss/tok 2.9387 (5.0501)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.160 (0.140)	Data 1.68e-04 (4.26e-04)	Tok/s 103432 (101723)	Loss/tok 3.7243 (5.0399)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.106 (0.140)	Data 2.51e-04 (4.24e-04)	Tok/s 98319 (101739)	Loss/tok 3.4877 (5.0284)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.159 (0.140)	Data 1.62e-04 (4.22e-04)	Tok/s 104541 (101774)	Loss/tok 3.6722 (5.0155)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.158 (0.140)	Data 1.34e-04 (4.20e-04)	Tok/s 108225 (101789)	Loss/tok 3.5999 (5.0036)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.270 (0.140)	Data 1.64e-04 (4.18e-04)	Tok/s 112050 (101790)	Loss/tok 4.1223 (4.9931)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1280/1938]	Time 0.273 (0.141)	Data 2.08e-04 (4.16e-04)	Tok/s 108808 (101802)	Loss/tok 4.0089 (4.9818)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.158 (0.141)	Data 1.83e-04 (4.14e-04)	Tok/s 105496 (101805)	Loss/tok 3.7272 (4.9718)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.107 (0.140)	Data 1.44e-04 (4.12e-04)	Tok/s 98109 (101776)	Loss/tok 3.4519 (4.9630)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1310/1938]	Time 0.107 (0.140)	Data 2.50e-04 (4.10e-04)	Tok/s 96090 (101773)	Loss/tok 3.4253 (4.9530)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.107 (0.140)	Data 1.65e-04 (4.09e-04)	Tok/s 96805 (101760)	Loss/tok 3.4320 (4.9438)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.272 (0.141)	Data 1.51e-04 (4.07e-04)	Tok/s 109359 (101788)	Loss/tok 4.0801 (4.9322)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.107 (0.141)	Data 1.46e-04 (4.05e-04)	Tok/s 96640 (101781)	Loss/tok 3.3935 (4.9230)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.107 (0.141)	Data 2.07e-04 (4.03e-04)	Tok/s 95680 (101772)	Loss/tok 3.4690 (4.9140)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.106 (0.141)	Data 1.57e-04 (4.02e-04)	Tok/s 97371 (101769)	Loss/tok 3.3982 (4.9051)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.159 (0.141)	Data 1.75e-04 (4.00e-04)	Tok/s 106110 (101787)	Loss/tok 3.7073 (4.8954)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.157 (0.141)	Data 1.89e-04 (3.98e-04)	Tok/s 105293 (101802)	Loss/tok 3.6997 (4.8856)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.107 (0.141)	Data 1.72e-04 (3.97e-04)	Tok/s 95649 (101807)	Loss/tok 3.4444 (4.8763)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.158 (0.141)	Data 1.53e-04 (3.95e-04)	Tok/s 107276 (101792)	Loss/tok 3.7013 (4.8683)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.158 (0.141)	Data 1.76e-04 (3.94e-04)	Tok/s 105403 (101784)	Loss/tok 3.4907 (4.8601)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.107 (0.141)	Data 2.33e-04 (3.92e-04)	Tok/s 99020 (101783)	Loss/tok 3.3979 (4.8514)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.106 (0.141)	Data 1.39e-04 (3.91e-04)	Tok/s 96601 (101771)	Loss/tok 3.4745 (4.8435)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1440/1938]	Time 0.158 (0.141)	Data 1.93e-04 (3.89e-04)	Tok/s 106262 (101774)	Loss/tok 3.6299 (4.8346)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.107 (0.141)	Data 1.50e-04 (3.88e-04)	Tok/s 97808 (101754)	Loss/tok 3.3166 (4.8268)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.060 (0.141)	Data 1.25e-04 (3.86e-04)	Tok/s 89593 (101750)	Loss/tok 2.9254 (4.8187)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.107 (0.141)	Data 1.78e-04 (3.85e-04)	Tok/s 97209 (101749)	Loss/tok 3.3427 (4.8106)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.106 (0.141)	Data 1.56e-04 (3.83e-04)	Tok/s 99096 (101757)	Loss/tok 3.4321 (4.8022)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.058 (0.141)	Data 1.77e-04 (3.82e-04)	Tok/s 92494 (101734)	Loss/tok 2.8207 (4.7956)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.160 (0.141)	Data 1.76e-04 (3.81e-04)	Tok/s 105729 (101722)	Loss/tok 3.7121 (4.7884)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.107 (0.141)	Data 1.61e-04 (3.79e-04)	Tok/s 96883 (101722)	Loss/tok 3.4033 (4.7804)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.107 (0.141)	Data 2.36e-04 (3.78e-04)	Tok/s 94304 (101725)	Loss/tok 3.4407 (4.7727)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.107 (0.141)	Data 2.07e-04 (3.77e-04)	Tok/s 95611 (101733)	Loss/tok 3.3953 (4.7649)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.107 (0.141)	Data 1.65e-04 (3.75e-04)	Tok/s 96309 (101757)	Loss/tok 3.2803 (4.7562)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.158 (0.141)	Data 1.65e-04 (3.74e-04)	Tok/s 107439 (101764)	Loss/tok 3.6285 (4.7485)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1560/1938]	Time 0.159 (0.141)	Data 1.63e-04 (3.73e-04)	Tok/s 104955 (101738)	Loss/tok 3.6941 (4.7422)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.106 (0.141)	Data 1.71e-04 (3.71e-04)	Tok/s 97829 (101731)	Loss/tok 3.3897 (4.7352)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1580/1938]	Time 0.107 (0.141)	Data 1.93e-04 (3.70e-04)	Tok/s 97812 (101730)	Loss/tok 3.3329 (4.7281)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.107 (0.141)	Data 2.79e-04 (3.69e-04)	Tok/s 97689 (101740)	Loss/tok 3.3789 (4.7205)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.106 (0.141)	Data 1.51e-04 (3.68e-04)	Tok/s 98888 (101742)	Loss/tok 3.3439 (4.7132)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.106 (0.141)	Data 1.85e-04 (3.67e-04)	Tok/s 96789 (101724)	Loss/tok 3.3598 (4.7069)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.106 (0.141)	Data 1.96e-04 (3.66e-04)	Tok/s 97081 (101698)	Loss/tok 3.4813 (4.7013)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.159 (0.141)	Data 1.49e-04 (3.65e-04)	Tok/s 106150 (101702)	Loss/tok 3.6048 (4.6944)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.107 (0.141)	Data 1.92e-04 (3.63e-04)	Tok/s 97509 (101687)	Loss/tok 3.4013 (4.6883)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.160 (0.141)	Data 1.71e-04 (3.62e-04)	Tok/s 106032 (101689)	Loss/tok 3.5423 (4.6816)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.211 (0.141)	Data 2.27e-04 (3.61e-04)	Tok/s 108847 (101705)	Loss/tok 3.8454 (4.6744)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.107 (0.141)	Data 1.44e-04 (3.60e-04)	Tok/s 96309 (101705)	Loss/tok 3.3594 (4.6678)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.159 (0.141)	Data 1.51e-04 (3.59e-04)	Tok/s 104195 (101719)	Loss/tok 3.5756 (4.6607)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.107 (0.141)	Data 2.08e-04 (3.58e-04)	Tok/s 97491 (101723)	Loss/tok 3.3788 (4.6540)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.059 (0.141)	Data 1.44e-04 (3.57e-04)	Tok/s 88357 (101700)	Loss/tok 2.8334 (4.6485)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1710/1938]	Time 0.158 (0.141)	Data 1.48e-04 (3.56e-04)	Tok/s 105792 (101708)	Loss/tok 3.6468 (4.6422)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.106 (0.141)	Data 1.75e-04 (3.55e-04)	Tok/s 98164 (101714)	Loss/tok 3.4141 (4.6354)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.106 (0.141)	Data 1.92e-04 (3.54e-04)	Tok/s 99985 (101722)	Loss/tok 3.3824 (4.6290)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.107 (0.141)	Data 1.58e-04 (3.53e-04)	Tok/s 96228 (101709)	Loss/tok 3.3326 (4.6232)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1750/1938]	Time 0.106 (0.141)	Data 2.12e-04 (3.52e-04)	Tok/s 97627 (101715)	Loss/tok 3.2748 (4.6171)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.159 (0.141)	Data 2.37e-04 (3.51e-04)	Tok/s 105550 (101698)	Loss/tok 3.5371 (4.6116)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.107 (0.141)	Data 1.40e-04 (3.50e-04)	Tok/s 96626 (101704)	Loss/tok 3.4015 (4.6055)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.158 (0.141)	Data 1.84e-04 (3.49e-04)	Tok/s 104959 (101725)	Loss/tok 3.5893 (4.5986)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.107 (0.141)	Data 1.69e-04 (3.48e-04)	Tok/s 97483 (101731)	Loss/tok 3.2922 (4.5925)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.107 (0.141)	Data 2.68e-04 (3.47e-04)	Tok/s 97748 (101732)	Loss/tok 3.2961 (4.5866)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.271 (0.141)	Data 2.21e-04 (3.46e-04)	Tok/s 111365 (101753)	Loss/tok 3.9229 (4.5803)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.159 (0.141)	Data 2.53e-04 (3.45e-04)	Tok/s 107373 (101754)	Loss/tok 3.5262 (4.5746)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.108 (0.141)	Data 1.73e-04 (3.44e-04)	Tok/s 95011 (101761)	Loss/tok 3.3528 (4.5688)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.107 (0.141)	Data 1.59e-04 (3.43e-04)	Tok/s 96863 (101733)	Loss/tok 3.2983 (4.5640)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.107 (0.141)	Data 1.52e-04 (3.43e-04)	Tok/s 96728 (101744)	Loss/tok 3.3122 (4.5579)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.107 (0.141)	Data 1.44e-04 (3.42e-04)	Tok/s 96860 (101723)	Loss/tok 3.3538 (4.5532)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.159 (0.141)	Data 1.77e-04 (3.41e-04)	Tok/s 106676 (101699)	Loss/tok 3.6051 (4.5486)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1880/1938]	Time 0.107 (0.141)	Data 1.39e-04 (3.40e-04)	Tok/s 99762 (101683)	Loss/tok 3.4340 (4.5438)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1890/1938]	Time 0.272 (0.141)	Data 1.73e-04 (3.39e-04)	Tok/s 108634 (101677)	Loss/tok 3.8751 (4.5386)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.107 (0.141)	Data 1.47e-04 (3.38e-04)	Tok/s 97745 (101664)	Loss/tok 3.3375 (4.5337)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.107 (0.141)	Data 1.73e-04 (3.38e-04)	Tok/s 95886 (101673)	Loss/tok 3.3750 (4.5281)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.107 (0.141)	Data 2.12e-04 (3.37e-04)	Tok/s 96340 (101653)	Loss/tok 3.2019 (4.5232)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.159 (0.141)	Data 1.83e-04 (3.36e-04)	Tok/s 105744 (101633)	Loss/tok 3.6003 (4.5188)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019436743, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019436743, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.712 (0.712)	Decoder iters 131.0 (131.0)	Tok/s 22755 (22755)
0: Running moses detokenizer
0: BLEU(score=20.1291462296654, counts=[34706, 16064, 8609, 4782], totals=[65743, 62740, 59737, 56739], precisions=[52.79041114643384, 25.604080331526937, 14.411503758139846, 8.428065351874372], bp=1.0, sys_len=65743, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019438644, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.20129999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019438645, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.5144	Test BLEU: 20.13
0: Performance: Epoch: 0	Training: 812880 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019438645, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019438645, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019438645, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2252958542
0: TRAIN [1][0/1938]	Time 0.365 (0.365)	Data 2.59e-01 (2.59e-01)	Tok/s 29041 (29041)	Loss/tok 3.1790 (3.1790)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.107 (0.149)	Data 1.60e-04 (2.37e-02)	Tok/s 95362 (92983)	Loss/tok 3.1721 (3.3769)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.211 (0.142)	Data 2.73e-04 (1.25e-02)	Tok/s 109738 (96591)	Loss/tok 3.6799 (3.4098)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.108 (0.139)	Data 1.71e-04 (8.55e-03)	Tok/s 97351 (97961)	Loss/tok 3.2532 (3.4115)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.211 (0.152)	Data 1.83e-04 (6.51e-03)	Tok/s 110636 (99856)	Loss/tok 3.6345 (3.5072)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.159 (0.152)	Data 1.62e-04 (5.27e-03)	Tok/s 106820 (100160)	Loss/tok 3.4044 (3.5108)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.211 (0.151)	Data 1.64e-04 (4.43e-03)	Tok/s 109693 (100396)	Loss/tok 3.6461 (3.5008)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.212 (0.152)	Data 1.70e-04 (3.83e-03)	Tok/s 110344 (101009)	Loss/tok 3.6735 (3.5053)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][80/1938]	Time 0.108 (0.152)	Data 1.54e-04 (3.38e-03)	Tok/s 97425 (101206)	Loss/tok 3.4066 (3.5103)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.108 (0.149)	Data 1.53e-04 (3.03e-03)	Tok/s 96211 (100983)	Loss/tok 3.3121 (3.4959)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.106 (0.150)	Data 1.39e-04 (2.74e-03)	Tok/s 95693 (101225)	Loss/tok 3.2561 (3.4993)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.059 (0.149)	Data 1.76e-04 (2.51e-03)	Tok/s 89333 (101095)	Loss/tok 2.6864 (3.4976)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.212 (0.152)	Data 1.97e-04 (2.32e-03)	Tok/s 109024 (101563)	Loss/tok 3.5878 (3.5106)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.211 (0.151)	Data 1.67e-04 (2.16e-03)	Tok/s 111161 (101568)	Loss/tok 3.6013 (3.5055)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.158 (0.151)	Data 2.08e-04 (2.02e-03)	Tok/s 104235 (101581)	Loss/tok 3.4075 (3.5072)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.212 (0.152)	Data 1.90e-04 (1.90e-03)	Tok/s 110892 (101821)	Loss/tok 3.5525 (3.5100)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.160 (0.152)	Data 1.89e-04 (1.79e-03)	Tok/s 104482 (101851)	Loss/tok 3.5320 (3.5062)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.160 (0.150)	Data 1.65e-04 (1.69e-03)	Tok/s 104878 (101710)	Loss/tok 3.4189 (3.4980)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.059 (0.148)	Data 1.48e-04 (1.61e-03)	Tok/s 91028 (101559)	Loss/tok 2.7355 (3.4896)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.107 (0.147)	Data 1.70e-04 (1.54e-03)	Tok/s 97619 (101479)	Loss/tok 3.2032 (3.4848)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.107 (0.146)	Data 1.91e-04 (1.47e-03)	Tok/s 96057 (101278)	Loss/tok 3.2919 (3.4813)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][210/1938]	Time 0.272 (0.146)	Data 1.47e-04 (1.41e-03)	Tok/s 108472 (101432)	Loss/tok 3.9786 (3.4866)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.211 (0.147)	Data 1.60e-04 (1.35e-03)	Tok/s 110564 (101559)	Loss/tok 3.5839 (3.4894)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.107 (0.148)	Data 1.49e-04 (1.30e-03)	Tok/s 95562 (101625)	Loss/tok 3.2999 (3.4904)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.159 (0.148)	Data 1.58e-04 (1.26e-03)	Tok/s 106459 (101661)	Loss/tok 3.4777 (3.4892)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.273 (0.148)	Data 1.79e-04 (1.21e-03)	Tok/s 108770 (101670)	Loss/tok 3.7644 (3.4866)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.107 (0.147)	Data 2.21e-04 (1.17e-03)	Tok/s 96406 (101681)	Loss/tok 3.2143 (3.4842)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.107 (0.147)	Data 2.17e-04 (1.14e-03)	Tok/s 94537 (101705)	Loss/tok 3.1823 (3.4836)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.160 (0.147)	Data 1.76e-04 (1.10e-03)	Tok/s 105356 (101713)	Loss/tok 3.4162 (3.4798)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.107 (0.147)	Data 2.47e-04 (1.07e-03)	Tok/s 97068 (101696)	Loss/tok 3.1658 (3.4782)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.213 (0.147)	Data 1.70e-04 (1.04e-03)	Tok/s 111175 (101718)	Loss/tok 3.5985 (3.4787)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.159 (0.147)	Data 2.19e-04 (1.01e-03)	Tok/s 105579 (101774)	Loss/tok 3.4132 (3.4773)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.212 (0.147)	Data 1.59e-04 (9.89e-04)	Tok/s 107833 (101781)	Loss/tok 3.6561 (3.4761)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.106 (0.147)	Data 1.87e-04 (9.65e-04)	Tok/s 97652 (101812)	Loss/tok 3.2504 (3.4771)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][340/1938]	Time 0.107 (0.146)	Data 1.63e-04 (9.41e-04)	Tok/s 95010 (101678)	Loss/tok 3.1848 (3.4742)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.106 (0.147)	Data 1.83e-04 (9.20e-04)	Tok/s 98346 (101689)	Loss/tok 3.1525 (3.4732)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.106 (0.147)	Data 2.16e-04 (9.00e-04)	Tok/s 97588 (101732)	Loss/tok 3.1647 (3.4717)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.107 (0.146)	Data 1.64e-04 (8.80e-04)	Tok/s 97144 (101618)	Loss/tok 3.2542 (3.4691)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][380/1938]	Time 0.272 (0.146)	Data 1.83e-04 (8.62e-04)	Tok/s 109267 (101600)	Loss/tok 3.7321 (3.4702)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.159 (0.146)	Data 1.63e-04 (8.45e-04)	Tok/s 105741 (101570)	Loss/tok 3.3515 (3.4691)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.211 (0.146)	Data 1.62e-04 (8.29e-04)	Tok/s 111647 (101633)	Loss/tok 3.5751 (3.4690)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.107 (0.146)	Data 1.60e-04 (8.13e-04)	Tok/s 98290 (101602)	Loss/tok 3.2456 (3.4662)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.107 (0.145)	Data 1.46e-04 (7.99e-04)	Tok/s 95149 (101575)	Loss/tok 3.2656 (3.4643)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.212 (0.145)	Data 1.91e-04 (7.84e-04)	Tok/s 109621 (101582)	Loss/tok 3.6909 (3.4640)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.158 (0.145)	Data 2.79e-04 (7.71e-04)	Tok/s 106889 (101539)	Loss/tok 3.4526 (3.4622)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.107 (0.145)	Data 2.27e-04 (7.58e-04)	Tok/s 97960 (101514)	Loss/tok 3.1498 (3.4619)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.160 (0.144)	Data 2.05e-04 (7.46e-04)	Tok/s 104502 (101458)	Loss/tok 3.4096 (3.4585)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.107 (0.144)	Data 2.38e-04 (7.34e-04)	Tok/s 97914 (101491)	Loss/tok 3.1768 (3.4579)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.108 (0.144)	Data 1.70e-04 (7.23e-04)	Tok/s 95942 (101522)	Loss/tok 3.1356 (3.4574)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][490/1938]	Time 0.272 (0.145)	Data 2.10e-04 (7.12e-04)	Tok/s 108230 (101594)	Loss/tok 3.8311 (3.4608)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.107 (0.145)	Data 1.65e-04 (7.02e-04)	Tok/s 97368 (101557)	Loss/tok 3.2638 (3.4585)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.271 (0.145)	Data 1.86e-04 (6.92e-04)	Tok/s 109325 (101565)	Loss/tok 3.9201 (3.4601)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.107 (0.145)	Data 1.82e-04 (6.82e-04)	Tok/s 96345 (101540)	Loss/tok 3.1721 (3.4590)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.059 (0.145)	Data 2.00e-04 (6.73e-04)	Tok/s 90260 (101541)	Loss/tok 2.7854 (3.4593)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.059 (0.145)	Data 1.58e-04 (6.64e-04)	Tok/s 90280 (101535)	Loss/tok 2.8318 (3.4602)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.107 (0.145)	Data 1.65e-04 (6.55e-04)	Tok/s 96197 (101532)	Loss/tok 3.2859 (3.4602)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.158 (0.144)	Data 2.09e-04 (6.47e-04)	Tok/s 105552 (101476)	Loss/tok 3.3978 (3.4583)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.107 (0.144)	Data 1.84e-04 (6.39e-04)	Tok/s 99422 (101481)	Loss/tok 3.2915 (3.4581)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.160 (0.144)	Data 1.66e-04 (6.31e-04)	Tok/s 104557 (101438)	Loss/tok 3.3785 (3.4565)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.108 (0.144)	Data 1.75e-04 (6.24e-04)	Tok/s 95986 (101445)	Loss/tok 3.1182 (3.4567)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.107 (0.144)	Data 1.64e-04 (6.16e-04)	Tok/s 97805 (101440)	Loss/tok 3.3836 (3.4573)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.211 (0.144)	Data 2.33e-04 (6.09e-04)	Tok/s 109676 (101427)	Loss/tok 3.6297 (3.4564)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][620/1938]	Time 0.107 (0.144)	Data 2.26e-04 (6.03e-04)	Tok/s 96053 (101491)	Loss/tok 3.2512 (3.4576)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.159 (0.144)	Data 2.55e-04 (5.96e-04)	Tok/s 105526 (101454)	Loss/tok 3.4365 (3.4567)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.107 (0.145)	Data 1.84e-04 (5.90e-04)	Tok/s 97027 (101503)	Loss/tok 3.2105 (3.4590)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.107 (0.145)	Data 1.39e-04 (5.84e-04)	Tok/s 97880 (101519)	Loss/tok 3.1363 (3.4576)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.160 (0.145)	Data 2.32e-04 (5.78e-04)	Tok/s 105923 (101540)	Loss/tok 3.4176 (3.4573)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.272 (0.145)	Data 1.92e-04 (5.72e-04)	Tok/s 108141 (101546)	Loss/tok 3.9081 (3.4572)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.107 (0.145)	Data 1.68e-04 (5.66e-04)	Tok/s 96725 (101534)	Loss/tok 3.1717 (3.4558)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.107 (0.145)	Data 1.76e-04 (5.61e-04)	Tok/s 94901 (101507)	Loss/tok 3.2156 (3.4561)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.107 (0.144)	Data 1.68e-04 (5.55e-04)	Tok/s 97522 (101479)	Loss/tok 3.2236 (3.4548)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.159 (0.145)	Data 1.79e-04 (5.50e-04)	Tok/s 106107 (101481)	Loss/tok 3.4217 (3.4546)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.160 (0.145)	Data 1.92e-04 (5.45e-04)	Tok/s 104803 (101531)	Loss/tok 3.3263 (3.4552)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.108 (0.145)	Data 1.71e-04 (5.41e-04)	Tok/s 95792 (101488)	Loss/tok 3.1224 (3.4528)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.159 (0.145)	Data 2.02e-04 (5.36e-04)	Tok/s 107455 (101491)	Loss/tok 3.4393 (3.4526)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][750/1938]	Time 0.159 (0.145)	Data 1.85e-04 (5.31e-04)	Tok/s 105158 (101503)	Loss/tok 3.4693 (3.4524)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.159 (0.145)	Data 2.08e-04 (5.27e-04)	Tok/s 104947 (101503)	Loss/tok 3.4881 (3.4524)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.107 (0.145)	Data 2.30e-04 (5.22e-04)	Tok/s 98415 (101507)	Loss/tok 3.1826 (3.4515)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.160 (0.145)	Data 1.96e-04 (5.18e-04)	Tok/s 103886 (101488)	Loss/tok 3.4457 (3.4502)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][790/1938]	Time 0.160 (0.145)	Data 2.49e-04 (5.14e-04)	Tok/s 104854 (101492)	Loss/tok 3.3694 (3.4499)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.106 (0.144)	Data 1.71e-04 (5.10e-04)	Tok/s 97898 (101462)	Loss/tok 3.0655 (3.4488)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.106 (0.144)	Data 2.59e-04 (5.06e-04)	Tok/s 96321 (101411)	Loss/tok 3.1655 (3.4469)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.160 (0.144)	Data 2.56e-04 (5.02e-04)	Tok/s 106823 (101406)	Loss/tok 3.3160 (3.4463)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.160 (0.144)	Data 1.85e-04 (4.98e-04)	Tok/s 105918 (101427)	Loss/tok 3.4205 (3.4466)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.107 (0.144)	Data 1.57e-04 (4.94e-04)	Tok/s 96230 (101425)	Loss/tok 3.0773 (3.4456)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.108 (0.144)	Data 1.12e-04 (4.91e-04)	Tok/s 95202 (101424)	Loss/tok 3.2236 (3.4454)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.212 (0.144)	Data 1.52e-04 (4.87e-04)	Tok/s 108244 (101413)	Loss/tok 3.6458 (3.4452)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.108 (0.144)	Data 1.85e-04 (4.84e-04)	Tok/s 96458 (101432)	Loss/tok 3.1626 (3.4461)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.107 (0.144)	Data 2.25e-04 (4.80e-04)	Tok/s 96323 (101395)	Loss/tok 3.1982 (3.4445)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.107 (0.144)	Data 1.85e-04 (4.77e-04)	Tok/s 96098 (101397)	Loss/tok 3.1799 (3.4440)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.160 (0.144)	Data 1.45e-04 (4.73e-04)	Tok/s 107622 (101397)	Loss/tok 3.3709 (3.4435)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][910/1938]	Time 0.213 (0.144)	Data 2.75e-04 (4.70e-04)	Tok/s 110372 (101433)	Loss/tok 3.3846 (3.4436)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.059 (0.144)	Data 1.45e-04 (4.67e-04)	Tok/s 88471 (101454)	Loss/tok 2.8499 (3.4438)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][930/1938]	Time 0.160 (0.144)	Data 1.94e-04 (4.64e-04)	Tok/s 104920 (101424)	Loss/tok 3.3783 (3.4429)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.213 (0.144)	Data 1.76e-04 (4.61e-04)	Tok/s 110363 (101406)	Loss/tok 3.4570 (3.4421)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.107 (0.144)	Data 1.64e-04 (4.58e-04)	Tok/s 98373 (101400)	Loss/tok 3.2173 (3.4408)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.061 (0.144)	Data 2.22e-04 (4.55e-04)	Tok/s 86878 (101380)	Loss/tok 2.7493 (3.4406)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.106 (0.144)	Data 2.09e-04 (4.52e-04)	Tok/s 96700 (101369)	Loss/tok 3.1133 (3.4406)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.160 (0.144)	Data 2.21e-04 (4.49e-04)	Tok/s 106996 (101378)	Loss/tok 3.3830 (3.4398)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.160 (0.144)	Data 1.67e-04 (4.47e-04)	Tok/s 103610 (101339)	Loss/tok 3.5078 (3.4384)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.107 (0.144)	Data 1.66e-04 (4.44e-04)	Tok/s 95237 (101337)	Loss/tok 3.2379 (3.4378)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.160 (0.144)	Data 1.45e-04 (4.41e-04)	Tok/s 106072 (101346)	Loss/tok 3.3817 (3.4369)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.271 (0.144)	Data 1.75e-04 (4.39e-04)	Tok/s 110279 (101377)	Loss/tok 3.7090 (3.4368)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.160 (0.144)	Data 1.67e-04 (4.36e-04)	Tok/s 104084 (101357)	Loss/tok 3.4038 (3.4360)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.106 (0.143)	Data 1.64e-04 (4.34e-04)	Tok/s 96672 (101316)	Loss/tok 3.2035 (3.4345)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.212 (0.143)	Data 1.50e-04 (4.31e-04)	Tok/s 109716 (101319)	Loss/tok 3.5094 (3.4352)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1060/1938]	Time 0.160 (0.143)	Data 1.71e-04 (4.29e-04)	Tok/s 103995 (101313)	Loss/tok 3.4497 (3.4351)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.211 (0.143)	Data 1.77e-04 (4.27e-04)	Tok/s 109641 (101316)	Loss/tok 3.5893 (3.4347)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.159 (0.143)	Data 2.01e-04 (4.24e-04)	Tok/s 106074 (101313)	Loss/tok 3.3361 (3.4349)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.108 (0.143)	Data 2.11e-04 (4.22e-04)	Tok/s 96646 (101307)	Loss/tok 3.2538 (3.4338)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.107 (0.143)	Data 1.64e-04 (4.19e-04)	Tok/s 95520 (101291)	Loss/tok 3.1979 (3.4345)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.108 (0.143)	Data 1.65e-04 (4.17e-04)	Tok/s 95101 (101239)	Loss/tok 3.0840 (3.4329)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.108 (0.143)	Data 1.79e-04 (4.15e-04)	Tok/s 96629 (101225)	Loss/tok 3.1862 (3.4315)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.060 (0.143)	Data 1.79e-04 (4.13e-04)	Tok/s 89673 (101183)	Loss/tok 2.6860 (3.4299)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.059 (0.143)	Data 1.87e-04 (4.10e-04)	Tok/s 89836 (101186)	Loss/tok 2.6680 (3.4298)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.159 (0.142)	Data 1.46e-04 (4.08e-04)	Tok/s 105409 (101167)	Loss/tok 3.2675 (3.4286)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.160 (0.143)	Data 1.78e-04 (4.06e-04)	Tok/s 104992 (101187)	Loss/tok 3.4414 (3.4285)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.159 (0.143)	Data 1.50e-04 (4.04e-04)	Tok/s 105310 (101193)	Loss/tok 3.4053 (3.4281)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1180/1938]	Time 0.107 (0.143)	Data 1.77e-04 (4.02e-04)	Tok/s 96460 (101195)	Loss/tok 3.2721 (3.4284)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1190/1938]	Time 0.158 (0.143)	Data 1.68e-04 (4.00e-04)	Tok/s 106681 (101190)	Loss/tok 3.3307 (3.4284)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.107 (0.143)	Data 1.64e-04 (3.98e-04)	Tok/s 98027 (101219)	Loss/tok 3.2513 (3.4287)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.160 (0.143)	Data 1.65e-04 (3.96e-04)	Tok/s 103829 (101231)	Loss/tok 3.4634 (3.4286)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.107 (0.143)	Data 1.74e-04 (3.94e-04)	Tok/s 95781 (101227)	Loss/tok 3.2087 (3.4283)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.059 (0.143)	Data 1.81e-04 (3.92e-04)	Tok/s 90027 (101186)	Loss/tok 2.7343 (3.4274)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.160 (0.143)	Data 1.54e-04 (3.90e-04)	Tok/s 105846 (101193)	Loss/tok 3.3602 (3.4266)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.107 (0.143)	Data 1.87e-04 (3.89e-04)	Tok/s 98185 (101189)	Loss/tok 3.0662 (3.4255)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.159 (0.143)	Data 1.51e-04 (3.87e-04)	Tok/s 106190 (101201)	Loss/tok 3.4265 (3.4251)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.212 (0.143)	Data 1.40e-04 (3.85e-04)	Tok/s 109686 (101204)	Loss/tok 3.5525 (3.4247)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.107 (0.143)	Data 1.45e-04 (3.83e-04)	Tok/s 96240 (101196)	Loss/tok 3.1570 (3.4237)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.107 (0.143)	Data 1.50e-04 (3.82e-04)	Tok/s 97296 (101188)	Loss/tok 3.0820 (3.4231)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.159 (0.143)	Data 1.66e-04 (3.80e-04)	Tok/s 106810 (101189)	Loss/tok 3.3555 (3.4221)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.108 (0.142)	Data 1.47e-04 (3.78e-04)	Tok/s 93839 (101166)	Loss/tok 3.0932 (3.4209)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1320/1938]	Time 0.212 (0.142)	Data 1.59e-04 (3.77e-04)	Tok/s 109626 (101167)	Loss/tok 3.5477 (3.4210)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.159 (0.142)	Data 1.86e-04 (3.75e-04)	Tok/s 105170 (101161)	Loss/tok 3.2899 (3.4202)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.160 (0.142)	Data 1.62e-04 (3.74e-04)	Tok/s 105542 (101157)	Loss/tok 3.3668 (3.4194)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.107 (0.142)	Data 1.53e-04 (3.72e-04)	Tok/s 96459 (101132)	Loss/tok 3.2334 (3.4185)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.108 (0.142)	Data 1.78e-04 (3.71e-04)	Tok/s 96231 (101119)	Loss/tok 3.1065 (3.4176)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1370/1938]	Time 0.106 (0.142)	Data 1.80e-04 (3.69e-04)	Tok/s 96318 (101109)	Loss/tok 3.0996 (3.4172)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.159 (0.142)	Data 1.73e-04 (3.68e-04)	Tok/s 106913 (101114)	Loss/tok 3.3267 (3.4165)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.212 (0.142)	Data 2.31e-04 (3.66e-04)	Tok/s 110148 (101092)	Loss/tok 3.6318 (3.4164)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.107 (0.142)	Data 1.71e-04 (3.65e-04)	Tok/s 94516 (101095)	Loss/tok 3.0668 (3.4153)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.160 (0.142)	Data 1.69e-04 (3.63e-04)	Tok/s 104281 (101103)	Loss/tok 3.3831 (3.4144)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.107 (0.142)	Data 1.89e-04 (3.62e-04)	Tok/s 98092 (101102)	Loss/tok 3.0860 (3.4138)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.106 (0.142)	Data 1.65e-04 (3.61e-04)	Tok/s 95376 (101080)	Loss/tok 3.1761 (3.4133)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.106 (0.142)	Data 1.49e-04 (3.59e-04)	Tok/s 95333 (101067)	Loss/tok 3.0803 (3.4124)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.108 (0.142)	Data 1.66e-04 (3.58e-04)	Tok/s 97476 (101065)	Loss/tok 3.1673 (3.4121)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.160 (0.142)	Data 1.64e-04 (3.57e-04)	Tok/s 104024 (101079)	Loss/tok 3.4357 (3.4120)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.160 (0.142)	Data 1.47e-04 (3.55e-04)	Tok/s 104485 (101092)	Loss/tok 3.2729 (3.4116)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.107 (0.142)	Data 1.66e-04 (3.54e-04)	Tok/s 95892 (101058)	Loss/tok 3.1680 (3.4109)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.107 (0.141)	Data 1.47e-04 (3.53e-04)	Tok/s 96607 (101041)	Loss/tok 3.1627 (3.4101)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1500/1938]	Time 0.160 (0.141)	Data 1.69e-04 (3.51e-04)	Tok/s 105189 (101029)	Loss/tok 3.3858 (3.4099)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.213 (0.141)	Data 1.61e-04 (3.50e-04)	Tok/s 109663 (101025)	Loss/tok 3.6905 (3.4095)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.160 (0.141)	Data 1.55e-04 (3.49e-04)	Tok/s 105333 (101031)	Loss/tok 3.3156 (3.4089)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.159 (0.141)	Data 1.55e-04 (3.48e-04)	Tok/s 106326 (101029)	Loss/tok 3.4040 (3.4084)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1540/1938]	Time 0.160 (0.141)	Data 1.54e-04 (3.47e-04)	Tok/s 105261 (101029)	Loss/tok 3.3702 (3.4080)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.107 (0.142)	Data 1.89e-04 (3.45e-04)	Tok/s 95848 (101051)	Loss/tok 3.0910 (3.4083)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.107 (0.142)	Data 1.74e-04 (3.44e-04)	Tok/s 95767 (101036)	Loss/tok 3.1139 (3.4078)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.161 (0.142)	Data 1.71e-04 (3.43e-04)	Tok/s 104312 (101047)	Loss/tok 3.5018 (3.4080)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.108 (0.142)	Data 1.81e-04 (3.42e-04)	Tok/s 95200 (101041)	Loss/tok 3.1245 (3.4073)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1590/1938]	Time 0.212 (0.142)	Data 1.73e-04 (3.41e-04)	Tok/s 108524 (101055)	Loss/tok 3.5664 (3.4074)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.107 (0.142)	Data 2.19e-04 (3.40e-04)	Tok/s 98653 (101062)	Loss/tok 3.1204 (3.4074)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.108 (0.142)	Data 1.46e-04 (3.39e-04)	Tok/s 95681 (101061)	Loss/tok 3.1781 (3.4069)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.059 (0.142)	Data 1.56e-04 (3.38e-04)	Tok/s 89308 (101049)	Loss/tok 2.7860 (3.4066)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.161 (0.142)	Data 1.39e-04 (3.37e-04)	Tok/s 104134 (101052)	Loss/tok 3.3194 (3.4061)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.272 (0.142)	Data 1.97e-04 (3.36e-04)	Tok/s 109346 (101049)	Loss/tok 3.6579 (3.4059)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.160 (0.142)	Data 1.85e-04 (3.35e-04)	Tok/s 102832 (101041)	Loss/tok 3.3811 (3.4056)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.108 (0.142)	Data 1.58e-04 (3.34e-04)	Tok/s 96620 (101054)	Loss/tok 3.1365 (3.4054)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.106 (0.142)	Data 1.75e-04 (3.33e-04)	Tok/s 95463 (101055)	Loss/tok 3.2312 (3.4051)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.059 (0.142)	Data 1.84e-04 (3.32e-04)	Tok/s 89224 (101069)	Loss/tok 2.5705 (3.4054)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.159 (0.142)	Data 1.47e-04 (3.31e-04)	Tok/s 104633 (101054)	Loss/tok 3.5248 (3.4052)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.160 (0.142)	Data 1.92e-04 (3.30e-04)	Tok/s 104750 (101056)	Loss/tok 3.4197 (3.4046)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.107 (0.142)	Data 1.88e-04 (3.29e-04)	Tok/s 94878 (101053)	Loss/tok 3.1123 (3.4045)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1720/1938]	Time 0.212 (0.142)	Data 1.80e-04 (3.28e-04)	Tok/s 109510 (101037)	Loss/tok 3.4762 (3.4040)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.107 (0.142)	Data 1.31e-04 (3.27e-04)	Tok/s 95891 (101029)	Loss/tok 3.1129 (3.4032)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.212 (0.142)	Data 2.29e-04 (3.26e-04)	Tok/s 110421 (101041)	Loss/tok 3.4478 (3.4032)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.159 (0.142)	Data 1.80e-04 (3.25e-04)	Tok/s 106138 (101052)	Loss/tok 3.4237 (3.4028)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.107 (0.142)	Data 1.87e-04 (3.25e-04)	Tok/s 95616 (101028)	Loss/tok 3.1152 (3.4025)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.159 (0.142)	Data 1.47e-04 (3.24e-04)	Tok/s 105583 (101026)	Loss/tok 3.3226 (3.4023)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.107 (0.142)	Data 1.67e-04 (3.23e-04)	Tok/s 96576 (101029)	Loss/tok 3.1265 (3.4019)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.107 (0.142)	Data 1.40e-04 (3.22e-04)	Tok/s 96152 (101013)	Loss/tok 3.0638 (3.4012)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.159 (0.142)	Data 1.75e-04 (3.21e-04)	Tok/s 105341 (101015)	Loss/tok 3.2826 (3.4006)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.159 (0.142)	Data 1.73e-04 (3.20e-04)	Tok/s 104235 (101015)	Loss/tok 3.4049 (3.4003)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1820/1938]	Time 0.107 (0.142)	Data 1.49e-04 (3.20e-04)	Tok/s 98912 (101034)	Loss/tok 3.1998 (3.4008)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.107 (0.142)	Data 1.91e-04 (3.19e-04)	Tok/s 98267 (101033)	Loss/tok 3.1504 (3.4003)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.107 (0.142)	Data 1.55e-04 (3.18e-04)	Tok/s 94332 (101036)	Loss/tok 3.0311 (3.3997)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.108 (0.142)	Data 1.87e-04 (3.17e-04)	Tok/s 95138 (101011)	Loss/tok 3.2729 (3.3992)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.160 (0.142)	Data 1.97e-04 (3.16e-04)	Tok/s 103932 (101000)	Loss/tok 3.3341 (3.3985)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.107 (0.142)	Data 1.62e-04 (3.16e-04)	Tok/s 96830 (100992)	Loss/tok 3.0946 (3.3978)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.108 (0.142)	Data 1.84e-04 (3.15e-04)	Tok/s 95011 (100999)	Loss/tok 3.1043 (3.3975)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.213 (0.142)	Data 1.54e-04 (3.14e-04)	Tok/s 109002 (101006)	Loss/tok 3.5584 (3.3972)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.108 (0.142)	Data 1.62e-04 (3.13e-04)	Tok/s 95448 (101005)	Loss/tok 3.1247 (3.3967)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.159 (0.142)	Data 1.52e-04 (3.13e-04)	Tok/s 105929 (101000)	Loss/tok 3.3256 (3.3961)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.107 (0.142)	Data 1.97e-04 (3.12e-04)	Tok/s 95856 (100987)	Loss/tok 3.1949 (3.3958)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.213 (0.142)	Data 2.44e-04 (3.11e-04)	Tok/s 109050 (100990)	Loss/tok 3.5271 (3.3955)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019713690, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019713691, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.578 (0.578)	Decoder iters 92.0 (92.0)	Tok/s 27623 (27623)
0: Running moses detokenizer
0: BLEU(score=22.148258182861518, counts=[35513, 17040, 9448, 5456], totals=[63518, 60515, 57512, 54516], precisions=[55.910135709562645, 28.15830785755598, 16.427875921546807, 10.008071025020177], bp=0.9819341276978837, sys_len=63518, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019715399, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22149999999999997, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019715399, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3973	Test BLEU: 22.15
0: Performance: Epoch: 1	Training: 807717 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019715399, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019715400, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019715400, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4230967098
0: TRAIN [2][0/1938]	Time 0.406 (0.406)	Data 3.03e-01 (3.03e-01)	Tok/s 24750 (24750)	Loss/tok 3.0596 (3.0596)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][10/1938]	Time 0.213 (0.173)	Data 1.38e-04 (2.77e-02)	Tok/s 109706 (95586)	Loss/tok 3.4427 (3.2635)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.059 (0.165)	Data 1.40e-04 (1.46e-02)	Tok/s 92480 (98626)	Loss/tok 2.6205 (3.3044)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.059 (0.167)	Data 1.34e-04 (9.91e-03)	Tok/s 88470 (100342)	Loss/tok 2.5963 (3.3288)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.160 (0.165)	Data 1.95e-04 (7.53e-03)	Tok/s 105966 (101319)	Loss/tok 3.2606 (3.3207)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.161 (0.161)	Data 1.24e-04 (6.08e-03)	Tok/s 103372 (101313)	Loss/tok 3.4163 (3.3136)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.163 (0.160)	Data 1.38e-04 (5.11e-03)	Tok/s 104190 (101533)	Loss/tok 3.2651 (3.3131)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.159 (0.156)	Data 1.65e-04 (4.41e-03)	Tok/s 106614 (101095)	Loss/tok 3.3287 (3.3070)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.215 (0.156)	Data 1.24e-04 (3.88e-03)	Tok/s 108661 (101377)	Loss/tok 3.4761 (3.3034)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.060 (0.156)	Data 1.68e-04 (3.47e-03)	Tok/s 86754 (101382)	Loss/tok 2.5910 (3.3053)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][100/1938]	Time 0.212 (0.153)	Data 1.23e-04 (3.14e-03)	Tok/s 110642 (101163)	Loss/tok 3.4525 (3.2989)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.107 (0.151)	Data 1.23e-04 (2.87e-03)	Tok/s 96908 (101004)	Loss/tok 3.0697 (3.2875)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.108 (0.149)	Data 1.30e-04 (2.64e-03)	Tok/s 93732 (100810)	Loss/tok 3.0519 (3.2818)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.108 (0.146)	Data 1.26e-04 (2.45e-03)	Tok/s 96157 (100602)	Loss/tok 3.1493 (3.2725)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.161 (0.147)	Data 1.31e-04 (2.29e-03)	Tok/s 104045 (100925)	Loss/tok 3.2890 (3.2728)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.107 (0.146)	Data 1.28e-04 (2.15e-03)	Tok/s 94195 (100734)	Loss/tok 3.0328 (3.2725)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.159 (0.146)	Data 1.39e-04 (2.02e-03)	Tok/s 105033 (100720)	Loss/tok 3.3821 (3.2779)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.159 (0.146)	Data 1.41e-04 (1.91e-03)	Tok/s 105476 (100768)	Loss/tok 3.2934 (3.2753)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.107 (0.146)	Data 1.63e-04 (1.81e-03)	Tok/s 95870 (100868)	Loss/tok 3.0572 (3.2735)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.107 (0.144)	Data 1.40e-04 (1.73e-03)	Tok/s 98407 (100781)	Loss/tok 3.0390 (3.2648)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.273 (0.144)	Data 1.21e-04 (1.65e-03)	Tok/s 110151 (100812)	Loss/tok 3.4923 (3.2637)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.160 (0.143)	Data 1.61e-04 (1.58e-03)	Tok/s 105225 (100714)	Loss/tok 3.2466 (3.2589)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.159 (0.143)	Data 1.18e-04 (1.51e-03)	Tok/s 106217 (100674)	Loss/tok 3.2930 (3.2590)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][230/1938]	Time 0.107 (0.142)	Data 1.39e-04 (1.45e-03)	Tok/s 96713 (100618)	Loss/tok 2.9460 (3.2539)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.108 (0.143)	Data 1.73e-04 (1.40e-03)	Tok/s 95395 (100667)	Loss/tok 3.0359 (3.2559)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.212 (0.143)	Data 1.20e-04 (1.35e-03)	Tok/s 110219 (100769)	Loss/tok 3.4124 (3.2608)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.160 (0.144)	Data 1.29e-04 (1.30e-03)	Tok/s 106186 (100924)	Loss/tok 3.3441 (3.2615)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.107 (0.144)	Data 1.30e-04 (1.26e-03)	Tok/s 97562 (100889)	Loss/tok 2.9427 (3.2598)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.059 (0.144)	Data 1.21e-04 (1.22e-03)	Tok/s 88397 (100877)	Loss/tok 2.7105 (3.2593)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.160 (0.144)	Data 1.43e-04 (1.18e-03)	Tok/s 105556 (100996)	Loss/tok 3.2678 (3.2617)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.108 (0.144)	Data 1.23e-04 (1.15e-03)	Tok/s 93264 (100904)	Loss/tok 3.0124 (3.2607)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.159 (0.145)	Data 1.87e-04 (1.11e-03)	Tok/s 107685 (101023)	Loss/tok 3.1778 (3.2631)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.159 (0.144)	Data 1.20e-04 (1.08e-03)	Tok/s 106322 (100884)	Loss/tok 3.4064 (3.2629)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.107 (0.144)	Data 1.23e-04 (1.06e-03)	Tok/s 98153 (100933)	Loss/tok 3.1302 (3.2648)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.107 (0.144)	Data 1.43e-04 (1.03e-03)	Tok/s 96663 (100894)	Loss/tok 2.9912 (3.2669)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][350/1938]	Time 0.060 (0.144)	Data 1.80e-04 (1.00e-03)	Tok/s 88875 (100885)	Loss/tok 2.6125 (3.2663)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.107 (0.143)	Data 1.21e-04 (9.80e-04)	Tok/s 98353 (100864)	Loss/tok 3.1462 (3.2666)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.108 (0.143)	Data 1.49e-04 (9.57e-04)	Tok/s 96165 (100833)	Loss/tok 3.0106 (3.2642)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1938]	Time 0.212 (0.143)	Data 1.46e-04 (9.36e-04)	Tok/s 110142 (100864)	Loss/tok 3.4580 (3.2644)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.212 (0.143)	Data 1.20e-04 (9.15e-04)	Tok/s 109085 (100858)	Loss/tok 3.4976 (3.2650)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.108 (0.143)	Data 1.29e-04 (8.96e-04)	Tok/s 96780 (100848)	Loss/tok 2.9797 (3.2641)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.273 (0.143)	Data 1.84e-04 (8.78e-04)	Tok/s 109780 (100877)	Loss/tok 3.6202 (3.2652)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.160 (0.143)	Data 2.28e-04 (8.60e-04)	Tok/s 105121 (100877)	Loss/tok 3.2805 (3.2650)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.060 (0.142)	Data 1.32e-04 (8.44e-04)	Tok/s 87268 (100766)	Loss/tok 2.7062 (3.2624)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.107 (0.142)	Data 1.63e-04 (8.28e-04)	Tok/s 95365 (100753)	Loss/tok 3.0740 (3.2615)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.213 (0.143)	Data 1.61e-04 (8.13e-04)	Tok/s 109184 (100782)	Loss/tok 3.4307 (3.2647)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.212 (0.143)	Data 1.76e-04 (7.99e-04)	Tok/s 109842 (100845)	Loss/tok 3.3489 (3.2649)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.161 (0.143)	Data 1.73e-04 (7.85e-04)	Tok/s 104742 (100810)	Loss/tok 3.1361 (3.2621)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.108 (0.143)	Data 1.41e-04 (7.72e-04)	Tok/s 96845 (100817)	Loss/tok 3.0625 (3.2628)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.159 (0.142)	Data 1.49e-04 (7.60e-04)	Tok/s 104828 (100782)	Loss/tok 3.2096 (3.2622)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.107 (0.142)	Data 1.44e-04 (7.48e-04)	Tok/s 97857 (100791)	Loss/tok 3.1189 (3.2630)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][510/1938]	Time 0.107 (0.142)	Data 1.38e-04 (7.36e-04)	Tok/s 95977 (100754)	Loss/tok 3.1360 (3.2623)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][520/1938]	Time 0.160 (0.143)	Data 1.46e-04 (7.25e-04)	Tok/s 104834 (100808)	Loss/tok 3.1596 (3.2663)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.108 (0.143)	Data 1.73e-04 (7.14e-04)	Tok/s 95631 (100851)	Loss/tok 3.1056 (3.2685)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.161 (0.143)	Data 1.41e-04 (7.03e-04)	Tok/s 104659 (100829)	Loss/tok 3.2477 (3.2682)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.213 (0.143)	Data 1.45e-04 (6.93e-04)	Tok/s 110446 (100801)	Loss/tok 3.4281 (3.2671)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.161 (0.143)	Data 1.41e-04 (6.83e-04)	Tok/s 103159 (100783)	Loss/tok 3.3388 (3.2663)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.059 (0.143)	Data 1.41e-04 (6.74e-04)	Tok/s 91798 (100784)	Loss/tok 2.6098 (3.2677)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.107 (0.143)	Data 1.26e-04 (6.65e-04)	Tok/s 95308 (100736)	Loss/tok 3.0777 (3.2658)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.212 (0.143)	Data 1.46e-04 (6.56e-04)	Tok/s 109105 (100757)	Loss/tok 3.5599 (3.2676)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.107 (0.142)	Data 1.42e-04 (6.48e-04)	Tok/s 96807 (100709)	Loss/tok 3.1277 (3.2659)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.108 (0.142)	Data 1.46e-04 (6.40e-04)	Tok/s 93672 (100737)	Loss/tok 2.9336 (3.2658)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.159 (0.143)	Data 1.51e-04 (6.32e-04)	Tok/s 104870 (100768)	Loss/tok 3.2020 (3.2672)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.212 (0.143)	Data 1.44e-04 (6.24e-04)	Tok/s 110448 (100740)	Loss/tok 3.3244 (3.2664)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.212 (0.143)	Data 1.60e-04 (6.16e-04)	Tok/s 109137 (100751)	Loss/tok 3.5794 (3.2663)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][650/1938]	Time 0.106 (0.143)	Data 1.20e-04 (6.09e-04)	Tok/s 97620 (100791)	Loss/tok 3.0089 (3.2684)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.107 (0.143)	Data 1.44e-04 (6.02e-04)	Tok/s 97923 (100840)	Loss/tok 3.0559 (3.2700)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.160 (0.143)	Data 1.96e-04 (5.95e-04)	Tok/s 104657 (100836)	Loss/tok 3.3243 (3.2699)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.109 (0.143)	Data 1.43e-04 (5.89e-04)	Tok/s 94377 (100805)	Loss/tok 3.1321 (3.2686)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.213 (0.143)	Data 1.61e-04 (5.83e-04)	Tok/s 108873 (100778)	Loss/tok 3.5389 (3.2688)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.213 (0.143)	Data 1.44e-04 (5.77e-04)	Tok/s 107756 (100802)	Loss/tok 3.3928 (3.2689)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.108 (0.144)	Data 1.58e-04 (5.71e-04)	Tok/s 94981 (100854)	Loss/tok 3.0046 (3.2718)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.108 (0.143)	Data 1.46e-04 (5.65e-04)	Tok/s 95898 (100816)	Loss/tok 2.9592 (3.2698)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.160 (0.143)	Data 1.22e-04 (5.59e-04)	Tok/s 105663 (100805)	Loss/tok 3.2262 (3.2686)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.107 (0.143)	Data 1.43e-04 (5.54e-04)	Tok/s 98375 (100836)	Loss/tok 3.1187 (3.2700)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.213 (0.143)	Data 1.91e-04 (5.49e-04)	Tok/s 109076 (100856)	Loss/tok 3.5088 (3.2696)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.160 (0.143)	Data 1.45e-04 (5.44e-04)	Tok/s 105190 (100832)	Loss/tok 3.3084 (3.2687)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.161 (0.143)	Data 1.49e-04 (5.39e-04)	Tok/s 105490 (100823)	Loss/tok 3.3903 (3.2689)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][780/1938]	Time 0.212 (0.143)	Data 1.50e-04 (5.34e-04)	Tok/s 109398 (100842)	Loss/tok 3.4902 (3.2706)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.107 (0.143)	Data 1.44e-04 (5.29e-04)	Tok/s 97593 (100829)	Loss/tok 2.9342 (3.2691)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.108 (0.143)	Data 1.92e-04 (5.24e-04)	Tok/s 96814 (100802)	Loss/tok 3.0365 (3.2678)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.107 (0.143)	Data 1.49e-04 (5.20e-04)	Tok/s 96294 (100827)	Loss/tok 2.9726 (3.2689)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][820/1938]	Time 0.160 (0.143)	Data 1.34e-04 (5.15e-04)	Tok/s 104301 (100837)	Loss/tok 3.3633 (3.2689)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.059 (0.143)	Data 1.43e-04 (5.11e-04)	Tok/s 91124 (100781)	Loss/tok 2.5898 (3.2678)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.108 (0.142)	Data 1.50e-04 (5.07e-04)	Tok/s 94848 (100764)	Loss/tok 3.0238 (3.2679)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.059 (0.142)	Data 1.49e-04 (5.03e-04)	Tok/s 88235 (100782)	Loss/tok 2.6855 (3.2676)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.160 (0.143)	Data 1.89e-04 (4.99e-04)	Tok/s 103889 (100810)	Loss/tok 3.2901 (3.2679)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.107 (0.143)	Data 1.47e-04 (4.95e-04)	Tok/s 95581 (100835)	Loss/tok 2.9983 (3.2683)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.107 (0.143)	Data 1.68e-04 (4.91e-04)	Tok/s 98209 (100854)	Loss/tok 3.0704 (3.2696)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.159 (0.143)	Data 2.00e-04 (4.87e-04)	Tok/s 105207 (100898)	Loss/tok 3.3236 (3.2706)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.058 (0.143)	Data 1.62e-04 (4.84e-04)	Tok/s 90503 (100872)	Loss/tok 2.5415 (3.2702)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.273 (0.143)	Data 1.60e-04 (4.80e-04)	Tok/s 110595 (100884)	Loss/tok 3.6700 (3.2708)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.107 (0.143)	Data 1.65e-04 (4.77e-04)	Tok/s 98116 (100891)	Loss/tok 2.9098 (3.2701)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.273 (0.143)	Data 1.76e-04 (4.74e-04)	Tok/s 109077 (100888)	Loss/tok 3.6291 (3.2709)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.107 (0.143)	Data 2.48e-04 (4.71e-04)	Tok/s 95707 (100890)	Loss/tok 2.9894 (3.2714)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][950/1938]	Time 0.160 (0.143)	Data 1.64e-04 (4.67e-04)	Tok/s 103702 (100903)	Loss/tok 3.2568 (3.2714)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.160 (0.143)	Data 1.75e-04 (4.64e-04)	Tok/s 105679 (100890)	Loss/tok 3.2989 (3.2716)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][970/1938]	Time 0.214 (0.144)	Data 1.62e-04 (4.61e-04)	Tok/s 108115 (100917)	Loss/tok 3.4714 (3.2730)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.108 (0.144)	Data 1.96e-04 (4.59e-04)	Tok/s 96410 (100939)	Loss/tok 3.0333 (3.2734)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.160 (0.144)	Data 1.64e-04 (4.56e-04)	Tok/s 103482 (100928)	Loss/tok 3.2007 (3.2723)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.109 (0.144)	Data 1.48e-04 (4.53e-04)	Tok/s 96064 (100927)	Loss/tok 3.0961 (3.2714)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.058 (0.144)	Data 1.79e-04 (4.50e-04)	Tok/s 91588 (100944)	Loss/tok 2.7426 (3.2720)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.108 (0.144)	Data 1.75e-04 (4.47e-04)	Tok/s 94537 (100927)	Loss/tok 2.9824 (3.2722)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.212 (0.144)	Data 1.67e-04 (4.45e-04)	Tok/s 111113 (100980)	Loss/tok 3.4636 (3.2733)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.060 (0.144)	Data 1.82e-04 (4.42e-04)	Tok/s 86839 (100967)	Loss/tok 2.6536 (3.2724)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.107 (0.144)	Data 1.55e-04 (4.40e-04)	Tok/s 96866 (100965)	Loss/tok 3.0293 (3.2716)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.059 (0.144)	Data 2.22e-04 (4.37e-04)	Tok/s 91019 (100979)	Loss/tok 2.6296 (3.2724)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.159 (0.144)	Data 2.58e-04 (4.35e-04)	Tok/s 105273 (100987)	Loss/tok 3.2821 (3.2717)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.160 (0.144)	Data 1.61e-04 (4.33e-04)	Tok/s 104164 (100988)	Loss/tok 3.2977 (3.2722)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.107 (0.144)	Data 2.34e-04 (4.30e-04)	Tok/s 96443 (101023)	Loss/tok 3.0137 (3.2729)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1100/1938]	Time 0.108 (0.144)	Data 1.63e-04 (4.28e-04)	Tok/s 94685 (101037)	Loss/tok 3.0366 (3.2727)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.059 (0.144)	Data 1.96e-04 (4.26e-04)	Tok/s 89918 (101003)	Loss/tok 2.6335 (3.2716)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.107 (0.144)	Data 1.70e-04 (4.24e-04)	Tok/s 97752 (100998)	Loss/tok 2.9888 (3.2712)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.107 (0.144)	Data 1.65e-04 (4.21e-04)	Tok/s 96932 (100976)	Loss/tok 2.9990 (3.2706)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.059 (0.143)	Data 1.97e-04 (4.19e-04)	Tok/s 90654 (100968)	Loss/tok 2.6311 (3.2709)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.107 (0.143)	Data 1.91e-04 (4.17e-04)	Tok/s 95866 (100945)	Loss/tok 3.1812 (3.2711)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.107 (0.143)	Data 1.75e-04 (4.15e-04)	Tok/s 95763 (100958)	Loss/tok 3.0387 (3.2713)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.160 (0.143)	Data 1.61e-04 (4.13e-04)	Tok/s 105056 (100964)	Loss/tok 3.2115 (3.2709)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1180/1938]	Time 0.213 (0.144)	Data 2.56e-04 (4.11e-04)	Tok/s 108114 (100977)	Loss/tok 3.6160 (3.2714)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.107 (0.143)	Data 2.58e-04 (4.09e-04)	Tok/s 97416 (100961)	Loss/tok 3.0842 (3.2711)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.059 (0.143)	Data 1.85e-04 (4.07e-04)	Tok/s 89977 (100928)	Loss/tok 2.5883 (3.2701)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.107 (0.143)	Data 1.60e-04 (4.05e-04)	Tok/s 94594 (100886)	Loss/tok 2.9597 (3.2689)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.107 (0.143)	Data 1.82e-04 (4.04e-04)	Tok/s 95098 (100879)	Loss/tok 2.9383 (3.2681)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.213 (0.143)	Data 1.69e-04 (4.02e-04)	Tok/s 108793 (100882)	Loss/tok 3.4332 (3.2681)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.107 (0.143)	Data 1.44e-04 (4.00e-04)	Tok/s 97130 (100864)	Loss/tok 3.0045 (3.2675)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.159 (0.143)	Data 1.82e-04 (3.98e-04)	Tok/s 105889 (100869)	Loss/tok 3.2193 (3.2673)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.160 (0.143)	Data 2.55e-04 (3.97e-04)	Tok/s 104638 (100903)	Loss/tok 3.3935 (3.2674)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.160 (0.143)	Data 2.02e-04 (3.95e-04)	Tok/s 106371 (100913)	Loss/tok 3.2775 (3.2673)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.161 (0.143)	Data 1.74e-04 (3.93e-04)	Tok/s 105316 (100934)	Loss/tok 3.3201 (3.2678)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.161 (0.143)	Data 2.01e-04 (3.92e-04)	Tok/s 104870 (100950)	Loss/tok 3.2482 (3.2683)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.107 (0.143)	Data 2.18e-04 (3.90e-04)	Tok/s 96469 (100936)	Loss/tok 3.0023 (3.2676)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1310/1938]	Time 0.108 (0.143)	Data 1.64e-04 (3.89e-04)	Tok/s 94532 (100952)	Loss/tok 2.9500 (3.2690)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1320/1938]	Time 0.107 (0.143)	Data 1.80e-04 (3.87e-04)	Tok/s 97493 (100916)	Loss/tok 3.1461 (3.2682)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.159 (0.143)	Data 2.44e-04 (3.85e-04)	Tok/s 106404 (100887)	Loss/tok 3.2081 (3.2674)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.107 (0.143)	Data 2.29e-04 (3.84e-04)	Tok/s 96726 (100875)	Loss/tok 3.1066 (3.2668)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.058 (0.143)	Data 1.68e-04 (3.82e-04)	Tok/s 91610 (100871)	Loss/tok 2.6284 (3.2663)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.107 (0.143)	Data 1.69e-04 (3.81e-04)	Tok/s 94755 (100837)	Loss/tok 3.0321 (3.2650)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.108 (0.142)	Data 1.48e-04 (3.80e-04)	Tok/s 94921 (100811)	Loss/tok 2.9468 (3.2640)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.214 (0.142)	Data 1.58e-04 (3.78e-04)	Tok/s 110066 (100816)	Loss/tok 3.3704 (3.2640)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.106 (0.142)	Data 2.07e-04 (3.77e-04)	Tok/s 96918 (100807)	Loss/tok 3.0631 (3.2634)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.159 (0.142)	Data 1.72e-04 (3.75e-04)	Tok/s 104623 (100803)	Loss/tok 3.3306 (3.2633)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.159 (0.142)	Data 1.46e-04 (3.74e-04)	Tok/s 107058 (100804)	Loss/tok 3.1960 (3.2632)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.107 (0.142)	Data 1.57e-04 (3.72e-04)	Tok/s 98053 (100759)	Loss/tok 3.1317 (3.2621)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.107 (0.142)	Data 1.62e-04 (3.71e-04)	Tok/s 96641 (100759)	Loss/tok 3.0090 (3.2615)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.108 (0.142)	Data 1.52e-04 (3.70e-04)	Tok/s 95289 (100761)	Loss/tok 2.9973 (3.2616)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1450/1938]	Time 0.108 (0.142)	Data 1.48e-04 (3.68e-04)	Tok/s 97087 (100762)	Loss/tok 3.0992 (3.2616)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1460/1938]	Time 0.213 (0.142)	Data 1.46e-04 (3.67e-04)	Tok/s 109979 (100773)	Loss/tok 3.3588 (3.2618)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.108 (0.142)	Data 1.70e-04 (3.65e-04)	Tok/s 95342 (100760)	Loss/tok 3.0644 (3.2614)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.107 (0.142)	Data 1.73e-04 (3.64e-04)	Tok/s 95721 (100758)	Loss/tok 3.1340 (3.2610)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.160 (0.142)	Data 1.48e-04 (3.63e-04)	Tok/s 104608 (100729)	Loss/tok 3.1897 (3.2601)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.107 (0.142)	Data 1.29e-04 (3.61e-04)	Tok/s 97031 (100728)	Loss/tok 3.0890 (3.2601)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.108 (0.141)	Data 1.30e-04 (3.60e-04)	Tok/s 95168 (100727)	Loss/tok 2.8902 (3.2596)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.109 (0.141)	Data 2.10e-04 (3.59e-04)	Tok/s 94196 (100711)	Loss/tok 3.0026 (3.2589)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.107 (0.141)	Data 1.76e-04 (3.57e-04)	Tok/s 95395 (100705)	Loss/tok 3.0301 (3.2588)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.107 (0.141)	Data 2.16e-04 (3.56e-04)	Tok/s 95656 (100705)	Loss/tok 3.0312 (3.2592)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.159 (0.142)	Data 1.73e-04 (3.55e-04)	Tok/s 104941 (100723)	Loss/tok 3.1265 (3.2598)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.159 (0.142)	Data 1.51e-04 (3.54e-04)	Tok/s 106024 (100739)	Loss/tok 3.1806 (3.2600)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.160 (0.142)	Data 1.34e-04 (3.52e-04)	Tok/s 105203 (100755)	Loss/tok 3.1317 (3.2603)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.160 (0.142)	Data 1.49e-04 (3.51e-04)	Tok/s 104479 (100750)	Loss/tok 3.3115 (3.2598)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1590/1938]	Time 0.059 (0.142)	Data 1.47e-04 (3.50e-04)	Tok/s 90910 (100738)	Loss/tok 2.6330 (3.2599)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.108 (0.142)	Data 1.65e-04 (3.49e-04)	Tok/s 96006 (100733)	Loss/tok 3.0117 (3.2595)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.159 (0.142)	Data 1.47e-04 (3.48e-04)	Tok/s 106012 (100738)	Loss/tok 3.1655 (3.2596)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.160 (0.142)	Data 1.61e-04 (3.46e-04)	Tok/s 104926 (100730)	Loss/tok 3.1966 (3.2588)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.107 (0.141)	Data 1.91e-04 (3.45e-04)	Tok/s 94800 (100722)	Loss/tok 3.0619 (3.2582)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.160 (0.141)	Data 1.29e-04 (3.44e-04)	Tok/s 104199 (100712)	Loss/tok 3.2323 (3.2576)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.160 (0.141)	Data 1.75e-04 (3.43e-04)	Tok/s 104036 (100721)	Loss/tok 3.2228 (3.2577)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.159 (0.141)	Data 1.65e-04 (3.42e-04)	Tok/s 105960 (100709)	Loss/tok 3.3610 (3.2575)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.107 (0.142)	Data 1.49e-04 (3.41e-04)	Tok/s 96379 (100721)	Loss/tok 3.0361 (3.2579)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.160 (0.142)	Data 1.67e-04 (3.40e-04)	Tok/s 104581 (100728)	Loss/tok 3.1799 (3.2578)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.060 (0.142)	Data 1.58e-04 (3.39e-04)	Tok/s 90461 (100733)	Loss/tok 2.6968 (3.2577)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1700/1938]	Time 0.160 (0.142)	Data 1.64e-04 (3.38e-04)	Tok/s 103944 (100738)	Loss/tok 3.2278 (3.2576)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.060 (0.142)	Data 1.48e-04 (3.37e-04)	Tok/s 88854 (100742)	Loss/tok 2.6056 (3.2581)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.161 (0.142)	Data 1.91e-04 (3.36e-04)	Tok/s 103732 (100744)	Loss/tok 3.2172 (3.2577)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.212 (0.142)	Data 1.92e-04 (3.35e-04)	Tok/s 111126 (100746)	Loss/tok 3.3287 (3.2579)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.273 (0.142)	Data 1.67e-04 (3.34e-04)	Tok/s 108875 (100755)	Loss/tok 3.6276 (3.2582)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.107 (0.142)	Data 1.84e-04 (3.33e-04)	Tok/s 98968 (100748)	Loss/tok 3.0156 (3.2580)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.272 (0.142)	Data 1.61e-04 (3.32e-04)	Tok/s 110830 (100757)	Loss/tok 3.4631 (3.2584)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.107 (0.142)	Data 1.64e-04 (3.31e-04)	Tok/s 97513 (100759)	Loss/tok 3.0171 (3.2591)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.160 (0.142)	Data 1.52e-04 (3.30e-04)	Tok/s 105352 (100759)	Loss/tok 3.1018 (3.2585)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.108 (0.142)	Data 1.66e-04 (3.29e-04)	Tok/s 95422 (100758)	Loss/tok 3.0210 (3.2591)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.108 (0.142)	Data 1.52e-04 (3.28e-04)	Tok/s 93649 (100756)	Loss/tok 3.1030 (3.2589)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.160 (0.142)	Data 1.44e-04 (3.27e-04)	Tok/s 103594 (100780)	Loss/tok 3.2498 (3.2593)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1820/1938]	Time 0.059 (0.142)	Data 1.56e-04 (3.26e-04)	Tok/s 89891 (100764)	Loss/tok 2.4892 (3.2589)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1830/1938]	Time 0.160 (0.142)	Data 1.34e-04 (3.25e-04)	Tok/s 105313 (100765)	Loss/tok 3.3100 (3.2590)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.213 (0.142)	Data 1.39e-04 (3.24e-04)	Tok/s 108890 (100775)	Loss/tok 3.4017 (3.2589)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.161 (0.142)	Data 1.47e-04 (3.23e-04)	Tok/s 103659 (100787)	Loss/tok 3.2425 (3.2592)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.159 (0.142)	Data 1.78e-04 (3.22e-04)	Tok/s 105496 (100776)	Loss/tok 3.2679 (3.2592)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.273 (0.142)	Data 1.60e-04 (3.21e-04)	Tok/s 108453 (100787)	Loss/tok 3.6440 (3.2596)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.108 (0.142)	Data 1.67e-04 (3.20e-04)	Tok/s 96403 (100791)	Loss/tok 3.0149 (3.2594)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.107 (0.142)	Data 1.33e-04 (3.19e-04)	Tok/s 97492 (100787)	Loss/tok 2.8968 (3.2590)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.159 (0.142)	Data 1.42e-04 (3.18e-04)	Tok/s 104690 (100780)	Loss/tok 3.2439 (3.2587)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.108 (0.142)	Data 1.41e-04 (3.18e-04)	Tok/s 97837 (100782)	Loss/tok 3.0759 (3.2586)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.107 (0.142)	Data 1.55e-04 (3.17e-04)	Tok/s 96662 (100771)	Loss/tok 3.0545 (3.2584)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1930/1938]	Time 0.274 (0.142)	Data 1.39e-04 (3.16e-04)	Tok/s 108128 (100770)	Loss/tok 3.5342 (3.2586)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019991199, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019991199, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.738 (0.738)	Decoder iters 142.0 (142.0)	Tok/s 22408 (22408)
0: Running moses detokenizer
0: BLEU(score=23.296505432850623, counts=[36648, 18026, 10160, 5966], totals=[65317, 62314, 59311, 56314], precisions=[56.10790452715219, 28.92768880187438, 17.130043330916692, 10.594168412828072], bp=1.0, sys_len=65317, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019993102, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.233, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019993102, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2576	Test BLEU: 23.30
0: Performance: Epoch: 2	Training: 805671 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593019993103, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019993103, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019993103, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3832583345
0: TRAIN [3][0/1938]	Time 0.373 (0.373)	Data 2.64e-01 (2.64e-01)	Tok/s 27568 (27568)	Loss/tok 2.8540 (2.8540)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.108 (0.146)	Data 1.39e-04 (2.42e-02)	Tok/s 94042 (92533)	Loss/tok 2.8601 (3.0357)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.106 (0.155)	Data 1.22e-04 (1.27e-02)	Tok/s 97639 (97835)	Loss/tok 2.8746 (3.1521)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.107 (0.152)	Data 1.44e-04 (8.66e-03)	Tok/s 95557 (98862)	Loss/tok 2.9704 (3.1448)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.161 (0.145)	Data 1.23e-04 (6.58e-03)	Tok/s 104046 (98813)	Loss/tok 3.1768 (3.1344)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.160 (0.143)	Data 1.29e-04 (5.32e-03)	Tok/s 105206 (98950)	Loss/tok 3.1815 (3.1294)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.112 (0.141)	Data 1.42e-04 (4.47e-03)	Tok/s 90886 (98951)	Loss/tok 3.0427 (3.1297)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.213 (0.144)	Data 1.43e-04 (3.86e-03)	Tok/s 108989 (99732)	Loss/tok 3.2975 (3.1444)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.107 (0.145)	Data 1.12e-04 (3.40e-03)	Tok/s 95534 (100033)	Loss/tok 3.0321 (3.1503)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.273 (0.144)	Data 1.31e-04 (3.04e-03)	Tok/s 107652 (100039)	Loss/tok 3.5933 (3.1521)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.059 (0.140)	Data 1.21e-04 (2.75e-03)	Tok/s 89784 (99606)	Loss/tok 2.4902 (3.1380)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.160 (0.141)	Data 1.29e-04 (2.52e-03)	Tok/s 105615 (99825)	Loss/tok 3.2172 (3.1459)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][120/1938]	Time 0.059 (0.141)	Data 1.24e-04 (2.32e-03)	Tok/s 90583 (100028)	Loss/tok 2.5489 (3.1451)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.059 (0.143)	Data 1.23e-04 (2.16e-03)	Tok/s 89550 (100240)	Loss/tok 2.5604 (3.1531)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.273 (0.145)	Data 1.22e-04 (2.01e-03)	Tok/s 108316 (100499)	Loss/tok 3.6042 (3.1670)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.160 (0.146)	Data 1.47e-04 (1.89e-03)	Tok/s 105086 (100671)	Loss/tok 3.0558 (3.1658)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.107 (0.145)	Data 1.20e-04 (1.78e-03)	Tok/s 95666 (100657)	Loss/tok 2.8730 (3.1633)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.212 (0.146)	Data 1.28e-04 (1.68e-03)	Tok/s 108462 (100717)	Loss/tok 3.4364 (3.1677)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.160 (0.146)	Data 1.34e-04 (1.60e-03)	Tok/s 105524 (100757)	Loss/tok 3.1529 (3.1657)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.161 (0.145)	Data 1.21e-04 (1.52e-03)	Tok/s 103704 (100726)	Loss/tok 3.1651 (3.1641)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.107 (0.145)	Data 1.42e-04 (1.45e-03)	Tok/s 94513 (100761)	Loss/tok 3.0299 (3.1635)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.107 (0.145)	Data 1.40e-04 (1.39e-03)	Tok/s 96225 (100785)	Loss/tok 3.0825 (3.1627)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.160 (0.145)	Data 1.67e-04 (1.34e-03)	Tok/s 104967 (100845)	Loss/tok 3.1124 (3.1640)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.108 (0.144)	Data 1.46e-04 (1.29e-03)	Tok/s 95527 (100844)	Loss/tok 3.0163 (3.1622)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][240/1938]	Time 0.109 (0.144)	Data 1.47e-04 (1.24e-03)	Tok/s 95538 (100840)	Loss/tok 2.9169 (3.1639)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.274 (0.146)	Data 1.52e-04 (1.20e-03)	Tok/s 109554 (100969)	Loss/tok 3.4155 (3.1682)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.107 (0.146)	Data 1.49e-04 (1.16e-03)	Tok/s 97322 (100954)	Loss/tok 2.9876 (3.1685)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][270/1938]	Time 0.107 (0.147)	Data 2.38e-04 (1.12e-03)	Tok/s 95843 (101099)	Loss/tok 2.9436 (3.1743)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.108 (0.146)	Data 1.60e-04 (1.09e-03)	Tok/s 95580 (100984)	Loss/tok 3.0337 (3.1724)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.212 (0.146)	Data 1.47e-04 (1.06e-03)	Tok/s 110673 (101010)	Loss/tok 3.2793 (3.1705)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.160 (0.146)	Data 2.50e-04 (1.03e-03)	Tok/s 103632 (100972)	Loss/tok 3.1233 (3.1701)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.107 (0.146)	Data 1.57e-04 (9.99e-04)	Tok/s 96322 (100982)	Loss/tok 2.8977 (3.1710)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.059 (0.146)	Data 1.25e-04 (9.73e-04)	Tok/s 91740 (100946)	Loss/tok 2.6215 (3.1717)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.108 (0.145)	Data 1.65e-04 (9.49e-04)	Tok/s 93750 (100859)	Loss/tok 2.9903 (3.1700)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.159 (0.145)	Data 1.44e-04 (9.26e-04)	Tok/s 105617 (100848)	Loss/tok 3.1709 (3.1685)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.274 (0.145)	Data 1.77e-04 (9.05e-04)	Tok/s 108841 (100855)	Loss/tok 3.5270 (3.1715)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.160 (0.145)	Data 1.46e-04 (8.84e-04)	Tok/s 107217 (100889)	Loss/tok 3.1220 (3.1728)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.060 (0.145)	Data 2.46e-04 (8.65e-04)	Tok/s 87755 (100823)	Loss/tok 2.7348 (3.1718)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.108 (0.145)	Data 1.83e-04 (8.47e-04)	Tok/s 95169 (100802)	Loss/tok 2.9796 (3.1713)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.108 (0.145)	Data 2.16e-04 (8.30e-04)	Tok/s 93983 (100782)	Loss/tok 3.0022 (3.1722)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][400/1938]	Time 0.060 (0.145)	Data 1.83e-04 (8.13e-04)	Tok/s 88870 (100829)	Loss/tok 2.5586 (3.1731)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.107 (0.145)	Data 1.56e-04 (7.98e-04)	Tok/s 95122 (100823)	Loss/tok 2.9805 (3.1733)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.161 (0.144)	Data 1.73e-04 (7.83e-04)	Tok/s 103894 (100765)	Loss/tok 3.1028 (3.1706)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.161 (0.145)	Data 1.91e-04 (7.69e-04)	Tok/s 105610 (100832)	Loss/tok 3.0888 (3.1724)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.107 (0.145)	Data 2.00e-04 (7.56e-04)	Tok/s 97703 (100810)	Loss/tok 3.1309 (3.1726)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.160 (0.144)	Data 1.72e-04 (7.44e-04)	Tok/s 105025 (100782)	Loss/tok 3.1943 (3.1715)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.108 (0.144)	Data 1.64e-04 (7.31e-04)	Tok/s 93884 (100721)	Loss/tok 3.0427 (3.1708)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.108 (0.144)	Data 1.95e-04 (7.19e-04)	Tok/s 95596 (100704)	Loss/tok 2.9495 (3.1700)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.214 (0.144)	Data 1.59e-04 (7.08e-04)	Tok/s 108293 (100702)	Loss/tok 3.2353 (3.1685)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.160 (0.143)	Data 2.37e-04 (6.98e-04)	Tok/s 104434 (100660)	Loss/tok 3.0991 (3.1685)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][500/1938]	Time 0.160 (0.143)	Data 1.96e-04 (6.87e-04)	Tok/s 104307 (100671)	Loss/tok 3.2986 (3.1698)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.160 (0.144)	Data 1.91e-04 (6.78e-04)	Tok/s 104869 (100703)	Loss/tok 3.1646 (3.1706)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.108 (0.144)	Data 1.85e-04 (6.68e-04)	Tok/s 94022 (100741)	Loss/tok 2.9841 (3.1719)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.107 (0.144)	Data 1.23e-04 (6.59e-04)	Tok/s 95664 (100707)	Loss/tok 2.9524 (3.1727)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][540/1938]	Time 0.273 (0.144)	Data 1.92e-04 (6.50e-04)	Tok/s 108858 (100734)	Loss/tok 3.5923 (3.1740)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.159 (0.144)	Data 1.84e-04 (6.42e-04)	Tok/s 106381 (100695)	Loss/tok 3.2003 (3.1734)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.160 (0.144)	Data 1.41e-04 (6.33e-04)	Tok/s 104379 (100687)	Loss/tok 3.1345 (3.1736)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.107 (0.144)	Data 2.39e-04 (6.26e-04)	Tok/s 96776 (100724)	Loss/tok 3.0461 (3.1745)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.214 (0.143)	Data 1.76e-04 (6.18e-04)	Tok/s 109134 (100684)	Loss/tok 3.3213 (3.1730)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.274 (0.144)	Data 1.61e-04 (6.10e-04)	Tok/s 106919 (100702)	Loss/tok 3.6543 (3.1746)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.160 (0.144)	Data 1.38e-04 (6.03e-04)	Tok/s 104569 (100713)	Loss/tok 3.1353 (3.1739)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.108 (0.143)	Data 1.62e-04 (5.96e-04)	Tok/s 97028 (100693)	Loss/tok 2.9776 (3.1729)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.107 (0.143)	Data 2.00e-04 (5.89e-04)	Tok/s 96902 (100679)	Loss/tok 2.9409 (3.1718)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.107 (0.143)	Data 2.40e-04 (5.83e-04)	Tok/s 97429 (100648)	Loss/tok 3.0983 (3.1709)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.108 (0.143)	Data 1.81e-04 (5.76e-04)	Tok/s 95098 (100608)	Loss/tok 2.9541 (3.1722)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.160 (0.142)	Data 1.59e-04 (5.70e-04)	Tok/s 104772 (100580)	Loss/tok 3.2630 (3.1712)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][660/1938]	Time 0.108 (0.143)	Data 1.81e-04 (5.65e-04)	Tok/s 99110 (100592)	Loss/tok 3.0378 (3.1728)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.107 (0.143)	Data 1.89e-04 (5.59e-04)	Tok/s 93846 (100633)	Loss/tok 3.0083 (3.1740)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.107 (0.143)	Data 1.77e-04 (5.53e-04)	Tok/s 97026 (100635)	Loss/tok 3.0894 (3.1740)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.108 (0.143)	Data 1.70e-04 (5.48e-04)	Tok/s 94978 (100662)	Loss/tok 2.8978 (3.1737)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.274 (0.144)	Data 2.08e-04 (5.42e-04)	Tok/s 108492 (100711)	Loss/tok 3.6594 (3.1759)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.159 (0.144)	Data 2.10e-04 (5.37e-04)	Tok/s 106998 (100721)	Loss/tok 3.1894 (3.1769)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.160 (0.144)	Data 1.65e-04 (5.32e-04)	Tok/s 106026 (100737)	Loss/tok 3.1076 (3.1762)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.108 (0.144)	Data 1.60e-04 (5.27e-04)	Tok/s 95870 (100736)	Loss/tok 2.9245 (3.1749)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.214 (0.144)	Data 1.42e-04 (5.23e-04)	Tok/s 108043 (100749)	Loss/tok 3.2486 (3.1746)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.213 (0.144)	Data 2.32e-04 (5.18e-04)	Tok/s 108745 (100766)	Loss/tok 3.3477 (3.1751)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.159 (0.144)	Data 2.02e-04 (5.14e-04)	Tok/s 105558 (100764)	Loss/tok 3.1430 (3.1733)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.211 (0.144)	Data 1.94e-04 (5.10e-04)	Tok/s 111049 (100779)	Loss/tok 3.2929 (3.1745)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.108 (0.144)	Data 1.44e-04 (5.06e-04)	Tok/s 96308 (100779)	Loss/tok 2.9371 (3.1743)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][790/1938]	Time 0.107 (0.144)	Data 1.81e-04 (5.01e-04)	Tok/s 97133 (100727)	Loss/tok 2.9169 (3.1740)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.214 (0.143)	Data 1.85e-04 (4.97e-04)	Tok/s 108781 (100696)	Loss/tok 3.3228 (3.1726)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.060 (0.144)	Data 2.28e-04 (4.93e-04)	Tok/s 89571 (100717)	Loss/tok 2.5923 (3.1733)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.274 (0.144)	Data 1.42e-04 (4.89e-04)	Tok/s 108891 (100710)	Loss/tok 3.4241 (3.1729)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.060 (0.144)	Data 1.80e-04 (4.86e-04)	Tok/s 86871 (100692)	Loss/tok 2.5892 (3.1713)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.212 (0.144)	Data 2.08e-04 (4.82e-04)	Tok/s 111677 (100719)	Loss/tok 3.2504 (3.1708)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.059 (0.144)	Data 2.32e-04 (4.79e-04)	Tok/s 87790 (100720)	Loss/tok 2.6111 (3.1714)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.213 (0.143)	Data 1.61e-04 (4.76e-04)	Tok/s 110550 (100669)	Loss/tok 3.3364 (3.1701)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.160 (0.144)	Data 1.87e-04 (4.72e-04)	Tok/s 103926 (100680)	Loss/tok 3.1761 (3.1706)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.161 (0.144)	Data 1.68e-04 (4.69e-04)	Tok/s 104405 (100705)	Loss/tok 3.1399 (3.1711)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.107 (0.144)	Data 1.83e-04 (4.66e-04)	Tok/s 95879 (100704)	Loss/tok 2.9024 (3.1713)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.108 (0.144)	Data 1.90e-04 (4.62e-04)	Tok/s 95846 (100716)	Loss/tok 2.9233 (3.1707)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.160 (0.144)	Data 2.03e-04 (4.60e-04)	Tok/s 104310 (100684)	Loss/tok 3.0762 (3.1699)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][920/1938]	Time 0.213 (0.144)	Data 1.85e-04 (4.56e-04)	Tok/s 108906 (100674)	Loss/tok 3.3424 (3.1697)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.107 (0.143)	Data 2.15e-04 (4.53e-04)	Tok/s 98328 (100650)	Loss/tok 3.0366 (3.1687)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.214 (0.144)	Data 1.90e-04 (4.51e-04)	Tok/s 109552 (100662)	Loss/tok 3.2710 (3.1698)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.060 (0.143)	Data 1.83e-04 (4.48e-04)	Tok/s 86636 (100650)	Loss/tok 2.5633 (3.1694)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.159 (0.144)	Data 1.88e-04 (4.45e-04)	Tok/s 106743 (100673)	Loss/tok 3.1696 (3.1700)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.107 (0.144)	Data 1.39e-04 (4.42e-04)	Tok/s 97658 (100684)	Loss/tok 2.8603 (3.1706)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.213 (0.144)	Data 1.65e-04 (4.39e-04)	Tok/s 109975 (100717)	Loss/tok 3.3528 (3.1706)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.107 (0.144)	Data 1.41e-04 (4.36e-04)	Tok/s 94594 (100692)	Loss/tok 2.9885 (3.1693)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.107 (0.144)	Data 2.14e-04 (4.34e-04)	Tok/s 96884 (100679)	Loss/tok 2.9330 (3.1688)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.212 (0.144)	Data 2.35e-04 (4.32e-04)	Tok/s 109664 (100686)	Loss/tok 3.2730 (3.1689)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.107 (0.144)	Data 2.31e-04 (4.29e-04)	Tok/s 94822 (100666)	Loss/tok 2.8790 (3.1681)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.108 (0.143)	Data 1.91e-04 (4.26e-04)	Tok/s 93935 (100639)	Loss/tok 2.9362 (3.1668)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.160 (0.143)	Data 1.61e-04 (4.24e-04)	Tok/s 103865 (100631)	Loss/tok 3.1520 (3.1664)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1050/1938]	Time 0.160 (0.143)	Data 1.74e-04 (4.21e-04)	Tok/s 106703 (100623)	Loss/tok 3.1130 (3.1661)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.060 (0.143)	Data 1.97e-04 (4.19e-04)	Tok/s 87777 (100597)	Loss/tok 2.5866 (3.1650)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.160 (0.143)	Data 1.40e-04 (4.17e-04)	Tok/s 105980 (100598)	Loss/tok 3.1123 (3.1645)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.159 (0.143)	Data 1.64e-04 (4.15e-04)	Tok/s 105329 (100589)	Loss/tok 3.2302 (3.1643)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.107 (0.143)	Data 2.64e-04 (4.13e-04)	Tok/s 95975 (100591)	Loss/tok 2.8861 (3.1645)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.108 (0.143)	Data 1.94e-04 (4.10e-04)	Tok/s 97452 (100592)	Loss/tok 2.9359 (3.1644)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.108 (0.143)	Data 1.39e-04 (4.08e-04)	Tok/s 97197 (100565)	Loss/tok 2.8537 (3.1638)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.213 (0.143)	Data 1.41e-04 (4.06e-04)	Tok/s 108116 (100583)	Loss/tok 3.2349 (3.1631)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.107 (0.143)	Data 1.52e-04 (4.04e-04)	Tok/s 96714 (100572)	Loss/tok 2.9396 (3.1623)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.107 (0.143)	Data 2.24e-04 (4.02e-04)	Tok/s 96500 (100567)	Loss/tok 2.7851 (3.1620)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.214 (0.143)	Data 1.70e-04 (4.00e-04)	Tok/s 110172 (100574)	Loss/tok 3.2125 (3.1615)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.059 (0.143)	Data 2.83e-04 (3.98e-04)	Tok/s 89998 (100597)	Loss/tok 2.5177 (3.1623)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.108 (0.143)	Data 1.40e-04 (3.96e-04)	Tok/s 95851 (100588)	Loss/tok 2.8737 (3.1613)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1180/1938]	Time 0.107 (0.143)	Data 1.39e-04 (3.95e-04)	Tok/s 96071 (100585)	Loss/tok 2.9917 (3.1606)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.059 (0.142)	Data 1.80e-04 (3.93e-04)	Tok/s 88574 (100538)	Loss/tok 2.5394 (3.1596)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.161 (0.142)	Data 1.59e-04 (3.91e-04)	Tok/s 105134 (100551)	Loss/tok 3.0959 (3.1597)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.108 (0.142)	Data 1.65e-04 (3.89e-04)	Tok/s 93571 (100548)	Loss/tok 2.9442 (3.1598)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.160 (0.142)	Data 1.26e-04 (3.87e-04)	Tok/s 103939 (100544)	Loss/tok 3.1493 (3.1594)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.108 (0.142)	Data 1.86e-04 (3.85e-04)	Tok/s 95282 (100536)	Loss/tok 2.9083 (3.1587)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.059 (0.142)	Data 1.54e-04 (3.84e-04)	Tok/s 88495 (100522)	Loss/tok 2.5589 (3.1584)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.108 (0.142)	Data 2.43e-04 (3.82e-04)	Tok/s 97407 (100532)	Loss/tok 2.9291 (3.1581)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.108 (0.142)	Data 2.24e-04 (3.80e-04)	Tok/s 96572 (100519)	Loss/tok 2.9502 (3.1572)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.107 (0.142)	Data 1.43e-04 (3.79e-04)	Tok/s 98036 (100538)	Loss/tok 2.9842 (3.1579)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.060 (0.142)	Data 1.67e-04 (3.77e-04)	Tok/s 85996 (100512)	Loss/tok 2.5208 (3.1574)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.160 (0.142)	Data 1.42e-04 (3.75e-04)	Tok/s 106214 (100533)	Loss/tok 3.1270 (3.1572)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1300/1938]	Time 0.107 (0.142)	Data 2.23e-04 (3.74e-04)	Tok/s 96082 (100530)	Loss/tok 2.8954 (3.1572)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.107 (0.142)	Data 2.55e-04 (3.73e-04)	Tok/s 95619 (100536)	Loss/tok 2.9180 (3.1570)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.214 (0.142)	Data 1.44e-04 (3.71e-04)	Tok/s 109083 (100542)	Loss/tok 3.3371 (3.1569)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.274 (0.142)	Data 1.45e-04 (3.70e-04)	Tok/s 109058 (100537)	Loss/tok 3.3928 (3.1568)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.107 (0.142)	Data 1.39e-04 (3.68e-04)	Tok/s 95495 (100552)	Loss/tok 2.8649 (3.1570)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.108 (0.142)	Data 1.58e-04 (3.67e-04)	Tok/s 94414 (100518)	Loss/tok 3.0213 (3.1560)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.160 (0.142)	Data 1.79e-04 (3.65e-04)	Tok/s 103770 (100527)	Loss/tok 3.0526 (3.1559)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.108 (0.142)	Data 1.74e-04 (3.64e-04)	Tok/s 93341 (100500)	Loss/tok 2.9466 (3.1550)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.107 (0.142)	Data 2.89e-04 (3.63e-04)	Tok/s 98251 (100497)	Loss/tok 2.9220 (3.1545)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.108 (0.142)	Data 1.79e-04 (3.61e-04)	Tok/s 95390 (100506)	Loss/tok 2.9752 (3.1547)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.161 (0.142)	Data 1.38e-04 (3.60e-04)	Tok/s 103705 (100511)	Loss/tok 3.2110 (3.1545)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.160 (0.142)	Data 1.78e-04 (3.59e-04)	Tok/s 104039 (100533)	Loss/tok 3.0935 (3.1547)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.275 (0.142)	Data 2.26e-04 (3.58e-04)	Tok/s 107494 (100537)	Loss/tok 3.5015 (3.1552)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1430/1938]	Time 0.161 (0.142)	Data 1.97e-04 (3.56e-04)	Tok/s 106549 (100549)	Loss/tok 3.0651 (3.1551)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.161 (0.142)	Data 1.67e-04 (3.55e-04)	Tok/s 103847 (100564)	Loss/tok 3.2167 (3.1554)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.213 (0.142)	Data 2.13e-04 (3.54e-04)	Tok/s 109515 (100553)	Loss/tok 3.1721 (3.1550)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.160 (0.142)	Data 1.39e-04 (3.53e-04)	Tok/s 103923 (100554)	Loss/tok 3.2085 (3.1546)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.275 (0.142)	Data 2.32e-04 (3.51e-04)	Tok/s 108203 (100542)	Loss/tok 3.4185 (3.1542)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.160 (0.142)	Data 1.65e-04 (3.50e-04)	Tok/s 104300 (100553)	Loss/tok 3.2038 (3.1540)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.160 (0.142)	Data 1.86e-04 (3.49e-04)	Tok/s 104204 (100524)	Loss/tok 3.1457 (3.1533)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.160 (0.142)	Data 1.25e-04 (3.48e-04)	Tok/s 104874 (100523)	Loss/tok 3.2403 (3.1530)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.108 (0.142)	Data 1.60e-04 (3.47e-04)	Tok/s 97518 (100539)	Loss/tok 2.9304 (3.1528)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.107 (0.142)	Data 2.53e-04 (3.45e-04)	Tok/s 96882 (100539)	Loss/tok 2.9123 (3.1526)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.109 (0.142)	Data 2.20e-04 (3.44e-04)	Tok/s 95595 (100536)	Loss/tok 2.9792 (3.1523)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.107 (0.142)	Data 1.82e-04 (3.43e-04)	Tok/s 96119 (100520)	Loss/tok 2.9022 (3.1518)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.212 (0.142)	Data 1.88e-04 (3.42e-04)	Tok/s 111234 (100517)	Loss/tok 3.2617 (3.1516)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1560/1938]	Time 0.160 (0.142)	Data 1.79e-04 (3.41e-04)	Tok/s 104892 (100495)	Loss/tok 3.1349 (3.1506)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.107 (0.142)	Data 2.34e-04 (3.40e-04)	Tok/s 97967 (100506)	Loss/tok 2.9727 (3.1508)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.212 (0.142)	Data 2.35e-04 (3.39e-04)	Tok/s 109107 (100509)	Loss/tok 3.3635 (3.1508)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.108 (0.142)	Data 1.41e-04 (3.38e-04)	Tok/s 96361 (100500)	Loss/tok 2.9240 (3.1502)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.161 (0.142)	Data 1.74e-04 (3.37e-04)	Tok/s 103312 (100510)	Loss/tok 3.1537 (3.1500)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.108 (0.142)	Data 1.41e-04 (3.35e-04)	Tok/s 96499 (100489)	Loss/tok 2.8869 (3.1496)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.159 (0.142)	Data 1.24e-04 (3.35e-04)	Tok/s 104741 (100481)	Loss/tok 3.1629 (3.1491)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.161 (0.141)	Data 1.79e-04 (3.34e-04)	Tok/s 104016 (100475)	Loss/tok 3.0812 (3.1487)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.108 (0.141)	Data 1.43e-04 (3.33e-04)	Tok/s 96194 (100472)	Loss/tok 2.9375 (3.1484)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.159 (0.142)	Data 2.38e-04 (3.32e-04)	Tok/s 103190 (100482)	Loss/tok 3.0897 (3.1484)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.107 (0.141)	Data 1.49e-04 (3.31e-04)	Tok/s 96340 (100452)	Loss/tok 2.9518 (3.1478)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.107 (0.141)	Data 1.99e-04 (3.30e-04)	Tok/s 96634 (100445)	Loss/tok 2.9710 (3.1473)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.160 (0.141)	Data 1.58e-04 (3.29e-04)	Tok/s 104625 (100453)	Loss/tok 3.1616 (3.1471)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1690/1938]	Time 0.108 (0.141)	Data 1.56e-04 (3.28e-04)	Tok/s 96625 (100456)	Loss/tok 2.9105 (3.1470)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.215 (0.141)	Data 1.80e-04 (3.27e-04)	Tok/s 108457 (100460)	Loss/tok 3.2247 (3.1466)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.274 (0.141)	Data 1.44e-04 (3.26e-04)	Tok/s 108738 (100455)	Loss/tok 3.4175 (3.1462)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.274 (0.142)	Data 1.49e-04 (3.26e-04)	Tok/s 109328 (100474)	Loss/tok 3.4143 (3.1466)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.214 (0.142)	Data 1.40e-04 (3.25e-04)	Tok/s 110102 (100492)	Loss/tok 3.1779 (3.1468)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.108 (0.142)	Data 1.70e-04 (3.24e-04)	Tok/s 94369 (100504)	Loss/tok 2.8905 (3.1468)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.060 (0.142)	Data 2.51e-04 (3.23e-04)	Tok/s 86543 (100487)	Loss/tok 2.5028 (3.1465)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.274 (0.142)	Data 1.40e-04 (3.22e-04)	Tok/s 108828 (100520)	Loss/tok 3.5244 (3.1480)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.059 (0.142)	Data 1.42e-04 (3.21e-04)	Tok/s 90676 (100508)	Loss/tok 2.5498 (3.1477)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.159 (0.142)	Data 1.36e-04 (3.20e-04)	Tok/s 106365 (100502)	Loss/tok 3.0808 (3.1472)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.159 (0.142)	Data 2.16e-04 (3.20e-04)	Tok/s 104942 (100503)	Loss/tok 3.1343 (3.1468)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.107 (0.142)	Data 1.44e-04 (3.19e-04)	Tok/s 94154 (100499)	Loss/tok 2.9207 (3.1465)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.108 (0.142)	Data 1.64e-04 (3.18e-04)	Tok/s 95120 (100498)	Loss/tok 2.9960 (3.1463)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1820/1938]	Time 0.213 (0.142)	Data 2.01e-04 (3.17e-04)	Tok/s 109454 (100505)	Loss/tok 3.3150 (3.1461)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.059 (0.142)	Data 1.42e-04 (3.17e-04)	Tok/s 87758 (100489)	Loss/tok 2.5362 (3.1459)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.107 (0.142)	Data 1.93e-04 (3.16e-04)	Tok/s 96948 (100489)	Loss/tok 2.9170 (3.1459)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.108 (0.142)	Data 1.71e-04 (3.15e-04)	Tok/s 93328 (100494)	Loss/tok 2.9341 (3.1456)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.107 (0.142)	Data 2.09e-04 (3.14e-04)	Tok/s 94435 (100499)	Loss/tok 2.9021 (3.1456)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.060 (0.142)	Data 1.39e-04 (3.13e-04)	Tok/s 88183 (100492)	Loss/tok 2.5370 (3.1452)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.160 (0.142)	Data 2.65e-04 (3.13e-04)	Tok/s 105111 (100494)	Loss/tok 3.0706 (3.1449)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.160 (0.142)	Data 1.73e-04 (3.12e-04)	Tok/s 105072 (100493)	Loss/tok 3.1259 (3.1447)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.108 (0.142)	Data 2.42e-04 (3.11e-04)	Tok/s 96123 (100518)	Loss/tok 2.9535 (3.1448)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.274 (0.142)	Data 1.51e-04 (3.11e-04)	Tok/s 108558 (100537)	Loss/tok 3.3910 (3.1455)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.161 (0.142)	Data 2.00e-04 (3.10e-04)	Tok/s 104542 (100543)	Loss/tok 3.1290 (3.1454)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.108 (0.142)	Data 1.62e-04 (3.09e-04)	Tok/s 94851 (100534)	Loss/tok 2.9929 (3.1451)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020269337, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020269338, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.630 (0.630)	Decoder iters 102.0 (102.0)	Tok/s 26133 (26133)
0: Running moses detokenizer
0: BLEU(score=24.40176821495318, counts=[37348, 18874, 10863, 6503], totals=[65813, 62810, 59807, 56809], precisions=[56.748666676796375, 30.049355198216844, 18.163425685956494, 11.447129856184759], bp=1.0, sys_len=65813, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020271136, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.244, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020271137, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1480	Test BLEU: 24.40
0: Performance: Epoch: 3	Training: 804450 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020271137, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020271138, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:37:55 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:05 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
ENDING TIMING RUN AT 2020-06-24 10:37:56 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:05 AM
