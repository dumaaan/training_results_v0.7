+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
srun: Job 467818 step creation temporarily disabled, retrying
srun: Step created for job 467818
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019151757, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019151793, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019151793, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019151793, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019151793, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: Job 467818 step creation temporarily disabled, retrying
srun: Step created for job 467818
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-457
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019157919, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177106/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ '[' -n 6 ']'
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-24 10:19:20 AM
+ declare -a CMD
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' -n 3 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019162238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162245, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019162246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1983556015
:::MLLOG {"namespace": "", "time_ms": 1593019168084, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1983556015, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1700939605
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019176137, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019176138, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019176138, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019176138, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019176138, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019177696, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019177696, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019177696, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019177954, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019177956, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019177956, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019177956, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019177957, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019177957, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019177957, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019177957, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019177957, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019177958, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019177958, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019177958, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 490597801
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.411 (0.411)	Data 3.00e-01 (3.00e-01)	Tok/s 25171 (25171)	Loss/tok 10.6468 (10.6468)	LR 2.047e-05
0: TRAIN [0][10/1938]	Time 0.155 (0.138)	Data 1.47e-04 (2.74e-02)	Tok/s 109067 (93291)	Loss/tok 9.8537 (10.1235)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.104 (0.118)	Data 1.70e-04 (1.44e-02)	Tok/s 97077 (95252)	Loss/tok 9.3430 (9.8266)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.104 (0.119)	Data 2.08e-04 (9.84e-03)	Tok/s 101651 (96867)	Loss/tok 9.0134 (9.6088)	LR 4.083e-05
0: TRAIN [0][40/1938]	Time 0.208 (0.122)	Data 1.97e-04 (7.48e-03)	Tok/s 112553 (98248)	Loss/tok 8.9147 (9.4145)	LR 5.141e-05
0: TRAIN [0][50/1938]	Time 0.104 (0.126)	Data 2.04e-04 (6.05e-03)	Tok/s 96625 (99254)	Loss/tok 8.4705 (9.2481)	LR 6.472e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][60/1938]	Time 0.158 (0.128)	Data 2.60e-04 (5.09e-03)	Tok/s 105959 (99966)	Loss/tok 8.4361 (9.1042)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.103 (0.133)	Data 2.11e-04 (4.40e-03)	Tok/s 100156 (100831)	Loss/tok 8.1506 (8.9636)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.267 (0.133)	Data 2.16e-04 (3.88e-03)	Tok/s 111735 (101042)	Loss/tok 8.3546 (8.8551)	LR 1.262e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][90/1938]	Time 0.105 (0.134)	Data 2.05e-04 (3.47e-03)	Tok/s 96908 (101212)	Loss/tok 8.0024 (8.7647)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.105 (0.134)	Data 1.94e-04 (3.15e-03)	Tok/s 98251 (101440)	Loss/tok 7.8276 (8.6852)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.155 (0.134)	Data 1.91e-04 (2.88e-03)	Tok/s 108136 (101516)	Loss/tok 7.9909 (8.6220)	LR 2.461e-04
0: TRAIN [0][120/1938]	Time 0.206 (0.134)	Data 1.24e-04 (2.66e-03)	Tok/s 111346 (101683)	Loss/tok 8.0242 (8.5617)	LR 3.098e-04
0: TRAIN [0][130/1938]	Time 0.207 (0.134)	Data 1.25e-04 (2.46e-03)	Tok/s 113062 (101769)	Loss/tok 8.0098 (8.5095)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.155 (0.134)	Data 1.92e-04 (2.30e-03)	Tok/s 107608 (101781)	Loss/tok 7.9585 (8.4603)	LR 4.909e-04
0: TRAIN [0][150/1938]	Time 0.058 (0.136)	Data 1.60e-04 (2.16e-03)	Tok/s 89417 (102017)	Loss/tok 7.0489 (8.4141)	LR 6.181e-04
0: TRAIN [0][160/1938]	Time 0.105 (0.137)	Data 1.62e-04 (2.04e-03)	Tok/s 98957 (102024)	Loss/tok 7.5967 (8.3733)	LR 7.781e-04
0: TRAIN [0][170/1938]	Time 0.157 (0.137)	Data 1.43e-04 (1.93e-03)	Tok/s 107014 (102199)	Loss/tok 7.6182 (8.3287)	LR 9.796e-04
0: TRAIN [0][180/1938]	Time 0.104 (0.137)	Data 1.44e-04 (1.83e-03)	Tok/s 100692 (102186)	Loss/tok 7.3279 (8.2851)	LR 1.233e-03
0: TRAIN [0][190/1938]	Time 0.156 (0.137)	Data 2.21e-04 (1.74e-03)	Tok/s 104613 (102183)	Loss/tok 7.5546 (8.2411)	LR 1.552e-03
0: TRAIN [0][200/1938]	Time 0.105 (0.135)	Data 1.39e-04 (1.66e-03)	Tok/s 98221 (101968)	Loss/tok 7.0171 (8.1985)	LR 1.954e-03
0: TRAIN [0][210/1938]	Time 0.156 (0.133)	Data 2.47e-04 (1.59e-03)	Tok/s 106330 (101792)	Loss/tok 7.1198 (8.1529)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][220/1938]	Time 0.158 (0.134)	Data 2.25e-04 (1.53e-03)	Tok/s 107114 (101800)	Loss/tok 6.9802 (8.0956)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.156 (0.133)	Data 1.54e-04 (1.47e-03)	Tok/s 108842 (101779)	Loss/tok 6.7486 (8.0396)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.156 (0.134)	Data 1.20e-04 (1.41e-03)	Tok/s 107690 (101921)	Loss/tok 6.5979 (7.9732)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.207 (0.135)	Data 1.23e-04 (1.36e-03)	Tok/s 112640 (102046)	Loss/tok 6.6371 (7.9031)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.106 (0.135)	Data 1.21e-04 (1.32e-03)	Tok/s 97120 (102095)	Loss/tok 5.9666 (7.8396)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.105 (0.135)	Data 1.20e-04 (1.27e-03)	Tok/s 99156 (102119)	Loss/tok 6.0009 (7.7775)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.106 (0.135)	Data 1.36e-04 (1.23e-03)	Tok/s 96163 (102122)	Loss/tok 5.8843 (7.7172)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.107 (0.134)	Data 1.19e-04 (1.20e-03)	Tok/s 97164 (101998)	Loss/tok 5.6662 (7.6663)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.106 (0.134)	Data 1.18e-04 (1.16e-03)	Tok/s 98236 (101968)	Loss/tok 5.7327 (7.6110)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.106 (0.134)	Data 1.66e-04 (1.13e-03)	Tok/s 99208 (101956)	Loss/tok 5.5508 (7.5548)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.105 (0.134)	Data 1.29e-04 (1.10e-03)	Tok/s 97836 (101951)	Loss/tok 5.4324 (7.4983)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.269 (0.134)	Data 1.19e-04 (1.07e-03)	Tok/s 110769 (101935)	Loss/tok 6.0680 (7.4412)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.156 (0.135)	Data 1.54e-04 (1.04e-03)	Tok/s 106504 (101993)	Loss/tok 5.5480 (7.3813)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][350/1938]	Time 0.058 (0.134)	Data 1.38e-04 (1.02e-03)	Tok/s 93790 (101962)	Loss/tok 4.3104 (7.3311)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.105 (0.134)	Data 1.36e-04 (9.91e-04)	Tok/s 102561 (102004)	Loss/tok 5.1744 (7.2759)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.106 (0.134)	Data 1.19e-04 (9.68e-04)	Tok/s 99551 (102017)	Loss/tok 5.0015 (7.2220)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.106 (0.135)	Data 1.61e-04 (9.47e-04)	Tok/s 97485 (102099)	Loss/tok 4.8818 (7.1605)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.106 (0.134)	Data 1.75e-04 (9.27e-04)	Tok/s 95445 (101963)	Loss/tok 4.7633 (7.1189)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.157 (0.135)	Data 1.19e-04 (9.07e-04)	Tok/s 107461 (101991)	Loss/tok 4.9726 (7.0618)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.209 (0.135)	Data 1.49e-04 (8.89e-04)	Tok/s 111640 (102063)	Loss/tok 5.3524 (7.0053)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.157 (0.136)	Data 1.55e-04 (8.71e-04)	Tok/s 107198 (102146)	Loss/tok 5.0376 (6.9464)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.156 (0.136)	Data 1.52e-04 (8.54e-04)	Tok/s 106272 (102124)	Loss/tok 4.7824 (6.8996)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.058 (0.136)	Data 1.57e-04 (8.38e-04)	Tok/s 90539 (102126)	Loss/tok 3.7256 (6.8494)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.106 (0.136)	Data 1.19e-04 (8.23e-04)	Tok/s 95351 (102108)	Loss/tok 4.3931 (6.8034)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.271 (0.136)	Data 1.38e-04 (8.08e-04)	Tok/s 110653 (102144)	Loss/tok 5.1438 (6.7509)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.271 (0.137)	Data 1.36e-04 (7.94e-04)	Tok/s 109824 (102176)	Loss/tok 5.1309 (6.7013)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][480/1938]	Time 0.107 (0.137)	Data 1.57e-04 (7.81e-04)	Tok/s 96767 (102204)	Loss/tok 4.3509 (6.6551)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.106 (0.137)	Data 1.34e-04 (7.68e-04)	Tok/s 99885 (102234)	Loss/tok 4.1788 (6.6080)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.059 (0.137)	Data 1.37e-04 (7.56e-04)	Tok/s 90024 (102146)	Loss/tok 3.5298 (6.5735)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.211 (0.137)	Data 1.20e-04 (7.43e-04)	Tok/s 111151 (102202)	Loss/tok 4.7910 (6.5254)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][520/1938]	Time 0.106 (0.137)	Data 1.20e-04 (7.32e-04)	Tok/s 93035 (102148)	Loss/tok 4.1141 (6.4878)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.157 (0.137)	Data 1.23e-04 (7.21e-04)	Tok/s 108032 (102137)	Loss/tok 4.2787 (6.4492)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.058 (0.137)	Data 1.38e-04 (7.10e-04)	Tok/s 89575 (102145)	Loss/tok 3.3663 (6.4060)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.210 (0.138)	Data 1.65e-04 (7.00e-04)	Tok/s 110007 (102161)	Loss/tok 4.4484 (6.3665)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.269 (0.138)	Data 2.09e-04 (6.90e-04)	Tok/s 108004 (102193)	Loss/tok 4.9508 (6.3264)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.106 (0.138)	Data 1.23e-04 (6.80e-04)	Tok/s 97007 (102143)	Loss/tok 3.9563 (6.2942)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.158 (0.138)	Data 1.19e-04 (6.71e-04)	Tok/s 105051 (102202)	Loss/tok 4.4543 (6.2527)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.106 (0.138)	Data 1.18e-04 (6.62e-04)	Tok/s 99102 (102167)	Loss/tok 3.9796 (6.2217)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.209 (0.139)	Data 1.20e-04 (6.53e-04)	Tok/s 112314 (102232)	Loss/tok 4.3648 (6.1813)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.107 (0.138)	Data 1.48e-04 (6.45e-04)	Tok/s 97048 (102173)	Loss/tok 4.1419 (6.1543)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.106 (0.138)	Data 1.60e-04 (6.37e-04)	Tok/s 95537 (102184)	Loss/tok 3.9964 (6.1199)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.105 (0.138)	Data 1.49e-04 (6.29e-04)	Tok/s 98867 (102190)	Loss/tok 3.9172 (6.0890)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.209 (0.139)	Data 1.26e-04 (6.21e-04)	Tok/s 111907 (102212)	Loss/tok 4.4895 (6.0566)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][650/1938]	Time 0.106 (0.138)	Data 1.36e-04 (6.14e-04)	Tok/s 96513 (102183)	Loss/tok 3.8988 (6.0293)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.210 (0.139)	Data 1.22e-04 (6.07e-04)	Tok/s 110283 (102171)	Loss/tok 4.4271 (6.0009)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.211 (0.138)	Data 1.40e-04 (6.00e-04)	Tok/s 109807 (102153)	Loss/tok 4.4864 (5.9741)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.058 (0.139)	Data 1.21e-04 (5.93e-04)	Tok/s 88031 (102187)	Loss/tok 3.1153 (5.9432)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.107 (0.139)	Data 1.62e-04 (5.87e-04)	Tok/s 97094 (102162)	Loss/tok 3.8051 (5.9160)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.106 (0.139)	Data 1.58e-04 (5.81e-04)	Tok/s 96204 (102159)	Loss/tok 3.8179 (5.8908)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.157 (0.139)	Data 1.29e-04 (5.74e-04)	Tok/s 107271 (102146)	Loss/tok 4.1344 (5.8661)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.211 (0.139)	Data 1.39e-04 (5.68e-04)	Tok/s 110773 (102168)	Loss/tok 4.3205 (5.8389)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.106 (0.139)	Data 1.21e-04 (5.63e-04)	Tok/s 99697 (102185)	Loss/tok 3.7488 (5.8130)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.210 (0.139)	Data 1.81e-04 (5.57e-04)	Tok/s 110707 (102221)	Loss/tok 4.2574 (5.7846)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.106 (0.139)	Data 2.04e-04 (5.52e-04)	Tok/s 95905 (102171)	Loss/tok 3.7527 (5.7640)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.158 (0.139)	Data 1.43e-04 (5.46e-04)	Tok/s 105763 (102177)	Loss/tok 3.9446 (5.7390)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.106 (0.139)	Data 1.59e-04 (5.41e-04)	Tok/s 96198 (102168)	Loss/tok 3.7015 (5.7153)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][780/1938]	Time 0.107 (0.139)	Data 1.83e-04 (5.36e-04)	Tok/s 96764 (102167)	Loss/tok 3.6552 (5.6923)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.158 (0.139)	Data 1.28e-04 (5.31e-04)	Tok/s 106432 (102145)	Loss/tok 3.9445 (5.6719)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][800/1938]	Time 0.159 (0.139)	Data 1.58e-04 (5.27e-04)	Tok/s 105862 (102126)	Loss/tok 4.0461 (5.6519)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.106 (0.139)	Data 1.20e-04 (5.22e-04)	Tok/s 99152 (102122)	Loss/tok 3.7394 (5.6307)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.210 (0.139)	Data 1.23e-04 (5.17e-04)	Tok/s 112136 (102118)	Loss/tok 4.1475 (5.6099)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.158 (0.139)	Data 1.36e-04 (5.13e-04)	Tok/s 105389 (102115)	Loss/tok 3.9774 (5.5890)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.106 (0.139)	Data 1.26e-04 (5.08e-04)	Tok/s 96487 (102112)	Loss/tok 3.6662 (5.5695)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.107 (0.139)	Data 1.41e-04 (5.04e-04)	Tok/s 97553 (102118)	Loss/tok 3.7011 (5.5493)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.159 (0.139)	Data 1.15e-04 (5.00e-04)	Tok/s 106565 (102112)	Loss/tok 3.8904 (5.5298)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.271 (0.139)	Data 1.37e-04 (4.96e-04)	Tok/s 109756 (102142)	Loss/tok 4.2524 (5.5087)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.107 (0.139)	Data 1.26e-04 (4.92e-04)	Tok/s 97950 (102132)	Loss/tok 3.6766 (5.4910)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.158 (0.139)	Data 1.78e-04 (4.88e-04)	Tok/s 107429 (102112)	Loss/tok 3.8662 (5.4739)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.107 (0.139)	Data 1.19e-04 (4.84e-04)	Tok/s 96228 (102081)	Loss/tok 3.6296 (5.4577)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.158 (0.139)	Data 1.36e-04 (4.80e-04)	Tok/s 107513 (102041)	Loss/tok 4.0159 (5.4427)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][920/1938]	Time 0.158 (0.139)	Data 1.37e-04 (4.77e-04)	Tok/s 105200 (102014)	Loss/tok 3.9320 (5.4264)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.107 (0.139)	Data 1.76e-04 (4.73e-04)	Tok/s 95673 (102010)	Loss/tok 3.6369 (5.4092)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.158 (0.139)	Data 1.28e-04 (4.70e-04)	Tok/s 105964 (102036)	Loss/tok 3.8363 (5.3905)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.107 (0.139)	Data 1.24e-04 (4.66e-04)	Tok/s 98565 (102046)	Loss/tok 3.6148 (5.3730)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.210 (0.139)	Data 1.57e-04 (4.63e-04)	Tok/s 110524 (102039)	Loss/tok 4.1189 (5.3568)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.107 (0.139)	Data 1.28e-04 (4.60e-04)	Tok/s 96529 (102032)	Loss/tok 3.5555 (5.3408)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][980/1938]	Time 0.107 (0.139)	Data 1.23e-04 (4.56e-04)	Tok/s 95798 (102037)	Loss/tok 3.6569 (5.3240)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.158 (0.139)	Data 1.55e-04 (4.53e-04)	Tok/s 106937 (102021)	Loss/tok 3.8240 (5.3095)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.159 (0.140)	Data 1.29e-04 (4.50e-04)	Tok/s 106472 (102079)	Loss/tok 3.9421 (5.2904)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.107 (0.140)	Data 2.06e-04 (4.47e-04)	Tok/s 97942 (102066)	Loss/tok 3.6448 (5.2765)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.058 (0.140)	Data 1.61e-04 (4.44e-04)	Tok/s 91241 (102034)	Loss/tok 3.0963 (5.2636)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.107 (0.139)	Data 1.25e-04 (4.41e-04)	Tok/s 96494 (101984)	Loss/tok 3.5336 (5.2518)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.107 (0.140)	Data 2.09e-04 (4.39e-04)	Tok/s 97838 (102015)	Loss/tok 3.4851 (5.2361)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.212 (0.140)	Data 1.27e-04 (4.36e-04)	Tok/s 108610 (102018)	Loss/tok 4.0755 (5.2224)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.059 (0.140)	Data 1.42e-04 (4.33e-04)	Tok/s 91241 (101988)	Loss/tok 2.8822 (5.2107)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.158 (0.140)	Data 1.95e-04 (4.31e-04)	Tok/s 105996 (102006)	Loss/tok 3.8099 (5.1962)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.157 (0.140)	Data 1.24e-04 (4.28e-04)	Tok/s 107715 (101982)	Loss/tok 3.8614 (5.1844)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.059 (0.140)	Data 1.23e-04 (4.25e-04)	Tok/s 90851 (101993)	Loss/tok 2.9564 (5.1704)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.107 (0.140)	Data 1.52e-04 (4.23e-04)	Tok/s 96828 (102007)	Loss/tok 3.3656 (5.1559)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1110/1938]	Time 0.158 (0.140)	Data 1.27e-04 (4.20e-04)	Tok/s 107271 (102009)	Loss/tok 3.6991 (5.1431)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.160 (0.140)	Data 1.21e-04 (4.18e-04)	Tok/s 104983 (101988)	Loss/tok 3.6974 (5.1319)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.158 (0.140)	Data 2.47e-04 (4.15e-04)	Tok/s 105954 (102010)	Loss/tok 3.8920 (5.1183)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.159 (0.140)	Data 1.40e-04 (4.13e-04)	Tok/s 107029 (102023)	Loss/tok 3.6714 (5.1051)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.059 (0.140)	Data 1.37e-04 (4.11e-04)	Tok/s 89759 (102004)	Loss/tok 3.0069 (5.0938)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.272 (0.140)	Data 1.41e-04 (4.09e-04)	Tok/s 109973 (102011)	Loss/tok 4.2119 (5.0815)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.059 (0.140)	Data 1.56e-04 (4.06e-04)	Tok/s 89352 (102014)	Loss/tok 2.9634 (5.0695)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.159 (0.140)	Data 1.46e-04 (4.04e-04)	Tok/s 105004 (101982)	Loss/tok 3.6593 (5.0596)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.059 (0.140)	Data 1.19e-04 (4.02e-04)	Tok/s 92193 (102003)	Loss/tok 2.9045 (5.0476)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.212 (0.140)	Data 1.38e-04 (4.00e-04)	Tok/s 109320 (101993)	Loss/tok 3.9631 (5.0368)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.107 (0.140)	Data 1.25e-04 (3.98e-04)	Tok/s 96780 (101980)	Loss/tok 3.5033 (5.0261)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.059 (0.140)	Data 1.49e-04 (3.96e-04)	Tok/s 87848 (101964)	Loss/tok 2.9859 (5.0162)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1230/1938]	Time 0.158 (0.140)	Data 1.53e-04 (3.94e-04)	Tok/s 105300 (101969)	Loss/tok 3.6920 (5.0052)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.272 (0.140)	Data 1.41e-04 (3.92e-04)	Tok/s 109283 (101962)	Loss/tok 4.0882 (4.9948)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.107 (0.140)	Data 1.40e-04 (3.90e-04)	Tok/s 98772 (101967)	Loss/tok 3.5479 (4.9840)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.159 (0.140)	Data 1.95e-04 (3.88e-04)	Tok/s 105995 (101961)	Loss/tok 3.6933 (4.9741)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.107 (0.140)	Data 2.17e-04 (3.86e-04)	Tok/s 99170 (101954)	Loss/tok 3.4320 (4.9643)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.158 (0.140)	Data 1.70e-04 (3.84e-04)	Tok/s 107740 (101973)	Loss/tok 3.6016 (4.9530)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.159 (0.140)	Data 1.37e-04 (3.82e-04)	Tok/s 106539 (101987)	Loss/tok 3.7263 (4.9425)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.107 (0.140)	Data 1.19e-04 (3.80e-04)	Tok/s 95029 (101989)	Loss/tok 3.4703 (4.9327)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.106 (0.140)	Data 1.89e-04 (3.78e-04)	Tok/s 98011 (101974)	Loss/tok 3.5289 (4.9238)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.059 (0.140)	Data 1.27e-04 (3.77e-04)	Tok/s 90719 (101949)	Loss/tok 2.9855 (4.9151)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.059 (0.140)	Data 1.50e-04 (3.75e-04)	Tok/s 90665 (101946)	Loss/tok 2.8984 (4.9058)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.159 (0.140)	Data 1.39e-04 (3.73e-04)	Tok/s 107354 (101953)	Loss/tok 3.6032 (4.8960)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.157 (0.140)	Data 1.22e-04 (3.72e-04)	Tok/s 108496 (101953)	Loss/tok 3.6101 (4.8866)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1360/1938]	Time 0.107 (0.140)	Data 1.24e-04 (3.70e-04)	Tok/s 94079 (101932)	Loss/tok 3.3428 (4.8781)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.212 (0.140)	Data 1.41e-04 (3.68e-04)	Tok/s 110264 (101958)	Loss/tok 3.9409 (4.8675)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.159 (0.141)	Data 1.23e-04 (3.66e-04)	Tok/s 107586 (101978)	Loss/tok 3.7192 (4.8575)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.058 (0.141)	Data 1.35e-04 (3.65e-04)	Tok/s 92608 (101959)	Loss/tok 2.9385 (4.8493)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.059 (0.140)	Data 1.54e-04 (3.63e-04)	Tok/s 91267 (101946)	Loss/tok 2.9870 (4.8412)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.107 (0.140)	Data 1.71e-04 (3.62e-04)	Tok/s 95695 (101936)	Loss/tok 3.4288 (4.8328)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.159 (0.140)	Data 1.21e-04 (3.60e-04)	Tok/s 104661 (101923)	Loss/tok 3.6849 (4.8249)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.212 (0.140)	Data 1.22e-04 (3.58e-04)	Tok/s 110448 (101908)	Loss/tok 3.8256 (4.8172)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.211 (0.140)	Data 1.20e-04 (3.57e-04)	Tok/s 110334 (101927)	Loss/tok 3.8520 (4.8081)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.107 (0.140)	Data 1.47e-04 (3.55e-04)	Tok/s 96984 (101924)	Loss/tok 3.3059 (4.8001)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.107 (0.140)	Data 1.28e-04 (3.54e-04)	Tok/s 95222 (101901)	Loss/tok 3.3401 (4.7929)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.159 (0.140)	Data 1.21e-04 (3.52e-04)	Tok/s 104752 (101887)	Loss/tok 3.6809 (4.7854)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1480/1938]	Time 0.107 (0.140)	Data 1.78e-04 (3.51e-04)	Tok/s 94997 (101854)	Loss/tok 3.3447 (4.7784)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1490/1938]	Time 0.212 (0.140)	Data 1.67e-04 (3.50e-04)	Tok/s 110948 (101831)	Loss/tok 3.8373 (4.7713)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.159 (0.140)	Data 1.33e-04 (3.48e-04)	Tok/s 106336 (101837)	Loss/tok 3.6621 (4.7633)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.160 (0.140)	Data 2.10e-04 (3.47e-04)	Tok/s 105549 (101827)	Loss/tok 3.6780 (4.7560)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.212 (0.140)	Data 1.22e-04 (3.46e-04)	Tok/s 109444 (101837)	Loss/tok 3.9193 (4.7479)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.159 (0.140)	Data 1.22e-04 (3.44e-04)	Tok/s 105591 (101854)	Loss/tok 3.6181 (4.7400)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.158 (0.140)	Data 1.59e-04 (3.43e-04)	Tok/s 105308 (101835)	Loss/tok 3.5707 (4.7334)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.211 (0.140)	Data 1.41e-04 (3.42e-04)	Tok/s 111715 (101835)	Loss/tok 3.7862 (4.7258)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.158 (0.140)	Data 1.49e-04 (3.41e-04)	Tok/s 106071 (101848)	Loss/tok 3.6459 (4.7182)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.107 (0.140)	Data 1.23e-04 (3.39e-04)	Tok/s 96418 (101848)	Loss/tok 3.2955 (4.7106)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.158 (0.140)	Data 1.21e-04 (3.38e-04)	Tok/s 107108 (101851)	Loss/tok 3.5397 (4.7035)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.107 (0.140)	Data 1.36e-04 (3.37e-04)	Tok/s 96583 (101834)	Loss/tok 3.3561 (4.6972)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.158 (0.140)	Data 1.22e-04 (3.36e-04)	Tok/s 105977 (101820)	Loss/tok 3.6521 (4.6909)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.212 (0.140)	Data 1.83e-04 (3.35e-04)	Tok/s 110695 (101807)	Loss/tok 3.7459 (4.6845)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1620/1938]	Time 0.160 (0.140)	Data 1.42e-04 (3.33e-04)	Tok/s 105751 (101799)	Loss/tok 3.6135 (4.6781)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.158 (0.140)	Data 1.51e-04 (3.32e-04)	Tok/s 104189 (101787)	Loss/tok 3.5961 (4.6716)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.106 (0.140)	Data 1.42e-04 (3.31e-04)	Tok/s 99821 (101774)	Loss/tok 3.4217 (4.6656)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.272 (0.140)	Data 1.32e-04 (3.30e-04)	Tok/s 108086 (101770)	Loss/tok 4.1678 (4.6591)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.107 (0.140)	Data 1.72e-04 (3.29e-04)	Tok/s 96867 (101744)	Loss/tok 3.3374 (4.6533)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1670/1938]	Time 0.106 (0.140)	Data 2.18e-04 (3.28e-04)	Tok/s 97547 (101738)	Loss/tok 3.2974 (4.6471)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.108 (0.140)	Data 1.76e-04 (3.27e-04)	Tok/s 94590 (101724)	Loss/tok 3.3882 (4.6409)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.108 (0.140)	Data 2.43e-04 (3.26e-04)	Tok/s 93295 (101712)	Loss/tok 3.3805 (4.6349)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.107 (0.140)	Data 1.63e-04 (3.25e-04)	Tok/s 96698 (101715)	Loss/tok 3.3141 (4.6284)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.160 (0.140)	Data 1.75e-04 (3.24e-04)	Tok/s 106107 (101708)	Loss/tok 3.4939 (4.6225)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.159 (0.140)	Data 1.58e-04 (3.23e-04)	Tok/s 106506 (101702)	Loss/tok 3.6128 (4.6165)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.159 (0.140)	Data 1.60e-04 (3.23e-04)	Tok/s 107631 (101701)	Loss/tok 3.6311 (4.6102)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.159 (0.140)	Data 1.71e-04 (3.22e-04)	Tok/s 105977 (101704)	Loss/tok 3.6270 (4.6038)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.212 (0.140)	Data 1.76e-04 (3.21e-04)	Tok/s 110061 (101715)	Loss/tok 3.8995 (4.5977)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.107 (0.140)	Data 2.32e-04 (3.20e-04)	Tok/s 97131 (101708)	Loss/tok 3.3725 (4.5921)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.160 (0.140)	Data 1.60e-04 (3.19e-04)	Tok/s 102929 (101720)	Loss/tok 3.6128 (4.5862)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.158 (0.140)	Data 1.26e-04 (3.18e-04)	Tok/s 105808 (101730)	Loss/tok 3.5889 (4.5799)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1790/1938]	Time 0.210 (0.140)	Data 1.53e-04 (3.17e-04)	Tok/s 109942 (101731)	Loss/tok 3.6864 (4.5738)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.107 (0.140)	Data 1.24e-04 (3.16e-04)	Tok/s 95519 (101734)	Loss/tok 3.3368 (4.5680)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.106 (0.140)	Data 1.68e-04 (3.15e-04)	Tok/s 98218 (101737)	Loss/tok 3.2558 (4.5622)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.107 (0.140)	Data 1.28e-04 (3.14e-04)	Tok/s 98066 (101725)	Loss/tok 3.3323 (4.5568)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.158 (0.140)	Data 1.37e-04 (3.13e-04)	Tok/s 105502 (101729)	Loss/tok 3.6829 (4.5510)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1840/1938]	Time 0.271 (0.140)	Data 1.39e-04 (3.12e-04)	Tok/s 110079 (101739)	Loss/tok 3.8805 (4.5453)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.272 (0.140)	Data 1.49e-04 (3.11e-04)	Tok/s 110382 (101739)	Loss/tok 3.9416 (4.5400)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.059 (0.141)	Data 1.45e-04 (3.11e-04)	Tok/s 90704 (101754)	Loss/tok 2.9351 (4.5339)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.159 (0.141)	Data 1.37e-04 (3.10e-04)	Tok/s 106879 (101753)	Loss/tok 3.4952 (4.5285)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.107 (0.141)	Data 1.19e-04 (3.09e-04)	Tok/s 95740 (101748)	Loss/tok 3.3889 (4.5232)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.159 (0.141)	Data 1.28e-04 (3.08e-04)	Tok/s 104409 (101752)	Loss/tok 3.5702 (4.5178)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.106 (0.141)	Data 1.16e-04 (3.07e-04)	Tok/s 97112 (101748)	Loss/tok 3.2678 (4.5127)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.160 (0.141)	Data 1.21e-04 (3.06e-04)	Tok/s 103865 (101734)	Loss/tok 3.5546 (4.5079)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.107 (0.141)	Data 1.45e-04 (3.05e-04)	Tok/s 95488 (101738)	Loss/tok 3.2637 (4.5026)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.106 (0.141)	Data 1.51e-04 (3.04e-04)	Tok/s 96984 (101731)	Loss/tok 3.1939 (4.4979)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019450894, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019450895, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.789 (0.789)	Decoder iters 149.0 (149.0)	Tok/s 21446 (21446)
0: Running moses detokenizer
0: BLEU(score=19.61819731040722, counts=[35036, 15992, 8524, 4726], totals=[67072, 64069, 61066, 58067], precisions=[52.23640267175573, 24.9605893645913, 13.958667671044443, 8.138874059276354], bp=1.0, sys_len=67072, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019452948, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1962, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019452949, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.4922	Test BLEU: 19.62
0: Performance: Epoch: 0	Training: 813736 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019452949, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019452950, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019452950, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2436059778
0: TRAIN [1][0/1938]	Time 0.417 (0.417)	Data 2.50e-01 (2.50e-01)	Tok/s 40205 (40205)	Loss/tok 3.4863 (3.4863)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.211 (0.168)	Data 1.86e-04 (2.29e-02)	Tok/s 110536 (96805)	Loss/tok 3.6622 (3.4462)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.159 (0.159)	Data 1.75e-04 (1.21e-02)	Tok/s 105666 (99395)	Loss/tok 3.4345 (3.4540)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][30/1938]	Time 0.107 (0.144)	Data 1.42e-04 (8.24e-03)	Tok/s 95787 (98651)	Loss/tok 3.2451 (3.4177)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.158 (0.143)	Data 1.77e-04 (6.27e-03)	Tok/s 106901 (99135)	Loss/tok 3.5115 (3.4300)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.059 (0.140)	Data 1.71e-04 (5.08e-03)	Tok/s 87502 (99163)	Loss/tok 2.7019 (3.4228)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.107 (0.138)	Data 1.30e-04 (4.27e-03)	Tok/s 96899 (99148)	Loss/tok 3.2440 (3.4160)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.212 (0.137)	Data 1.42e-04 (3.69e-03)	Tok/s 108312 (99202)	Loss/tok 3.6277 (3.4114)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.061 (0.135)	Data 3.04e-04 (3.26e-03)	Tok/s 88037 (99156)	Loss/tok 2.8018 (3.4035)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][90/1938]	Time 0.060 (0.138)	Data 1.52e-04 (2.92e-03)	Tok/s 87734 (99516)	Loss/tok 2.7393 (3.4260)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.160 (0.137)	Data 1.52e-04 (2.65e-03)	Tok/s 104847 (99506)	Loss/tok 3.4671 (3.4210)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.210 (0.139)	Data 1.60e-04 (2.43e-03)	Tok/s 112455 (99790)	Loss/tok 3.6473 (3.4339)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.211 (0.139)	Data 2.05e-04 (2.24e-03)	Tok/s 111370 (99896)	Loss/tok 3.7795 (3.4391)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.106 (0.138)	Data 1.31e-04 (2.08e-03)	Tok/s 98249 (99968)	Loss/tok 3.1801 (3.4345)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.107 (0.138)	Data 1.62e-04 (1.95e-03)	Tok/s 97435 (99973)	Loss/tok 3.2165 (3.4301)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.159 (0.136)	Data 1.33e-04 (1.83e-03)	Tok/s 105499 (99806)	Loss/tok 3.4120 (3.4257)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.159 (0.139)	Data 1.88e-04 (1.73e-03)	Tok/s 105089 (100035)	Loss/tok 3.5301 (3.4384)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.159 (0.138)	Data 1.50e-04 (1.64e-03)	Tok/s 106093 (100092)	Loss/tok 3.3764 (3.4337)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.108 (0.138)	Data 1.74e-04 (1.56e-03)	Tok/s 95191 (100117)	Loss/tok 3.1737 (3.4322)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.212 (0.139)	Data 1.46e-04 (1.48e-03)	Tok/s 110892 (100266)	Loss/tok 3.7223 (3.4339)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.159 (0.138)	Data 1.77e-04 (1.42e-03)	Tok/s 106765 (100188)	Loss/tok 3.4733 (3.4299)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][210/1938]	Time 0.107 (0.138)	Data 1.58e-04 (1.36e-03)	Tok/s 96100 (100275)	Loss/tok 3.1818 (3.4297)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.160 (0.138)	Data 1.55e-04 (1.30e-03)	Tok/s 104497 (100318)	Loss/tok 3.5953 (3.4322)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.107 (0.138)	Data 1.66e-04 (1.26e-03)	Tok/s 98025 (100291)	Loss/tok 3.3919 (3.4304)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.159 (0.139)	Data 1.46e-04 (1.21e-03)	Tok/s 105285 (100402)	Loss/tok 3.4723 (3.4378)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.213 (0.139)	Data 1.50e-04 (1.17e-03)	Tok/s 109405 (100411)	Loss/tok 3.7226 (3.4373)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.159 (0.139)	Data 1.78e-04 (1.13e-03)	Tok/s 105295 (100461)	Loss/tok 3.4212 (3.4366)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.060 (0.138)	Data 2.34e-04 (1.10e-03)	Tok/s 89507 (100374)	Loss/tok 2.7497 (3.4323)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.158 (0.139)	Data 2.34e-04 (1.06e-03)	Tok/s 105145 (100479)	Loss/tok 3.4858 (3.4381)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.106 (0.139)	Data 2.49e-04 (1.03e-03)	Tok/s 96910 (100502)	Loss/tok 3.2307 (3.4382)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.106 (0.138)	Data 2.38e-04 (1.01e-03)	Tok/s 97720 (100431)	Loss/tok 3.1885 (3.4351)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.211 (0.139)	Data 1.67e-04 (9.79e-04)	Tok/s 109196 (100411)	Loss/tok 3.7233 (3.4388)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][320/1938]	Time 0.211 (0.139)	Data 1.32e-04 (9.55e-04)	Tok/s 109919 (100487)	Loss/tok 3.6365 (3.4412)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.059 (0.139)	Data 1.80e-04 (9.31e-04)	Tok/s 89821 (100502)	Loss/tok 2.7452 (3.4429)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.161 (0.139)	Data 1.87e-04 (9.10e-04)	Tok/s 104985 (100522)	Loss/tok 3.5160 (3.4414)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.160 (0.139)	Data 2.14e-04 (8.89e-04)	Tok/s 107819 (100514)	Loss/tok 3.3392 (3.4417)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.159 (0.139)	Data 1.45e-04 (8.69e-04)	Tok/s 104467 (100478)	Loss/tok 3.4448 (3.4399)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.108 (0.140)	Data 1.64e-04 (8.50e-04)	Tok/s 96554 (100596)	Loss/tok 3.2295 (3.4435)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.107 (0.139)	Data 1.56e-04 (8.32e-04)	Tok/s 97100 (100531)	Loss/tok 3.2092 (3.4408)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.159 (0.139)	Data 1.50e-04 (8.16e-04)	Tok/s 106621 (100529)	Loss/tok 3.3808 (3.4393)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.160 (0.139)	Data 1.63e-04 (8.00e-04)	Tok/s 103500 (100538)	Loss/tok 3.4452 (3.4388)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.107 (0.139)	Data 1.66e-04 (7.85e-04)	Tok/s 96486 (100554)	Loss/tok 3.2514 (3.4413)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.270 (0.139)	Data 1.67e-04 (7.71e-04)	Tok/s 110893 (100508)	Loss/tok 3.7578 (3.4420)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.059 (0.139)	Data 1.96e-04 (7.57e-04)	Tok/s 89544 (100484)	Loss/tok 2.7736 (3.4408)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.107 (0.139)	Data 1.59e-04 (7.44e-04)	Tok/s 98373 (100529)	Loss/tok 3.2120 (3.4431)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][450/1938]	Time 0.107 (0.139)	Data 1.65e-04 (7.32e-04)	Tok/s 95036 (100469)	Loss/tok 3.1898 (3.4421)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.160 (0.139)	Data 2.20e-04 (7.20e-04)	Tok/s 103896 (100503)	Loss/tok 3.5404 (3.4429)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.159 (0.140)	Data 1.66e-04 (7.08e-04)	Tok/s 105240 (100515)	Loss/tok 3.3908 (3.4438)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.108 (0.140)	Data 1.69e-04 (6.97e-04)	Tok/s 95690 (100561)	Loss/tok 3.1891 (3.4439)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.159 (0.140)	Data 1.63e-04 (6.87e-04)	Tok/s 104972 (100572)	Loss/tok 3.4315 (3.4428)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.159 (0.140)	Data 2.23e-04 (6.77e-04)	Tok/s 105995 (100606)	Loss/tok 3.3909 (3.4446)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.106 (0.140)	Data 1.71e-04 (6.67e-04)	Tok/s 98582 (100599)	Loss/tok 3.1728 (3.4453)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.214 (0.140)	Data 3.00e-04 (6.58e-04)	Tok/s 109602 (100584)	Loss/tok 3.7121 (3.4442)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.107 (0.140)	Data 1.44e-04 (6.49e-04)	Tok/s 93763 (100593)	Loss/tok 3.2485 (3.4430)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.107 (0.140)	Data 1.26e-04 (6.40e-04)	Tok/s 95204 (100608)	Loss/tok 3.1781 (3.4420)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.107 (0.141)	Data 1.48e-04 (6.31e-04)	Tok/s 97016 (100658)	Loss/tok 3.1794 (3.4423)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.106 (0.140)	Data 1.48e-04 (6.23e-04)	Tok/s 98408 (100631)	Loss/tok 3.1659 (3.4419)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.059 (0.141)	Data 1.35e-04 (6.15e-04)	Tok/s 88476 (100680)	Loss/tok 2.8383 (3.4435)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][580/1938]	Time 0.108 (0.141)	Data 2.67e-04 (6.07e-04)	Tok/s 94693 (100752)	Loss/tok 3.1057 (3.4449)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.212 (0.141)	Data 2.89e-04 (6.00e-04)	Tok/s 109465 (100751)	Loss/tok 3.5078 (3.4447)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.107 (0.142)	Data 2.07e-04 (5.93e-04)	Tok/s 97005 (100779)	Loss/tok 3.0805 (3.4437)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.213 (0.141)	Data 1.58e-04 (5.85e-04)	Tok/s 109067 (100772)	Loss/tok 3.6396 (3.4431)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.107 (0.142)	Data 1.61e-04 (5.79e-04)	Tok/s 97711 (100800)	Loss/tok 3.1724 (3.4427)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.274 (0.142)	Data 1.51e-04 (5.72e-04)	Tok/s 109748 (100850)	Loss/tok 3.7085 (3.4440)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.161 (0.142)	Data 2.81e-04 (5.66e-04)	Tok/s 104285 (100842)	Loss/tok 3.4761 (3.4440)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.108 (0.142)	Data 1.48e-04 (5.59e-04)	Tok/s 96273 (100832)	Loss/tok 3.2654 (3.4426)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.107 (0.142)	Data 1.41e-04 (5.53e-04)	Tok/s 96722 (100850)	Loss/tok 3.1157 (3.4420)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.108 (0.142)	Data 1.64e-04 (5.47e-04)	Tok/s 95165 (100875)	Loss/tok 3.1600 (3.4427)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.060 (0.142)	Data 2.65e-04 (5.42e-04)	Tok/s 85703 (100881)	Loss/tok 2.6825 (3.4419)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.059 (0.142)	Data 1.24e-04 (5.37e-04)	Tok/s 90870 (100879)	Loss/tok 2.8061 (3.4407)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.107 (0.141)	Data 1.44e-04 (5.32e-04)	Tok/s 95423 (100802)	Loss/tok 3.1633 (3.4379)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][710/1938]	Time 0.108 (0.141)	Data 1.67e-04 (5.26e-04)	Tok/s 95620 (100823)	Loss/tok 3.2192 (3.4375)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][720/1938]	Time 0.160 (0.142)	Data 1.77e-04 (5.21e-04)	Tok/s 104371 (100873)	Loss/tok 3.3253 (3.4387)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.108 (0.142)	Data 2.42e-04 (5.17e-04)	Tok/s 96219 (100861)	Loss/tok 3.1777 (3.4382)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.211 (0.142)	Data 1.82e-04 (5.12e-04)	Tok/s 110800 (100930)	Loss/tok 3.6606 (3.4396)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.160 (0.142)	Data 1.26e-04 (5.07e-04)	Tok/s 106232 (100959)	Loss/tok 3.4901 (3.4390)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.060 (0.142)	Data 2.35e-04 (5.03e-04)	Tok/s 88465 (100927)	Loss/tok 2.6544 (3.4379)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.060 (0.142)	Data 2.07e-04 (4.99e-04)	Tok/s 88644 (100915)	Loss/tok 2.7129 (3.4375)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.212 (0.142)	Data 1.39e-04 (4.94e-04)	Tok/s 109792 (100968)	Loss/tok 3.6285 (3.4391)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.107 (0.142)	Data 2.05e-04 (4.90e-04)	Tok/s 94242 (100941)	Loss/tok 3.1184 (3.4372)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.108 (0.142)	Data 1.45e-04 (4.86e-04)	Tok/s 96895 (100949)	Loss/tok 3.1544 (3.4367)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.160 (0.142)	Data 1.41e-04 (4.82e-04)	Tok/s 106164 (100952)	Loss/tok 3.3726 (3.4362)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.107 (0.142)	Data 2.07e-04 (4.78e-04)	Tok/s 94672 (100952)	Loss/tok 3.1590 (3.4357)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.107 (0.142)	Data 1.47e-04 (4.75e-04)	Tok/s 97798 (100905)	Loss/tok 3.0053 (3.4335)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.107 (0.142)	Data 1.78e-04 (4.71e-04)	Tok/s 98976 (100957)	Loss/tok 3.1525 (3.4340)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][850/1938]	Time 0.159 (0.142)	Data 1.45e-04 (4.67e-04)	Tok/s 106391 (100971)	Loss/tok 3.4489 (3.4338)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.107 (0.142)	Data 1.62e-04 (4.64e-04)	Tok/s 96669 (100983)	Loss/tok 3.1908 (3.4344)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][870/1938]	Time 0.160 (0.142)	Data 1.54e-04 (4.60e-04)	Tok/s 104179 (100997)	Loss/tok 3.4200 (3.4335)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.060 (0.142)	Data 1.58e-04 (4.57e-04)	Tok/s 88060 (100945)	Loss/tok 2.7358 (3.4320)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.108 (0.142)	Data 1.45e-04 (4.54e-04)	Tok/s 96959 (100921)	Loss/tok 3.1262 (3.4303)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.160 (0.142)	Data 2.52e-04 (4.51e-04)	Tok/s 104208 (100942)	Loss/tok 3.4382 (3.4305)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.213 (0.142)	Data 1.41e-04 (4.48e-04)	Tok/s 108430 (100964)	Loss/tok 3.5621 (3.4330)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.060 (0.142)	Data 1.86e-04 (4.45e-04)	Tok/s 87189 (100947)	Loss/tok 2.6786 (3.4328)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.107 (0.142)	Data 2.17e-04 (4.42e-04)	Tok/s 94688 (100959)	Loss/tok 3.1515 (3.4325)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.213 (0.142)	Data 1.79e-04 (4.39e-04)	Tok/s 108291 (100946)	Loss/tok 3.6348 (3.4318)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.108 (0.142)	Data 1.48e-04 (4.36e-04)	Tok/s 95364 (100921)	Loss/tok 3.1494 (3.4307)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.160 (0.142)	Data 1.31e-04 (4.33e-04)	Tok/s 105480 (100925)	Loss/tok 3.2776 (3.4306)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.159 (0.142)	Data 1.70e-04 (4.31e-04)	Tok/s 105215 (100893)	Loss/tok 3.3705 (3.4293)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.108 (0.142)	Data 1.47e-04 (4.28e-04)	Tok/s 95531 (100929)	Loss/tok 3.1583 (3.4307)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.107 (0.142)	Data 1.55e-04 (4.25e-04)	Tok/s 96213 (100944)	Loss/tok 3.1041 (3.4300)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1000/1938]	Time 0.159 (0.142)	Data 1.66e-04 (4.23e-04)	Tok/s 105244 (100923)	Loss/tok 3.4118 (3.4291)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1010/1938]	Time 0.160 (0.142)	Data 1.52e-04 (4.20e-04)	Tok/s 104619 (100925)	Loss/tok 3.3354 (3.4296)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.108 (0.142)	Data 1.63e-04 (4.18e-04)	Tok/s 97186 (100950)	Loss/tok 3.2662 (3.4304)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.059 (0.142)	Data 1.48e-04 (4.16e-04)	Tok/s 90267 (100924)	Loss/tok 2.7013 (3.4295)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.158 (0.143)	Data 1.83e-04 (4.13e-04)	Tok/s 104524 (100940)	Loss/tok 3.3718 (3.4304)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.158 (0.143)	Data 1.55e-04 (4.11e-04)	Tok/s 106823 (100961)	Loss/tok 3.3155 (3.4304)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.058 (0.143)	Data 1.42e-04 (4.08e-04)	Tok/s 92023 (100949)	Loss/tok 2.7385 (3.4297)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.161 (0.143)	Data 2.06e-04 (4.06e-04)	Tok/s 105031 (100937)	Loss/tok 3.3114 (3.4296)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.107 (0.142)	Data 1.58e-04 (4.04e-04)	Tok/s 94207 (100931)	Loss/tok 3.2475 (3.4286)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.214 (0.143)	Data 1.99e-04 (4.02e-04)	Tok/s 108422 (100938)	Loss/tok 3.6654 (3.4293)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.109 (0.143)	Data 1.88e-04 (4.00e-04)	Tok/s 98164 (100927)	Loss/tok 3.2352 (3.4286)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.059 (0.143)	Data 2.30e-04 (3.98e-04)	Tok/s 89201 (100945)	Loss/tok 2.7002 (3.4285)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.160 (0.143)	Data 1.62e-04 (3.96e-04)	Tok/s 104543 (100976)	Loss/tok 3.3700 (3.4291)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.159 (0.143)	Data 1.74e-04 (3.94e-04)	Tok/s 104907 (100989)	Loss/tok 3.3755 (3.4282)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1140/1938]	Time 0.112 (0.143)	Data 1.29e-04 (3.92e-04)	Tok/s 92580 (100973)	Loss/tok 3.1830 (3.4277)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.161 (0.143)	Data 1.45e-04 (3.90e-04)	Tok/s 103714 (100979)	Loss/tok 3.3169 (3.4269)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1160/1938]	Time 0.212 (0.143)	Data 1.45e-04 (3.88e-04)	Tok/s 109564 (100974)	Loss/tok 3.6339 (3.4268)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1170/1938]	Time 0.159 (0.143)	Data 1.62e-04 (3.86e-04)	Tok/s 105939 (100995)	Loss/tok 3.4027 (3.4280)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.059 (0.143)	Data 1.81e-04 (3.84e-04)	Tok/s 87632 (100961)	Loss/tok 2.7637 (3.4267)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.158 (0.143)	Data 1.80e-04 (3.82e-04)	Tok/s 106734 (100947)	Loss/tok 3.2265 (3.4257)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.059 (0.143)	Data 1.86e-04 (3.81e-04)	Tok/s 90743 (100925)	Loss/tok 2.6799 (3.4242)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.107 (0.142)	Data 1.79e-04 (3.79e-04)	Tok/s 97471 (100915)	Loss/tok 3.1652 (3.4231)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.107 (0.142)	Data 1.49e-04 (3.78e-04)	Tok/s 96895 (100906)	Loss/tok 3.2316 (3.4218)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.107 (0.142)	Data 1.89e-04 (3.76e-04)	Tok/s 95145 (100880)	Loss/tok 3.0864 (3.4210)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.159 (0.142)	Data 1.67e-04 (3.74e-04)	Tok/s 105096 (100886)	Loss/tok 3.3592 (3.4201)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.107 (0.142)	Data 1.68e-04 (3.73e-04)	Tok/s 97680 (100867)	Loss/tok 3.0772 (3.4190)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.108 (0.142)	Data 1.53e-04 (3.71e-04)	Tok/s 96468 (100830)	Loss/tok 3.2888 (3.4182)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.059 (0.142)	Data 1.69e-04 (3.70e-04)	Tok/s 89446 (100844)	Loss/tok 2.7518 (3.4181)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.107 (0.142)	Data 1.73e-04 (3.68e-04)	Tok/s 93896 (100837)	Loss/tok 3.0734 (3.4179)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.212 (0.142)	Data 1.77e-04 (3.67e-04)	Tok/s 109776 (100858)	Loss/tok 3.3997 (3.4177)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1300/1938]	Time 0.107 (0.142)	Data 1.74e-04 (3.65e-04)	Tok/s 97298 (100882)	Loss/tok 3.1958 (3.4182)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.160 (0.142)	Data 1.70e-04 (3.64e-04)	Tok/s 107649 (100873)	Loss/tok 3.4382 (3.4181)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.215 (0.142)	Data 1.84e-04 (3.63e-04)	Tok/s 109016 (100905)	Loss/tok 3.4348 (3.4184)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.108 (0.142)	Data 1.59e-04 (3.61e-04)	Tok/s 95301 (100897)	Loss/tok 3.0459 (3.4175)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.214 (0.142)	Data 1.52e-04 (3.60e-04)	Tok/s 108701 (100893)	Loss/tok 3.5644 (3.4175)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1350/1938]	Time 0.274 (0.142)	Data 1.62e-04 (3.58e-04)	Tok/s 106814 (100890)	Loss/tok 3.9075 (3.4176)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.159 (0.142)	Data 1.83e-04 (3.57e-04)	Tok/s 105500 (100891)	Loss/tok 3.3671 (3.4176)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.212 (0.143)	Data 1.55e-04 (3.56e-04)	Tok/s 110987 (100908)	Loss/tok 3.5047 (3.4188)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.107 (0.143)	Data 1.75e-04 (3.55e-04)	Tok/s 96870 (100885)	Loss/tok 3.2259 (3.4177)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.213 (0.143)	Data 2.37e-04 (3.53e-04)	Tok/s 110037 (100886)	Loss/tok 3.5302 (3.4171)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.160 (0.142)	Data 1.72e-04 (3.52e-04)	Tok/s 106908 (100847)	Loss/tok 3.3455 (3.4158)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.213 (0.142)	Data 2.28e-04 (3.51e-04)	Tok/s 109561 (100843)	Loss/tok 3.6059 (3.4152)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.109 (0.142)	Data 2.98e-04 (3.50e-04)	Tok/s 93054 (100831)	Loss/tok 3.1088 (3.4143)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.108 (0.142)	Data 2.12e-04 (3.49e-04)	Tok/s 96780 (100841)	Loss/tok 3.0822 (3.4144)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.212 (0.142)	Data 2.04e-04 (3.48e-04)	Tok/s 110625 (100823)	Loss/tok 3.5585 (3.4135)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.107 (0.142)	Data 1.96e-04 (3.47e-04)	Tok/s 96213 (100796)	Loss/tok 3.1920 (3.4124)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.107 (0.142)	Data 1.47e-04 (3.46e-04)	Tok/s 96139 (100797)	Loss/tok 3.1152 (3.4119)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.107 (0.142)	Data 1.92e-04 (3.45e-04)	Tok/s 94722 (100800)	Loss/tok 3.1886 (3.4118)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1480/1938]	Time 0.107 (0.142)	Data 2.66e-04 (3.44e-04)	Tok/s 96400 (100765)	Loss/tok 3.1939 (3.4106)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.159 (0.142)	Data 1.89e-04 (3.42e-04)	Tok/s 107177 (100772)	Loss/tok 3.2617 (3.4104)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.159 (0.142)	Data 1.81e-04 (3.41e-04)	Tok/s 104875 (100777)	Loss/tok 3.4076 (3.4110)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.160 (0.142)	Data 2.50e-04 (3.40e-04)	Tok/s 105632 (100789)	Loss/tok 3.3340 (3.4109)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.059 (0.142)	Data 1.71e-04 (3.39e-04)	Tok/s 89129 (100776)	Loss/tok 2.6927 (3.4103)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.213 (0.142)	Data 1.52e-04 (3.38e-04)	Tok/s 109127 (100790)	Loss/tok 3.4871 (3.4096)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.160 (0.142)	Data 1.61e-04 (3.37e-04)	Tok/s 106116 (100795)	Loss/tok 3.3648 (3.4092)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.160 (0.142)	Data 2.04e-04 (3.37e-04)	Tok/s 106035 (100804)	Loss/tok 3.2873 (3.4095)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.158 (0.142)	Data 1.35e-04 (3.35e-04)	Tok/s 107138 (100787)	Loss/tok 3.3787 (3.4090)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.273 (0.142)	Data 1.87e-04 (3.34e-04)	Tok/s 109653 (100788)	Loss/tok 3.6859 (3.4088)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.106 (0.142)	Data 1.72e-04 (3.33e-04)	Tok/s 98158 (100780)	Loss/tok 3.1876 (3.4084)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.214 (0.142)	Data 1.42e-04 (3.32e-04)	Tok/s 108885 (100790)	Loss/tok 3.5108 (3.4079)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.159 (0.142)	Data 2.00e-04 (3.32e-04)	Tok/s 105334 (100808)	Loss/tok 3.3185 (3.4077)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1610/1938]	Time 0.159 (0.142)	Data 2.37e-04 (3.31e-04)	Tok/s 105504 (100811)	Loss/tok 3.3930 (3.4071)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.061 (0.142)	Data 2.04e-04 (3.30e-04)	Tok/s 88898 (100812)	Loss/tok 2.8278 (3.4067)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.160 (0.142)	Data 1.69e-04 (3.29e-04)	Tok/s 105778 (100829)	Loss/tok 3.3676 (3.4072)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.211 (0.142)	Data 1.22e-04 (3.28e-04)	Tok/s 109287 (100836)	Loss/tok 3.5725 (3.4067)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.106 (0.142)	Data 1.50e-04 (3.27e-04)	Tok/s 98682 (100845)	Loss/tok 3.1466 (3.4064)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.107 (0.142)	Data 1.67e-04 (3.26e-04)	Tok/s 97702 (100841)	Loss/tok 3.3036 (3.4066)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.212 (0.142)	Data 1.36e-04 (3.25e-04)	Tok/s 109266 (100826)	Loss/tok 3.5310 (3.4059)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.107 (0.142)	Data 1.29e-04 (3.24e-04)	Tok/s 96726 (100837)	Loss/tok 3.2789 (3.4061)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.107 (0.142)	Data 1.83e-04 (3.23e-04)	Tok/s 97496 (100845)	Loss/tok 3.0299 (3.4057)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.158 (0.142)	Data 1.34e-04 (3.22e-04)	Tok/s 102707 (100845)	Loss/tok 3.3039 (3.4049)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.160 (0.142)	Data 1.41e-04 (3.21e-04)	Tok/s 105459 (100847)	Loss/tok 3.2154 (3.4043)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.213 (0.142)	Data 1.82e-04 (3.20e-04)	Tok/s 110547 (100843)	Loss/tok 3.5325 (3.4037)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.212 (0.142)	Data 1.21e-04 (3.19e-04)	Tok/s 109619 (100852)	Loss/tok 3.5255 (3.4041)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1740/1938]	Time 0.159 (0.142)	Data 1.29e-04 (3.18e-04)	Tok/s 106149 (100847)	Loss/tok 3.3777 (3.4035)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.159 (0.142)	Data 1.58e-04 (3.17e-04)	Tok/s 104533 (100865)	Loss/tok 3.4186 (3.4043)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.158 (0.142)	Data 1.56e-04 (3.16e-04)	Tok/s 107057 (100859)	Loss/tok 3.3281 (3.4035)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.107 (0.142)	Data 1.56e-04 (3.15e-04)	Tok/s 99369 (100839)	Loss/tok 3.2058 (3.4027)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.213 (0.142)	Data 1.44e-04 (3.14e-04)	Tok/s 108554 (100843)	Loss/tok 3.4872 (3.4028)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.214 (0.142)	Data 1.32e-04 (3.13e-04)	Tok/s 109723 (100835)	Loss/tok 3.5686 (3.4022)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.107 (0.142)	Data 1.58e-04 (3.12e-04)	Tok/s 96647 (100847)	Loss/tok 3.2303 (3.4028)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.159 (0.142)	Data 1.45e-04 (3.11e-04)	Tok/s 105398 (100839)	Loss/tok 3.2859 (3.4020)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.212 (0.142)	Data 1.39e-04 (3.10e-04)	Tok/s 110240 (100844)	Loss/tok 3.4979 (3.4020)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1830/1938]	Time 0.159 (0.142)	Data 1.38e-04 (3.09e-04)	Tok/s 103805 (100838)	Loss/tok 3.3440 (3.4015)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.059 (0.142)	Data 1.82e-04 (3.08e-04)	Tok/s 88091 (100843)	Loss/tok 2.7184 (3.4012)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.059 (0.142)	Data 1.38e-04 (3.07e-04)	Tok/s 90359 (100847)	Loss/tok 2.7308 (3.4014)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.159 (0.142)	Data 1.36e-04 (3.06e-04)	Tok/s 106316 (100845)	Loss/tok 3.3861 (3.4010)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.107 (0.142)	Data 1.49e-04 (3.05e-04)	Tok/s 98471 (100842)	Loss/tok 3.1833 (3.4004)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.213 (0.142)	Data 1.50e-04 (3.05e-04)	Tok/s 108122 (100857)	Loss/tok 3.4890 (3.4004)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.160 (0.142)	Data 1.23e-04 (3.04e-04)	Tok/s 105698 (100861)	Loss/tok 3.3142 (3.4001)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.159 (0.142)	Data 1.27e-04 (3.03e-04)	Tok/s 106236 (100847)	Loss/tok 3.4895 (3.3995)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.159 (0.142)	Data 1.64e-04 (3.02e-04)	Tok/s 107009 (100837)	Loss/tok 3.2747 (3.3989)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.107 (0.142)	Data 1.22e-04 (3.01e-04)	Tok/s 95588 (100810)	Loss/tok 3.1494 (3.3980)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.107 (0.142)	Data 1.24e-04 (3.00e-04)	Tok/s 95021 (100811)	Loss/tok 3.1162 (3.3974)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019728388, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019728389, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.766 (0.766)	Decoder iters 149.0 (149.0)	Tok/s 21173 (21173)
0: Running moses detokenizer
0: BLEU(score=22.47704698887773, counts=[35846, 17364, 9627, 5538], totals=[64228, 61225, 58222, 55223], precisions=[55.81054991592452, 28.360963658636177, 16.534986774758682, 10.028430183075892], bp=0.9930491188798977, sys_len=64228, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019730306, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2248, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019730306, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3957	Test BLEU: 22.48
0: Performance: Epoch: 1	Training: 806466 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019730307, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019730307, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019730307, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1484108604
0: TRAIN [2][0/1938]	Time 0.471 (0.471)	Data 2.52e-01 (2.52e-01)	Tok/s 49813 (49813)	Loss/tok 3.3497 (3.3497)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.212 (0.198)	Data 1.43e-04 (2.31e-02)	Tok/s 109952 (99297)	Loss/tok 3.4235 (3.3645)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][20/1938]	Time 0.158 (0.173)	Data 1.62e-04 (1.22e-02)	Tok/s 106357 (100123)	Loss/tok 3.3129 (3.3294)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][30/1938]	Time 0.108 (0.167)	Data 1.65e-04 (8.31e-03)	Tok/s 96477 (100975)	Loss/tok 2.9948 (3.3085)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.212 (0.167)	Data 2.10e-04 (6.33e-03)	Tok/s 110756 (101604)	Loss/tok 3.3843 (3.3206)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.108 (0.165)	Data 1.66e-04 (5.13e-03)	Tok/s 96357 (101920)	Loss/tok 3.0539 (3.3073)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.160 (0.160)	Data 1.96e-04 (4.32e-03)	Tok/s 104269 (101741)	Loss/tok 3.2524 (3.2967)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.107 (0.157)	Data 1.72e-04 (3.74e-03)	Tok/s 98181 (101519)	Loss/tok 3.0174 (3.2906)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.108 (0.153)	Data 2.15e-04 (3.30e-03)	Tok/s 95610 (101163)	Loss/tok 3.0931 (3.2777)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.108 (0.154)	Data 1.78e-04 (2.96e-03)	Tok/s 95362 (101222)	Loss/tok 3.0312 (3.2918)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.213 (0.153)	Data 3.21e-04 (2.68e-03)	Tok/s 108447 (101244)	Loss/tok 3.4755 (3.2906)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][110/1938]	Time 0.160 (0.154)	Data 1.74e-04 (2.46e-03)	Tok/s 105056 (101451)	Loss/tok 3.3652 (3.2963)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.060 (0.153)	Data 1.66e-04 (2.27e-03)	Tok/s 88951 (101428)	Loss/tok 2.6080 (3.2916)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.274 (0.154)	Data 1.57e-04 (2.11e-03)	Tok/s 110616 (101555)	Loss/tok 3.5637 (3.2955)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.160 (0.154)	Data 1.47e-04 (1.97e-03)	Tok/s 104529 (101623)	Loss/tok 3.2486 (3.2933)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.108 (0.152)	Data 1.68e-04 (1.86e-03)	Tok/s 97236 (101500)	Loss/tok 3.1492 (3.2859)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.108 (0.152)	Data 1.75e-04 (1.75e-03)	Tok/s 94215 (101520)	Loss/tok 3.0129 (3.2893)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.212 (0.152)	Data 1.61e-04 (1.66e-03)	Tok/s 110904 (101586)	Loss/tok 3.4892 (3.2883)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.158 (0.151)	Data 2.41e-04 (1.58e-03)	Tok/s 105743 (101479)	Loss/tok 3.3173 (3.2831)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.212 (0.150)	Data 1.75e-04 (1.51e-03)	Tok/s 110134 (101518)	Loss/tok 3.5578 (3.2800)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.060 (0.150)	Data 1.74e-04 (1.44e-03)	Tok/s 86629 (101441)	Loss/tok 2.5826 (3.2769)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.108 (0.149)	Data 2.39e-04 (1.38e-03)	Tok/s 95681 (101394)	Loss/tok 2.9880 (3.2725)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.159 (0.148)	Data 2.37e-04 (1.33e-03)	Tok/s 105725 (101382)	Loss/tok 3.1846 (3.2693)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.107 (0.147)	Data 1.62e-04 (1.28e-03)	Tok/s 96537 (101212)	Loss/tok 3.0386 (3.2646)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][240/1938]	Time 0.160 (0.147)	Data 1.65e-04 (1.23e-03)	Tok/s 104289 (101268)	Loss/tok 3.3022 (3.2653)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.059 (0.146)	Data 1.75e-04 (1.19e-03)	Tok/s 91424 (101164)	Loss/tok 2.6811 (3.2623)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.213 (0.146)	Data 1.93e-04 (1.15e-03)	Tok/s 108262 (101160)	Loss/tok 3.4902 (3.2638)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.108 (0.145)	Data 2.54e-04 (1.12e-03)	Tok/s 97759 (101095)	Loss/tok 3.0975 (3.2616)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.109 (0.145)	Data 2.23e-04 (1.08e-03)	Tok/s 95151 (101093)	Loss/tok 3.1098 (3.2600)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.161 (0.145)	Data 2.08e-04 (1.05e-03)	Tok/s 103767 (101056)	Loss/tok 3.2614 (3.2600)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.108 (0.145)	Data 2.09e-04 (1.02e-03)	Tok/s 96693 (101070)	Loss/tok 3.0996 (3.2597)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.160 (0.146)	Data 1.87e-04 (9.96e-04)	Tok/s 105869 (101162)	Loss/tok 3.2672 (3.2602)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.107 (0.145)	Data 1.87e-04 (9.71e-04)	Tok/s 95453 (101104)	Loss/tok 3.0411 (3.2583)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.107 (0.146)	Data 1.92e-04 (9.47e-04)	Tok/s 95459 (101168)	Loss/tok 3.1276 (3.2638)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.107 (0.147)	Data 1.79e-04 (9.25e-04)	Tok/s 94371 (101216)	Loss/tok 3.0091 (3.2655)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.107 (0.146)	Data 1.93e-04 (9.03e-04)	Tok/s 97055 (101134)	Loss/tok 3.0456 (3.2623)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.213 (0.146)	Data 1.49e-04 (8.83e-04)	Tok/s 109563 (101212)	Loss/tok 3.4564 (3.2647)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][370/1938]	Time 0.108 (0.146)	Data 1.66e-04 (8.64e-04)	Tok/s 95748 (101139)	Loss/tok 3.0647 (3.2631)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.161 (0.146)	Data 1.73e-04 (8.45e-04)	Tok/s 104670 (101130)	Loss/tok 3.2652 (3.2648)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][390/1938]	Time 0.107 (0.146)	Data 1.85e-04 (8.28e-04)	Tok/s 96124 (101130)	Loss/tok 3.1284 (3.2681)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.108 (0.146)	Data 2.50e-04 (8.12e-04)	Tok/s 96609 (101065)	Loss/tok 3.0728 (3.2662)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.108 (0.145)	Data 1.70e-04 (7.97e-04)	Tok/s 94454 (101023)	Loss/tok 3.1171 (3.2645)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.107 (0.146)	Data 1.37e-04 (7.82e-04)	Tok/s 96705 (101065)	Loss/tok 3.1054 (3.2660)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.212 (0.146)	Data 2.14e-04 (7.79e-04)	Tok/s 109335 (101100)	Loss/tok 3.4824 (3.2674)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.274 (0.146)	Data 2.14e-04 (7.66e-04)	Tok/s 107978 (101090)	Loss/tok 3.5733 (3.2672)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.160 (0.146)	Data 1.26e-04 (7.52e-04)	Tok/s 106201 (101066)	Loss/tok 3.2877 (3.2662)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.108 (0.145)	Data 1.44e-04 (7.40e-04)	Tok/s 97010 (101032)	Loss/tok 3.0166 (3.2643)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.107 (0.146)	Data 1.40e-04 (7.27e-04)	Tok/s 97705 (101091)	Loss/tok 3.0283 (3.2658)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.107 (0.145)	Data 1.85e-04 (7.16e-04)	Tok/s 96488 (101081)	Loss/tok 3.0719 (3.2655)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.159 (0.146)	Data 1.67e-04 (7.05e-04)	Tok/s 105794 (101129)	Loss/tok 3.3371 (3.2649)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.107 (0.145)	Data 1.62e-04 (6.94e-04)	Tok/s 97572 (101040)	Loss/tok 3.0147 (3.2621)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.107 (0.144)	Data 1.90e-04 (6.84e-04)	Tok/s 96229 (100945)	Loss/tok 2.9150 (3.2595)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][520/1938]	Time 0.159 (0.144)	Data 1.46e-04 (6.74e-04)	Tok/s 104483 (100972)	Loss/tok 3.3122 (3.2609)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.108 (0.144)	Data 1.77e-04 (6.65e-04)	Tok/s 95186 (100940)	Loss/tok 3.0296 (3.2611)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.108 (0.145)	Data 1.57e-04 (6.56e-04)	Tok/s 94523 (100974)	Loss/tok 3.0930 (3.2617)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.159 (0.144)	Data 1.40e-04 (6.48e-04)	Tok/s 105739 (100939)	Loss/tok 3.0438 (3.2601)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.108 (0.144)	Data 1.33e-04 (6.39e-04)	Tok/s 95711 (100902)	Loss/tok 3.0712 (3.2600)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.108 (0.145)	Data 1.74e-04 (6.31e-04)	Tok/s 97975 (100923)	Loss/tok 3.0170 (3.2622)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.107 (0.144)	Data 1.71e-04 (6.23e-04)	Tok/s 98602 (100905)	Loss/tok 3.0566 (3.2611)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.060 (0.144)	Data 2.09e-04 (6.16e-04)	Tok/s 89032 (100854)	Loss/tok 2.6265 (3.2596)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.107 (0.144)	Data 1.61e-04 (6.08e-04)	Tok/s 97927 (100841)	Loss/tok 3.0794 (3.2583)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.160 (0.144)	Data 1.70e-04 (6.01e-04)	Tok/s 105847 (100899)	Loss/tok 3.2380 (3.2593)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][620/1938]	Time 0.273 (0.143)	Data 1.41e-04 (5.94e-04)	Tok/s 108090 (100809)	Loss/tok 3.7005 (3.2587)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.160 (0.143)	Data 1.48e-04 (5.87e-04)	Tok/s 103659 (100797)	Loss/tok 3.2690 (3.2582)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.059 (0.143)	Data 1.50e-04 (5.81e-04)	Tok/s 91519 (100788)	Loss/tok 2.5981 (3.2592)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.212 (0.143)	Data 1.68e-04 (5.74e-04)	Tok/s 109877 (100762)	Loss/tok 3.4957 (3.2581)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.108 (0.143)	Data 1.79e-04 (5.68e-04)	Tok/s 95287 (100743)	Loss/tok 3.0225 (3.2580)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.059 (0.142)	Data 1.78e-04 (5.62e-04)	Tok/s 89580 (100711)	Loss/tok 2.5976 (3.2571)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.272 (0.143)	Data 1.47e-04 (5.56e-04)	Tok/s 109274 (100800)	Loss/tok 3.6780 (3.2600)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.108 (0.143)	Data 2.49e-04 (5.51e-04)	Tok/s 93655 (100794)	Loss/tok 3.1063 (3.2606)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.059 (0.143)	Data 1.37e-04 (5.45e-04)	Tok/s 88209 (100747)	Loss/tok 2.6753 (3.2586)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.106 (0.143)	Data 1.44e-04 (5.40e-04)	Tok/s 96844 (100767)	Loss/tok 3.0808 (3.2596)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.108 (0.143)	Data 1.25e-04 (5.34e-04)	Tok/s 95066 (100715)	Loss/tok 3.0404 (3.2579)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.161 (0.142)	Data 2.45e-04 (5.29e-04)	Tok/s 104881 (100683)	Loss/tok 3.1912 (3.2579)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.059 (0.142)	Data 1.42e-04 (5.24e-04)	Tok/s 86821 (100663)	Loss/tok 2.5446 (3.2580)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][750/1938]	Time 0.108 (0.142)	Data 1.49e-04 (5.20e-04)	Tok/s 95023 (100628)	Loss/tok 3.1536 (3.2569)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.159 (0.142)	Data 1.25e-04 (5.15e-04)	Tok/s 105981 (100598)	Loss/tok 3.2554 (3.2559)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.160 (0.142)	Data 1.74e-04 (5.11e-04)	Tok/s 104143 (100597)	Loss/tok 3.3195 (3.2552)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.059 (0.142)	Data 2.22e-04 (5.06e-04)	Tok/s 87998 (100607)	Loss/tok 2.5747 (3.2551)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.107 (0.142)	Data 1.70e-04 (5.02e-04)	Tok/s 95500 (100607)	Loss/tok 2.9971 (3.2548)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.160 (0.141)	Data 1.57e-04 (4.98e-04)	Tok/s 105967 (100572)	Loss/tok 3.1613 (3.2549)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.160 (0.142)	Data 1.62e-04 (4.94e-04)	Tok/s 103699 (100638)	Loss/tok 3.2289 (3.2553)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.211 (0.142)	Data 1.52e-04 (4.90e-04)	Tok/s 112259 (100658)	Loss/tok 3.3575 (3.2549)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.159 (0.142)	Data 1.51e-04 (4.86e-04)	Tok/s 106275 (100670)	Loss/tok 3.3395 (3.2556)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][840/1938]	Time 0.160 (0.142)	Data 1.25e-04 (4.82e-04)	Tok/s 105289 (100650)	Loss/tok 3.3191 (3.2560)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.108 (0.142)	Data 3.00e-04 (4.79e-04)	Tok/s 96573 (100653)	Loss/tok 3.0767 (3.2577)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.214 (0.142)	Data 1.48e-04 (4.75e-04)	Tok/s 108795 (100669)	Loss/tok 3.4496 (3.2594)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.059 (0.142)	Data 1.35e-04 (4.71e-04)	Tok/s 88755 (100631)	Loss/tok 2.7028 (3.2582)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.160 (0.142)	Data 1.76e-04 (4.68e-04)	Tok/s 106077 (100662)	Loss/tok 3.2957 (3.2590)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.214 (0.142)	Data 2.08e-04 (4.64e-04)	Tok/s 109774 (100675)	Loss/tok 3.4457 (3.2592)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.107 (0.142)	Data 1.42e-04 (4.61e-04)	Tok/s 96775 (100650)	Loss/tok 2.9511 (3.2583)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.107 (0.142)	Data 1.45e-04 (4.58e-04)	Tok/s 95211 (100639)	Loss/tok 3.0311 (3.2584)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.059 (0.142)	Data 2.17e-04 (4.55e-04)	Tok/s 91145 (100594)	Loss/tok 2.7584 (3.2585)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.274 (0.142)	Data 2.37e-04 (4.52e-04)	Tok/s 108673 (100629)	Loss/tok 3.5096 (3.2600)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.059 (0.142)	Data 1.42e-04 (4.49e-04)	Tok/s 88137 (100580)	Loss/tok 2.5903 (3.2589)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.059 (0.142)	Data 1.92e-04 (4.46e-04)	Tok/s 89827 (100578)	Loss/tok 2.6229 (3.2589)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.107 (0.142)	Data 1.54e-04 (4.43e-04)	Tok/s 98606 (100587)	Loss/tok 3.0364 (3.2584)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][970/1938]	Time 0.108 (0.142)	Data 1.75e-04 (4.41e-04)	Tok/s 95261 (100585)	Loss/tok 3.0004 (3.2578)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.160 (0.142)	Data 1.46e-04 (4.38e-04)	Tok/s 105580 (100608)	Loss/tok 3.2803 (3.2582)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.058 (0.141)	Data 2.08e-04 (4.35e-04)	Tok/s 88494 (100562)	Loss/tok 2.6234 (3.2576)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.107 (0.141)	Data 1.63e-04 (4.32e-04)	Tok/s 97647 (100550)	Loss/tok 3.0496 (3.2569)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.159 (0.141)	Data 2.46e-04 (4.30e-04)	Tok/s 106042 (100574)	Loss/tok 3.2885 (3.2567)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.107 (0.141)	Data 1.72e-04 (4.27e-04)	Tok/s 97612 (100569)	Loss/tok 3.0696 (3.2572)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.273 (0.142)	Data 1.45e-04 (4.24e-04)	Tok/s 108503 (100578)	Loss/tok 3.6411 (3.2588)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.159 (0.142)	Data 1.57e-04 (4.22e-04)	Tok/s 105535 (100617)	Loss/tok 3.1949 (3.2600)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.214 (0.142)	Data 1.48e-04 (4.20e-04)	Tok/s 109077 (100622)	Loss/tok 3.5270 (3.2603)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.162 (0.142)	Data 1.87e-04 (4.17e-04)	Tok/s 104947 (100598)	Loss/tok 3.1909 (3.2591)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.107 (0.142)	Data 1.90e-04 (4.15e-04)	Tok/s 98046 (100565)	Loss/tok 3.0648 (3.2587)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.107 (0.142)	Data 1.55e-04 (4.13e-04)	Tok/s 96825 (100592)	Loss/tok 3.0537 (3.2586)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1090/1938]	Time 0.108 (0.142)	Data 1.46e-04 (4.11e-04)	Tok/s 96000 (100602)	Loss/tok 3.1566 (3.2601)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.107 (0.142)	Data 1.49e-04 (4.08e-04)	Tok/s 94887 (100607)	Loss/tok 3.1658 (3.2597)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.108 (0.142)	Data 1.98e-04 (4.06e-04)	Tok/s 96921 (100653)	Loss/tok 2.9046 (3.2612)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.213 (0.142)	Data 1.56e-04 (4.04e-04)	Tok/s 109524 (100649)	Loss/tok 3.4349 (3.2610)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.213 (0.142)	Data 2.79e-04 (4.02e-04)	Tok/s 107779 (100679)	Loss/tok 3.5122 (3.2617)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.107 (0.142)	Data 2.02e-04 (4.00e-04)	Tok/s 97124 (100672)	Loss/tok 3.1170 (3.2617)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.059 (0.142)	Data 1.65e-04 (3.98e-04)	Tok/s 90316 (100646)	Loss/tok 2.6388 (3.2607)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.272 (0.142)	Data 1.80e-04 (3.96e-04)	Tok/s 109461 (100635)	Loss/tok 3.7483 (3.2607)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.159 (0.142)	Data 1.35e-04 (3.94e-04)	Tok/s 106149 (100639)	Loss/tok 3.1836 (3.2603)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.108 (0.142)	Data 1.54e-04 (3.92e-04)	Tok/s 96612 (100657)	Loss/tok 3.0991 (3.2603)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.108 (0.142)	Data 1.80e-04 (3.90e-04)	Tok/s 96531 (100660)	Loss/tok 2.9456 (3.2596)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.160 (0.142)	Data 2.83e-04 (3.89e-04)	Tok/s 105833 (100645)	Loss/tok 3.2095 (3.2590)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.160 (0.142)	Data 1.49e-04 (3.87e-04)	Tok/s 105002 (100647)	Loss/tok 3.3020 (3.2591)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1220/1938]	Time 0.107 (0.142)	Data 2.20e-04 (3.85e-04)	Tok/s 95544 (100651)	Loss/tok 2.8962 (3.2592)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1230/1938]	Time 0.108 (0.142)	Data 2.19e-04 (3.83e-04)	Tok/s 95017 (100672)	Loss/tok 2.9803 (3.2602)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.159 (0.142)	Data 1.44e-04 (3.82e-04)	Tok/s 105319 (100651)	Loss/tok 3.2044 (3.2598)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.159 (0.142)	Data 2.30e-04 (3.80e-04)	Tok/s 105333 (100677)	Loss/tok 3.2452 (3.2599)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.161 (0.142)	Data 2.06e-04 (3.79e-04)	Tok/s 103623 (100699)	Loss/tok 3.3257 (3.2608)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.162 (0.142)	Data 1.88e-04 (3.77e-04)	Tok/s 103388 (100689)	Loss/tok 3.2196 (3.2607)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.160 (0.142)	Data 1.74e-04 (3.75e-04)	Tok/s 105919 (100709)	Loss/tok 3.3375 (3.2608)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.273 (0.142)	Data 1.76e-04 (3.74e-04)	Tok/s 109797 (100709)	Loss/tok 3.6533 (3.2615)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.108 (0.142)	Data 1.64e-04 (3.72e-04)	Tok/s 97529 (100711)	Loss/tok 3.0901 (3.2615)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.108 (0.142)	Data 1.92e-04 (3.71e-04)	Tok/s 96956 (100705)	Loss/tok 3.0131 (3.2610)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.69e-04)	Tok/s 96732 (100676)	Loss/tok 3.1863 (3.2599)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.273 (0.142)	Data 1.45e-04 (3.67e-04)	Tok/s 109236 (100680)	Loss/tok 3.5862 (3.2599)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.108 (0.142)	Data 1.40e-04 (3.66e-04)	Tok/s 94627 (100697)	Loss/tok 3.1132 (3.2601)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.109 (0.142)	Data 1.43e-04 (3.65e-04)	Tok/s 95709 (100714)	Loss/tok 3.0978 (3.2608)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1360/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.63e-04)	Tok/s 95101 (100717)	Loss/tok 3.1131 (3.2615)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.212 (0.142)	Data 1.49e-04 (3.62e-04)	Tok/s 109610 (100710)	Loss/tok 3.4196 (3.2613)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.108 (0.142)	Data 1.29e-04 (3.60e-04)	Tok/s 95162 (100719)	Loss/tok 2.9960 (3.2611)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.213 (0.143)	Data 2.18e-04 (3.59e-04)	Tok/s 108458 (100732)	Loss/tok 3.4212 (3.2613)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1400/1938]	Time 0.273 (0.143)	Data 1.49e-04 (3.57e-04)	Tok/s 108073 (100750)	Loss/tok 3.7058 (3.2628)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.213 (0.143)	Data 1.42e-04 (3.56e-04)	Tok/s 110912 (100768)	Loss/tok 3.3940 (3.2630)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.159 (0.143)	Data 1.45e-04 (3.55e-04)	Tok/s 105579 (100756)	Loss/tok 3.3489 (3.2627)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.212 (0.143)	Data 1.62e-04 (3.53e-04)	Tok/s 109219 (100751)	Loss/tok 3.5796 (3.2623)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.108 (0.143)	Data 1.50e-04 (3.52e-04)	Tok/s 94477 (100748)	Loss/tok 3.0780 (3.2621)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.107 (0.142)	Data 1.58e-04 (3.51e-04)	Tok/s 96056 (100707)	Loss/tok 2.9203 (3.2612)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.213 (0.143)	Data 1.39e-04 (3.49e-04)	Tok/s 109282 (100732)	Loss/tok 3.4780 (3.2619)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.212 (0.143)	Data 1.54e-04 (3.48e-04)	Tok/s 109143 (100754)	Loss/tok 3.4407 (3.2625)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.108 (0.143)	Data 3.00e-04 (3.47e-04)	Tok/s 97476 (100754)	Loss/tok 2.9267 (3.2618)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.107 (0.142)	Data 2.04e-04 (3.46e-04)	Tok/s 96685 (100738)	Loss/tok 2.9547 (3.2609)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.159 (0.143)	Data 1.78e-04 (3.45e-04)	Tok/s 105232 (100752)	Loss/tok 3.1622 (3.2611)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.106 (0.142)	Data 1.43e-04 (3.44e-04)	Tok/s 96619 (100717)	Loss/tok 2.9362 (3.2600)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.059 (0.142)	Data 1.45e-04 (3.43e-04)	Tok/s 89191 (100704)	Loss/tok 2.5811 (3.2600)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1530/1938]	Time 0.107 (0.142)	Data 2.09e-04 (3.42e-04)	Tok/s 94670 (100719)	Loss/tok 2.9931 (3.2606)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.107 (0.142)	Data 2.05e-04 (3.40e-04)	Tok/s 98527 (100720)	Loss/tok 2.9822 (3.2608)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.273 (0.142)	Data 1.42e-04 (3.39e-04)	Tok/s 110061 (100707)	Loss/tok 3.6513 (3.2608)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.38e-04)	Tok/s 94276 (100702)	Loss/tok 3.0129 (3.2613)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.37e-04)	Tok/s 96401 (100723)	Loss/tok 3.1660 (3.2616)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.060 (0.142)	Data 1.81e-04 (3.36e-04)	Tok/s 85009 (100718)	Loss/tok 2.6192 (3.2613)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.108 (0.142)	Data 1.56e-04 (3.35e-04)	Tok/s 95640 (100698)	Loss/tok 3.1247 (3.2605)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.058 (0.142)	Data 1.30e-04 (3.34e-04)	Tok/s 91816 (100687)	Loss/tok 2.7482 (3.2599)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.059 (0.142)	Data 1.61e-04 (3.32e-04)	Tok/s 90551 (100686)	Loss/tok 2.7269 (3.2596)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.159 (0.142)	Data 1.46e-04 (3.31e-04)	Tok/s 107014 (100686)	Loss/tok 3.3115 (3.2597)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.106 (0.142)	Data 1.76e-04 (3.31e-04)	Tok/s 95173 (100657)	Loss/tok 3.1007 (3.2589)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.108 (0.142)	Data 2.26e-04 (3.30e-04)	Tok/s 96390 (100653)	Loss/tok 3.0708 (3.2586)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.059 (0.142)	Data 1.48e-04 (3.29e-04)	Tok/s 89139 (100624)	Loss/tok 2.5784 (3.2577)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1660/1938]	Time 0.213 (0.142)	Data 2.21e-04 (3.28e-04)	Tok/s 108861 (100621)	Loss/tok 3.4126 (3.2572)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.159 (0.142)	Data 1.61e-04 (3.27e-04)	Tok/s 105630 (100636)	Loss/tok 3.2041 (3.2573)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.106 (0.142)	Data 2.05e-04 (3.26e-04)	Tok/s 95160 (100643)	Loss/tok 3.1072 (3.2574)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.107 (0.142)	Data 2.24e-04 (3.25e-04)	Tok/s 96478 (100640)	Loss/tok 3.1413 (3.2576)	LR 2.000e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1700/1938]	Time 0.212 (0.142)	Data 1.63e-04 (3.24e-04)	Tok/s 109120 (100651)	Loss/tok 3.3254 (3.2582)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.108 (0.142)	Data 1.47e-04 (3.23e-04)	Tok/s 95865 (100655)	Loss/tok 3.0527 (3.2579)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.161 (0.142)	Data 2.35e-04 (3.23e-04)	Tok/s 104603 (100670)	Loss/tok 3.3038 (3.2578)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.160 (0.142)	Data 2.25e-04 (3.22e-04)	Tok/s 104725 (100674)	Loss/tok 3.2282 (3.2578)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.107 (0.142)	Data 1.79e-04 (3.21e-04)	Tok/s 95702 (100671)	Loss/tok 3.0123 (3.2581)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.159 (0.142)	Data 1.53e-04 (3.20e-04)	Tok/s 104350 (100667)	Loss/tok 3.2923 (3.2576)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.271 (0.142)	Data 1.73e-04 (3.19e-04)	Tok/s 110530 (100673)	Loss/tok 3.5769 (3.2580)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.109 (0.142)	Data 2.10e-04 (3.18e-04)	Tok/s 95664 (100665)	Loss/tok 2.9967 (3.2577)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.107 (0.142)	Data 1.45e-04 (3.18e-04)	Tok/s 95691 (100655)	Loss/tok 3.0629 (3.2575)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.107 (0.142)	Data 1.71e-04 (3.17e-04)	Tok/s 93857 (100641)	Loss/tok 3.0633 (3.2575)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.108 (0.142)	Data 1.65e-04 (3.16e-04)	Tok/s 96642 (100647)	Loss/tok 3.0323 (3.2573)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.160 (0.142)	Data 1.46e-04 (3.15e-04)	Tok/s 105570 (100656)	Loss/tok 3.3226 (3.2569)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1820/1938]	Time 0.160 (0.142)	Data 2.51e-04 (3.14e-04)	Tok/s 105569 (100666)	Loss/tok 3.2634 (3.2572)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.108 (0.142)	Data 2.21e-04 (3.14e-04)	Tok/s 96586 (100661)	Loss/tok 3.0019 (3.2571)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.160 (0.142)	Data 2.03e-04 (3.13e-04)	Tok/s 105059 (100644)	Loss/tok 3.2206 (3.2564)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.107 (0.141)	Data 1.46e-04 (3.12e-04)	Tok/s 95683 (100633)	Loss/tok 3.1311 (3.2560)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.158 (0.141)	Data 2.26e-04 (3.11e-04)	Tok/s 106302 (100636)	Loss/tok 3.2524 (3.2558)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.107 (0.142)	Data 1.27e-04 (3.10e-04)	Tok/s 94095 (100636)	Loss/tok 3.0067 (3.2563)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.160 (0.142)	Data 1.42e-04 (3.10e-04)	Tok/s 104927 (100659)	Loss/tok 3.2907 (3.2567)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1890/1938]	Time 0.059 (0.142)	Data 1.48e-04 (3.09e-04)	Tok/s 88186 (100671)	Loss/tok 2.6709 (3.2571)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.107 (0.142)	Data 1.88e-04 (3.08e-04)	Tok/s 96147 (100680)	Loss/tok 2.9697 (3.2572)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.161 (0.142)	Data 1.46e-04 (3.07e-04)	Tok/s 104419 (100686)	Loss/tok 3.2634 (3.2576)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.107 (0.142)	Data 1.49e-04 (3.07e-04)	Tok/s 98504 (100691)	Loss/tok 3.0080 (3.2574)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.211 (0.142)	Data 1.70e-04 (3.06e-04)	Tok/s 110907 (100693)	Loss/tok 3.4107 (3.2575)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593020006137, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020006137, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.753 (0.753)	Decoder iters 149.0 (149.0)	Tok/s 22079 (22079)
0: Running moses detokenizer
0: BLEU(score=22.842360514092935, counts=[36466, 17877, 10028, 5850], totals=[65815, 62812, 59810, 56813], precisions=[55.40682215300463, 28.46112207858371, 16.76642701889316, 10.296939080844172], bp=1.0, sys_len=65815, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020008072, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2284, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020008072, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2577	Test BLEU: 22.84
0: Performance: Epoch: 2	Training: 805369 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593020008073, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020008073, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593020008073, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1213797664
0: TRAIN [3][0/1938]	Time 0.475 (0.475)	Data 2.48e-01 (2.48e-01)	Tok/s 49178 (49178)	Loss/tok 3.3242 (3.3242)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.107 (0.183)	Data 2.38e-04 (2.28e-02)	Tok/s 95617 (98537)	Loss/tok 2.9756 (3.1995)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.107 (0.175)	Data 1.68e-04 (1.20e-02)	Tok/s 97458 (101301)	Loss/tok 3.1260 (3.2274)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.108 (0.163)	Data 2.22e-04 (8.19e-03)	Tok/s 96884 (100785)	Loss/tok 2.9411 (3.2079)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.107 (0.156)	Data 2.16e-04 (6.24e-03)	Tok/s 96204 (100466)	Loss/tok 3.0203 (3.2031)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.108 (0.151)	Data 2.81e-04 (5.05e-03)	Tok/s 93234 (100102)	Loss/tok 2.9986 (3.1878)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.163 (0.151)	Data 1.93e-04 (4.25e-03)	Tok/s 102451 (100383)	Loss/tok 3.1199 (3.1828)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.158 (0.148)	Data 1.91e-04 (3.68e-03)	Tok/s 104364 (100244)	Loss/tok 3.1306 (3.1722)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][80/1938]	Time 0.213 (0.151)	Data 1.46e-04 (3.24e-03)	Tok/s 110106 (100773)	Loss/tok 3.3474 (3.1877)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.107 (0.148)	Data 2.17e-04 (2.91e-03)	Tok/s 95117 (100555)	Loss/tok 3.0407 (3.1740)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.107 (0.148)	Data 1.24e-04 (2.63e-03)	Tok/s 95556 (100626)	Loss/tok 3.0587 (3.1719)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.159 (0.148)	Data 1.43e-04 (2.41e-03)	Tok/s 105759 (100703)	Loss/tok 3.0995 (3.1697)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.161 (0.148)	Data 1.96e-04 (2.23e-03)	Tok/s 104695 (100858)	Loss/tok 3.2788 (3.1720)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.108 (0.148)	Data 1.49e-04 (2.07e-03)	Tok/s 94520 (100898)	Loss/tok 2.9498 (3.1659)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.061 (0.145)	Data 2.47e-04 (1.94e-03)	Tok/s 87799 (100599)	Loss/tok 2.5320 (3.1595)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.108 (0.147)	Data 3.05e-04 (1.82e-03)	Tok/s 95255 (100877)	Loss/tok 2.9063 (3.1696)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.107 (0.146)	Data 2.41e-04 (1.72e-03)	Tok/s 95251 (100663)	Loss/tok 2.9360 (3.1643)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.108 (0.146)	Data 1.64e-04 (1.63e-03)	Tok/s 94086 (100712)	Loss/tok 2.8093 (3.1701)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.159 (0.147)	Data 1.62e-04 (1.55e-03)	Tok/s 107986 (100825)	Loss/tok 3.1967 (3.1741)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.272 (0.149)	Data 2.19e-04 (1.48e-03)	Tok/s 109650 (101026)	Loss/tok 3.4922 (3.1792)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.108 (0.149)	Data 1.60e-04 (1.42e-03)	Tok/s 96069 (101073)	Loss/tok 3.0039 (3.1827)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][210/1938]	Time 0.214 (0.150)	Data 1.71e-04 (1.36e-03)	Tok/s 108141 (101080)	Loss/tok 3.3710 (3.1863)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.214 (0.150)	Data 1.82e-04 (1.30e-03)	Tok/s 110486 (101173)	Loss/tok 3.2614 (3.1875)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.107 (0.150)	Data 1.98e-04 (1.25e-03)	Tok/s 97138 (101104)	Loss/tok 3.0262 (3.1872)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.212 (0.149)	Data 2.09e-04 (1.21e-03)	Tok/s 109752 (100966)	Loss/tok 3.3463 (3.1848)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.107 (0.147)	Data 2.64e-04 (1.17e-03)	Tok/s 96346 (100826)	Loss/tok 3.0389 (3.1814)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.107 (0.147)	Data 1.73e-04 (1.13e-03)	Tok/s 97808 (100874)	Loss/tok 3.0210 (3.1796)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.160 (0.147)	Data 2.00e-04 (1.10e-03)	Tok/s 103801 (100909)	Loss/tok 3.2361 (3.1776)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.160 (0.148)	Data 1.45e-04 (1.06e-03)	Tok/s 103594 (101014)	Loss/tok 3.0838 (3.1810)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.107 (0.147)	Data 1.75e-04 (1.03e-03)	Tok/s 94432 (100989)	Loss/tok 2.8872 (3.1784)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.274 (0.147)	Data 1.63e-04 (1.01e-03)	Tok/s 109749 (101027)	Loss/tok 3.3783 (3.1791)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.108 (0.146)	Data 1.80e-04 (9.79e-04)	Tok/s 96496 (100916)	Loss/tok 2.9214 (3.1751)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.212 (0.147)	Data 1.49e-04 (9.55e-04)	Tok/s 109880 (100981)	Loss/tok 3.4465 (3.1771)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.159 (0.147)	Data 1.93e-04 (9.31e-04)	Tok/s 106951 (101081)	Loss/tok 3.2478 (3.1782)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][340/1938]	Time 0.212 (0.147)	Data 1.63e-04 (9.09e-04)	Tok/s 110336 (101004)	Loss/tok 3.3583 (3.1768)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.107 (0.147)	Data 1.78e-04 (8.89e-04)	Tok/s 97668 (101060)	Loss/tok 2.8901 (3.1782)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.159 (0.147)	Data 1.78e-04 (8.69e-04)	Tok/s 105469 (101024)	Loss/tok 3.1466 (3.1768)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.107 (0.147)	Data 1.54e-04 (8.50e-04)	Tok/s 94283 (101069)	Loss/tok 2.9970 (3.1774)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.106 (0.146)	Data 2.51e-04 (8.33e-04)	Tok/s 95887 (100923)	Loss/tok 2.9998 (3.1768)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.107 (0.146)	Data 1.28e-04 (8.16e-04)	Tok/s 95493 (100944)	Loss/tok 2.9454 (3.1783)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.107 (0.147)	Data 1.59e-04 (8.00e-04)	Tok/s 97312 (101006)	Loss/tok 2.9232 (3.1813)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.059 (0.147)	Data 2.17e-04 (7.85e-04)	Tok/s 89459 (100994)	Loss/tok 2.5209 (3.1813)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.213 (0.147)	Data 1.72e-04 (7.70e-04)	Tok/s 110190 (101091)	Loss/tok 3.3435 (3.1834)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.160 (0.147)	Data 1.75e-04 (7.57e-04)	Tok/s 105689 (101051)	Loss/tok 3.1000 (3.1815)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.107 (0.147)	Data 1.41e-04 (7.43e-04)	Tok/s 97700 (101069)	Loss/tok 2.9496 (3.1831)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.160 (0.147)	Data 1.57e-04 (7.32e-04)	Tok/s 104662 (101041)	Loss/tok 3.1433 (3.1827)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.272 (0.147)	Data 1.46e-04 (7.19e-04)	Tok/s 109765 (101022)	Loss/tok 3.4897 (3.1840)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][470/1938]	Time 0.107 (0.146)	Data 1.49e-04 (7.08e-04)	Tok/s 95883 (100984)	Loss/tok 3.0024 (3.1828)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.107 (0.146)	Data 1.81e-04 (6.97e-04)	Tok/s 99654 (100908)	Loss/tok 2.9399 (3.1808)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.160 (0.146)	Data 1.69e-04 (6.87e-04)	Tok/s 105319 (100936)	Loss/tok 3.2064 (3.1803)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.107 (0.146)	Data 1.78e-04 (6.77e-04)	Tok/s 97056 (100974)	Loss/tok 2.9536 (3.1795)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.160 (0.145)	Data 2.15e-04 (6.67e-04)	Tok/s 103550 (100946)	Loss/tok 3.1508 (3.1784)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.273 (0.146)	Data 1.70e-04 (6.57e-04)	Tok/s 108500 (101012)	Loss/tok 3.5755 (3.1804)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.107 (0.146)	Data 2.34e-04 (6.48e-04)	Tok/s 94617 (100982)	Loss/tok 3.0007 (3.1798)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][540/1938]	Time 0.107 (0.145)	Data 1.58e-04 (6.40e-04)	Tok/s 94667 (100931)	Loss/tok 3.0298 (3.1785)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.159 (0.146)	Data 1.40e-04 (6.32e-04)	Tok/s 103470 (100949)	Loss/tok 3.1485 (3.1787)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.159 (0.145)	Data 1.51e-04 (6.23e-04)	Tok/s 105289 (100951)	Loss/tok 3.0749 (3.1781)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.059 (0.145)	Data 1.49e-04 (6.15e-04)	Tok/s 88938 (100858)	Loss/tok 2.6143 (3.1756)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.108 (0.145)	Data 1.64e-04 (6.08e-04)	Tok/s 96721 (100884)	Loss/tok 3.1439 (3.1765)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.107 (0.145)	Data 1.82e-04 (6.01e-04)	Tok/s 95097 (100901)	Loss/tok 3.0224 (3.1776)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.160 (0.145)	Data 1.55e-04 (5.94e-04)	Tok/s 106578 (100927)	Loss/tok 3.0515 (3.1779)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.160 (0.145)	Data 1.49e-04 (5.87e-04)	Tok/s 104382 (100884)	Loss/tok 3.1954 (3.1769)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.160 (0.144)	Data 2.34e-04 (5.81e-04)	Tok/s 104378 (100822)	Loss/tok 3.1977 (3.1756)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.060 (0.144)	Data 1.74e-04 (5.74e-04)	Tok/s 88296 (100756)	Loss/tok 2.6365 (3.1744)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.272 (0.144)	Data 2.25e-04 (5.68e-04)	Tok/s 108275 (100767)	Loss/tok 3.4748 (3.1760)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.212 (0.144)	Data 2.14e-04 (5.62e-04)	Tok/s 110574 (100728)	Loss/tok 3.3084 (3.1750)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.159 (0.144)	Data 1.25e-04 (5.56e-04)	Tok/s 106715 (100761)	Loss/tok 3.1747 (3.1759)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][670/1938]	Time 0.160 (0.144)	Data 1.63e-04 (5.51e-04)	Tok/s 105735 (100803)	Loss/tok 3.2687 (3.1760)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.108 (0.144)	Data 1.77e-04 (5.45e-04)	Tok/s 94555 (100778)	Loss/tok 2.9975 (3.1750)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.159 (0.144)	Data 1.53e-04 (5.40e-04)	Tok/s 105478 (100750)	Loss/tok 3.3152 (3.1753)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.160 (0.144)	Data 1.80e-04 (5.35e-04)	Tok/s 104744 (100755)	Loss/tok 2.9978 (3.1748)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.107 (0.144)	Data 2.53e-04 (5.30e-04)	Tok/s 96678 (100746)	Loss/tok 2.9390 (3.1738)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.159 (0.144)	Data 1.47e-04 (5.25e-04)	Tok/s 105408 (100784)	Loss/tok 3.0380 (3.1751)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.059 (0.144)	Data 1.57e-04 (5.20e-04)	Tok/s 91687 (100735)	Loss/tok 2.5816 (3.1737)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.213 (0.144)	Data 1.32e-04 (5.15e-04)	Tok/s 109982 (100777)	Loss/tok 3.3440 (3.1751)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.159 (0.143)	Data 1.49e-04 (5.10e-04)	Tok/s 105288 (100742)	Loss/tok 3.1454 (3.1737)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.212 (0.144)	Data 1.75e-04 (5.06e-04)	Tok/s 109660 (100746)	Loss/tok 3.3021 (3.1739)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.107 (0.143)	Data 2.02e-04 (5.01e-04)	Tok/s 94027 (100712)	Loss/tok 3.0182 (3.1728)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.159 (0.143)	Data 1.56e-04 (4.97e-04)	Tok/s 104926 (100702)	Loss/tok 3.1063 (3.1741)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.107 (0.143)	Data 1.49e-04 (4.93e-04)	Tok/s 95532 (100714)	Loss/tok 3.0624 (3.1742)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][800/1938]	Time 0.161 (0.143)	Data 1.59e-04 (4.88e-04)	Tok/s 106129 (100728)	Loss/tok 3.1335 (3.1737)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.107 (0.143)	Data 1.70e-04 (4.84e-04)	Tok/s 96318 (100693)	Loss/tok 2.9250 (3.1725)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.160 (0.143)	Data 2.29e-04 (4.81e-04)	Tok/s 103774 (100727)	Loss/tok 3.1325 (3.1725)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.107 (0.143)	Data 2.25e-04 (4.77e-04)	Tok/s 96500 (100744)	Loss/tok 2.9419 (3.1719)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.159 (0.143)	Data 1.61e-04 (4.73e-04)	Tok/s 105493 (100740)	Loss/tok 3.1948 (3.1715)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.107 (0.143)	Data 2.01e-04 (4.70e-04)	Tok/s 98011 (100761)	Loss/tok 2.9539 (3.1708)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.060 (0.143)	Data 1.27e-04 (4.66e-04)	Tok/s 90361 (100742)	Loss/tok 2.5674 (3.1707)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.108 (0.143)	Data 2.78e-04 (4.63e-04)	Tok/s 96398 (100688)	Loss/tok 2.9269 (3.1692)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.107 (0.143)	Data 1.54e-04 (4.59e-04)	Tok/s 96818 (100676)	Loss/tok 2.8988 (3.1690)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.159 (0.143)	Data 1.76e-04 (4.56e-04)	Tok/s 106337 (100668)	Loss/tok 3.0449 (3.1681)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.273 (0.143)	Data 1.46e-04 (4.53e-04)	Tok/s 107535 (100685)	Loss/tok 3.5852 (3.1695)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.159 (0.143)	Data 1.28e-04 (4.50e-04)	Tok/s 105804 (100713)	Loss/tok 3.1707 (3.1693)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][920/1938]	Time 0.107 (0.143)	Data 1.22e-04 (4.47e-04)	Tok/s 96396 (100693)	Loss/tok 3.0814 (3.1680)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.160 (0.143)	Data 2.29e-04 (4.44e-04)	Tok/s 105548 (100678)	Loss/tok 3.1115 (3.1666)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.212 (0.143)	Data 1.52e-04 (4.41e-04)	Tok/s 109397 (100681)	Loss/tok 3.3977 (3.1663)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.107 (0.142)	Data 1.64e-04 (4.39e-04)	Tok/s 96687 (100643)	Loss/tok 3.0125 (3.1655)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.107 (0.142)	Data 1.49e-04 (4.36e-04)	Tok/s 95850 (100663)	Loss/tok 2.9848 (3.1652)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.161 (0.142)	Data 2.24e-04 (4.34e-04)	Tok/s 103921 (100647)	Loss/tok 3.1053 (3.1642)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.161 (0.143)	Data 1.80e-04 (4.31e-04)	Tok/s 104320 (100660)	Loss/tok 3.1582 (3.1652)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.107 (0.142)	Data 1.91e-04 (4.28e-04)	Tok/s 95479 (100644)	Loss/tok 2.8559 (3.1641)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.160 (0.142)	Data 1.56e-04 (4.26e-04)	Tok/s 103758 (100641)	Loss/tok 3.2268 (3.1640)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.107 (0.143)	Data 1.55e-04 (4.24e-04)	Tok/s 97192 (100664)	Loss/tok 2.9187 (3.1644)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.213 (0.143)	Data 2.54e-04 (4.21e-04)	Tok/s 110910 (100671)	Loss/tok 3.2495 (3.1637)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.273 (0.143)	Data 1.92e-04 (4.19e-04)	Tok/s 108694 (100684)	Loss/tok 3.5329 (3.1646)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.107 (0.143)	Data 1.96e-04 (4.17e-04)	Tok/s 95750 (100685)	Loss/tok 2.8993 (3.1640)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1050/1938]	Time 0.107 (0.143)	Data 1.63e-04 (4.14e-04)	Tok/s 97267 (100698)	Loss/tok 2.8838 (3.1638)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.159 (0.143)	Data 1.88e-04 (4.12e-04)	Tok/s 105903 (100745)	Loss/tok 3.1635 (3.1648)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.107 (0.143)	Data 2.23e-04 (4.10e-04)	Tok/s 97643 (100755)	Loss/tok 2.9786 (3.1654)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.160 (0.143)	Data 1.96e-04 (4.08e-04)	Tok/s 104527 (100768)	Loss/tok 3.1700 (3.1669)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.159 (0.143)	Data 2.89e-04 (4.06e-04)	Tok/s 105817 (100767)	Loss/tok 3.1227 (3.1668)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.160 (0.143)	Data 1.89e-04 (4.03e-04)	Tok/s 104809 (100745)	Loss/tok 3.1559 (3.1667)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.160 (0.143)	Data 1.75e-04 (4.01e-04)	Tok/s 105503 (100738)	Loss/tok 3.1030 (3.1657)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.108 (0.143)	Data 1.78e-04 (3.99e-04)	Tok/s 94554 (100721)	Loss/tok 2.9151 (3.1656)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.108 (0.143)	Data 1.91e-04 (3.97e-04)	Tok/s 95873 (100702)	Loss/tok 2.9321 (3.1651)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.059 (0.143)	Data 2.15e-04 (3.95e-04)	Tok/s 89616 (100703)	Loss/tok 2.5933 (3.1652)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.272 (0.143)	Data 1.53e-04 (3.93e-04)	Tok/s 110313 (100722)	Loss/tok 3.4362 (3.1656)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.059 (0.143)	Data 1.48e-04 (3.92e-04)	Tok/s 91008 (100710)	Loss/tok 2.5450 (3.1647)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.160 (0.143)	Data 1.45e-04 (3.90e-04)	Tok/s 106645 (100711)	Loss/tok 3.1283 (3.1644)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1180/1938]	Time 0.159 (0.143)	Data 1.72e-04 (3.88e-04)	Tok/s 105227 (100693)	Loss/tok 3.1962 (3.1636)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.213 (0.143)	Data 1.58e-04 (3.86e-04)	Tok/s 110127 (100724)	Loss/tok 3.2483 (3.1635)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.106 (0.143)	Data 1.58e-04 (3.85e-04)	Tok/s 97936 (100727)	Loss/tok 2.9511 (3.1640)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.160 (0.143)	Data 1.57e-04 (3.83e-04)	Tok/s 105578 (100729)	Loss/tok 3.1609 (3.1638)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.108 (0.143)	Data 1.61e-04 (3.81e-04)	Tok/s 95686 (100729)	Loss/tok 2.8670 (3.1635)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.161 (0.143)	Data 3.38e-04 (3.79e-04)	Tok/s 105652 (100762)	Loss/tok 3.1008 (3.1637)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.107 (0.143)	Data 1.45e-04 (3.78e-04)	Tok/s 96356 (100739)	Loss/tok 2.8732 (3.1630)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.160 (0.143)	Data 1.66e-04 (3.76e-04)	Tok/s 105095 (100735)	Loss/tok 3.0834 (3.1621)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.274 (0.143)	Data 1.60e-04 (3.75e-04)	Tok/s 107973 (100755)	Loss/tok 3.5325 (3.1634)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.274 (0.143)	Data 1.74e-04 (3.73e-04)	Tok/s 108891 (100753)	Loss/tok 3.3660 (3.1630)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.108 (0.143)	Data 2.13e-04 (3.72e-04)	Tok/s 95640 (100729)	Loss/tok 2.9543 (3.1619)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.107 (0.143)	Data 1.62e-04 (3.70e-04)	Tok/s 96825 (100731)	Loss/tok 2.9859 (3.1626)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.060 (0.143)	Data 1.81e-04 (3.69e-04)	Tok/s 88834 (100699)	Loss/tok 2.4726 (3.1619)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1310/1938]	Time 0.159 (0.143)	Data 1.48e-04 (3.67e-04)	Tok/s 104430 (100697)	Loss/tok 3.1455 (3.1611)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.159 (0.143)	Data 1.42e-04 (3.66e-04)	Tok/s 106386 (100699)	Loss/tok 3.0778 (3.1610)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.107 (0.143)	Data 1.70e-04 (3.64e-04)	Tok/s 97485 (100697)	Loss/tok 2.9355 (3.1604)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.272 (0.143)	Data 1.99e-04 (3.63e-04)	Tok/s 109940 (100705)	Loss/tok 3.3296 (3.1602)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.108 (0.143)	Data 1.46e-04 (3.61e-04)	Tok/s 95119 (100703)	Loss/tok 2.9607 (3.1599)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.107 (0.143)	Data 1.84e-04 (3.60e-04)	Tok/s 97613 (100690)	Loss/tok 2.9152 (3.1593)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.060 (0.143)	Data 1.54e-04 (3.59e-04)	Tok/s 85249 (100684)	Loss/tok 2.6074 (3.1589)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.108 (0.142)	Data 3.19e-04 (3.58e-04)	Tok/s 96927 (100680)	Loss/tok 2.8609 (3.1581)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.160 (0.142)	Data 2.25e-04 (3.56e-04)	Tok/s 104502 (100659)	Loss/tok 3.1841 (3.1572)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.160 (0.142)	Data 1.70e-04 (3.55e-04)	Tok/s 107309 (100661)	Loss/tok 3.1343 (3.1570)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.159 (0.142)	Data 1.67e-04 (3.54e-04)	Tok/s 104149 (100685)	Loss/tok 3.1719 (3.1567)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.108 (0.142)	Data 2.46e-04 (3.53e-04)	Tok/s 96945 (100682)	Loss/tok 2.9462 (3.1567)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.107 (0.142)	Data 2.29e-04 (3.52e-04)	Tok/s 96369 (100676)	Loss/tok 2.8537 (3.1559)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1440/1938]	Time 0.158 (0.142)	Data 1.66e-04 (3.50e-04)	Tok/s 105327 (100695)	Loss/tok 3.0931 (3.1565)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.107 (0.142)	Data 1.63e-04 (3.49e-04)	Tok/s 91329 (100672)	Loss/tok 2.8856 (3.1555)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.107 (0.142)	Data 1.70e-04 (3.48e-04)	Tok/s 97616 (100693)	Loss/tok 3.0929 (3.1556)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.159 (0.142)	Data 1.64e-04 (3.47e-04)	Tok/s 106425 (100710)	Loss/tok 3.1109 (3.1555)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.160 (0.143)	Data 1.57e-04 (3.46e-04)	Tok/s 104964 (100724)	Loss/tok 3.1558 (3.1558)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.159 (0.142)	Data 1.83e-04 (3.44e-04)	Tok/s 107895 (100716)	Loss/tok 3.0969 (3.1553)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.159 (0.142)	Data 1.75e-04 (3.43e-04)	Tok/s 105082 (100723)	Loss/tok 3.0705 (3.1552)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.159 (0.142)	Data 1.91e-04 (3.42e-04)	Tok/s 105729 (100728)	Loss/tok 3.0416 (3.1547)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.060 (0.142)	Data 1.46e-04 (3.41e-04)	Tok/s 88077 (100703)	Loss/tok 2.6325 (3.1545)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.107 (0.142)	Data 2.52e-04 (3.40e-04)	Tok/s 94255 (100711)	Loss/tok 2.9287 (3.1544)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.107 (0.142)	Data 1.81e-04 (3.39e-04)	Tok/s 94942 (100714)	Loss/tok 2.8727 (3.1542)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.160 (0.143)	Data 1.85e-04 (3.38e-04)	Tok/s 103404 (100725)	Loss/tok 3.1481 (3.1544)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1560/1938]	Time 0.108 (0.143)	Data 1.30e-04 (3.37e-04)	Tok/s 96637 (100725)	Loss/tok 2.9493 (3.1545)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.107 (0.142)	Data 1.44e-04 (3.36e-04)	Tok/s 96303 (100717)	Loss/tok 2.9709 (3.1539)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.161 (0.143)	Data 1.49e-04 (3.35e-04)	Tok/s 106052 (100725)	Loss/tok 2.9928 (3.1538)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.159 (0.143)	Data 1.54e-04 (3.34e-04)	Tok/s 104851 (100723)	Loss/tok 3.2631 (3.1543)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.159 (0.143)	Data 1.50e-04 (3.33e-04)	Tok/s 104762 (100736)	Loss/tok 3.1629 (3.1544)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.213 (0.143)	Data 2.09e-04 (3.32e-04)	Tok/s 110527 (100731)	Loss/tok 3.1859 (3.1541)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.060 (0.143)	Data 1.54e-04 (3.31e-04)	Tok/s 85013 (100723)	Loss/tok 2.6063 (3.1536)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.059 (0.143)	Data 1.54e-04 (3.30e-04)	Tok/s 89112 (100730)	Loss/tok 2.4917 (3.1538)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.159 (0.143)	Data 1.68e-04 (3.29e-04)	Tok/s 105097 (100725)	Loss/tok 3.1232 (3.1531)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.108 (0.142)	Data 2.99e-04 (3.28e-04)	Tok/s 96106 (100706)	Loss/tok 3.0140 (3.1525)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.107 (0.142)	Data 1.76e-04 (3.27e-04)	Tok/s 94476 (100706)	Loss/tok 3.0127 (3.1522)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.107 (0.142)	Data 1.80e-04 (3.26e-04)	Tok/s 98756 (100676)	Loss/tok 2.8320 (3.1512)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.107 (0.142)	Data 2.42e-04 (3.25e-04)	Tok/s 95068 (100673)	Loss/tok 2.8722 (3.1511)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1690/1938]	Time 0.159 (0.142)	Data 1.56e-04 (3.24e-04)	Tok/s 105365 (100677)	Loss/tok 3.2538 (3.1510)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.213 (0.142)	Data 1.62e-04 (3.23e-04)	Tok/s 107281 (100675)	Loss/tok 3.3969 (3.1506)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.108 (0.142)	Data 2.03e-04 (3.23e-04)	Tok/s 96267 (100686)	Loss/tok 2.9966 (3.1505)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.059 (0.142)	Data 1.74e-04 (3.22e-04)	Tok/s 90092 (100685)	Loss/tok 2.5845 (3.1506)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.161 (0.142)	Data 1.67e-04 (3.21e-04)	Tok/s 105349 (100697)	Loss/tok 3.1248 (3.1505)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.059 (0.142)	Data 1.19e-04 (3.20e-04)	Tok/s 88419 (100685)	Loss/tok 2.4530 (3.1498)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.159 (0.142)	Data 1.80e-04 (3.19e-04)	Tok/s 106150 (100698)	Loss/tok 3.1443 (3.1496)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.159 (0.142)	Data 2.00e-04 (3.18e-04)	Tok/s 105872 (100694)	Loss/tok 3.1490 (3.1494)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.159 (0.142)	Data 2.09e-04 (3.17e-04)	Tok/s 105085 (100686)	Loss/tok 3.2473 (3.1489)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.059 (0.142)	Data 1.45e-04 (3.17e-04)	Tok/s 89394 (100671)	Loss/tok 2.4949 (3.1483)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.107 (0.142)	Data 1.47e-04 (3.16e-04)	Tok/s 94943 (100695)	Loss/tok 2.8690 (3.1487)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.159 (0.142)	Data 1.80e-04 (3.15e-04)	Tok/s 103652 (100699)	Loss/tok 3.0905 (3.1488)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.159 (0.142)	Data 1.82e-04 (3.14e-04)	Tok/s 105259 (100694)	Loss/tok 3.1048 (3.1482)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1820/1938]	Time 0.059 (0.142)	Data 2.04e-04 (3.14e-04)	Tok/s 91224 (100697)	Loss/tok 2.5227 (3.1478)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.059 (0.142)	Data 1.81e-04 (3.13e-04)	Tok/s 90618 (100688)	Loss/tok 2.4850 (3.1473)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.107 (0.142)	Data 2.63e-04 (3.12e-04)	Tok/s 98254 (100692)	Loss/tok 2.9276 (3.1471)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.107 (0.142)	Data 2.28e-04 (3.12e-04)	Tok/s 96652 (100695)	Loss/tok 3.0190 (3.1472)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.159 (0.142)	Data 1.96e-04 (3.11e-04)	Tok/s 104778 (100697)	Loss/tok 3.1435 (3.1474)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.159 (0.142)	Data 1.60e-04 (3.10e-04)	Tok/s 104658 (100684)	Loss/tok 3.0965 (3.1468)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.160 (0.142)	Data 1.46e-04 (3.09e-04)	Tok/s 104945 (100681)	Loss/tok 3.2369 (3.1463)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.274 (0.142)	Data 1.78e-04 (3.09e-04)	Tok/s 107067 (100685)	Loss/tok 3.5049 (3.1468)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.107 (0.142)	Data 1.67e-04 (3.08e-04)	Tok/s 94734 (100688)	Loss/tok 2.9771 (3.1466)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.159 (0.142)	Data 1.42e-04 (3.07e-04)	Tok/s 105447 (100680)	Loss/tok 3.0141 (3.1461)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.059 (0.142)	Data 1.88e-04 (3.06e-04)	Tok/s 86306 (100679)	Loss/tok 2.5327 (3.1461)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.059 (0.142)	Data 2.48e-04 (3.06e-04)	Tok/s 88834 (100673)	Loss/tok 2.4913 (3.1459)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020283895, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020283895, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.662 (0.662)	Decoder iters 115.0 (115.0)	Tok/s 25016 (25016)
0: Running moses detokenizer
0: BLEU(score=24.17953649381891, counts=[37112, 18705, 10670, 6335], totals=[65465, 62462, 59459, 56461], precisions=[56.68983426258306, 29.946207294034775, 17.945138666980608, 11.22013425196153], bp=1.0, sys_len=65465, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020285700, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2418, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020285701, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1483	Test BLEU: 24.18
0: Performance: Epoch: 3	Training: 805324 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020285701, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020285702, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:38:10 AM
RESULT,RNN_TRANSLATOR,,1130,nvidia,2020-06-24 10:19:20 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
slurmstepd: error: _is_a_lwp: open() /proc/49942/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
ENDING TIMING RUN AT 2020-06-24 10:38:11 AM
RESULT,RNN_TRANSLATOR,,1131,nvidia,2020-06-24 10:19:20 AM
