+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
srun: Job 467815 step creation temporarily disabled, retrying
srun: Step created for job 467815
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019119809, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019119845, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019119845, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019119845, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019119845, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: Job 467815 step creation temporarily disabled, retrying
srun: Step created for job 467815
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-772
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019126254, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177093/results:/results ./run_and_time.sh
srun: Job 467815 step creation temporarily disabled, retrying
srun: Step created for job 467815
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=256
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=128
+ LR=2.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ REMAIN_STEPS=6453
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ LR=2.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ '[' -n 4 ']'
+ TARGET=24.0
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:18:48 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019130644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019130649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3072121201
:::MLLOG {"namespace": "", "time_ms": 1593019136626, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3072121201, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3650668135
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019144589, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019144589, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019144590, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019144590, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019144590, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019146209, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019146209, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019146210, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019146479, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019146480, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019146481, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019146481, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019146482, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019146482, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019146482, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019146482, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019146482, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019146482, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019146483, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019146483, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3181709195
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.459 (0.459)	Data 3.01e-01 (3.01e-01)	Tok/s 36424 (36424)	Loss/tok 10.6828 (10.6828)	LR 2.047e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/1938]	Time 0.104 (0.153)	Data 1.45e-04 (2.75e-02)	Tok/s 99245 (94154)	Loss/tok 9.6090 (10.1185)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.104 (0.140)	Data 1.56e-04 (1.45e-02)	Tok/s 99421 (97745)	Loss/tok 9.1912 (9.7870)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.104 (0.140)	Data 1.61e-04 (9.89e-03)	Tok/s 98735 (100378)	Loss/tok 8.9280 (9.5545)	LR 4.083e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1938]	Time 0.206 (0.140)	Data 2.42e-04 (7.52e-03)	Tok/s 110805 (100890)	Loss/tok 8.9861 (9.4718)	LR 4.909e-05
0: TRAIN [0][50/1938]	Time 0.104 (0.139)	Data 2.54e-04 (6.08e-03)	Tok/s 97760 (100935)	Loss/tok 8.5486 (9.3345)	LR 6.181e-05
0: TRAIN [0][60/1938]	Time 0.206 (0.137)	Data 1.69e-04 (5.11e-03)	Tok/s 114293 (101231)	Loss/tok 8.5124 (9.1975)	LR 7.781e-05
0: TRAIN [0][70/1938]	Time 0.105 (0.138)	Data 1.74e-04 (4.42e-03)	Tok/s 98583 (101721)	Loss/tok 8.0933 (9.0630)	LR 9.796e-05
0: TRAIN [0][80/1938]	Time 0.058 (0.139)	Data 1.55e-04 (3.89e-03)	Tok/s 91222 (102104)	Loss/tok 7.7520 (8.9387)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.154 (0.136)	Data 1.52e-04 (3.48e-03)	Tok/s 108923 (101914)	Loss/tok 8.0641 (8.8564)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.105 (0.136)	Data 1.65e-04 (3.15e-03)	Tok/s 98216 (101917)	Loss/tok 7.7299 (8.7695)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.155 (0.137)	Data 1.70e-04 (2.89e-03)	Tok/s 108287 (102205)	Loss/tok 7.9243 (8.6889)	LR 2.461e-04
0: TRAIN [0][120/1938]	Time 0.104 (0.136)	Data 1.96e-04 (2.66e-03)	Tok/s 98220 (102183)	Loss/tok 7.8107 (8.6272)	LR 3.098e-04
0: TRAIN [0][130/1938]	Time 0.059 (0.134)	Data 1.62e-04 (2.47e-03)	Tok/s 91453 (101978)	Loss/tok 7.0868 (8.5735)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.104 (0.136)	Data 2.39e-04 (2.31e-03)	Tok/s 100617 (102236)	Loss/tok 7.6110 (8.5185)	LR 4.909e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][150/1938]	Time 0.208 (0.137)	Data 1.78e-04 (2.17e-03)	Tok/s 113044 (102392)	Loss/tok 7.9828 (8.4670)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.207 (0.137)	Data 1.64e-04 (2.04e-03)	Tok/s 112763 (102509)	Loss/tok 7.7147 (8.4165)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.105 (0.137)	Data 1.63e-04 (1.94e-03)	Tok/s 98522 (102602)	Loss/tok 7.3854 (8.3673)	LR 9.573e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][180/1938]	Time 0.156 (0.136)	Data 1.88e-04 (1.84e-03)	Tok/s 107170 (102404)	Loss/tok 7.5426 (8.3275)	LR 1.178e-03
0: TRAIN [0][190/1938]	Time 0.268 (0.137)	Data 1.72e-04 (1.75e-03)	Tok/s 110333 (102539)	Loss/tok 7.4722 (8.2701)	LR 1.483e-03
0: TRAIN [0][200/1938]	Time 0.266 (0.138)	Data 1.55e-04 (1.67e-03)	Tok/s 111222 (102554)	Loss/tok 7.3182 (8.2124)	LR 1.867e-03
0: TRAIN [0][210/1938]	Time 0.059 (0.137)	Data 2.42e-04 (1.60e-03)	Tok/s 90096 (102468)	Loss/tok 6.1395 (8.1620)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.207 (0.136)	Data 2.06e-04 (1.54e-03)	Tok/s 111662 (102400)	Loss/tok 7.2341 (8.1097)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.208 (0.137)	Data 1.78e-04 (1.48e-03)	Tok/s 112293 (102528)	Loss/tok 6.8955 (8.0453)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.156 (0.137)	Data 1.39e-04 (1.43e-03)	Tok/s 106933 (102548)	Loss/tok 6.6632 (7.9844)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.105 (0.137)	Data 1.42e-04 (1.38e-03)	Tok/s 98818 (102539)	Loss/tok 6.2571 (7.9230)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.156 (0.138)	Data 2.23e-04 (1.33e-03)	Tok/s 108272 (102618)	Loss/tok 6.2386 (7.8567)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.058 (0.137)	Data 1.44e-04 (1.29e-03)	Tok/s 89611 (102503)	Loss/tok 5.0259 (7.8025)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.208 (0.138)	Data 2.29e-04 (1.25e-03)	Tok/s 111855 (102698)	Loss/tok 6.2189 (7.7299)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.106 (0.138)	Data 1.19e-04 (1.21e-03)	Tok/s 97160 (102662)	Loss/tok 5.6459 (7.6743)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.059 (0.138)	Data 2.59e-04 (1.18e-03)	Tok/s 90586 (102654)	Loss/tok 4.7528 (7.6152)	LR 2.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][310/1938]	Time 0.106 (0.139)	Data 1.53e-04 (1.14e-03)	Tok/s 97234 (102695)	Loss/tok 5.5588 (7.5557)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.058 (0.138)	Data 1.80e-04 (1.11e-03)	Tok/s 91752 (102635)	Loss/tok 4.6136 (7.5032)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.106 (0.138)	Data 1.57e-04 (1.09e-03)	Tok/s 95406 (102559)	Loss/tok 5.3163 (7.4533)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.105 (0.138)	Data 1.78e-04 (1.06e-03)	Tok/s 98072 (102616)	Loss/tok 5.2659 (7.3943)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.267 (0.138)	Data 1.82e-04 (1.03e-03)	Tok/s 112374 (102602)	Loss/tok 5.9132 (7.3392)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.106 (0.139)	Data 1.89e-04 (1.01e-03)	Tok/s 97966 (102700)	Loss/tok 5.1419 (7.2797)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.106 (0.139)	Data 1.50e-04 (9.88e-04)	Tok/s 97749 (102752)	Loss/tok 5.1580 (7.2216)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.268 (0.139)	Data 1.39e-04 (9.67e-04)	Tok/s 111651 (102662)	Loss/tok 5.6503 (7.1750)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.105 (0.140)	Data 1.40e-04 (9.46e-04)	Tok/s 99313 (102718)	Loss/tok 4.8743 (7.1147)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.158 (0.139)	Data 1.60e-04 (9.27e-04)	Tok/s 106508 (102612)	Loss/tok 5.2356 (7.0685)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.157 (0.139)	Data 1.37e-04 (9.08e-04)	Tok/s 107444 (102578)	Loss/tok 5.0692 (7.0213)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.210 (0.139)	Data 1.38e-04 (8.90e-04)	Tok/s 111159 (102631)	Loss/tok 5.1757 (6.9668)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][430/1938]	Time 0.157 (0.140)	Data 1.65e-04 (8.73e-04)	Tok/s 106091 (102651)	Loss/tok 4.8358 (6.9152)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.105 (0.139)	Data 2.46e-04 (8.58e-04)	Tok/s 101344 (102619)	Loss/tok 4.4220 (6.8693)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.106 (0.139)	Data 2.15e-04 (8.42e-04)	Tok/s 97677 (102516)	Loss/tok 4.4159 (6.8296)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.269 (0.139)	Data 1.44e-04 (8.28e-04)	Tok/s 111763 (102551)	Loss/tok 5.1778 (6.7766)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.106 (0.139)	Data 2.09e-04 (8.14e-04)	Tok/s 98146 (102464)	Loss/tok 4.3242 (6.7392)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.158 (0.139)	Data 1.49e-04 (8.01e-04)	Tok/s 106087 (102501)	Loss/tok 4.6704 (6.6907)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.158 (0.139)	Data 1.40e-04 (7.88e-04)	Tok/s 105729 (102440)	Loss/tok 4.6798 (6.6532)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.158 (0.139)	Data 1.37e-04 (7.75e-04)	Tok/s 107001 (102449)	Loss/tok 4.5032 (6.6097)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.157 (0.139)	Data 2.01e-04 (7.63e-04)	Tok/s 106610 (102457)	Loss/tok 4.4817 (6.5676)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.210 (0.139)	Data 2.35e-04 (7.52e-04)	Tok/s 110023 (102452)	Loss/tok 4.8668 (6.5271)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.210 (0.139)	Data 1.73e-04 (7.41e-04)	Tok/s 110631 (102422)	Loss/tok 4.6071 (6.4888)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.158 (0.139)	Data 1.99e-04 (7.30e-04)	Tok/s 107231 (102412)	Loss/tok 4.5536 (6.4511)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.212 (0.139)	Data 1.46e-04 (7.20e-04)	Tok/s 107295 (102403)	Loss/tok 4.8291 (6.4136)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][560/1938]	Time 0.210 (0.139)	Data 1.40e-04 (7.10e-04)	Tok/s 110163 (102449)	Loss/tok 4.4615 (6.3706)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.212 (0.139)	Data 1.63e-04 (7.00e-04)	Tok/s 108602 (102478)	Loss/tok 4.6293 (6.3310)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.158 (0.139)	Data 2.41e-04 (6.92e-04)	Tok/s 105959 (102471)	Loss/tok 4.3198 (6.2952)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.106 (0.140)	Data 1.40e-04 (6.82e-04)	Tok/s 97549 (102496)	Loss/tok 4.0402 (6.2574)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.158 (0.140)	Data 2.21e-04 (6.74e-04)	Tok/s 107781 (102497)	Loss/tok 4.2923 (6.2227)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.059 (0.139)	Data 1.63e-04 (6.66e-04)	Tok/s 89570 (102464)	Loss/tok 3.2541 (6.1918)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.107 (0.139)	Data 1.41e-04 (6.58e-04)	Tok/s 97012 (102430)	Loss/tok 3.8854 (6.1619)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.271 (0.139)	Data 1.60e-04 (6.50e-04)	Tok/s 107679 (102440)	Loss/tok 4.7656 (6.1282)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.271 (0.140)	Data 1.77e-04 (6.42e-04)	Tok/s 109178 (102430)	Loss/tok 4.7482 (6.0975)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.107 (0.140)	Data 1.79e-04 (6.35e-04)	Tok/s 95688 (102385)	Loss/tok 3.8861 (6.0708)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.158 (0.139)	Data 1.36e-04 (6.28e-04)	Tok/s 107014 (102364)	Loss/tok 4.0501 (6.0424)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.212 (0.140)	Data 1.42e-04 (6.21e-04)	Tok/s 110422 (102397)	Loss/tok 4.2536 (6.0100)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.106 (0.140)	Data 1.42e-04 (6.15e-04)	Tok/s 95585 (102420)	Loss/tok 3.7877 (5.9785)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][690/1938]	Time 0.212 (0.140)	Data 1.62e-04 (6.08e-04)	Tok/s 108820 (102408)	Loss/tok 4.4445 (5.9508)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.160 (0.140)	Data 2.03e-04 (6.02e-04)	Tok/s 105267 (102370)	Loss/tok 4.1214 (5.9260)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.159 (0.140)	Data 2.54e-04 (5.96e-04)	Tok/s 107295 (102327)	Loss/tok 4.1026 (5.9017)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.108 (0.140)	Data 1.86e-04 (5.90e-04)	Tok/s 93059 (102279)	Loss/tok 3.9240 (5.8773)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.214 (0.140)	Data 1.37e-04 (5.85e-04)	Tok/s 108299 (102236)	Loss/tok 4.2911 (5.8526)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.060 (0.140)	Data 2.77e-04 (5.79e-04)	Tok/s 90119 (102203)	Loss/tok 3.1593 (5.8287)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.160 (0.140)	Data 1.65e-04 (5.74e-04)	Tok/s 105005 (102180)	Loss/tok 4.0125 (5.8041)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.109 (0.140)	Data 1.83e-04 (5.69e-04)	Tok/s 96875 (102178)	Loss/tok 3.7677 (5.7786)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.108 (0.140)	Data 2.40e-04 (5.64e-04)	Tok/s 96001 (102166)	Loss/tok 3.7167 (5.7546)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.159 (0.140)	Data 1.68e-04 (5.59e-04)	Tok/s 107584 (102112)	Loss/tok 4.0333 (5.7349)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.109 (0.140)	Data 1.65e-04 (5.54e-04)	Tok/s 93926 (102090)	Loss/tok 3.6896 (5.7119)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.060 (0.140)	Data 1.67e-04 (5.50e-04)	Tok/s 87598 (102082)	Loss/tok 3.2458 (5.6885)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][810/1938]	Time 0.161 (0.140)	Data 1.95e-04 (5.45e-04)	Tok/s 102714 (102068)	Loss/tok 3.9878 (5.6660)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.214 (0.141)	Data 2.31e-04 (5.41e-04)	Tok/s 108825 (102058)	Loss/tok 4.1375 (5.6438)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.060 (0.140)	Data 1.84e-04 (5.36e-04)	Tok/s 87603 (102018)	Loss/tok 3.2036 (5.6247)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.161 (0.140)	Data 1.72e-04 (5.32e-04)	Tok/s 104927 (101979)	Loss/tok 3.9952 (5.6054)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.108 (0.141)	Data 1.57e-04 (5.28e-04)	Tok/s 93344 (101960)	Loss/tok 3.6905 (5.5854)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.109 (0.141)	Data 1.61e-04 (5.24e-04)	Tok/s 94167 (101932)	Loss/tok 3.7054 (5.5665)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.274 (0.141)	Data 1.74e-04 (5.20e-04)	Tok/s 109295 (101922)	Loss/tok 4.4999 (5.5470)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.160 (0.140)	Data 2.27e-04 (5.17e-04)	Tok/s 104971 (101887)	Loss/tok 3.8449 (5.5298)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.160 (0.141)	Data 1.60e-04 (5.13e-04)	Tok/s 105297 (101888)	Loss/tok 3.9531 (5.5107)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.108 (0.141)	Data 2.25e-04 (5.09e-04)	Tok/s 96283 (101894)	Loss/tok 3.6913 (5.4909)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.109 (0.141)	Data 1.71e-04 (5.06e-04)	Tok/s 96716 (101906)	Loss/tok 3.7022 (5.4709)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.109 (0.141)	Data 1.65e-04 (5.03e-04)	Tok/s 97750 (101891)	Loss/tok 3.5486 (5.4534)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.160 (0.141)	Data 1.98e-04 (4.99e-04)	Tok/s 103554 (101849)	Loss/tok 3.7827 (5.4376)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][940/1938]	Time 0.161 (0.141)	Data 1.43e-04 (4.96e-04)	Tok/s 103722 (101857)	Loss/tok 3.7286 (5.4197)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.160 (0.141)	Data 2.26e-04 (4.92e-04)	Tok/s 104390 (101833)	Loss/tok 3.8385 (5.4041)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.214 (0.141)	Data 1.63e-04 (4.89e-04)	Tok/s 109213 (101850)	Loss/tok 3.9579 (5.3860)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.109 (0.141)	Data 2.37e-04 (4.86e-04)	Tok/s 95518 (101793)	Loss/tok 3.6110 (5.3725)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.215 (0.142)	Data 1.74e-04 (4.83e-04)	Tok/s 110155 (101828)	Loss/tok 3.9695 (5.3530)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.273 (0.142)	Data 1.86e-04 (4.80e-04)	Tok/s 108069 (101840)	Loss/tok 4.2101 (5.3355)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.108 (0.142)	Data 1.71e-04 (4.77e-04)	Tok/s 96369 (101825)	Loss/tok 3.6720 (5.3205)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.109 (0.142)	Data 1.75e-04 (4.74e-04)	Tok/s 94608 (101797)	Loss/tok 3.6108 (5.3064)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.160 (0.142)	Data 1.63e-04 (4.71e-04)	Tok/s 105079 (101790)	Loss/tok 3.8125 (5.2911)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.060 (0.142)	Data 1.83e-04 (4.68e-04)	Tok/s 87956 (101779)	Loss/tok 3.0731 (5.2766)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.060 (0.142)	Data 1.82e-04 (4.66e-04)	Tok/s 86191 (101750)	Loss/tok 3.0239 (5.2631)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.109 (0.142)	Data 1.40e-04 (4.63e-04)	Tok/s 94759 (101715)	Loss/tok 3.4839 (5.2499)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.108 (0.142)	Data 1.39e-04 (4.60e-04)	Tok/s 94167 (101669)	Loss/tok 3.5194 (5.2375)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1070/1938]	Time 0.160 (0.142)	Data 2.46e-04 (4.58e-04)	Tok/s 106765 (101642)	Loss/tok 3.7504 (5.2250)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.215 (0.142)	Data 1.95e-04 (4.55e-04)	Tok/s 108557 (101622)	Loss/tok 3.9483 (5.2114)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.108 (0.142)	Data 1.59e-04 (4.53e-04)	Tok/s 94472 (101611)	Loss/tok 3.3573 (5.1973)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.108 (0.142)	Data 1.97e-04 (4.50e-04)	Tok/s 96016 (101601)	Loss/tok 3.4910 (5.1836)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.215 (0.142)	Data 1.44e-04 (4.48e-04)	Tok/s 107584 (101540)	Loss/tok 4.0077 (5.1732)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.109 (0.142)	Data 1.62e-04 (4.45e-04)	Tok/s 92324 (101508)	Loss/tok 3.4882 (5.1618)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.108 (0.141)	Data 2.24e-04 (4.43e-04)	Tok/s 94406 (101468)	Loss/tok 3.4033 (5.1507)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.273 (0.142)	Data 1.35e-04 (4.41e-04)	Tok/s 109598 (101476)	Loss/tok 4.0740 (5.1369)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.274 (0.142)	Data 1.40e-04 (4.38e-04)	Tok/s 108353 (101469)	Loss/tok 4.0336 (5.1243)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.110 (0.142)	Data 1.58e-04 (4.36e-04)	Tok/s 94043 (101426)	Loss/tok 3.4541 (5.1141)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.215 (0.142)	Data 1.25e-04 (4.33e-04)	Tok/s 109177 (101447)	Loss/tok 4.0048 (5.1004)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.109 (0.142)	Data 1.20e-04 (4.31e-04)	Tok/s 96210 (101414)	Loss/tok 3.5185 (5.0897)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.215 (0.142)	Data 1.76e-04 (4.28e-04)	Tok/s 109408 (101411)	Loss/tok 3.9148 (5.0778)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1200/1938]	Time 0.060 (0.142)	Data 1.03e-04 (4.26e-04)	Tok/s 89793 (101378)	Loss/tok 2.9248 (5.0675)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.060 (0.142)	Data 1.48e-04 (4.24e-04)	Tok/s 89149 (101370)	Loss/tok 2.8956 (5.0559)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.161 (0.142)	Data 1.36e-04 (4.21e-04)	Tok/s 104573 (101366)	Loss/tok 3.7298 (5.0444)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.214 (0.142)	Data 1.31e-04 (4.19e-04)	Tok/s 108058 (101374)	Loss/tok 3.9799 (5.0328)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.161 (0.142)	Data 1.16e-04 (4.17e-04)	Tok/s 103782 (101359)	Loss/tok 3.7061 (5.0223)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.109 (0.142)	Data 1.33e-04 (4.15e-04)	Tok/s 95429 (101345)	Loss/tok 3.3686 (5.0117)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.161 (0.142)	Data 1.27e-04 (4.13e-04)	Tok/s 104585 (101342)	Loss/tok 3.5775 (5.0010)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.214 (0.142)	Data 1.39e-04 (4.11e-04)	Tok/s 108741 (101349)	Loss/tok 3.8583 (4.9898)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.110 (0.142)	Data 1.91e-04 (4.09e-04)	Tok/s 94098 (101291)	Loss/tok 3.3945 (4.9818)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.108 (0.142)	Data 1.47e-04 (4.07e-04)	Tok/s 94899 (101257)	Loss/tok 3.3378 (4.9730)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.109 (0.142)	Data 1.45e-04 (4.05e-04)	Tok/s 96026 (101240)	Loss/tok 3.5346 (4.9634)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.060 (0.141)	Data 1.58e-04 (4.03e-04)	Tok/s 88434 (101219)	Loss/tok 2.9171 (4.9543)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.109 (0.141)	Data 1.42e-04 (4.01e-04)	Tok/s 93530 (101180)	Loss/tok 3.4618 (4.9462)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1330/1938]	Time 0.109 (0.141)	Data 1.51e-04 (3.99e-04)	Tok/s 97051 (101162)	Loss/tok 3.4022 (4.9372)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.275 (0.142)	Data 1.47e-04 (3.97e-04)	Tok/s 108676 (101181)	Loss/tok 4.0086 (4.9263)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.109 (0.141)	Data 1.62e-04 (3.96e-04)	Tok/s 94367 (101165)	Loss/tok 3.4808 (4.9174)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1360/1938]	Time 0.161 (0.142)	Data 1.34e-04 (3.94e-04)	Tok/s 104120 (101196)	Loss/tok 3.6628 (4.9061)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.214 (0.142)	Data 1.19e-04 (3.92e-04)	Tok/s 108616 (101199)	Loss/tok 3.9101 (4.8964)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.060 (0.142)	Data 1.45e-04 (3.90e-04)	Tok/s 86436 (101152)	Loss/tok 3.0225 (4.8892)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.060 (0.142)	Data 1.52e-04 (3.89e-04)	Tok/s 90722 (101143)	Loss/tok 2.9124 (4.8807)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.161 (0.142)	Data 1.44e-04 (3.87e-04)	Tok/s 102874 (101147)	Loss/tok 3.6187 (4.8713)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.109 (0.142)	Data 1.38e-04 (3.85e-04)	Tok/s 93454 (101152)	Loss/tok 3.5101 (4.8617)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.160 (0.142)	Data 1.49e-04 (3.84e-04)	Tok/s 105609 (101130)	Loss/tok 3.5675 (4.8537)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.109 (0.142)	Data 1.22e-04 (3.82e-04)	Tok/s 93972 (101121)	Loss/tok 3.4954 (4.8452)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.108 (0.142)	Data 1.36e-04 (3.80e-04)	Tok/s 96705 (101121)	Loss/tok 3.2939 (4.8363)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.109 (0.142)	Data 1.41e-04 (3.79e-04)	Tok/s 93128 (101102)	Loss/tok 3.3782 (4.8286)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.109 (0.142)	Data 1.53e-04 (3.77e-04)	Tok/s 97396 (101076)	Loss/tok 3.3455 (4.8213)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.163 (0.142)	Data 1.79e-04 (3.75e-04)	Tok/s 103359 (101059)	Loss/tok 3.5908 (4.8135)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.109 (0.142)	Data 1.34e-04 (3.74e-04)	Tok/s 94023 (101047)	Loss/tok 3.3496 (4.8054)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1490/1938]	Time 0.275 (0.142)	Data 1.18e-04 (3.72e-04)	Tok/s 109074 (101024)	Loss/tok 3.8554 (4.7977)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.160 (0.142)	Data 2.15e-04 (3.71e-04)	Tok/s 104300 (101021)	Loss/tok 3.7150 (4.7898)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.161 (0.142)	Data 1.79e-04 (3.70e-04)	Tok/s 104481 (101027)	Loss/tok 3.7024 (4.7812)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.214 (0.142)	Data 1.19e-04 (3.68e-04)	Tok/s 110220 (101023)	Loss/tok 3.7468 (4.7735)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.109 (0.142)	Data 2.30e-04 (3.67e-04)	Tok/s 94886 (100989)	Loss/tok 3.3490 (4.7669)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.161 (0.142)	Data 1.25e-04 (3.66e-04)	Tok/s 104200 (100998)	Loss/tok 3.5993 (4.7587)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.060 (0.142)	Data 1.71e-04 (3.64e-04)	Tok/s 86881 (100973)	Loss/tok 2.8112 (4.7519)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.109 (0.141)	Data 2.31e-04 (3.63e-04)	Tok/s 94113 (100936)	Loss/tok 3.4027 (4.7458)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.162 (0.141)	Data 1.55e-04 (3.62e-04)	Tok/s 104607 (100920)	Loss/tok 3.5967 (4.7391)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.162 (0.141)	Data 1.39e-04 (3.61e-04)	Tok/s 103596 (100910)	Loss/tok 3.5618 (4.7319)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.160 (0.141)	Data 2.17e-04 (3.59e-04)	Tok/s 104866 (100911)	Loss/tok 3.6134 (4.7247)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.160 (0.141)	Data 1.81e-04 (3.58e-04)	Tok/s 105175 (100914)	Loss/tok 3.6314 (4.7171)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.060 (0.141)	Data 1.38e-04 (3.57e-04)	Tok/s 89538 (100898)	Loss/tok 2.8846 (4.7101)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [0][1620/1938]	Time 0.109 (0.141)	Data 1.68e-04 (3.56e-04)	Tok/s 94139 (100878)	Loss/tok 3.4241 (4.7037)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1630/1938]	Time 0.110 (0.141)	Data 1.65e-04 (3.54e-04)	Tok/s 92754 (100863)	Loss/tok 3.4448 (4.6971)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.215 (0.141)	Data 1.79e-04 (3.53e-04)	Tok/s 108274 (100853)	Loss/tok 3.7027 (4.6903)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.109 (0.141)	Data 1.42e-04 (3.52e-04)	Tok/s 95543 (100833)	Loss/tok 3.3957 (4.6840)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.110 (0.141)	Data 2.29e-04 (3.51e-04)	Tok/s 95617 (100834)	Loss/tok 3.3404 (4.6772)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.162 (0.141)	Data 1.40e-04 (3.50e-04)	Tok/s 104249 (100822)	Loss/tok 3.7543 (4.6709)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.109 (0.141)	Data 1.96e-04 (3.49e-04)	Tok/s 94804 (100783)	Loss/tok 3.4421 (4.6656)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.110 (0.141)	Data 1.47e-04 (3.48e-04)	Tok/s 92772 (100783)	Loss/tok 3.2983 (4.6591)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.060 (0.141)	Data 2.16e-04 (3.47e-04)	Tok/s 89183 (100753)	Loss/tok 2.8161 (4.6533)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.110 (0.141)	Data 2.03e-04 (3.46e-04)	Tok/s 93105 (100758)	Loss/tok 3.3989 (4.6466)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.109 (0.141)	Data 1.67e-04 (3.45e-04)	Tok/s 94659 (100768)	Loss/tok 3.3151 (4.6395)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.109 (0.141)	Data 1.38e-04 (3.44e-04)	Tok/s 94997 (100757)	Loss/tok 3.3809 (4.6335)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.162 (0.141)	Data 1.86e-04 (3.43e-04)	Tok/s 103850 (100747)	Loss/tok 3.5299 (4.6275)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.060 (0.141)	Data 1.74e-04 (3.42e-04)	Tok/s 87703 (100741)	Loss/tok 2.8706 (4.6218)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1760/1938]	Time 0.109 (0.141)	Data 1.40e-04 (3.41e-04)	Tok/s 94638 (100729)	Loss/tok 3.2202 (4.6160)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1770/1938]	Time 0.213 (0.141)	Data 1.49e-04 (3.40e-04)	Tok/s 109588 (100746)	Loss/tok 3.7508 (4.6092)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.110 (0.141)	Data 1.93e-04 (3.39e-04)	Tok/s 94749 (100753)	Loss/tok 3.2505 (4.6027)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.109 (0.141)	Data 2.37e-04 (3.38e-04)	Tok/s 95477 (100745)	Loss/tok 3.3352 (4.5969)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.110 (0.141)	Data 2.03e-04 (3.37e-04)	Tok/s 93878 (100739)	Loss/tok 3.4507 (4.5910)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.109 (0.141)	Data 1.52e-04 (3.36e-04)	Tok/s 95551 (100741)	Loss/tok 3.2113 (4.5850)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.162 (0.141)	Data 1.44e-04 (3.35e-04)	Tok/s 104195 (100739)	Loss/tok 3.4658 (4.5792)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.161 (0.141)	Data 1.20e-04 (3.34e-04)	Tok/s 105408 (100729)	Loss/tok 3.5570 (4.5736)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.161 (0.141)	Data 1.18e-04 (3.33e-04)	Tok/s 103379 (100731)	Loss/tok 3.4431 (4.5676)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.162 (0.141)	Data 1.74e-04 (3.32e-04)	Tok/s 103341 (100728)	Loss/tok 3.6335 (4.5619)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.161 (0.142)	Data 1.36e-04 (3.31e-04)	Tok/s 104802 (100726)	Loss/tok 3.4551 (4.5561)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.109 (0.142)	Data 1.33e-04 (3.30e-04)	Tok/s 92404 (100719)	Loss/tok 3.2008 (4.5507)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.110 (0.142)	Data 1.19e-04 (3.29e-04)	Tok/s 95310 (100710)	Loss/tok 3.3014 (4.5452)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.109 (0.142)	Data 1.43e-04 (3.29e-04)	Tok/s 93606 (100718)	Loss/tok 3.2703 (4.5394)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1900/1938]	Time 0.162 (0.142)	Data 1.85e-04 (3.28e-04)	Tok/s 104093 (100722)	Loss/tok 3.5001 (4.5336)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.215 (0.142)	Data 1.46e-04 (3.27e-04)	Tok/s 108944 (100713)	Loss/tok 3.6360 (4.5283)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.162 (0.142)	Data 1.55e-04 (3.26e-04)	Tok/s 104015 (100731)	Loss/tok 3.4461 (4.5221)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.213 (0.142)	Data 2.09e-04 (3.25e-04)	Tok/s 108971 (100725)	Loss/tok 3.7448 (4.5172)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
:::MLLOG {"namespace": "", "time_ms": 1593019422137, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019422137, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.763 (0.763)	Decoder iters 149.0 (149.0)	Tok/s 21404 (21404)
0: Running moses detokenizer
0: BLEU(score=20.25227117424916, counts=[34705, 15977, 8544, 4741], totals=[65044, 62041, 59038, 56041], precisions=[53.3561896562327, 25.752325075353394, 14.472034960533893, 8.459877589621884], bp=1.0, sys_len=65044, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019424061, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2025, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019424062, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.5118	Test BLEU: 20.25
0: Performance: Epoch: 0	Training: 805361 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019424062, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019424062, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019424063, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 894807296
0: TRAIN [1][0/1938]	Time 0.419 (0.419)	Data 2.62e-01 (2.62e-01)	Tok/s 40471 (40471)	Loss/tok 3.5494 (3.5494)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.160 (0.155)	Data 1.79e-04 (2.40e-02)	Tok/s 106428 (94122)	Loss/tok 3.3009 (3.3728)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.108 (0.152)	Data 1.41e-04 (1.26e-02)	Tok/s 94706 (98229)	Loss/tok 3.1964 (3.4278)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.108 (0.147)	Data 1.44e-04 (8.61e-03)	Tok/s 94526 (98446)	Loss/tok 3.3168 (3.4471)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.107 (0.147)	Data 1.86e-04 (6.56e-03)	Tok/s 95791 (99223)	Loss/tok 3.1148 (3.4455)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.108 (0.145)	Data 1.56e-04 (5.31e-03)	Tok/s 97660 (99556)	Loss/tok 3.2030 (3.4320)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.159 (0.147)	Data 1.80e-04 (4.47e-03)	Tok/s 107029 (99933)	Loss/tok 3.4618 (3.4475)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.159 (0.144)	Data 1.76e-04 (3.87e-03)	Tok/s 104745 (99796)	Loss/tok 3.4058 (3.4371)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.107 (0.144)	Data 2.32e-04 (3.41e-03)	Tok/s 95984 (100042)	Loss/tok 3.1704 (3.4334)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.273 (0.142)	Data 1.93e-04 (3.06e-03)	Tok/s 110074 (99811)	Loss/tok 3.7706 (3.4287)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.107 (0.141)	Data 2.16e-04 (2.77e-03)	Tok/s 95377 (99714)	Loss/tok 3.2149 (3.4276)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.107 (0.140)	Data 1.33e-04 (2.54e-03)	Tok/s 98640 (99666)	Loss/tok 3.1926 (3.4298)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.107 (0.140)	Data 2.11e-04 (2.34e-03)	Tok/s 94518 (99740)	Loss/tok 3.2284 (3.4294)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][130/1938]	Time 0.212 (0.139)	Data 2.59e-04 (2.18e-03)	Tok/s 109927 (99613)	Loss/tok 3.6752 (3.4278)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.108 (0.139)	Data 1.43e-04 (2.04e-03)	Tok/s 98822 (99654)	Loss/tok 3.3180 (3.4310)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.213 (0.140)	Data 2.25e-04 (1.91e-03)	Tok/s 110206 (99889)	Loss/tok 3.6044 (3.4362)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.107 (0.141)	Data 2.01e-04 (1.81e-03)	Tok/s 96727 (100074)	Loss/tok 3.2789 (3.4406)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.160 (0.141)	Data 1.67e-04 (1.71e-03)	Tok/s 104030 (100114)	Loss/tok 3.4528 (3.4377)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.213 (0.143)	Data 1.82e-04 (1.63e-03)	Tok/s 109217 (100254)	Loss/tok 3.7199 (3.4465)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.060 (0.142)	Data 2.34e-04 (1.55e-03)	Tok/s 89243 (100247)	Loss/tok 2.8013 (3.4452)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.211 (0.143)	Data 1.79e-04 (1.48e-03)	Tok/s 110097 (100342)	Loss/tok 3.6440 (3.4484)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.107 (0.144)	Data 1.43e-04 (1.42e-03)	Tok/s 97801 (100460)	Loss/tok 3.3086 (3.4513)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.108 (0.143)	Data 1.78e-04 (1.36e-03)	Tok/s 96448 (100412)	Loss/tok 3.1878 (3.4486)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.108 (0.143)	Data 1.50e-04 (1.31e-03)	Tok/s 95751 (100369)	Loss/tok 3.2212 (3.4467)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.108 (0.143)	Data 1.61e-04 (1.26e-03)	Tok/s 96426 (100402)	Loss/tok 3.2596 (3.4456)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][250/1938]	Time 0.213 (0.143)	Data 1.82e-04 (1.22e-03)	Tok/s 108528 (100462)	Loss/tok 3.7494 (3.4505)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][260/1938]	Time 0.160 (0.143)	Data 2.10e-04 (1.18e-03)	Tok/s 105383 (100499)	Loss/tok 3.4300 (3.4530)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.107 (0.143)	Data 2.04e-04 (1.15e-03)	Tok/s 95649 (100425)	Loss/tok 3.1594 (3.4520)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.271 (0.143)	Data 1.58e-04 (1.11e-03)	Tok/s 109688 (100432)	Loss/tok 3.7194 (3.4544)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.214 (0.144)	Data 1.52e-04 (1.08e-03)	Tok/s 110514 (100422)	Loss/tok 3.6046 (3.4557)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.059 (0.143)	Data 2.49e-04 (1.05e-03)	Tok/s 88071 (100360)	Loss/tok 2.7059 (3.4512)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.212 (0.144)	Data 1.23e-04 (1.02e-03)	Tok/s 111322 (100467)	Loss/tok 3.4615 (3.4527)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.108 (0.144)	Data 1.49e-04 (9.94e-04)	Tok/s 95608 (100512)	Loss/tok 3.2641 (3.4554)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.160 (0.144)	Data 2.26e-04 (9.70e-04)	Tok/s 103527 (100485)	Loss/tok 3.4245 (3.4521)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.160 (0.145)	Data 1.54e-04 (9.46e-04)	Tok/s 105703 (100642)	Loss/tok 3.3736 (3.4537)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.108 (0.144)	Data 1.93e-04 (9.24e-04)	Tok/s 93922 (100545)	Loss/tok 3.2837 (3.4517)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.159 (0.144)	Data 2.35e-04 (9.05e-04)	Tok/s 103981 (100540)	Loss/tok 3.4895 (3.4514)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.108 (0.144)	Data 2.87e-04 (8.85e-04)	Tok/s 98040 (100533)	Loss/tok 3.1581 (3.4531)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.214 (0.144)	Data 1.46e-04 (8.67e-04)	Tok/s 108230 (100557)	Loss/tok 3.5860 (3.4528)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][390/1938]	Time 0.160 (0.144)	Data 1.78e-04 (8.49e-04)	Tok/s 104997 (100605)	Loss/tok 3.3944 (3.4524)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.108 (0.144)	Data 1.80e-04 (8.33e-04)	Tok/s 94092 (100646)	Loss/tok 3.1495 (3.4521)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][410/1938]	Time 0.159 (0.144)	Data 1.35e-04 (8.17e-04)	Tok/s 105520 (100675)	Loss/tok 3.3523 (3.4508)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.109 (0.144)	Data 1.29e-04 (8.01e-04)	Tok/s 96479 (100647)	Loss/tok 3.1338 (3.4498)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.110 (0.144)	Data 1.38e-04 (7.86e-04)	Tok/s 95247 (100566)	Loss/tok 3.1909 (3.4473)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.163 (0.144)	Data 1.83e-04 (7.73e-04)	Tok/s 104243 (100589)	Loss/tok 3.3358 (3.4499)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.161 (0.144)	Data 2.30e-04 (7.59e-04)	Tok/s 104527 (100579)	Loss/tok 3.5612 (3.4508)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.161 (0.145)	Data 1.64e-04 (7.47e-04)	Tok/s 103822 (100630)	Loss/tok 3.4981 (3.4522)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.162 (0.145)	Data 1.98e-04 (7.35e-04)	Tok/s 103687 (100636)	Loss/tok 3.4112 (3.4508)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.109 (0.144)	Data 1.52e-04 (7.24e-04)	Tok/s 97314 (100568)	Loss/tok 3.2244 (3.4484)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.161 (0.144)	Data 1.51e-04 (7.12e-04)	Tok/s 104100 (100570)	Loss/tok 3.4714 (3.4478)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.059 (0.144)	Data 1.91e-04 (7.02e-04)	Tok/s 90879 (100492)	Loss/tok 2.7802 (3.4465)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.109 (0.144)	Data 2.32e-04 (6.91e-04)	Tok/s 95576 (100462)	Loss/tok 3.2362 (3.4456)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.109 (0.144)	Data 1.80e-04 (6.81e-04)	Tok/s 94164 (100488)	Loss/tok 3.2550 (3.4453)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.060 (0.144)	Data 1.37e-04 (6.71e-04)	Tok/s 91587 (100478)	Loss/tok 2.7671 (3.4449)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][540/1938]	Time 0.109 (0.144)	Data 1.39e-04 (6.62e-04)	Tok/s 93661 (100463)	Loss/tok 3.1094 (3.4452)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.109 (0.144)	Data 1.80e-04 (6.53e-04)	Tok/s 95326 (100444)	Loss/tok 3.1841 (3.4432)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.110 (0.144)	Data 1.40e-04 (6.45e-04)	Tok/s 95734 (100391)	Loss/tok 3.2062 (3.4415)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.109 (0.144)	Data 1.57e-04 (6.37e-04)	Tok/s 94031 (100382)	Loss/tok 3.2259 (3.4415)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][580/1938]	Time 0.160 (0.144)	Data 1.48e-04 (6.28e-04)	Tok/s 104183 (100364)	Loss/tok 3.3776 (3.4437)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.215 (0.144)	Data 1.24e-04 (6.20e-04)	Tok/s 109360 (100356)	Loss/tok 3.6145 (3.4443)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.110 (0.144)	Data 1.24e-04 (6.12e-04)	Tok/s 95164 (100342)	Loss/tok 3.2349 (3.4437)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.161 (0.144)	Data 1.24e-04 (6.05e-04)	Tok/s 105066 (100335)	Loss/tok 3.4187 (3.4451)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.274 (0.145)	Data 1.23e-04 (5.97e-04)	Tok/s 107510 (100368)	Loss/tok 3.9746 (3.4466)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.109 (0.144)	Data 1.23e-04 (5.90e-04)	Tok/s 93610 (100324)	Loss/tok 3.1038 (3.4447)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.162 (0.144)	Data 1.35e-04 (5.83e-04)	Tok/s 104658 (100303)	Loss/tok 3.3841 (3.4448)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.163 (0.145)	Data 1.25e-04 (5.76e-04)	Tok/s 102747 (100327)	Loss/tok 3.3373 (3.4463)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.060 (0.145)	Data 1.08e-04 (5.69e-04)	Tok/s 88556 (100322)	Loss/tok 2.8207 (3.4475)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.109 (0.145)	Data 1.14e-04 (5.63e-04)	Tok/s 96909 (100315)	Loss/tok 3.2336 (3.4474)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.215 (0.145)	Data 1.27e-04 (5.57e-04)	Tok/s 108503 (100301)	Loss/tok 3.6190 (3.4463)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.109 (0.145)	Data 1.64e-04 (5.51e-04)	Tok/s 93698 (100274)	Loss/tok 3.2232 (3.4457)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.109 (0.145)	Data 1.87e-04 (5.45e-04)	Tok/s 95416 (100278)	Loss/tok 3.2618 (3.4452)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][710/1938]	Time 0.215 (0.145)	Data 1.21e-04 (5.39e-04)	Tok/s 107418 (100272)	Loss/tok 3.5728 (3.4448)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.109 (0.145)	Data 1.30e-04 (5.34e-04)	Tok/s 96332 (100265)	Loss/tok 3.1751 (3.4447)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.213 (0.145)	Data 1.16e-04 (5.28e-04)	Tok/s 109449 (100268)	Loss/tok 3.5705 (3.4458)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.214 (0.145)	Data 1.25e-04 (5.23e-04)	Tok/s 108667 (100290)	Loss/tok 3.5981 (3.4453)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.161 (0.145)	Data 1.30e-04 (5.18e-04)	Tok/s 103197 (100285)	Loss/tok 3.4230 (3.4442)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.109 (0.145)	Data 1.34e-04 (5.13e-04)	Tok/s 93198 (100288)	Loss/tok 3.1239 (3.4443)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.275 (0.145)	Data 1.25e-04 (5.08e-04)	Tok/s 106958 (100254)	Loss/tok 3.8287 (3.4437)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][780/1938]	Time 0.162 (0.145)	Data 1.11e-04 (5.03e-04)	Tok/s 103983 (100271)	Loss/tok 3.3967 (3.4448)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.161 (0.146)	Data 1.54e-04 (4.99e-04)	Tok/s 104948 (100298)	Loss/tok 3.4273 (3.4451)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.215 (0.146)	Data 1.43e-04 (4.95e-04)	Tok/s 107312 (100304)	Loss/tok 3.4893 (3.4451)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.275 (0.146)	Data 1.31e-04 (4.90e-04)	Tok/s 107895 (100304)	Loss/tok 3.8218 (3.4462)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.060 (0.146)	Data 1.28e-04 (4.86e-04)	Tok/s 90117 (100331)	Loss/tok 2.6248 (3.4471)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.110 (0.146)	Data 1.23e-04 (4.82e-04)	Tok/s 96100 (100347)	Loss/tok 3.1749 (3.4467)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.215 (0.146)	Data 1.40e-04 (4.78e-04)	Tok/s 109047 (100342)	Loss/tok 3.5126 (3.4462)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.215 (0.147)	Data 1.26e-04 (4.74e-04)	Tok/s 108411 (100342)	Loss/tok 3.5590 (3.4468)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.110 (0.146)	Data 1.24e-04 (4.70e-04)	Tok/s 94700 (100314)	Loss/tok 3.1376 (3.4457)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.162 (0.146)	Data 1.39e-04 (4.66e-04)	Tok/s 102679 (100274)	Loss/tok 3.4421 (3.4443)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.274 (0.146)	Data 1.24e-04 (4.62e-04)	Tok/s 108608 (100252)	Loss/tok 3.6546 (3.4435)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.109 (0.146)	Data 1.49e-04 (4.58e-04)	Tok/s 94477 (100239)	Loss/tok 3.2956 (3.4436)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.060 (0.146)	Data 1.30e-04 (4.55e-04)	Tok/s 87048 (100221)	Loss/tok 2.7024 (3.4427)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][910/1938]	Time 0.110 (0.146)	Data 1.08e-04 (4.51e-04)	Tok/s 95540 (100174)	Loss/tok 3.0611 (3.4409)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.109 (0.145)	Data 1.41e-04 (4.48e-04)	Tok/s 95018 (100109)	Loss/tok 3.2413 (3.4390)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.161 (0.145)	Data 1.27e-04 (4.45e-04)	Tok/s 105748 (100072)	Loss/tok 3.4831 (3.4375)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.059 (0.144)	Data 1.42e-04 (4.42e-04)	Tok/s 87888 (100014)	Loss/tok 2.6239 (3.4359)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.110 (0.144)	Data 1.98e-04 (4.39e-04)	Tok/s 94081 (99991)	Loss/tok 3.1826 (3.4353)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.109 (0.144)	Data 1.40e-04 (4.35e-04)	Tok/s 93917 (99986)	Loss/tok 3.1593 (3.4352)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.161 (0.144)	Data 1.24e-04 (4.32e-04)	Tok/s 102859 (99983)	Loss/tok 3.4882 (3.4355)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.162 (0.144)	Data 1.20e-04 (4.30e-04)	Tok/s 105194 (99956)	Loss/tok 3.3871 (3.4341)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.109 (0.144)	Data 1.49e-04 (4.27e-04)	Tok/s 95424 (99958)	Loss/tok 3.1790 (3.4331)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.160 (0.144)	Data 1.86e-04 (4.24e-04)	Tok/s 107264 (99954)	Loss/tok 3.2830 (3.4325)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.109 (0.144)	Data 1.24e-04 (4.21e-04)	Tok/s 96265 (99990)	Loss/tok 3.2549 (3.4328)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.108 (0.144)	Data 1.45e-04 (4.19e-04)	Tok/s 97459 (100004)	Loss/tok 3.1111 (3.4320)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.108 (0.144)	Data 1.47e-04 (4.16e-04)	Tok/s 95887 (99978)	Loss/tok 3.2863 (3.4308)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1040/1938]	Time 0.108 (0.144)	Data 1.28e-04 (4.14e-04)	Tok/s 97868 (99981)	Loss/tok 3.0594 (3.4300)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.108 (0.144)	Data 1.31e-04 (4.11e-04)	Tok/s 94923 (99956)	Loss/tok 3.1214 (3.4283)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.160 (0.143)	Data 1.66e-04 (4.09e-04)	Tok/s 104472 (99923)	Loss/tok 3.3304 (3.4269)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.109 (0.143)	Data 1.68e-04 (4.07e-04)	Tok/s 95420 (99899)	Loss/tok 3.1547 (3.4265)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.059 (0.143)	Data 1.81e-04 (4.04e-04)	Tok/s 89673 (99852)	Loss/tok 2.6986 (3.4252)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.213 (0.143)	Data 1.46e-04 (4.02e-04)	Tok/s 109392 (99844)	Loss/tok 3.4894 (3.4241)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.059 (0.143)	Data 1.60e-04 (4.00e-04)	Tok/s 90411 (99824)	Loss/tok 2.7121 (3.4230)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1110/1938]	Time 0.160 (0.143)	Data 1.87e-04 (3.98e-04)	Tok/s 106146 (99822)	Loss/tok 3.3382 (3.4231)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.161 (0.143)	Data 1.44e-04 (3.95e-04)	Tok/s 105422 (99811)	Loss/tok 3.4109 (3.4231)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.160 (0.143)	Data 1.45e-04 (3.93e-04)	Tok/s 104758 (99835)	Loss/tok 3.4506 (3.4238)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.108 (0.143)	Data 1.45e-04 (3.91e-04)	Tok/s 95280 (99840)	Loss/tok 3.1458 (3.4232)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.108 (0.143)	Data 1.67e-04 (3.89e-04)	Tok/s 93649 (99825)	Loss/tok 3.0353 (3.4221)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.109 (0.143)	Data 1.29e-04 (3.87e-04)	Tok/s 96146 (99838)	Loss/tok 3.2323 (3.4217)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.272 (0.143)	Data 1.57e-04 (3.85e-04)	Tok/s 109638 (99837)	Loss/tok 3.6856 (3.4218)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.060 (0.143)	Data 1.42e-04 (3.83e-04)	Tok/s 89717 (99850)	Loss/tok 2.7115 (3.4222)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.108 (0.143)	Data 1.42e-04 (3.81e-04)	Tok/s 94803 (99827)	Loss/tok 3.1008 (3.4207)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.212 (0.143)	Data 1.67e-04 (3.79e-04)	Tok/s 109238 (99866)	Loss/tok 3.6901 (3.4215)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.108 (0.143)	Data 1.43e-04 (3.77e-04)	Tok/s 96022 (99876)	Loss/tok 3.1792 (3.4208)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.061 (0.143)	Data 2.22e-04 (3.75e-04)	Tok/s 86062 (99859)	Loss/tok 2.6661 (3.4196)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1230/1938]	Time 0.274 (0.143)	Data 2.16e-04 (3.73e-04)	Tok/s 110566 (99877)	Loss/tok 3.7021 (3.4195)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.213 (0.143)	Data 1.43e-04 (3.71e-04)	Tok/s 110089 (99862)	Loss/tok 3.5067 (3.4184)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.160 (0.143)	Data 1.46e-04 (3.70e-04)	Tok/s 104824 (99877)	Loss/tok 3.3728 (3.4180)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.059 (0.143)	Data 1.39e-04 (3.68e-04)	Tok/s 90568 (99885)	Loss/tok 2.7420 (3.4173)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.160 (0.143)	Data 1.34e-04 (3.66e-04)	Tok/s 104583 (99873)	Loss/tok 3.3222 (3.4164)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.213 (0.143)	Data 1.48e-04 (3.65e-04)	Tok/s 109620 (99880)	Loss/tok 3.5673 (3.4158)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.214 (0.143)	Data 1.85e-04 (3.63e-04)	Tok/s 109213 (99879)	Loss/tok 3.6751 (3.4155)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.160 (0.142)	Data 1.50e-04 (3.61e-04)	Tok/s 105207 (99876)	Loss/tok 3.3628 (3.4148)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.214 (0.143)	Data 1.56e-04 (3.60e-04)	Tok/s 109879 (99898)	Loss/tok 3.5431 (3.4153)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.109 (0.143)	Data 1.93e-04 (3.58e-04)	Tok/s 94134 (99902)	Loss/tok 3.2724 (3.4150)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.108 (0.143)	Data 1.41e-04 (3.57e-04)	Tok/s 96244 (99916)	Loss/tok 3.0912 (3.4147)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.160 (0.143)	Data 1.27e-04 (3.55e-04)	Tok/s 105139 (99920)	Loss/tok 3.3800 (3.4142)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1350/1938]	Time 0.108 (0.143)	Data 1.22e-04 (3.54e-04)	Tok/s 94795 (99917)	Loss/tok 3.1087 (3.4144)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.160 (0.143)	Data 1.41e-04 (3.52e-04)	Tok/s 106202 (99915)	Loss/tok 3.3449 (3.4142)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.059 (0.142)	Data 1.47e-04 (3.51e-04)	Tok/s 87311 (99885)	Loss/tok 2.7713 (3.4131)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.108 (0.143)	Data 1.40e-04 (3.49e-04)	Tok/s 95528 (99910)	Loss/tok 3.0230 (3.4133)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.159 (0.143)	Data 1.45e-04 (3.48e-04)	Tok/s 105663 (99924)	Loss/tok 3.2517 (3.4130)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.060 (0.143)	Data 1.47e-04 (3.47e-04)	Tok/s 88826 (99940)	Loss/tok 2.7331 (3.4132)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.160 (0.143)	Data 1.52e-04 (3.45e-04)	Tok/s 103498 (99938)	Loss/tok 3.4405 (3.4126)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.160 (0.143)	Data 1.41e-04 (3.44e-04)	Tok/s 104470 (99957)	Loss/tok 3.2305 (3.4123)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.108 (0.143)	Data 2.60e-04 (3.43e-04)	Tok/s 93832 (99953)	Loss/tok 3.1344 (3.4123)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.161 (0.143)	Data 1.26e-04 (3.41e-04)	Tok/s 103616 (99970)	Loss/tok 3.4238 (3.4115)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.273 (0.143)	Data 1.47e-04 (3.40e-04)	Tok/s 108406 (99978)	Loss/tok 3.7485 (3.4117)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.213 (0.143)	Data 1.77e-04 (3.39e-04)	Tok/s 110710 (99996)	Loss/tok 3.5603 (3.4114)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.060 (0.143)	Data 1.82e-04 (3.37e-04)	Tok/s 87943 (100003)	Loss/tok 2.6745 (3.4110)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1480/1938]	Time 0.159 (0.143)	Data 1.42e-04 (3.36e-04)	Tok/s 105269 (100036)	Loss/tok 3.3584 (3.4119)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1490/1938]	Time 0.212 (0.143)	Data 1.46e-04 (3.35e-04)	Tok/s 109209 (100044)	Loss/tok 3.5942 (3.4122)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.108 (0.143)	Data 1.77e-04 (3.34e-04)	Tok/s 96924 (100047)	Loss/tok 3.0316 (3.4117)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.160 (0.143)	Data 1.59e-04 (3.33e-04)	Tok/s 104047 (100048)	Loss/tok 3.3894 (3.4110)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.108 (0.143)	Data 2.00e-04 (3.32e-04)	Tok/s 95562 (100053)	Loss/tok 3.2135 (3.4108)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.160 (0.143)	Data 1.43e-04 (3.30e-04)	Tok/s 103410 (100055)	Loss/tok 3.4859 (3.4103)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.160 (0.143)	Data 1.27e-04 (3.29e-04)	Tok/s 104938 (100064)	Loss/tok 3.3827 (3.4102)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.108 (0.143)	Data 1.37e-04 (3.28e-04)	Tok/s 94687 (100055)	Loss/tok 3.2261 (3.4098)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.107 (0.143)	Data 1.54e-04 (3.27e-04)	Tok/s 96532 (100035)	Loss/tok 3.1241 (3.4088)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.107 (0.143)	Data 1.18e-04 (3.26e-04)	Tok/s 92440 (100029)	Loss/tok 3.0959 (3.4080)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.108 (0.143)	Data 1.42e-04 (3.24e-04)	Tok/s 95171 (100002)	Loss/tok 3.1337 (3.4067)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.108 (0.143)	Data 1.35e-04 (3.23e-04)	Tok/s 96661 (100002)	Loss/tok 3.1166 (3.4061)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.108 (0.143)	Data 1.25e-04 (3.22e-04)	Tok/s 93652 (99997)	Loss/tok 3.1101 (3.4060)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.107 (0.142)	Data 1.37e-04 (3.21e-04)	Tok/s 98262 (99992)	Loss/tok 3.2030 (3.4058)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1620/1938]	Time 0.060 (0.142)	Data 1.24e-04 (3.20e-04)	Tok/s 90259 (99978)	Loss/tok 2.7828 (3.4050)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.108 (0.142)	Data 1.25e-04 (3.19e-04)	Tok/s 96063 (99977)	Loss/tok 3.1271 (3.4044)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.107 (0.142)	Data 1.44e-04 (3.17e-04)	Tok/s 97074 (100001)	Loss/tok 3.1788 (3.4045)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.213 (0.142)	Data 1.18e-04 (3.16e-04)	Tok/s 108722 (100009)	Loss/tok 3.5377 (3.4046)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.160 (0.142)	Data 1.41e-04 (3.15e-04)	Tok/s 104020 (100009)	Loss/tok 3.4253 (3.4041)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.214 (0.143)	Data 1.19e-04 (3.14e-04)	Tok/s 110480 (100012)	Loss/tok 3.5993 (3.4041)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1680/1938]	Time 0.109 (0.143)	Data 1.26e-04 (3.13e-04)	Tok/s 93341 (100018)	Loss/tok 3.1887 (3.4046)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.109 (0.143)	Data 1.27e-04 (3.12e-04)	Tok/s 96519 (100019)	Loss/tok 3.2369 (3.4044)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.160 (0.143)	Data 1.42e-04 (3.11e-04)	Tok/s 105291 (100024)	Loss/tok 3.3494 (3.4041)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.160 (0.143)	Data 1.42e-04 (3.10e-04)	Tok/s 103876 (100026)	Loss/tok 3.3261 (3.4034)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.108 (0.143)	Data 1.43e-04 (3.10e-04)	Tok/s 94553 (100032)	Loss/tok 3.1348 (3.4030)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.160 (0.143)	Data 1.24e-04 (3.09e-04)	Tok/s 105531 (100024)	Loss/tok 3.3562 (3.4025)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.108 (0.143)	Data 1.36e-04 (3.08e-04)	Tok/s 94076 (100022)	Loss/tok 3.1090 (3.4022)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.160 (0.143)	Data 1.71e-04 (3.07e-04)	Tok/s 104014 (100028)	Loss/tok 3.2771 (3.4020)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.213 (0.143)	Data 1.56e-04 (3.06e-04)	Tok/s 108255 (100028)	Loss/tok 3.5975 (3.4020)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.109 (0.143)	Data 1.78e-04 (3.05e-04)	Tok/s 94945 (100036)	Loss/tok 3.1295 (3.4021)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.108 (0.143)	Data 1.41e-04 (3.04e-04)	Tok/s 92968 (100041)	Loss/tok 3.1762 (3.4018)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.160 (0.143)	Data 1.58e-04 (3.03e-04)	Tok/s 105148 (100043)	Loss/tok 3.3444 (3.4011)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.161 (0.143)	Data 1.22e-04 (3.02e-04)	Tok/s 104635 (100061)	Loss/tok 3.2813 (3.4011)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1810/1938]	Time 0.108 (0.143)	Data 1.41e-04 (3.02e-04)	Tok/s 97076 (100069)	Loss/tok 3.1244 (3.4014)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.108 (0.143)	Data 1.40e-04 (3.01e-04)	Tok/s 95770 (100059)	Loss/tok 3.2099 (3.4009)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.108 (0.143)	Data 1.43e-04 (3.00e-04)	Tok/s 95201 (100053)	Loss/tok 3.1074 (3.4006)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.108 (0.143)	Data 1.42e-04 (2.99e-04)	Tok/s 95406 (100052)	Loss/tok 3.1943 (3.4005)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.109 (0.143)	Data 1.90e-04 (2.98e-04)	Tok/s 94590 (100040)	Loss/tok 3.1743 (3.3997)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.108 (0.143)	Data 1.24e-04 (2.98e-04)	Tok/s 94462 (100058)	Loss/tok 3.0544 (3.3996)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.212 (0.143)	Data 1.29e-04 (2.97e-04)	Tok/s 110507 (100057)	Loss/tok 3.5693 (3.3992)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.108 (0.143)	Data 1.18e-04 (2.96e-04)	Tok/s 95271 (100065)	Loss/tok 3.1829 (3.3989)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.107 (0.143)	Data 1.27e-04 (2.95e-04)	Tok/s 97319 (100065)	Loss/tok 3.0719 (3.3985)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.160 (0.143)	Data 1.17e-04 (2.94e-04)	Tok/s 104951 (100071)	Loss/tok 3.3710 (3.3980)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.108 (0.143)	Data 1.04e-04 (2.93e-04)	Tok/s 96477 (100065)	Loss/tok 3.1702 (3.3969)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.107 (0.143)	Data 1.25e-04 (2.93e-04)	Tok/s 95948 (100080)	Loss/tok 3.0999 (3.3972)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1930/1938]	Time 0.160 (0.143)	Data 1.07e-04 (2.92e-04)	Tok/s 103793 (100084)	Loss/tok 3.2692 (3.3967)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019701316, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019701316, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.647 (0.647)	Decoder iters 111.0 (111.0)	Tok/s 25433 (25433)
0: Running moses detokenizer
0: BLEU(score=22.26479766809554, counts=[35920, 17352, 9594, 5550], totals=[65217, 62214, 59211, 56215], precisions=[55.07766379931613, 27.8908284308998, 16.203070375436997, 9.872809748287823], bp=1.0, sys_len=65217, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019703195, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22260000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019703195, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3946	Test BLEU: 22.26
0: Performance: Epoch: 1	Training: 800897 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019703196, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019703196, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019703196, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 874776123
0: TRAIN [2][0/1938]	Time 0.419 (0.419)	Data 2.51e-01 (2.51e-01)	Tok/s 40180 (40180)	Loss/tok 3.2253 (3.2253)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.213 (0.194)	Data 1.99e-04 (2.30e-02)	Tok/s 110929 (97833)	Loss/tok 3.4845 (3.3725)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.160 (0.173)	Data 1.44e-04 (1.21e-02)	Tok/s 106582 (99497)	Loss/tok 3.1323 (3.3182)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.159 (0.167)	Data 2.35e-04 (8.28e-03)	Tok/s 105194 (100746)	Loss/tok 3.3138 (3.3015)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.158 (0.155)	Data 1.79e-04 (6.30e-03)	Tok/s 105946 (100080)	Loss/tok 3.3976 (3.2735)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.108 (0.151)	Data 1.54e-04 (5.10e-03)	Tok/s 98004 (100232)	Loss/tok 3.0893 (3.2689)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.112 (0.147)	Data 1.41e-04 (4.29e-03)	Tok/s 92474 (99783)	Loss/tok 3.0414 (3.2595)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.160 (0.146)	Data 1.59e-04 (3.71e-03)	Tok/s 102666 (99864)	Loss/tok 3.3049 (3.2616)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.059 (0.144)	Data 1.23e-04 (3.27e-03)	Tok/s 90526 (99647)	Loss/tok 2.5680 (3.2562)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.214 (0.142)	Data 2.24e-04 (2.93e-03)	Tok/s 110000 (99577)	Loss/tok 3.3169 (3.2467)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.108 (0.142)	Data 2.22e-04 (2.66e-03)	Tok/s 93984 (99776)	Loss/tok 2.9556 (3.2449)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.059 (0.141)	Data 1.83e-04 (2.44e-03)	Tok/s 87856 (99726)	Loss/tok 2.6859 (3.2377)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][120/1938]	Time 0.160 (0.142)	Data 2.22e-04 (2.25e-03)	Tok/s 105789 (99842)	Loss/tok 3.2784 (3.2449)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.108 (0.142)	Data 1.99e-04 (2.09e-03)	Tok/s 96081 (99978)	Loss/tok 2.9975 (3.2441)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.108 (0.144)	Data 1.89e-04 (1.96e-03)	Tok/s 96017 (100189)	Loss/tok 3.0279 (3.2519)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][150/1938]	Time 0.273 (0.148)	Data 1.24e-04 (1.84e-03)	Tok/s 107200 (100558)	Loss/tok 3.6076 (3.2716)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.059 (0.146)	Data 1.47e-04 (1.73e-03)	Tok/s 89362 (100421)	Loss/tok 2.7022 (3.2655)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][170/1938]	Time 0.160 (0.147)	Data 1.98e-04 (1.64e-03)	Tok/s 104700 (100486)	Loss/tok 3.3102 (3.2728)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.107 (0.145)	Data 2.22e-04 (1.56e-03)	Tok/s 97426 (100396)	Loss/tok 3.0224 (3.2649)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.059 (0.144)	Data 1.54e-04 (1.49e-03)	Tok/s 89285 (100252)	Loss/tok 2.6450 (3.2630)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.060 (0.144)	Data 1.79e-04 (1.43e-03)	Tok/s 88253 (100314)	Loss/tok 2.5717 (3.2589)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.273 (0.146)	Data 2.09e-04 (1.37e-03)	Tok/s 109743 (100578)	Loss/tok 3.5154 (3.2709)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.107 (0.145)	Data 1.36e-04 (1.31e-03)	Tok/s 95268 (100501)	Loss/tok 2.9941 (3.2657)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.107 (0.145)	Data 2.27e-04 (1.26e-03)	Tok/s 95597 (100463)	Loss/tok 3.0170 (3.2642)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.108 (0.145)	Data 1.57e-04 (1.22e-03)	Tok/s 93176 (100426)	Loss/tok 3.1686 (3.2637)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.212 (0.145)	Data 2.26e-04 (1.18e-03)	Tok/s 109673 (100444)	Loss/tok 3.4662 (3.2653)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.107 (0.144)	Data 1.72e-04 (1.14e-03)	Tok/s 97011 (100455)	Loss/tok 3.1232 (3.2629)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.107 (0.145)	Data 1.84e-04 (1.10e-03)	Tok/s 96301 (100537)	Loss/tok 2.9967 (3.2639)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.107 (0.145)	Data 2.18e-04 (1.07e-03)	Tok/s 96642 (100545)	Loss/tok 3.0625 (3.2639)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.159 (0.145)	Data 1.96e-04 (1.04e-03)	Tok/s 104489 (100624)	Loss/tok 3.2890 (3.2641)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][300/1938]	Time 0.108 (0.145)	Data 2.05e-04 (1.01e-03)	Tok/s 95765 (100556)	Loss/tok 3.1647 (3.2615)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.107 (0.145)	Data 2.65e-04 (9.86e-04)	Tok/s 94312 (100578)	Loss/tok 3.1022 (3.2608)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.107 (0.145)	Data 1.42e-04 (9.60e-04)	Tok/s 97858 (100649)	Loss/tok 3.1047 (3.2606)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.160 (0.145)	Data 1.49e-04 (9.37e-04)	Tok/s 107335 (100659)	Loss/tok 3.2813 (3.2585)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.273 (0.145)	Data 1.21e-04 (9.15e-04)	Tok/s 108177 (100740)	Loss/tok 3.5684 (3.2625)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][350/1938]	Time 0.160 (0.145)	Data 1.36e-04 (8.93e-04)	Tok/s 104777 (100740)	Loss/tok 3.2515 (3.2633)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.212 (0.146)	Data 1.58e-04 (8.73e-04)	Tok/s 110196 (100865)	Loss/tok 3.4602 (3.2682)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.107 (0.145)	Data 2.33e-04 (8.55e-04)	Tok/s 96874 (100792)	Loss/tok 3.0619 (3.2652)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.108 (0.145)	Data 1.58e-04 (8.37e-04)	Tok/s 95702 (100708)	Loss/tok 3.1401 (3.2636)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.107 (0.144)	Data 1.95e-04 (8.20e-04)	Tok/s 96330 (100666)	Loss/tok 2.9394 (3.2623)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.272 (0.145)	Data 1.71e-04 (8.04e-04)	Tok/s 110432 (100698)	Loss/tok 3.5879 (3.2647)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.108 (0.145)	Data 1.62e-04 (7.88e-04)	Tok/s 93874 (100631)	Loss/tok 3.0530 (3.2633)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.159 (0.144)	Data 1.74e-04 (7.73e-04)	Tok/s 104612 (100628)	Loss/tok 3.3134 (3.2634)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.159 (0.144)	Data 1.37e-04 (7.59e-04)	Tok/s 106143 (100614)	Loss/tok 3.2248 (3.2615)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.160 (0.144)	Data 1.72e-04 (7.46e-04)	Tok/s 105007 (100575)	Loss/tok 3.1691 (3.2608)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.108 (0.144)	Data 1.87e-04 (7.33e-04)	Tok/s 96913 (100603)	Loss/tok 2.9065 (3.2616)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.161 (0.144)	Data 2.12e-04 (7.21e-04)	Tok/s 102906 (100600)	Loss/tok 3.2590 (3.2609)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.107 (0.144)	Data 2.12e-04 (7.09e-04)	Tok/s 94441 (100625)	Loss/tok 3.0894 (3.2628)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][480/1938]	Time 0.160 (0.144)	Data 1.38e-04 (6.98e-04)	Tok/s 104594 (100642)	Loss/tok 3.3564 (3.2659)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.160 (0.144)	Data 1.23e-04 (6.87e-04)	Tok/s 105027 (100620)	Loss/tok 3.2697 (3.2638)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.273 (0.144)	Data 1.81e-04 (6.76e-04)	Tok/s 110035 (100640)	Loss/tok 3.5656 (3.2636)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.160 (0.144)	Data 1.17e-04 (6.65e-04)	Tok/s 103517 (100592)	Loss/tok 3.2503 (3.2627)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.059 (0.143)	Data 1.22e-04 (6.56e-04)	Tok/s 89499 (100580)	Loss/tok 2.6334 (3.2615)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.109 (0.143)	Data 1.71e-04 (6.47e-04)	Tok/s 94390 (100500)	Loss/tok 3.0523 (3.2599)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.108 (0.143)	Data 1.68e-04 (6.38e-04)	Tok/s 94226 (100475)	Loss/tok 3.0201 (3.2588)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.108 (0.143)	Data 1.85e-04 (6.29e-04)	Tok/s 95696 (100490)	Loss/tok 3.0463 (3.2598)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.160 (0.142)	Data 2.20e-04 (6.21e-04)	Tok/s 105410 (100455)	Loss/tok 3.2583 (3.2581)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.107 (0.142)	Data 1.43e-04 (6.13e-04)	Tok/s 96762 (100408)	Loss/tok 2.9595 (3.2570)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.108 (0.142)	Data 2.36e-04 (6.05e-04)	Tok/s 97041 (100426)	Loss/tok 3.0509 (3.2561)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.059 (0.142)	Data 1.17e-04 (5.97e-04)	Tok/s 88750 (100444)	Loss/tok 2.5738 (3.2568)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.160 (0.142)	Data 1.20e-04 (5.90e-04)	Tok/s 105764 (100454)	Loss/tok 3.3410 (3.2583)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][610/1938]	Time 0.160 (0.142)	Data 2.26e-04 (5.83e-04)	Tok/s 105393 (100486)	Loss/tok 3.2846 (3.2576)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.059 (0.142)	Data 1.60e-04 (5.76e-04)	Tok/s 88028 (100464)	Loss/tok 2.5779 (3.2573)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.107 (0.142)	Data 1.34e-04 (5.69e-04)	Tok/s 95644 (100461)	Loss/tok 3.1034 (3.2567)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.159 (0.142)	Data 1.65e-04 (5.63e-04)	Tok/s 105632 (100483)	Loss/tok 3.2875 (3.2574)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.160 (0.142)	Data 1.14e-04 (5.57e-04)	Tok/s 104022 (100477)	Loss/tok 3.3321 (3.2571)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.107 (0.142)	Data 1.22e-04 (5.51e-04)	Tok/s 96140 (100509)	Loss/tok 3.1624 (3.2576)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.160 (0.142)	Data 1.42e-04 (5.45e-04)	Tok/s 105334 (100529)	Loss/tok 3.2394 (3.2577)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.160 (0.142)	Data 1.40e-04 (5.39e-04)	Tok/s 105005 (100506)	Loss/tok 3.2993 (3.2565)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.107 (0.142)	Data 1.44e-04 (5.33e-04)	Tok/s 94863 (100513)	Loss/tok 3.1594 (3.2561)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.160 (0.142)	Data 1.31e-04 (5.28e-04)	Tok/s 105222 (100515)	Loss/tok 3.3248 (3.2558)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.274 (0.143)	Data 1.44e-04 (5.22e-04)	Tok/s 107596 (100569)	Loss/tok 3.7629 (3.2592)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.274 (0.143)	Data 1.42e-04 (5.17e-04)	Tok/s 108771 (100548)	Loss/tok 3.5982 (3.2585)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][730/1938]	Time 0.159 (0.143)	Data 1.51e-04 (5.12e-04)	Tok/s 104275 (100558)	Loss/tok 3.2928 (3.2583)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.108 (0.142)	Data 1.52e-04 (5.07e-04)	Tok/s 96688 (100507)	Loss/tok 2.9014 (3.2573)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.161 (0.142)	Data 1.44e-04 (5.03e-04)	Tok/s 104806 (100535)	Loss/tok 3.2877 (3.2568)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.108 (0.142)	Data 1.29e-04 (4.98e-04)	Tok/s 95467 (100532)	Loss/tok 2.9976 (3.2561)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.108 (0.142)	Data 1.30e-04 (4.93e-04)	Tok/s 95592 (100533)	Loss/tok 3.0354 (3.2565)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.161 (0.142)	Data 1.54e-04 (4.89e-04)	Tok/s 104401 (100566)	Loss/tok 3.2059 (3.2572)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.275 (0.142)	Data 1.18e-04 (4.84e-04)	Tok/s 108799 (100551)	Loss/tok 3.7176 (3.2574)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][800/1938]	Time 0.059 (0.142)	Data 1.33e-04 (4.80e-04)	Tok/s 87882 (100512)	Loss/tok 2.6411 (3.2560)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.059 (0.142)	Data 1.41e-04 (4.76e-04)	Tok/s 91082 (100510)	Loss/tok 2.7209 (3.2556)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.107 (0.142)	Data 1.43e-04 (4.72e-04)	Tok/s 95792 (100514)	Loss/tok 3.0647 (3.2556)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.273 (0.142)	Data 1.49e-04 (4.68e-04)	Tok/s 108742 (100538)	Loss/tok 3.7531 (3.2572)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.059 (0.142)	Data 1.46e-04 (4.64e-04)	Tok/s 89280 (100583)	Loss/tok 2.5959 (3.2586)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.108 (0.142)	Data 1.24e-04 (4.60e-04)	Tok/s 95549 (100581)	Loss/tok 3.0793 (3.2585)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.213 (0.143)	Data 1.38e-04 (4.57e-04)	Tok/s 110022 (100623)	Loss/tok 3.5367 (3.2594)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.107 (0.143)	Data 1.77e-04 (4.53e-04)	Tok/s 99842 (100667)	Loss/tok 3.1100 (3.2610)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.213 (0.143)	Data 1.44e-04 (4.50e-04)	Tok/s 109169 (100669)	Loss/tok 3.5005 (3.2611)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.107 (0.143)	Data 1.20e-04 (4.46e-04)	Tok/s 97224 (100677)	Loss/tok 3.1564 (3.2606)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.108 (0.143)	Data 1.38e-04 (4.43e-04)	Tok/s 95528 (100671)	Loss/tok 3.0406 (3.2603)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.159 (0.143)	Data 1.22e-04 (4.40e-04)	Tok/s 105620 (100679)	Loss/tok 3.3099 (3.2606)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.160 (0.143)	Data 1.40e-04 (4.36e-04)	Tok/s 104131 (100647)	Loss/tok 3.2347 (3.2597)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][930/1938]	Time 0.107 (0.142)	Data 1.57e-04 (4.34e-04)	Tok/s 96776 (100634)	Loss/tok 3.0359 (3.2586)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.213 (0.142)	Data 1.36e-04 (4.30e-04)	Tok/s 109545 (100650)	Loss/tok 3.3955 (3.2588)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.107 (0.142)	Data 1.26e-04 (4.28e-04)	Tok/s 95427 (100640)	Loss/tok 3.0418 (3.2587)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.108 (0.142)	Data 1.45e-04 (4.25e-04)	Tok/s 96235 (100632)	Loss/tok 3.0176 (3.2580)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.108 (0.142)	Data 1.91e-04 (4.22e-04)	Tok/s 97655 (100613)	Loss/tok 3.0698 (3.2573)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.160 (0.142)	Data 1.55e-04 (4.19e-04)	Tok/s 106057 (100599)	Loss/tok 3.2465 (3.2562)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.160 (0.142)	Data 1.24e-04 (4.16e-04)	Tok/s 106267 (100650)	Loss/tok 3.2956 (3.2566)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.160 (0.142)	Data 1.95e-04 (4.13e-04)	Tok/s 105434 (100666)	Loss/tok 3.3296 (3.2565)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.159 (0.142)	Data 1.59e-04 (4.11e-04)	Tok/s 105162 (100673)	Loss/tok 3.2201 (3.2558)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.272 (0.142)	Data 1.37e-04 (4.08e-04)	Tok/s 110565 (100670)	Loss/tok 3.5205 (3.2558)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1030/1938]	Time 0.060 (0.142)	Data 1.42e-04 (4.06e-04)	Tok/s 89975 (100676)	Loss/tok 2.7999 (3.2567)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.059 (0.142)	Data 1.81e-04 (4.03e-04)	Tok/s 86765 (100660)	Loss/tok 2.6804 (3.2564)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.059 (0.142)	Data 1.71e-04 (4.01e-04)	Tok/s 88155 (100655)	Loss/tok 2.6695 (3.2561)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.108 (0.142)	Data 1.20e-04 (3.98e-04)	Tok/s 96402 (100632)	Loss/tok 3.0841 (3.2554)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.059 (0.142)	Data 1.59e-04 (3.96e-04)	Tok/s 89160 (100646)	Loss/tok 2.6711 (3.2553)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.107 (0.142)	Data 1.46e-04 (3.94e-04)	Tok/s 95409 (100641)	Loss/tok 3.0156 (3.2551)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.159 (0.142)	Data 1.23e-04 (3.91e-04)	Tok/s 103679 (100638)	Loss/tok 3.2941 (3.2545)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.160 (0.142)	Data 1.37e-04 (3.89e-04)	Tok/s 105985 (100638)	Loss/tok 3.1783 (3.2552)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.160 (0.142)	Data 1.19e-04 (3.87e-04)	Tok/s 105408 (100614)	Loss/tok 3.2412 (3.2541)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.160 (0.142)	Data 1.40e-04 (3.85e-04)	Tok/s 106484 (100605)	Loss/tok 3.2870 (3.2536)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.107 (0.141)	Data 1.40e-04 (3.82e-04)	Tok/s 94810 (100592)	Loss/tok 2.9662 (3.2530)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.060 (0.141)	Data 1.46e-04 (3.80e-04)	Tok/s 89135 (100582)	Loss/tok 2.6034 (3.2527)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1150/1938]	Time 0.107 (0.142)	Data 1.91e-04 (3.78e-04)	Tok/s 96120 (100607)	Loss/tok 3.0994 (3.2533)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.159 (0.142)	Data 1.35e-04 (3.76e-04)	Tok/s 105080 (100618)	Loss/tok 3.1606 (3.2533)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.107 (0.142)	Data 1.50e-04 (3.74e-04)	Tok/s 96320 (100599)	Loss/tok 3.0795 (3.2534)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.273 (0.142)	Data 1.35e-04 (3.72e-04)	Tok/s 109366 (100630)	Loss/tok 3.6840 (3.2546)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.108 (0.142)	Data 1.53e-04 (3.70e-04)	Tok/s 96634 (100632)	Loss/tok 3.0508 (3.2545)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1200/1938]	Time 0.161 (0.142)	Data 1.33e-04 (3.68e-04)	Tok/s 103557 (100626)	Loss/tok 3.1309 (3.2544)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.059 (0.142)	Data 1.45e-04 (3.67e-04)	Tok/s 88738 (100620)	Loss/tok 2.5193 (3.2540)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.108 (0.142)	Data 1.21e-04 (3.65e-04)	Tok/s 93501 (100608)	Loss/tok 3.0847 (3.2534)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.158 (0.142)	Data 1.19e-04 (3.63e-04)	Tok/s 106407 (100602)	Loss/tok 3.3429 (3.2531)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.106 (0.141)	Data 1.19e-04 (3.61e-04)	Tok/s 97325 (100583)	Loss/tok 3.0796 (3.2525)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.107 (0.142)	Data 1.31e-04 (3.60e-04)	Tok/s 94211 (100612)	Loss/tok 3.0304 (3.2531)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.059 (0.142)	Data 1.17e-04 (3.58e-04)	Tok/s 90299 (100600)	Loss/tok 2.6406 (3.2532)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.213 (0.142)	Data 1.20e-04 (3.56e-04)	Tok/s 110596 (100611)	Loss/tok 3.4687 (3.2537)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.107 (0.142)	Data 1.38e-04 (3.54e-04)	Tok/s 96781 (100628)	Loss/tok 3.0774 (3.2554)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.108 (0.142)	Data 1.92e-04 (3.53e-04)	Tok/s 97207 (100660)	Loss/tok 3.0731 (3.2563)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.160 (0.142)	Data 1.23e-04 (3.51e-04)	Tok/s 104626 (100665)	Loss/tok 3.3043 (3.2568)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.160 (0.142)	Data 1.36e-04 (3.50e-04)	Tok/s 104257 (100674)	Loss/tok 3.1436 (3.2570)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.214 (0.142)	Data 1.83e-04 (3.48e-04)	Tok/s 108657 (100675)	Loss/tok 3.3684 (3.2566)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1330/1938]	Time 0.107 (0.142)	Data 1.20e-04 (3.47e-04)	Tok/s 96715 (100681)	Loss/tok 3.0894 (3.2563)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.214 (0.142)	Data 1.40e-04 (3.45e-04)	Tok/s 108560 (100685)	Loss/tok 3.4643 (3.2567)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.107 (0.142)	Data 1.18e-04 (3.44e-04)	Tok/s 94378 (100679)	Loss/tok 3.0274 (3.2561)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.108 (0.142)	Data 1.83e-04 (3.42e-04)	Tok/s 96725 (100665)	Loss/tok 3.0028 (3.2561)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.107 (0.142)	Data 1.18e-04 (3.41e-04)	Tok/s 95708 (100663)	Loss/tok 3.0115 (3.2560)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.160 (0.142)	Data 1.26e-04 (3.39e-04)	Tok/s 104085 (100672)	Loss/tok 3.3046 (3.2560)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.107 (0.142)	Data 1.22e-04 (3.38e-04)	Tok/s 96388 (100679)	Loss/tok 3.0960 (3.2564)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.213 (0.142)	Data 1.59e-04 (3.36e-04)	Tok/s 108902 (100660)	Loss/tok 3.4229 (3.2561)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.108 (0.142)	Data 1.94e-04 (3.35e-04)	Tok/s 95626 (100669)	Loss/tok 3.0079 (3.2563)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1420/1938]	Time 0.160 (0.142)	Data 1.39e-04 (3.34e-04)	Tok/s 105748 (100675)	Loss/tok 3.2512 (3.2564)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.274 (0.142)	Data 1.55e-04 (3.32e-04)	Tok/s 108090 (100681)	Loss/tok 3.6334 (3.2570)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.106 (0.142)	Data 1.44e-04 (3.31e-04)	Tok/s 95772 (100673)	Loss/tok 2.9945 (3.2574)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.214 (0.143)	Data 1.48e-04 (3.30e-04)	Tok/s 109505 (100696)	Loss/tok 3.3389 (3.2581)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.273 (0.143)	Data 1.21e-04 (3.29e-04)	Tok/s 108128 (100699)	Loss/tok 3.7109 (3.2585)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.108 (0.142)	Data 1.17e-04 (3.27e-04)	Tok/s 94500 (100657)	Loss/tok 2.9570 (3.2576)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.060 (0.142)	Data 1.48e-04 (3.26e-04)	Tok/s 88794 (100642)	Loss/tok 2.7043 (3.2573)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.107 (0.142)	Data 1.47e-04 (3.25e-04)	Tok/s 96308 (100614)	Loss/tok 3.0341 (3.2565)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.161 (0.142)	Data 1.69e-04 (3.24e-04)	Tok/s 105486 (100621)	Loss/tok 3.2245 (3.2579)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.059 (0.142)	Data 1.52e-04 (3.23e-04)	Tok/s 91394 (100636)	Loss/tok 2.6327 (3.2580)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.107 (0.142)	Data 1.17e-04 (3.22e-04)	Tok/s 99260 (100634)	Loss/tok 2.9632 (3.2577)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.214 (0.142)	Data 1.22e-04 (3.20e-04)	Tok/s 109051 (100647)	Loss/tok 3.4873 (3.2582)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1540/1938]	Time 0.213 (0.142)	Data 1.51e-04 (3.19e-04)	Tok/s 110129 (100672)	Loss/tok 3.4498 (3.2584)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.274 (0.143)	Data 1.23e-04 (3.18e-04)	Tok/s 107538 (100677)	Loss/tok 3.6229 (3.2592)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.107 (0.143)	Data 1.16e-04 (3.17e-04)	Tok/s 95016 (100684)	Loss/tok 3.0505 (3.2598)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.213 (0.143)	Data 1.34e-04 (3.16e-04)	Tok/s 108978 (100701)	Loss/tok 3.4352 (3.2606)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.160 (0.143)	Data 1.44e-04 (3.15e-04)	Tok/s 106125 (100692)	Loss/tok 3.3233 (3.2599)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.160 (0.143)	Data 1.42e-04 (3.14e-04)	Tok/s 105243 (100691)	Loss/tok 3.1864 (3.2598)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.159 (0.143)	Data 1.24e-04 (3.13e-04)	Tok/s 105984 (100685)	Loss/tok 3.2506 (3.2593)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.108 (0.143)	Data 1.60e-04 (3.12e-04)	Tok/s 94775 (100671)	Loss/tok 3.1994 (3.2590)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.107 (0.143)	Data 1.21e-04 (3.11e-04)	Tok/s 96402 (100674)	Loss/tok 3.1193 (3.2593)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.108 (0.143)	Data 1.35e-04 (3.10e-04)	Tok/s 94024 (100687)	Loss/tok 3.0315 (3.2593)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.213 (0.143)	Data 1.36e-04 (3.09e-04)	Tok/s 110212 (100686)	Loss/tok 3.4288 (3.2595)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.160 (0.143)	Data 1.40e-04 (3.08e-04)	Tok/s 105345 (100687)	Loss/tok 3.2757 (3.2591)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.213 (0.143)	Data 1.19e-04 (3.07e-04)	Tok/s 109700 (100696)	Loss/tok 3.3236 (3.2593)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1670/1938]	Time 0.160 (0.143)	Data 1.34e-04 (3.06e-04)	Tok/s 104172 (100681)	Loss/tok 3.1977 (3.2587)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.059 (0.143)	Data 1.67e-04 (3.05e-04)	Tok/s 89504 (100673)	Loss/tok 2.6029 (3.2588)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.107 (0.142)	Data 1.42e-04 (3.04e-04)	Tok/s 96896 (100656)	Loss/tok 2.9864 (3.2581)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.160 (0.142)	Data 2.04e-04 (3.03e-04)	Tok/s 104976 (100647)	Loss/tok 3.2331 (3.2578)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.107 (0.142)	Data 1.40e-04 (3.02e-04)	Tok/s 97588 (100636)	Loss/tok 3.0334 (3.2573)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.107 (0.142)	Data 1.45e-04 (3.01e-04)	Tok/s 97573 (100645)	Loss/tok 2.9566 (3.2573)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.107 (0.142)	Data 1.22e-04 (3.00e-04)	Tok/s 96336 (100637)	Loss/tok 2.9451 (3.2568)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1740/1938]	Time 0.108 (0.142)	Data 1.73e-04 (2.99e-04)	Tok/s 95247 (100642)	Loss/tok 2.9729 (3.2571)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.107 (0.142)	Data 1.50e-04 (2.99e-04)	Tok/s 97579 (100646)	Loss/tok 2.9698 (3.2567)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.107 (0.142)	Data 1.16e-04 (2.98e-04)	Tok/s 96936 (100651)	Loss/tok 3.1052 (3.2570)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.159 (0.142)	Data 1.50e-04 (2.97e-04)	Tok/s 106784 (100653)	Loss/tok 3.1389 (3.2566)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.108 (0.142)	Data 1.56e-04 (2.96e-04)	Tok/s 94034 (100648)	Loss/tok 3.1006 (3.2563)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.107 (0.142)	Data 1.41e-04 (2.95e-04)	Tok/s 96438 (100633)	Loss/tok 2.9386 (3.2557)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.108 (0.142)	Data 1.96e-04 (2.94e-04)	Tok/s 96351 (100638)	Loss/tok 3.0154 (3.2554)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.158 (0.142)	Data 1.75e-04 (2.94e-04)	Tok/s 107056 (100650)	Loss/tok 3.2378 (3.2554)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.213 (0.142)	Data 1.82e-04 (2.93e-04)	Tok/s 110320 (100671)	Loss/tok 3.4516 (3.2561)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.159 (0.142)	Data 1.67e-04 (2.92e-04)	Tok/s 105604 (100674)	Loss/tok 3.1929 (3.2560)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.211 (0.142)	Data 1.22e-04 (2.91e-04)	Tok/s 111042 (100657)	Loss/tok 3.3992 (3.2555)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.107 (0.142)	Data 1.38e-04 (2.91e-04)	Tok/s 98864 (100645)	Loss/tok 3.0794 (3.2552)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.107 (0.142)	Data 1.35e-04 (2.90e-04)	Tok/s 97413 (100644)	Loss/tok 3.0550 (3.2551)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1870/1938]	Time 0.107 (0.142)	Data 1.39e-04 (2.89e-04)	Tok/s 97857 (100627)	Loss/tok 3.0096 (3.2544)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.213 (0.142)	Data 1.40e-04 (2.88e-04)	Tok/s 110755 (100629)	Loss/tok 3.3858 (3.2543)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.059 (0.142)	Data 1.34e-04 (2.88e-04)	Tok/s 87425 (100626)	Loss/tok 2.6550 (3.2540)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1900/1938]	Time 0.106 (0.142)	Data 1.94e-04 (2.87e-04)	Tok/s 96520 (100634)	Loss/tok 3.1580 (3.2543)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.159 (0.142)	Data 2.01e-04 (2.86e-04)	Tok/s 106019 (100644)	Loss/tok 3.2117 (3.2543)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.108 (0.142)	Data 1.23e-04 (2.86e-04)	Tok/s 97262 (100663)	Loss/tok 2.9777 (3.2547)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.160 (0.142)	Data 1.40e-04 (2.85e-04)	Tok/s 104666 (100673)	Loss/tok 3.2690 (3.2547)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593019979132, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019979132, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.610 (0.610)	Decoder iters 100.0 (100.0)	Tok/s 26954 (26954)
0: Running moses detokenizer
0: BLEU(score=22.715259426555207, counts=[36595, 17880, 9979, 5777], totals=[65946, 62943, 59941, 56944], precisions=[55.49237254723562, 28.40665363900672, 16.648037236616005, 10.145054790671537], bp=1.0, sys_len=65946, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019980947, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22719999999999999, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019980947, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2554	Test BLEU: 22.72
0: Performance: Epoch: 2	Training: 805129 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593019980948, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593019980948, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019980948, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 292248126
0: TRAIN [3][0/1938]	Time 0.420 (0.420)	Data 2.58e-01 (2.58e-01)	Tok/s 40139 (40139)	Loss/tok 3.2149 (3.2149)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.108 (0.165)	Data 1.76e-04 (2.36e-02)	Tok/s 95967 (94606)	Loss/tok 2.9095 (3.1604)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.160 (0.151)	Data 1.58e-04 (1.24e-02)	Tok/s 102533 (97187)	Loss/tok 3.1602 (3.1398)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.108 (0.150)	Data 1.66e-04 (8.49e-03)	Tok/s 95969 (98894)	Loss/tok 3.0579 (3.1493)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.108 (0.151)	Data 1.35e-04 (6.46e-03)	Tok/s 94723 (99833)	Loss/tok 3.0086 (3.1569)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.107 (0.148)	Data 1.75e-04 (5.23e-03)	Tok/s 95991 (99637)	Loss/tok 3.0197 (3.1472)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.216 (0.155)	Data 1.54e-04 (4.40e-03)	Tok/s 108246 (100354)	Loss/tok 3.3158 (3.1897)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.159 (0.153)	Data 1.90e-04 (3.80e-03)	Tok/s 105596 (100685)	Loss/tok 3.0137 (3.1803)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.107 (0.152)	Data 1.05e-04 (3.35e-03)	Tok/s 96389 (100693)	Loss/tok 2.8442 (3.1830)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][90/1938]	Time 0.161 (0.152)	Data 1.29e-04 (3.00e-03)	Tok/s 105608 (100785)	Loss/tok 3.1038 (3.1757)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.161 (0.154)	Data 1.73e-04 (2.72e-03)	Tok/s 104371 (101076)	Loss/tok 3.1152 (3.1848)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][110/1938]	Time 0.106 (0.155)	Data 2.59e-04 (2.50e-03)	Tok/s 96858 (101263)	Loss/tok 2.9745 (3.1916)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.108 (0.153)	Data 1.64e-04 (2.30e-03)	Tok/s 95445 (101155)	Loss/tok 3.0734 (3.1865)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.107 (0.152)	Data 2.00e-04 (2.14e-03)	Tok/s 99079 (101108)	Loss/tok 2.9925 (3.1856)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.213 (0.151)	Data 1.70e-04 (2.00e-03)	Tok/s 109342 (101087)	Loss/tok 3.2459 (3.1834)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.160 (0.148)	Data 1.65e-04 (1.88e-03)	Tok/s 103936 (100803)	Loss/tok 3.2272 (3.1769)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.108 (0.148)	Data 1.57e-04 (1.77e-03)	Tok/s 94635 (100846)	Loss/tok 2.9500 (3.1757)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.108 (0.147)	Data 1.93e-04 (1.68e-03)	Tok/s 96961 (100755)	Loss/tok 2.9997 (3.1723)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.160 (0.148)	Data 1.65e-04 (1.60e-03)	Tok/s 104777 (100970)	Loss/tok 3.1041 (3.1753)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.060 (0.147)	Data 1.45e-04 (1.52e-03)	Tok/s 86470 (100790)	Loss/tok 2.4941 (3.1693)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.160 (0.147)	Data 1.25e-04 (1.46e-03)	Tok/s 104304 (100857)	Loss/tok 3.2235 (3.1719)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.159 (0.146)	Data 1.34e-04 (1.39e-03)	Tok/s 106013 (100858)	Loss/tok 3.2170 (3.1712)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.214 (0.147)	Data 1.43e-04 (1.34e-03)	Tok/s 109722 (100926)	Loss/tok 3.3984 (3.1787)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.159 (0.147)	Data 1.85e-04 (1.29e-03)	Tok/s 107411 (101009)	Loss/tok 3.1971 (3.1802)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][240/1938]	Time 0.160 (0.148)	Data 1.52e-04 (1.24e-03)	Tok/s 104002 (101110)	Loss/tok 3.2500 (3.1842)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.159 (0.148)	Data 1.56e-04 (1.20e-03)	Tok/s 103733 (101053)	Loss/tok 3.2425 (3.1824)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.159 (0.148)	Data 1.75e-04 (1.16e-03)	Tok/s 105646 (101024)	Loss/tok 3.1177 (3.1797)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.160 (0.147)	Data 1.47e-04 (1.12e-03)	Tok/s 105206 (100979)	Loss/tok 3.2736 (3.1795)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.274 (0.148)	Data 1.71e-04 (1.09e-03)	Tok/s 108677 (101037)	Loss/tok 3.5201 (3.1818)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.107 (0.147)	Data 1.21e-04 (1.05e-03)	Tok/s 95052 (101036)	Loss/tok 2.9867 (3.1803)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.108 (0.147)	Data 1.63e-04 (1.03e-03)	Tok/s 95311 (101001)	Loss/tok 2.9340 (3.1769)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.213 (0.147)	Data 1.60e-04 (9.97e-04)	Tok/s 108568 (100982)	Loss/tok 3.3347 (3.1753)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.160 (0.146)	Data 1.49e-04 (9.71e-04)	Tok/s 105417 (100893)	Loss/tok 3.1952 (3.1752)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.160 (0.145)	Data 1.47e-04 (9.46e-04)	Tok/s 104265 (100839)	Loss/tok 3.2471 (3.1731)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.107 (0.146)	Data 1.42e-04 (9.23e-04)	Tok/s 95814 (100878)	Loss/tok 2.9840 (3.1748)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.214 (0.145)	Data 1.39e-04 (9.01e-04)	Tok/s 107953 (100776)	Loss/tok 3.4278 (3.1724)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.108 (0.145)	Data 1.28e-04 (8.81e-04)	Tok/s 96474 (100761)	Loss/tok 3.0530 (3.1712)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][370/1938]	Time 0.107 (0.145)	Data 1.12e-04 (8.61e-04)	Tok/s 95718 (100762)	Loss/tok 3.0122 (3.1725)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.214 (0.145)	Data 1.22e-04 (8.43e-04)	Tok/s 108373 (100766)	Loss/tok 3.3960 (3.1739)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.107 (0.145)	Data 1.44e-04 (8.25e-04)	Tok/s 95478 (100781)	Loss/tok 2.8663 (3.1728)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.159 (0.145)	Data 1.57e-04 (8.09e-04)	Tok/s 105901 (100757)	Loss/tok 3.1427 (3.1711)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.107 (0.145)	Data 1.40e-04 (7.93e-04)	Tok/s 95531 (100760)	Loss/tok 3.0727 (3.1700)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.108 (0.145)	Data 1.21e-04 (7.78e-04)	Tok/s 96436 (100799)	Loss/tok 2.9185 (3.1728)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.160 (0.145)	Data 1.42e-04 (7.63e-04)	Tok/s 105500 (100810)	Loss/tok 3.2140 (3.1730)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.107 (0.145)	Data 1.19e-04 (7.49e-04)	Tok/s 94708 (100790)	Loss/tok 3.0225 (3.1716)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][450/1938]	Time 0.108 (0.145)	Data 1.18e-04 (7.36e-04)	Tok/s 97303 (100774)	Loss/tok 3.0162 (3.1714)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.215 (0.144)	Data 1.27e-04 (7.23e-04)	Tok/s 108983 (100746)	Loss/tok 3.3710 (3.1713)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.160 (0.145)	Data 1.40e-04 (7.12e-04)	Tok/s 105061 (100819)	Loss/tok 3.1267 (3.1731)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.108 (0.145)	Data 1.60e-04 (7.00e-04)	Tok/s 98538 (100765)	Loss/tok 3.0437 (3.1723)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.107 (0.144)	Data 1.61e-04 (6.89e-04)	Tok/s 95746 (100731)	Loss/tok 3.0007 (3.1716)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.160 (0.144)	Data 1.28e-04 (6.78e-04)	Tok/s 103131 (100665)	Loss/tok 3.2096 (3.1701)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.059 (0.143)	Data 1.24e-04 (6.67e-04)	Tok/s 90825 (100642)	Loss/tok 2.6577 (3.1692)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.273 (0.144)	Data 1.63e-04 (6.58e-04)	Tok/s 108280 (100661)	Loss/tok 3.5695 (3.1726)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.107 (0.143)	Data 1.24e-04 (6.48e-04)	Tok/s 96659 (100596)	Loss/tok 3.0692 (3.1709)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.160 (0.143)	Data 1.41e-04 (6.39e-04)	Tok/s 106376 (100597)	Loss/tok 3.1526 (3.1715)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.272 (0.143)	Data 1.48e-04 (6.31e-04)	Tok/s 109839 (100619)	Loss/tok 3.4497 (3.1725)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.107 (0.143)	Data 1.26e-04 (6.22e-04)	Tok/s 96139 (100566)	Loss/tok 2.9226 (3.1712)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][570/1938]	Time 0.060 (0.143)	Data 1.70e-04 (6.14e-04)	Tok/s 87809 (100533)	Loss/tok 2.6143 (3.1702)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.060 (0.142)	Data 1.49e-04 (6.06e-04)	Tok/s 89544 (100482)	Loss/tok 2.6171 (3.1696)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.107 (0.142)	Data 1.28e-04 (5.98e-04)	Tok/s 98234 (100482)	Loss/tok 2.9511 (3.1713)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.107 (0.142)	Data 1.24e-04 (5.91e-04)	Tok/s 95868 (100490)	Loss/tok 2.9952 (3.1710)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.160 (0.143)	Data 1.54e-04 (5.84e-04)	Tok/s 105615 (100541)	Loss/tok 3.2448 (3.1723)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.273 (0.144)	Data 1.27e-04 (5.76e-04)	Tok/s 110478 (100623)	Loss/tok 3.5653 (3.1761)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.108 (0.143)	Data 1.25e-04 (5.70e-04)	Tok/s 97129 (100616)	Loss/tok 3.0488 (3.1756)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.214 (0.143)	Data 1.20e-04 (5.63e-04)	Tok/s 108802 (100620)	Loss/tok 3.3668 (3.1761)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.212 (0.143)	Data 1.28e-04 (5.57e-04)	Tok/s 109259 (100586)	Loss/tok 3.5088 (3.1761)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][660/1938]	Time 0.160 (0.143)	Data 1.56e-04 (5.51e-04)	Tok/s 103715 (100620)	Loss/tok 3.1716 (3.1780)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.059 (0.143)	Data 1.51e-04 (5.45e-04)	Tok/s 88665 (100618)	Loss/tok 2.5747 (3.1779)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.107 (0.143)	Data 1.44e-04 (5.39e-04)	Tok/s 96996 (100623)	Loss/tok 3.0933 (3.1794)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.160 (0.144)	Data 1.22e-04 (5.34e-04)	Tok/s 105011 (100648)	Loss/tok 3.1449 (3.1795)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.107 (0.143)	Data 1.43e-04 (5.28e-04)	Tok/s 97759 (100582)	Loss/tok 3.0310 (3.1774)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.274 (0.143)	Data 1.79e-04 (5.23e-04)	Tok/s 107532 (100593)	Loss/tok 3.6152 (3.1788)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.272 (0.143)	Data 1.59e-04 (5.18e-04)	Tok/s 108542 (100590)	Loss/tok 3.3716 (3.1781)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.159 (0.143)	Data 1.24e-04 (5.13e-04)	Tok/s 104320 (100578)	Loss/tok 3.1199 (3.1771)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.107 (0.143)	Data 1.38e-04 (5.08e-04)	Tok/s 97907 (100595)	Loss/tok 3.0370 (3.1765)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.059 (0.143)	Data 1.48e-04 (5.04e-04)	Tok/s 88974 (100603)	Loss/tok 2.5639 (3.1764)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.159 (0.143)	Data 1.84e-04 (4.99e-04)	Tok/s 106310 (100622)	Loss/tok 3.2532 (3.1758)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.160 (0.143)	Data 1.25e-04 (4.95e-04)	Tok/s 106402 (100619)	Loss/tok 3.1405 (3.1746)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.160 (0.143)	Data 1.35e-04 (4.90e-04)	Tok/s 104311 (100631)	Loss/tok 3.0908 (3.1743)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][790/1938]	Time 0.107 (0.143)	Data 1.39e-04 (4.86e-04)	Tok/s 95605 (100619)	Loss/tok 3.0569 (3.1730)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.159 (0.143)	Data 1.30e-04 (4.82e-04)	Tok/s 104203 (100636)	Loss/tok 3.1815 (3.1726)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.107 (0.142)	Data 1.39e-04 (4.78e-04)	Tok/s 94778 (100602)	Loss/tok 2.9220 (3.1710)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.213 (0.143)	Data 1.47e-04 (4.74e-04)	Tok/s 108328 (100620)	Loss/tok 3.2555 (3.1716)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.213 (0.143)	Data 1.47e-04 (4.70e-04)	Tok/s 109750 (100615)	Loss/tok 3.4747 (3.1716)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.160 (0.143)	Data 1.47e-04 (4.66e-04)	Tok/s 104810 (100637)	Loss/tok 3.0956 (3.1713)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.107 (0.142)	Data 1.56e-04 (4.63e-04)	Tok/s 94655 (100598)	Loss/tok 2.8864 (3.1700)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.107 (0.142)	Data 1.84e-04 (4.59e-04)	Tok/s 96642 (100604)	Loss/tok 2.9574 (3.1704)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.108 (0.142)	Data 1.39e-04 (4.56e-04)	Tok/s 97982 (100582)	Loss/tok 2.9159 (3.1692)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.060 (0.142)	Data 1.44e-04 (4.53e-04)	Tok/s 87706 (100592)	Loss/tok 2.5738 (3.1691)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.108 (0.142)	Data 1.78e-04 (4.49e-04)	Tok/s 97405 (100607)	Loss/tok 2.8569 (3.1695)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.060 (0.142)	Data 1.88e-04 (4.46e-04)	Tok/s 88099 (100600)	Loss/tok 2.5811 (3.1690)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.061 (0.142)	Data 1.71e-04 (4.43e-04)	Tok/s 87109 (100569)	Loss/tok 2.5758 (3.1681)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][920/1938]	Time 0.108 (0.142)	Data 1.75e-04 (4.40e-04)	Tok/s 95569 (100580)	Loss/tok 2.9173 (3.1675)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.059 (0.142)	Data 1.74e-04 (4.37e-04)	Tok/s 90875 (100575)	Loss/tok 2.6282 (3.1675)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.108 (0.142)	Data 1.99e-04 (4.35e-04)	Tok/s 93683 (100589)	Loss/tok 2.9733 (3.1678)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.160 (0.142)	Data 1.66e-04 (4.32e-04)	Tok/s 104153 (100607)	Loss/tok 3.1424 (3.1680)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.107 (0.142)	Data 1.90e-04 (4.30e-04)	Tok/s 96157 (100588)	Loss/tok 2.9947 (3.1670)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.060 (0.142)	Data 1.82e-04 (4.27e-04)	Tok/s 89550 (100587)	Loss/tok 2.6342 (3.1663)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.108 (0.142)	Data 2.08e-04 (4.25e-04)	Tok/s 96825 (100578)	Loss/tok 2.8730 (3.1664)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.107 (0.142)	Data 1.90e-04 (4.22e-04)	Tok/s 96479 (100563)	Loss/tok 2.9511 (3.1656)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.160 (0.142)	Data 1.68e-04 (4.20e-04)	Tok/s 104243 (100570)	Loss/tok 3.1838 (3.1649)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.059 (0.142)	Data 2.11e-04 (4.18e-04)	Tok/s 87689 (100546)	Loss/tok 2.4694 (3.1640)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.108 (0.142)	Data 1.61e-04 (4.15e-04)	Tok/s 95343 (100561)	Loss/tok 2.9430 (3.1642)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.214 (0.142)	Data 1.65e-04 (4.13e-04)	Tok/s 110281 (100567)	Loss/tok 3.3240 (3.1642)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.107 (0.142)	Data 1.59e-04 (4.11e-04)	Tok/s 97815 (100565)	Loss/tok 2.9838 (3.1641)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1050/1938]	Time 0.213 (0.142)	Data 1.72e-04 (4.09e-04)	Tok/s 108178 (100585)	Loss/tok 3.2807 (3.1647)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.108 (0.142)	Data 1.65e-04 (4.07e-04)	Tok/s 96496 (100588)	Loss/tok 2.9858 (3.1640)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.107 (0.142)	Data 2.16e-04 (4.05e-04)	Tok/s 97257 (100590)	Loss/tok 2.9669 (3.1640)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.160 (0.142)	Data 1.65e-04 (4.03e-04)	Tok/s 105292 (100598)	Loss/tok 3.1773 (3.1635)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.108 (0.142)	Data 1.65e-04 (4.00e-04)	Tok/s 95445 (100588)	Loss/tok 2.9327 (3.1626)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.107 (0.142)	Data 2.11e-04 (3.99e-04)	Tok/s 97801 (100590)	Loss/tok 2.9871 (3.1625)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.160 (0.142)	Data 1.52e-04 (3.97e-04)	Tok/s 105675 (100580)	Loss/tok 3.1153 (3.1615)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.108 (0.142)	Data 1.55e-04 (3.95e-04)	Tok/s 96250 (100557)	Loss/tok 2.9346 (3.1612)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.107 (0.142)	Data 1.88e-04 (3.93e-04)	Tok/s 96271 (100585)	Loss/tok 2.9343 (3.1627)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.159 (0.142)	Data 1.74e-04 (3.91e-04)	Tok/s 106276 (100606)	Loss/tok 3.1146 (3.1634)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.159 (0.142)	Data 1.50e-04 (3.89e-04)	Tok/s 106479 (100605)	Loss/tok 3.1675 (3.1638)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.106 (0.142)	Data 1.79e-04 (3.87e-04)	Tok/s 96870 (100567)	Loss/tok 2.9351 (3.1630)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.108 (0.142)	Data 1.62e-04 (3.85e-04)	Tok/s 96116 (100569)	Loss/tok 2.9217 (3.1630)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1180/1938]	Time 0.107 (0.142)	Data 1.79e-04 (3.84e-04)	Tok/s 98775 (100562)	Loss/tok 2.9026 (3.1621)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.108 (0.142)	Data 1.88e-04 (3.82e-04)	Tok/s 92552 (100575)	Loss/tok 2.8802 (3.1623)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.273 (0.142)	Data 1.87e-04 (3.80e-04)	Tok/s 109175 (100582)	Loss/tok 3.5010 (3.1623)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.213 (0.143)	Data 1.90e-04 (3.79e-04)	Tok/s 108808 (100630)	Loss/tok 3.4176 (3.1643)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.160 (0.142)	Data 2.96e-04 (3.77e-04)	Tok/s 103919 (100638)	Loss/tok 3.1137 (3.1633)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.059 (0.142)	Data 1.96e-04 (3.75e-04)	Tok/s 91327 (100640)	Loss/tok 2.5568 (3.1633)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.107 (0.142)	Data 2.17e-04 (3.74e-04)	Tok/s 96251 (100632)	Loss/tok 2.9919 (3.1633)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.108 (0.142)	Data 1.82e-04 (3.72e-04)	Tok/s 95366 (100622)	Loss/tok 3.0353 (3.1632)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.160 (0.142)	Data 1.94e-04 (3.71e-04)	Tok/s 104769 (100621)	Loss/tok 3.1585 (3.1631)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.160 (0.143)	Data 1.69e-04 (3.69e-04)	Tok/s 102892 (100642)	Loss/tok 3.1163 (3.1633)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.107 (0.143)	Data 1.86e-04 (3.67e-04)	Tok/s 96763 (100642)	Loss/tok 2.9272 (3.1632)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.159 (0.143)	Data 2.16e-04 (3.66e-04)	Tok/s 105040 (100635)	Loss/tok 3.1467 (3.1628)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.108 (0.142)	Data 2.02e-04 (3.65e-04)	Tok/s 97720 (100614)	Loss/tok 2.9337 (3.1619)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1310/1938]	Time 0.108 (0.142)	Data 1.68e-04 (3.63e-04)	Tok/s 94572 (100605)	Loss/tok 2.9822 (3.1615)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.107 (0.142)	Data 1.59e-04 (3.62e-04)	Tok/s 96571 (100594)	Loss/tok 2.9123 (3.1609)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.215 (0.142)	Data 1.56e-04 (3.60e-04)	Tok/s 108208 (100618)	Loss/tok 3.3047 (3.1609)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.108 (0.142)	Data 2.09e-04 (3.59e-04)	Tok/s 95030 (100620)	Loss/tok 2.9897 (3.1607)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.107 (0.142)	Data 2.43e-04 (3.58e-04)	Tok/s 98302 (100606)	Loss/tok 2.8736 (3.1596)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.107 (0.142)	Data 2.27e-04 (3.57e-04)	Tok/s 95992 (100611)	Loss/tok 2.9854 (3.1592)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.107 (0.142)	Data 1.59e-04 (3.55e-04)	Tok/s 94724 (100616)	Loss/tok 2.9679 (3.1591)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.160 (0.142)	Data 1.56e-04 (3.54e-04)	Tok/s 105350 (100635)	Loss/tok 3.1593 (3.1591)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.108 (0.142)	Data 1.76e-04 (3.53e-04)	Tok/s 94966 (100625)	Loss/tok 2.9387 (3.1584)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.160 (0.142)	Data 1.71e-04 (3.52e-04)	Tok/s 104587 (100633)	Loss/tok 3.1680 (3.1583)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.159 (0.142)	Data 2.13e-04 (3.50e-04)	Tok/s 107191 (100623)	Loss/tok 3.0534 (3.1579)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.107 (0.142)	Data 1.73e-04 (3.49e-04)	Tok/s 96753 (100622)	Loss/tok 2.8035 (3.1576)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.107 (0.142)	Data 1.29e-04 (3.48e-04)	Tok/s 94007 (100621)	Loss/tok 3.0029 (3.1574)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1440/1938]	Time 0.107 (0.142)	Data 1.90e-04 (3.47e-04)	Tok/s 96872 (100644)	Loss/tok 2.9890 (3.1578)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.161 (0.142)	Data 1.84e-04 (3.46e-04)	Tok/s 104954 (100635)	Loss/tok 3.1332 (3.1574)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.107 (0.142)	Data 1.65e-04 (3.44e-04)	Tok/s 95128 (100634)	Loss/tok 3.0280 (3.1574)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.107 (0.142)	Data 1.73e-04 (3.43e-04)	Tok/s 96000 (100619)	Loss/tok 2.8283 (3.1569)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.159 (0.142)	Data 2.33e-04 (3.42e-04)	Tok/s 105951 (100611)	Loss/tok 3.2243 (3.1563)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.214 (0.142)	Data 1.39e-04 (3.41e-04)	Tok/s 109543 (100629)	Loss/tok 3.2356 (3.1568)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.273 (0.142)	Data 1.73e-04 (3.40e-04)	Tok/s 109060 (100613)	Loss/tok 3.4837 (3.1566)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.106 (0.142)	Data 2.32e-04 (3.39e-04)	Tok/s 96801 (100623)	Loss/tok 2.8858 (3.1566)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.108 (0.142)	Data 1.56e-04 (3.38e-04)	Tok/s 95337 (100619)	Loss/tok 3.0145 (3.1557)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.160 (0.142)	Data 2.26e-04 (3.37e-04)	Tok/s 104517 (100640)	Loss/tok 3.2327 (3.1563)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.107 (0.142)	Data 1.76e-04 (3.36e-04)	Tok/s 95809 (100631)	Loss/tok 2.9618 (3.1560)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.108 (0.142)	Data 1.63e-04 (3.35e-04)	Tok/s 94396 (100628)	Loss/tok 2.8874 (3.1552)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.160 (0.142)	Data 1.75e-04 (3.34e-04)	Tok/s 105093 (100633)	Loss/tok 3.1624 (3.1550)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1570/1938]	Time 0.213 (0.143)	Data 1.60e-04 (3.33e-04)	Tok/s 109577 (100665)	Loss/tok 3.2542 (3.1556)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.160 (0.143)	Data 1.63e-04 (3.32e-04)	Tok/s 104786 (100675)	Loss/tok 3.0231 (3.1554)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.109 (0.143)	Data 1.71e-04 (3.31e-04)	Tok/s 95976 (100667)	Loss/tok 2.9316 (3.1547)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.107 (0.143)	Data 1.77e-04 (3.30e-04)	Tok/s 96437 (100676)	Loss/tok 3.0073 (3.1548)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.108 (0.143)	Data 1.85e-04 (3.29e-04)	Tok/s 95300 (100673)	Loss/tok 2.8215 (3.1544)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.160 (0.143)	Data 1.57e-04 (3.28e-04)	Tok/s 105338 (100682)	Loss/tok 3.0959 (3.1542)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.108 (0.143)	Data 1.75e-04 (3.27e-04)	Tok/s 95157 (100664)	Loss/tok 2.8196 (3.1535)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.160 (0.142)	Data 2.49e-04 (3.26e-04)	Tok/s 104571 (100664)	Loss/tok 3.1893 (3.1537)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.160 (0.142)	Data 1.61e-04 (3.25e-04)	Tok/s 103635 (100640)	Loss/tok 3.0822 (3.1531)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.107 (0.142)	Data 2.33e-04 (3.24e-04)	Tok/s 94718 (100642)	Loss/tok 2.9144 (3.1527)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.107 (0.142)	Data 2.37e-04 (3.24e-04)	Tok/s 95910 (100651)	Loss/tok 2.9531 (3.1526)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.059 (0.142)	Data 1.72e-04 (3.23e-04)	Tok/s 92251 (100630)	Loss/tok 2.5631 (3.1518)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1690/1938]	Time 0.060 (0.142)	Data 1.67e-04 (3.22e-04)	Tok/s 86372 (100597)	Loss/tok 2.5014 (3.1509)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.107 (0.142)	Data 2.39e-04 (3.21e-04)	Tok/s 96903 (100596)	Loss/tok 2.9817 (3.1509)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.160 (0.142)	Data 2.02e-04 (3.20e-04)	Tok/s 106804 (100614)	Loss/tok 3.0000 (3.1511)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.060 (0.142)	Data 1.72e-04 (3.19e-04)	Tok/s 86538 (100600)	Loss/tok 2.5784 (3.1506)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.107 (0.142)	Data 1.86e-04 (3.18e-04)	Tok/s 98279 (100611)	Loss/tok 2.9031 (3.1504)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.159 (0.142)	Data 1.56e-04 (3.18e-04)	Tok/s 103639 (100593)	Loss/tok 3.0986 (3.1495)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.160 (0.142)	Data 1.89e-04 (3.17e-04)	Tok/s 106105 (100590)	Loss/tok 3.0523 (3.1489)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.107 (0.142)	Data 1.93e-04 (3.16e-04)	Tok/s 96402 (100584)	Loss/tok 3.0115 (3.1487)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.213 (0.142)	Data 1.46e-04 (3.15e-04)	Tok/s 108333 (100591)	Loss/tok 3.3133 (3.1483)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.108 (0.142)	Data 1.78e-04 (3.15e-04)	Tok/s 93957 (100605)	Loss/tok 2.8823 (3.1488)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.109 (0.142)	Data 2.00e-04 (3.14e-04)	Tok/s 94962 (100608)	Loss/tok 2.8772 (3.1483)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.274 (0.142)	Data 1.77e-04 (3.13e-04)	Tok/s 109401 (100609)	Loss/tok 3.3344 (3.1483)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.107 (0.142)	Data 1.67e-04 (3.12e-04)	Tok/s 96458 (100606)	Loss/tok 2.8552 (3.1477)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1820/1938]	Time 0.159 (0.142)	Data 1.71e-04 (3.12e-04)	Tok/s 107214 (100612)	Loss/tok 3.0769 (3.1473)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.108 (0.142)	Data 2.09e-04 (3.11e-04)	Tok/s 95501 (100629)	Loss/tok 2.8857 (3.1477)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.160 (0.142)	Data 2.02e-04 (3.10e-04)	Tok/s 104266 (100618)	Loss/tok 3.1206 (3.1472)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.059 (0.142)	Data 1.85e-04 (3.10e-04)	Tok/s 91012 (100600)	Loss/tok 2.5269 (3.1466)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.107 (0.142)	Data 1.83e-04 (3.09e-04)	Tok/s 93826 (100618)	Loss/tok 2.9304 (3.1468)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.059 (0.142)	Data 1.85e-04 (3.08e-04)	Tok/s 88590 (100597)	Loss/tok 2.4480 (3.1463)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.108 (0.142)	Data 1.73e-04 (3.08e-04)	Tok/s 94508 (100614)	Loss/tok 2.9087 (3.1465)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.107 (0.142)	Data 1.96e-04 (3.07e-04)	Tok/s 97978 (100618)	Loss/tok 2.9533 (3.1463)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.214 (0.142)	Data 1.56e-04 (3.06e-04)	Tok/s 108719 (100612)	Loss/tok 3.2421 (3.1461)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.108 (0.142)	Data 1.80e-04 (3.06e-04)	Tok/s 95053 (100607)	Loss/tok 2.9540 (3.1459)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.108 (0.142)	Data 1.62e-04 (3.05e-04)	Tok/s 96627 (100618)	Loss/tok 2.9038 (3.1460)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.160 (0.142)	Data 1.68e-04 (3.05e-04)	Tok/s 104554 (100632)	Loss/tok 3.0299 (3.1458)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020256966, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020256966, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.617 (0.617)	Decoder iters 99.0 (99.0)	Tok/s 26677 (26677)
0: Running moses detokenizer
0: BLEU(score=24.14762350046795, counts=[37145, 18680, 10709, 6408], totals=[65769, 62766, 59763, 56764], precisions=[56.477975946114434, 29.761335755026607, 17.91911383297358, 11.288845042632655], bp=1.0, sys_len=65769, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020258786, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2415, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020258787, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1466	Test BLEU: 24.15
0: Performance: Epoch: 3	Training: 804837 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020258787, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020258787, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:37:43 AM
RESULT,RNN_TRANSLATOR,,1135,nvidia,2020-06-24 10:18:48 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
ENDING TIMING RUN AT 2020-06-24 10:37:44 AM
RESULT,RNN_TRANSLATOR,,1136,nvidia,2020-06-24 10:18:48 AM
