+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ srun --ntasks=1 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019160849, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593019160885, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593019160885, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593019160885, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593019160886, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xNVIDIA DGX-1", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: Job 467819 step creation temporarily disabled, retrying
srun: Step created for job 467819
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on sc-sdgx-783
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593019167256, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=8 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/home/svcnvdlfw/logs/14177108/results:/results ./run_and_time.sh
srun: Job 467819 step creation temporarily disabled, retrying
srun: Step created for job 467819
slurmstepd: pyxis: reusing existing container
slurmstepd: pyxis: running "enroot start" ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ '[' -n 2 ']'
+ '[' 8 -gt 1 ']'
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ '[' -n 0 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
running benchmark
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
running benchmark
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-24 10:19:29 AM
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ '[' -n 6 ']'
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 8 -gt 1 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 1    --dwu-num-chunks 2    --dwu-num-rs-pg 2    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ '[' 8 -gt 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX1 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=35-39,75-79 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=15-19,55-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
num_sockets = 2 num_nodes=2 cores_per_socket=20
num_sockets = 2 num_nodes=2 cores_per_socket=20
+ exec numactl --physcpubind=20-24,60-64 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=10-14,50-54 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=0-4,40-44 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=5-9,45-49 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=30-34,70-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
+ exec numactl --physcpubind=25-29,65-69 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --distributed-weight-update 2 --dwu-num-blocks 1 --dwu-num-chunks 2 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-grad-norm --fused-attention --fused-xentropy --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593019171457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171459, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171467, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171475, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593019171483, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=1, dwu_num_chunks=2, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2501100494
:::MLLOG {"namespace": "", "time_ms": 1593019177417, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2501100494, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2449280114
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
self._net_total_param_size=160671360, self._total_param_size=160673792, dwu_min_page_size=4096, self._block_size=160673792, self._chunk_size=80336896, self._shard_size=10042112
[0]
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([3131392]), new_param_packed_fragment.size()=torch.Size([3131392]), master_param_fragment.size()=torch.Size([3131392])
model_param_fragment.size()=torch.Size([1931840]), new_param_packed_fragment.size()=torch.Size([1931840]), master_param_fragment.size()=torch.Size([1931840])
model_param_fragment.size()=torch.Size([2219520]), new_param_packed_fragment.size()=torch.Size([2219520]), master_param_fragment.size()=torch.Size([2219520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8102080]), new_param_packed_fragment.size()=torch.Size([8102080]), master_param_fragment.size()=torch.Size([8102080])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3620096]), new_param_packed_fragment.size()=torch.Size([3620096]), master_param_fragment.size()=torch.Size([3620096])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([608960]), new_param_packed_fragment.size()=torch.Size([608960]), master_param_fragment.size()=torch.Size([608960])
model_param_fragment.size()=torch.Size([1315840]), new_param_packed_fragment.size()=torch.Size([1315840]), master_param_fragment.size()=torch.Size([1315840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([321280]), new_param_packed_fragment.size()=torch.Size([321280]), master_param_fragment.size()=torch.Size([321280])
model_param_fragment.size()=torch.Size([2835520]), new_param_packed_fragment.size()=torch.Size([2835520]), master_param_fragment.size()=torch.Size([2835520])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3004096]), new_param_packed_fragment.size()=torch.Size([3004096]), master_param_fragment.size()=torch.Size([3004096])
model_param_fragment.size()=torch.Size([574208]), new_param_packed_fragment.size()=torch.Size([574208]), master_param_fragment.size()=torch.Size([574208])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([8067328]), new_param_packed_fragment.size()=torch.Size([8067328]), master_param_fragment.size()=torch.Size([8067328])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1062912]), new_param_packed_fragment.size()=torch.Size([1062912]), master_param_fragment.size()=torch.Size([1062912])
model_param_fragment.size()=torch.Size([1974784]), new_param_packed_fragment.size()=torch.Size([1974784]), master_param_fragment.size()=torch.Size([1974784])
model_param_fragment.size()=torch.Size([10007360]), new_param_packed_fragment.size()=torch.Size([10007360]), master_param_fragment.size()=torch.Size([10007360])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([2969344]), new_param_packed_fragment.size()=torch.Size([2969344]), master_param_fragment.size()=torch.Size([2969344])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([2878464]), new_param_packed_fragment.size()=torch.Size([2878464]), master_param_fragment.size()=torch.Size([2878464])
model_param_fragment.size()=torch.Size([286528]), new_param_packed_fragment.size()=torch.Size([286528]), master_param_fragment.size()=torch.Size([286528])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([5553088]), new_param_packed_fragment.size()=torch.Size([5553088]), master_param_fragment.size()=torch.Size([5553088])
model_param_fragment.size()=torch.Size([10042112]), new_param_packed_fragment.size()=torch.Size([10042112]), master_param_fragment.size()=torch.Size([10042112])
model_param_fragment.size()=torch.Size([7779648]), new_param_packed_fragment.size()=torch.Size([7779648]), master_param_fragment.size()=torch.Size([7779648])
model_param_fragment.size()=torch.Size([2262464]), new_param_packed_fragment.size()=torch.Size([2262464]), master_param_fragment.size()=torch.Size([2262464])
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593019185499, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593019185499, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593019185500, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593019185500, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593019185500, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593019187100, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593019187101, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593019187101, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593019187355, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2048, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593019187356, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3969024, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593019187356, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593019187357, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593019187357, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593019187358, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 809, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593019187358, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593019187358, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593019187358, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 6453, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593019187358, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593019187358, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019187359, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364850639
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1938]	Time 0.407 (0.407)	Data 3.01e-01 (3.01e-01)	Tok/s 25818 (25818)	Loss/tok 10.6491 (10.6491)	LR 2.047e-05
0: TRAIN [0][10/1938]	Time 0.104 (0.161)	Data 1.85e-04 (2.75e-02)	Tok/s 100536 (96148)	Loss/tok 9.5419 (10.1064)	LR 2.576e-05
0: TRAIN [0][20/1938]	Time 0.105 (0.147)	Data 1.55e-04 (1.45e-02)	Tok/s 96681 (98799)	Loss/tok 9.2008 (9.7925)	LR 3.244e-05
0: TRAIN [0][30/1938]	Time 0.106 (0.137)	Data 1.23e-04 (9.88e-03)	Tok/s 98514 (99034)	Loss/tok 8.9720 (9.5885)	LR 4.083e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1938]	Time 0.209 (0.141)	Data 1.50e-04 (7.51e-03)	Tok/s 112006 (100706)	Loss/tok 8.8579 (9.3964)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.156 (0.146)	Data 1.44e-04 (6.07e-03)	Tok/s 107454 (102009)	Loss/tok 8.5686 (9.2372)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.106 (0.142)	Data 1.24e-04 (5.10e-03)	Tok/s 98974 (101937)	Loss/tok 8.2608 (9.1244)	LR 7.962e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][70/1938]	Time 0.157 (0.144)	Data 1.53e-04 (4.40e-03)	Tok/s 108407 (102533)	Loss/tok 8.2149 (8.9962)	LR 9.796e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][80/1938]	Time 0.105 (0.141)	Data 1.64e-04 (3.88e-03)	Tok/s 100407 (102241)	Loss/tok 8.0835 (8.9204)	LR 1.205e-04
0: TRAIN [0][90/1938]	Time 0.105 (0.137)	Data 1.69e-04 (3.47e-03)	Tok/s 98627 (101798)	Loss/tok 7.8290 (8.8425)	LR 1.517e-04
0: TRAIN [0][100/1938]	Time 0.105 (0.137)	Data 1.69e-04 (3.14e-03)	Tok/s 98475 (101979)	Loss/tok 7.8413 (8.7590)	LR 1.910e-04
0: TRAIN [0][110/1938]	Time 0.157 (0.136)	Data 1.37e-04 (2.88e-03)	Tok/s 107670 (101974)	Loss/tok 7.8748 (8.6856)	LR 2.405e-04
0: TRAIN [0][120/1938]	Time 0.268 (0.139)	Data 1.68e-04 (2.65e-03)	Tok/s 112091 (102297)	Loss/tok 8.1803 (8.6112)	LR 3.027e-04
0: TRAIN [0][130/1938]	Time 0.157 (0.141)	Data 1.71e-04 (2.46e-03)	Tok/s 108080 (102497)	Loss/tok 8.0325 (8.5569)	LR 3.811e-04
0: TRAIN [0][140/1938]	Time 0.156 (0.141)	Data 1.87e-04 (2.30e-03)	Tok/s 104901 (102476)	Loss/tok 7.8713 (8.5099)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.105 (0.140)	Data 1.83e-04 (2.16e-03)	Tok/s 97677 (102469)	Loss/tok 7.6581 (8.4643)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.106 (0.138)	Data 1.97e-04 (2.03e-03)	Tok/s 99158 (102311)	Loss/tok 7.5663 (8.4233)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.105 (0.137)	Data 1.43e-04 (1.93e-03)	Tok/s 96943 (102098)	Loss/tok 7.4638 (8.3814)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.106 (0.137)	Data 1.74e-04 (1.83e-03)	Tok/s 99400 (102084)	Loss/tok 7.3773 (8.3358)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.159 (0.137)	Data 1.79e-04 (1.74e-03)	Tok/s 104428 (102093)	Loss/tok 7.2970 (8.2847)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.158 (0.136)	Data 1.66e-04 (1.66e-03)	Tok/s 105416 (101980)	Loss/tok 7.1110 (8.2331)	LR 1.910e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][210/1938]	Time 0.105 (0.135)	Data 1.96e-04 (1.59e-03)	Tok/s 98819 (101813)	Loss/tok 6.7492 (8.1792)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.106 (0.135)	Data 1.83e-04 (1.53e-03)	Tok/s 98487 (101806)	Loss/tok 6.6238 (8.1206)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.159 (0.135)	Data 1.90e-04 (1.47e-03)	Tok/s 106901 (101733)	Loss/tok 6.7381 (8.0634)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.210 (0.134)	Data 1.48e-04 (1.42e-03)	Tok/s 109945 (101639)	Loss/tok 6.7839 (8.0078)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.158 (0.134)	Data 2.10e-04 (1.37e-03)	Tok/s 105640 (101527)	Loss/tok 6.5954 (7.9564)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.107 (0.133)	Data 1.74e-04 (1.32e-03)	Tok/s 96556 (101440)	Loss/tok 6.2322 (7.9031)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.158 (0.134)	Data 1.45e-04 (1.28e-03)	Tok/s 107036 (101597)	Loss/tok 6.2606 (7.8304)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.106 (0.134)	Data 1.95e-04 (1.24e-03)	Tok/s 97369 (101578)	Loss/tok 5.8776 (7.7721)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.107 (0.134)	Data 1.65e-04 (1.20e-03)	Tok/s 95808 (101621)	Loss/tok 5.7597 (7.7099)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.211 (0.135)	Data 1.62e-04 (1.17e-03)	Tok/s 110253 (101632)	Loss/tok 6.2304 (7.6487)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.106 (0.135)	Data 1.71e-04 (1.13e-03)	Tok/s 97311 (101690)	Loss/tok 5.5703 (7.5828)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.159 (0.136)	Data 1.62e-04 (1.10e-03)	Tok/s 105197 (101694)	Loss/tok 5.8961 (7.5261)	LR 2.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][330/1938]	Time 0.106 (0.135)	Data 1.69e-04 (1.08e-03)	Tok/s 96464 (101655)	Loss/tok 5.2930 (7.4708)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.107 (0.137)	Data 1.45e-04 (1.05e-03)	Tok/s 94263 (101720)	Loss/tok 5.3414 (7.4043)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.106 (0.136)	Data 1.60e-04 (1.02e-03)	Tok/s 100096 (101677)	Loss/tok 5.3648 (7.3554)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.107 (0.136)	Data 1.68e-04 (1.00e-03)	Tok/s 98396 (101690)	Loss/tok 5.2000 (7.3017)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.059 (0.136)	Data 1.63e-04 (9.78e-04)	Tok/s 88418 (101599)	Loss/tok 4.2134 (7.2542)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.108 (0.136)	Data 1.57e-04 (9.57e-04)	Tok/s 96661 (101566)	Loss/tok 4.8362 (7.2027)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.107 (0.136)	Data 1.78e-04 (9.37e-04)	Tok/s 96857 (101552)	Loss/tok 4.8782 (7.1513)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.158 (0.136)	Data 1.82e-04 (9.17e-04)	Tok/s 106487 (101583)	Loss/tok 5.1698 (7.0928)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.059 (0.136)	Data 2.07e-04 (8.99e-04)	Tok/s 89274 (101543)	Loss/tok 3.8791 (7.0472)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.107 (0.136)	Data 1.57e-04 (8.82e-04)	Tok/s 97256 (101534)	Loss/tok 4.6822 (6.9972)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.059 (0.136)	Data 1.65e-04 (8.65e-04)	Tok/s 92019 (101577)	Loss/tok 3.8107 (6.9447)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.107 (0.136)	Data 1.76e-04 (8.49e-04)	Tok/s 98944 (101556)	Loss/tok 4.6232 (6.8990)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.107 (0.136)	Data 1.69e-04 (8.34e-04)	Tok/s 96396 (101582)	Loss/tok 4.4695 (6.8481)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][460/1938]	Time 0.159 (0.137)	Data 1.30e-04 (8.19e-04)	Tok/s 106311 (101656)	Loss/tok 4.6817 (6.7932)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.159 (0.138)	Data 1.65e-04 (8.05e-04)	Tok/s 106124 (101687)	Loss/tok 4.6194 (6.7440)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.106 (0.137)	Data 1.47e-04 (7.92e-04)	Tok/s 98775 (101644)	Loss/tok 4.2731 (6.7018)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][490/1938]	Time 0.212 (0.138)	Data 1.71e-04 (7.80e-04)	Tok/s 110840 (101663)	Loss/tok 4.9092 (6.6544)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.107 (0.139)	Data 1.63e-04 (7.67e-04)	Tok/s 96752 (101758)	Loss/tok 4.1727 (6.5993)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.160 (0.139)	Data 1.44e-04 (7.55e-04)	Tok/s 105115 (101727)	Loss/tok 4.5604 (6.5598)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.059 (0.139)	Data 1.70e-04 (7.44e-04)	Tok/s 89421 (101723)	Loss/tok 3.5171 (6.5169)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.107 (0.138)	Data 1.78e-04 (7.33e-04)	Tok/s 95404 (101656)	Loss/tok 4.1475 (6.4824)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.059 (0.139)	Data 1.66e-04 (7.23e-04)	Tok/s 87843 (101699)	Loss/tok 3.4099 (6.4398)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.107 (0.139)	Data 1.34e-04 (7.13e-04)	Tok/s 95901 (101714)	Loss/tok 3.9587 (6.3988)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.158 (0.139)	Data 1.74e-04 (7.03e-04)	Tok/s 105433 (101710)	Loss/tok 4.3203 (6.3622)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.059 (0.139)	Data 1.77e-04 (6.94e-04)	Tok/s 91281 (101678)	Loss/tok 3.4101 (6.3274)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.159 (0.138)	Data 1.54e-04 (6.84e-04)	Tok/s 105782 (101598)	Loss/tok 4.2882 (6.2989)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.158 (0.138)	Data 1.55e-04 (6.76e-04)	Tok/s 106523 (101613)	Loss/tok 4.2703 (6.2628)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.106 (0.138)	Data 1.67e-04 (6.67e-04)	Tok/s 97280 (101635)	Loss/tok 3.9601 (6.2273)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.159 (0.139)	Data 1.72e-04 (6.59e-04)	Tok/s 104891 (101666)	Loss/tok 4.1141 (6.1897)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][620/1938]	Time 0.159 (0.139)	Data 1.83e-04 (6.51e-04)	Tok/s 106045 (101721)	Loss/tok 4.2003 (6.1511)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.211 (0.139)	Data 1.96e-04 (6.43e-04)	Tok/s 109466 (101730)	Loss/tok 4.5258 (6.1185)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.106 (0.140)	Data 1.65e-04 (6.36e-04)	Tok/s 97341 (101735)	Loss/tok 3.7812 (6.0874)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.211 (0.140)	Data 1.62e-04 (6.29e-04)	Tok/s 110515 (101787)	Loss/tok 4.3330 (6.0513)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.106 (0.140)	Data 2.06e-04 (6.22e-04)	Tok/s 95036 (101806)	Loss/tok 3.7966 (6.0197)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.107 (0.141)	Data 1.57e-04 (6.15e-04)	Tok/s 95717 (101793)	Loss/tok 3.7875 (5.9913)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.107 (0.140)	Data 1.66e-04 (6.09e-04)	Tok/s 97126 (101731)	Loss/tok 3.8016 (5.9681)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.214 (0.141)	Data 1.80e-04 (6.02e-04)	Tok/s 108821 (101769)	Loss/tok 4.4439 (5.9355)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.272 (0.141)	Data 1.62e-04 (5.96e-04)	Tok/s 110638 (101769)	Loss/tok 4.5882 (5.9091)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.107 (0.141)	Data 1.67e-04 (5.90e-04)	Tok/s 95455 (101782)	Loss/tok 3.7429 (5.8821)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.158 (0.141)	Data 2.17e-04 (5.85e-04)	Tok/s 108348 (101790)	Loss/tok 4.0499 (5.8558)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.107 (0.141)	Data 1.84e-04 (5.79e-04)	Tok/s 96517 (101799)	Loss/tok 3.7482 (5.8293)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.159 (0.141)	Data 1.66e-04 (5.73e-04)	Tok/s 105657 (101734)	Loss/tok 4.1152 (5.8096)	LR 2.000e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][750/1938]	Time 0.107 (0.140)	Data 1.67e-04 (5.68e-04)	Tok/s 95540 (101682)	Loss/tok 3.8193 (5.7892)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.106 (0.140)	Data 1.69e-04 (5.63e-04)	Tok/s 98442 (101675)	Loss/tok 3.6679 (5.7664)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.106 (0.140)	Data 1.51e-04 (5.58e-04)	Tok/s 97214 (101649)	Loss/tok 3.8313 (5.7446)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.107 (0.140)	Data 1.91e-04 (5.53e-04)	Tok/s 98728 (101646)	Loss/tok 3.7407 (5.7218)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.272 (0.140)	Data 1.76e-04 (5.48e-04)	Tok/s 108565 (101596)	Loss/tok 4.5064 (5.7031)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.106 (0.140)	Data 1.71e-04 (5.44e-04)	Tok/s 97122 (101571)	Loss/tok 3.7566 (5.6821)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.107 (0.140)	Data 1.67e-04 (5.39e-04)	Tok/s 96100 (101533)	Loss/tok 3.5520 (5.6623)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.107 (0.140)	Data 1.67e-04 (5.34e-04)	Tok/s 93924 (101519)	Loss/tok 3.5183 (5.6407)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.159 (0.140)	Data 1.99e-04 (5.30e-04)	Tok/s 105005 (101523)	Loss/tok 3.9397 (5.6201)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.107 (0.140)	Data 1.75e-04 (5.26e-04)	Tok/s 97321 (101538)	Loss/tok 3.7286 (5.5990)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.108 (0.139)	Data 1.83e-04 (5.22e-04)	Tok/s 95886 (101488)	Loss/tok 3.7831 (5.5833)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.106 (0.139)	Data 1.65e-04 (5.17e-04)	Tok/s 97789 (101506)	Loss/tok 3.8492 (5.5628)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.107 (0.140)	Data 1.96e-04 (5.13e-04)	Tok/s 97503 (101548)	Loss/tok 3.5614 (5.5406)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][880/1938]	Time 0.107 (0.140)	Data 2.01e-04 (5.10e-04)	Tok/s 97898 (101530)	Loss/tok 3.6313 (5.5235)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][890/1938]	Time 0.158 (0.140)	Data 1.64e-04 (5.06e-04)	Tok/s 107138 (101536)	Loss/tok 3.9047 (5.5045)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.108 (0.140)	Data 1.78e-04 (5.02e-04)	Tok/s 97164 (101528)	Loss/tok 3.4936 (5.4868)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.108 (0.140)	Data 1.60e-04 (4.99e-04)	Tok/s 95702 (101552)	Loss/tok 3.6855 (5.4678)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.060 (0.140)	Data 2.06e-04 (4.95e-04)	Tok/s 87114 (101542)	Loss/tok 3.1243 (5.4505)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.060 (0.140)	Data 1.75e-04 (4.92e-04)	Tok/s 87388 (101554)	Loss/tok 3.0082 (5.4314)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.159 (0.140)	Data 2.12e-04 (4.88e-04)	Tok/s 106373 (101547)	Loss/tok 3.7541 (5.4151)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.059 (0.140)	Data 1.48e-04 (4.85e-04)	Tok/s 88775 (101542)	Loss/tok 2.9429 (5.3982)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.160 (0.140)	Data 1.65e-04 (4.82e-04)	Tok/s 103727 (101516)	Loss/tok 3.8360 (5.3833)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.107 (0.140)	Data 1.88e-04 (4.79e-04)	Tok/s 97817 (101505)	Loss/tok 3.7399 (5.3683)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.107 (0.140)	Data 1.42e-04 (4.76e-04)	Tok/s 96696 (101514)	Loss/tok 3.6909 (5.3519)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.107 (0.140)	Data 2.11e-04 (4.73e-04)	Tok/s 97385 (101529)	Loss/tok 3.6431 (5.3355)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.159 (0.140)	Data 2.03e-04 (4.70e-04)	Tok/s 104989 (101509)	Loss/tok 3.9648 (5.3218)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.158 (0.140)	Data 2.06e-04 (4.67e-04)	Tok/s 106766 (101502)	Loss/tok 3.8048 (5.3071)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1020/1938]	Time 0.107 (0.140)	Data 1.84e-04 (4.64e-04)	Tok/s 96313 (101491)	Loss/tok 3.5334 (5.2930)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.160 (0.140)	Data 1.73e-04 (4.61e-04)	Tok/s 104565 (101482)	Loss/tok 3.7596 (5.2791)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.159 (0.140)	Data 2.40e-04 (4.58e-04)	Tok/s 106307 (101506)	Loss/tok 3.9277 (5.2634)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.107 (0.140)	Data 1.99e-04 (4.55e-04)	Tok/s 94484 (101479)	Loss/tok 3.4501 (5.2505)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1060/1938]	Time 0.107 (0.140)	Data 2.00e-04 (4.53e-04)	Tok/s 97305 (101497)	Loss/tok 3.5534 (5.2355)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.161 (0.140)	Data 1.45e-04 (4.50e-04)	Tok/s 103175 (101515)	Loss/tok 3.7524 (5.2203)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.212 (0.140)	Data 1.64e-04 (4.48e-04)	Tok/s 111425 (101499)	Loss/tok 3.9741 (5.2079)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.272 (0.140)	Data 1.66e-04 (4.45e-04)	Tok/s 107931 (101524)	Loss/tok 4.2225 (5.1928)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.159 (0.140)	Data 1.76e-04 (4.43e-04)	Tok/s 103293 (101529)	Loss/tok 3.9420 (5.1795)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.159 (0.140)	Data 1.78e-04 (4.40e-04)	Tok/s 106978 (101533)	Loss/tok 3.7224 (5.1666)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.160 (0.141)	Data 2.01e-04 (4.38e-04)	Tok/s 103971 (101563)	Loss/tok 3.7831 (5.1523)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.159 (0.141)	Data 1.40e-04 (4.36e-04)	Tok/s 104827 (101556)	Loss/tok 3.8122 (5.1396)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.060 (0.140)	Data 1.25e-04 (4.33e-04)	Tok/s 85488 (101523)	Loss/tok 2.9744 (5.1291)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.107 (0.140)	Data 1.42e-04 (4.31e-04)	Tok/s 95952 (101514)	Loss/tok 3.4912 (5.1174)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.107 (0.141)	Data 1.28e-04 (4.29e-04)	Tok/s 97497 (101532)	Loss/tok 3.4888 (5.1043)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.107 (0.141)	Data 1.90e-04 (4.26e-04)	Tok/s 96467 (101513)	Loss/tok 3.3949 (5.0932)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1180/1938]	Time 0.158 (0.141)	Data 1.71e-04 (4.24e-04)	Tok/s 107028 (101513)	Loss/tok 3.6149 (5.0812)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.107 (0.140)	Data 1.42e-04 (4.22e-04)	Tok/s 94435 (101488)	Loss/tok 3.4335 (5.0708)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.160 (0.140)	Data 1.24e-04 (4.20e-04)	Tok/s 103861 (101470)	Loss/tok 3.8268 (5.0602)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.108 (0.140)	Data 1.43e-04 (4.18e-04)	Tok/s 95399 (101437)	Loss/tok 3.4119 (5.0500)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.160 (0.140)	Data 2.19e-04 (4.16e-04)	Tok/s 102451 (101420)	Loss/tok 3.7828 (5.0401)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.160 (0.140)	Data 1.64e-04 (4.14e-04)	Tok/s 104841 (101419)	Loss/tok 3.6922 (5.0292)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.107 (0.140)	Data 1.67e-04 (4.12e-04)	Tok/s 97376 (101418)	Loss/tok 3.4367 (5.0181)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.107 (0.140)	Data 1.43e-04 (4.10e-04)	Tok/s 95579 (101399)	Loss/tok 3.3607 (5.0084)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.108 (0.140)	Data 1.73e-04 (4.08e-04)	Tok/s 96545 (101384)	Loss/tok 3.4378 (4.9985)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.107 (0.140)	Data 1.30e-04 (4.06e-04)	Tok/s 97436 (101384)	Loss/tok 3.4767 (4.9882)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.107 (0.140)	Data 1.86e-04 (4.04e-04)	Tok/s 97917 (101372)	Loss/tok 3.5097 (4.9788)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.059 (0.140)	Data 1.69e-04 (4.02e-04)	Tok/s 91315 (101378)	Loss/tok 2.8902 (4.9680)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.272 (0.140)	Data 1.68e-04 (4.00e-04)	Tok/s 109571 (101385)	Loss/tok 4.0754 (4.9576)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1310/1938]	Time 0.159 (0.140)	Data 1.65e-04 (3.99e-04)	Tok/s 105803 (101388)	Loss/tok 3.7282 (4.9476)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.213 (0.141)	Data 1.20e-04 (3.97e-04)	Tok/s 109302 (101389)	Loss/tok 3.8282 (4.9374)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.159 (0.141)	Data 1.43e-04 (3.95e-04)	Tok/s 106894 (101397)	Loss/tok 3.8560 (4.9275)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1340/1938]	Time 0.160 (0.141)	Data 1.37e-04 (3.93e-04)	Tok/s 105002 (101415)	Loss/tok 3.6802 (4.9171)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.159 (0.141)	Data 1.44e-04 (3.91e-04)	Tok/s 104861 (101426)	Loss/tok 3.5276 (4.9072)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.161 (0.141)	Data 1.26e-04 (3.89e-04)	Tok/s 105973 (101460)	Loss/tok 3.6918 (4.8964)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.159 (0.141)	Data 1.61e-04 (3.88e-04)	Tok/s 106359 (101470)	Loss/tok 3.7215 (4.8868)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.159 (0.141)	Data 1.27e-04 (3.86e-04)	Tok/s 106041 (101462)	Loss/tok 3.7362 (4.8784)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.059 (0.141)	Data 1.35e-04 (3.84e-04)	Tok/s 86985 (101459)	Loss/tok 2.8313 (4.8692)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.107 (0.141)	Data 1.43e-04 (3.82e-04)	Tok/s 96908 (101458)	Loss/tok 3.5092 (4.8605)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.107 (0.141)	Data 1.26e-04 (3.81e-04)	Tok/s 97404 (101461)	Loss/tok 3.3887 (4.8518)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1420/1938]	Time 0.059 (0.141)	Data 1.23e-04 (3.79e-04)	Tok/s 88461 (101449)	Loss/tok 2.8643 (4.8433)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.059 (0.141)	Data 1.48e-04 (3.77e-04)	Tok/s 89288 (101444)	Loss/tok 2.9247 (4.8352)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.211 (0.141)	Data 1.25e-04 (3.76e-04)	Tok/s 111850 (101439)	Loss/tok 3.8063 (4.8270)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.212 (0.141)	Data 1.26e-04 (3.74e-04)	Tok/s 108846 (101426)	Loss/tok 3.9943 (4.8192)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.160 (0.141)	Data 1.62e-04 (3.73e-04)	Tok/s 105361 (101402)	Loss/tok 3.6887 (4.8121)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.059 (0.141)	Data 1.44e-04 (3.71e-04)	Tok/s 88624 (101409)	Loss/tok 2.8267 (4.8039)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.107 (0.141)	Data 1.77e-04 (3.70e-04)	Tok/s 97042 (101413)	Loss/tok 3.4131 (4.7957)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.213 (0.141)	Data 1.43e-04 (3.68e-04)	Tok/s 108750 (101432)	Loss/tok 3.7840 (4.7866)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.107 (0.141)	Data 1.60e-04 (3.67e-04)	Tok/s 97170 (101410)	Loss/tok 3.2718 (4.7799)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.273 (0.141)	Data 2.29e-04 (3.65e-04)	Tok/s 110274 (101451)	Loss/tok 3.9101 (4.7702)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.273 (0.141)	Data 1.20e-04 (3.64e-04)	Tok/s 109079 (101422)	Loss/tok 4.0330 (4.7639)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.159 (0.141)	Data 1.26e-04 (3.62e-04)	Tok/s 104902 (101420)	Loss/tok 3.6645 (4.7563)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.107 (0.141)	Data 1.49e-04 (3.61e-04)	Tok/s 94290 (101404)	Loss/tok 3.2926 (4.7491)	LR 2.000e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1550/1938]	Time 0.107 (0.141)	Data 1.64e-04 (3.59e-04)	Tok/s 97049 (101400)	Loss/tok 3.4426 (4.7418)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.107 (0.141)	Data 1.63e-04 (3.58e-04)	Tok/s 97787 (101391)	Loss/tok 3.3062 (4.7350)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.107 (0.141)	Data 1.55e-04 (3.57e-04)	Tok/s 96921 (101381)	Loss/tok 3.3270 (4.7281)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.213 (0.141)	Data 1.58e-04 (3.56e-04)	Tok/s 110601 (101374)	Loss/tok 3.8533 (4.7210)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.106 (0.141)	Data 1.49e-04 (3.54e-04)	Tok/s 96165 (101378)	Loss/tok 3.2819 (4.7139)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.160 (0.141)	Data 1.71e-04 (3.53e-04)	Tok/s 105440 (101377)	Loss/tok 3.6818 (4.7073)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.160 (0.141)	Data 1.68e-04 (3.52e-04)	Tok/s 104821 (101361)	Loss/tok 3.5854 (4.7009)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.107 (0.141)	Data 2.09e-04 (3.51e-04)	Tok/s 96380 (101342)	Loss/tok 3.3184 (4.6945)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.159 (0.141)	Data 1.58e-04 (3.50e-04)	Tok/s 107118 (101363)	Loss/tok 3.5187 (4.6867)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.160 (0.141)	Data 1.74e-04 (3.49e-04)	Tok/s 104277 (101367)	Loss/tok 3.5927 (4.6799)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.160 (0.141)	Data 1.43e-04 (3.48e-04)	Tok/s 104448 (101395)	Loss/tok 3.5381 (4.6723)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.107 (0.141)	Data 1.30e-04 (3.47e-04)	Tok/s 97753 (101380)	Loss/tok 3.4317 (4.6666)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.46e-04)	Tok/s 95204 (101368)	Loss/tok 3.4076 (4.6604)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [0][1680/1938]	Time 0.160 (0.141)	Data 1.19e-04 (3.45e-04)	Tok/s 104397 (101349)	Loss/tok 3.6134 (4.6545)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.059 (0.141)	Data 1.24e-04 (3.43e-04)	Tok/s 87658 (101334)	Loss/tok 2.8154 (4.6485)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.107 (0.141)	Data 1.54e-04 (3.42e-04)	Tok/s 96817 (101319)	Loss/tok 3.3982 (4.6424)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.107 (0.141)	Data 1.24e-04 (3.41e-04)	Tok/s 95919 (101310)	Loss/tok 3.2466 (4.6366)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.107 (0.141)	Data 1.26e-04 (3.40e-04)	Tok/s 96322 (101299)	Loss/tok 3.2690 (4.6306)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.108 (0.141)	Data 1.88e-04 (3.39e-04)	Tok/s 95275 (101286)	Loss/tok 3.2379 (4.6248)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.107 (0.141)	Data 1.38e-04 (3.38e-04)	Tok/s 95737 (101261)	Loss/tok 3.3548 (4.6195)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.213 (0.141)	Data 1.28e-04 (3.37e-04)	Tok/s 108968 (101253)	Loss/tok 3.9206 (4.6137)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.108 (0.141)	Data 1.63e-04 (3.36e-04)	Tok/s 95425 (101253)	Loss/tok 3.3741 (4.6074)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.107 (0.141)	Data 1.54e-04 (3.35e-04)	Tok/s 96988 (101254)	Loss/tok 3.3082 (4.6013)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.159 (0.141)	Data 1.32e-04 (3.34e-04)	Tok/s 105412 (101253)	Loss/tok 3.5199 (4.5955)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.158 (0.141)	Data 1.85e-04 (3.32e-04)	Tok/s 106417 (101243)	Loss/tok 3.4833 (4.5897)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.107 (0.141)	Data 1.44e-04 (3.31e-04)	Tok/s 96569 (101248)	Loss/tok 3.2971 (4.5836)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [0][1810/1938]	Time 0.108 (0.141)	Data 1.38e-04 (3.30e-04)	Tok/s 95061 (101255)	Loss/tok 3.2292 (4.5776)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.160 (0.141)	Data 1.72e-04 (3.29e-04)	Tok/s 104252 (101270)	Loss/tok 3.5602 (4.5714)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.159 (0.141)	Data 1.49e-04 (3.28e-04)	Tok/s 103953 (101270)	Loss/tok 3.5983 (4.5657)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.274 (0.141)	Data 1.33e-04 (3.27e-04)	Tok/s 110071 (101277)	Loss/tok 3.8804 (4.5597)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.160 (0.141)	Data 1.48e-04 (3.27e-04)	Tok/s 104713 (101273)	Loss/tok 3.5990 (4.5544)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.107 (0.141)	Data 1.39e-04 (3.26e-04)	Tok/s 95891 (101283)	Loss/tok 3.4113 (4.5484)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.159 (0.141)	Data 1.64e-04 (3.25e-04)	Tok/s 105556 (101298)	Loss/tok 3.6059 (4.5425)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.161 (0.141)	Data 1.45e-04 (3.24e-04)	Tok/s 103242 (101278)	Loss/tok 3.5782 (4.5378)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.107 (0.141)	Data 1.54e-04 (3.23e-04)	Tok/s 95614 (101268)	Loss/tok 3.3388 (4.5329)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.213 (0.141)	Data 1.47e-04 (3.22e-04)	Tok/s 112158 (101273)	Loss/tok 3.5374 (4.5274)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.159 (0.141)	Data 1.67e-04 (3.21e-04)	Tok/s 104146 (101273)	Loss/tok 3.6609 (4.5224)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.108 (0.141)	Data 1.24e-04 (3.20e-04)	Tok/s 94395 (101270)	Loss/tok 3.2779 (4.5172)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.161 (0.141)	Data 1.49e-04 (3.19e-04)	Tok/s 104414 (101276)	Loss/tok 3.5351 (4.5117)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
:::MLLOG {"namespace": "", "time_ms": 1593019461743, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019461744, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.750 (0.750)	Decoder iters 149.0 (149.0)	Tok/s 22030 (22030)
0: Running moses detokenizer
0: BLEU(score=19.53958433433022, counts=[34754, 15905, 8484, 4737], totals=[67074, 64071, 61068, 58070], precisions=[51.81441393088231, 24.824023349097097, 13.89270976616231, 8.157396245910109], bp=1.0, sys_len=67074, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019464024, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1954, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019464025, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.5097	Test BLEU: 19.54
0: Performance: Epoch: 0	Training: 809710 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593019464025, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019464025, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019464025, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1240659771
0: TRAIN [1][0/1938]	Time 0.418 (0.418)	Data 2.60e-01 (2.60e-01)	Tok/s 40102 (40102)	Loss/tok 3.4605 (3.4605)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][10/1938]	Time 0.160 (0.178)	Data 1.76e-04 (2.38e-02)	Tok/s 106849 (96936)	Loss/tok 3.3558 (3.5105)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.108 (0.150)	Data 1.68e-04 (1.25e-02)	Tok/s 96802 (97528)	Loss/tok 3.3365 (3.4533)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.107 (0.141)	Data 1.62e-04 (8.55e-03)	Tok/s 96232 (98175)	Loss/tok 3.2705 (3.4199)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.107 (0.142)	Data 1.48e-04 (6.50e-03)	Tok/s 96484 (98844)	Loss/tok 3.1748 (3.4367)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.213 (0.145)	Data 1.96e-04 (5.26e-03)	Tok/s 110316 (99495)	Loss/tok 3.5653 (3.4513)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.272 (0.147)	Data 1.50e-04 (4.42e-03)	Tok/s 110060 (99582)	Loss/tok 4.0021 (3.4809)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.107 (0.144)	Data 1.57e-04 (3.82e-03)	Tok/s 99241 (99423)	Loss/tok 3.1364 (3.4737)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.160 (0.143)	Data 1.80e-04 (3.37e-03)	Tok/s 102826 (99512)	Loss/tok 3.5410 (3.4739)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.158 (0.142)	Data 1.61e-04 (3.02e-03)	Tok/s 106091 (99676)	Loss/tok 3.2337 (3.4574)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.107 (0.140)	Data 1.63e-04 (2.74e-03)	Tok/s 96672 (99492)	Loss/tok 3.2847 (3.4496)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.107 (0.141)	Data 1.79e-04 (2.51e-03)	Tok/s 99116 (99664)	Loss/tok 3.2782 (3.4530)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.107 (0.142)	Data 1.64e-04 (2.31e-03)	Tok/s 96966 (99808)	Loss/tok 3.1885 (3.4561)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][130/1938]	Time 0.213 (0.143)	Data 1.64e-04 (2.15e-03)	Tok/s 109708 (100091)	Loss/tok 3.6618 (3.4587)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.213 (0.143)	Data 1.39e-04 (2.01e-03)	Tok/s 109281 (100169)	Loss/tok 3.7089 (3.4631)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.060 (0.143)	Data 1.91e-04 (1.89e-03)	Tok/s 88103 (100235)	Loss/tok 2.7587 (3.4632)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.059 (0.142)	Data 1.42e-04 (1.78e-03)	Tok/s 89791 (100125)	Loss/tok 2.7266 (3.4608)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.108 (0.143)	Data 1.48e-04 (1.68e-03)	Tok/s 97229 (100293)	Loss/tok 3.2712 (3.4605)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.107 (0.142)	Data 1.68e-04 (1.60e-03)	Tok/s 97609 (100197)	Loss/tok 3.1287 (3.4548)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.213 (0.141)	Data 1.72e-04 (1.53e-03)	Tok/s 108867 (100048)	Loss/tok 3.7092 (3.4531)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.159 (0.141)	Data 1.44e-04 (1.46e-03)	Tok/s 106440 (100025)	Loss/tok 3.4326 (3.4482)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.108 (0.141)	Data 1.43e-04 (1.40e-03)	Tok/s 97734 (100150)	Loss/tok 3.3549 (3.4496)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.060 (0.141)	Data 1.74e-04 (1.34e-03)	Tok/s 87397 (100161)	Loss/tok 2.7768 (3.4462)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.212 (0.142)	Data 1.25e-04 (1.29e-03)	Tok/s 110289 (100242)	Loss/tok 3.6468 (3.4486)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.059 (0.142)	Data 1.30e-04 (1.24e-03)	Tok/s 86787 (100273)	Loss/tok 2.8285 (3.4485)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.212 (0.142)	Data 1.54e-04 (1.20e-03)	Tok/s 109878 (100359)	Loss/tok 3.6960 (3.4474)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][260/1938]	Time 0.107 (0.141)	Data 1.43e-04 (1.16e-03)	Tok/s 95959 (100349)	Loss/tok 3.3182 (3.4445)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][270/1938]	Time 0.059 (0.142)	Data 1.50e-04 (1.13e-03)	Tok/s 89515 (100438)	Loss/tok 2.6936 (3.4474)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.108 (0.142)	Data 1.61e-04 (1.09e-03)	Tok/s 97560 (100410)	Loss/tok 3.2120 (3.4477)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.107 (0.142)	Data 2.29e-04 (1.06e-03)	Tok/s 97416 (100511)	Loss/tok 3.1356 (3.4457)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.059 (0.141)	Data 1.74e-04 (1.03e-03)	Tok/s 90832 (100478)	Loss/tok 2.7475 (3.4433)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.107 (0.142)	Data 1.67e-04 (1.00e-03)	Tok/s 95848 (100511)	Loss/tok 3.1982 (3.4462)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.159 (0.141)	Data 1.84e-04 (9.77e-04)	Tok/s 107043 (100499)	Loss/tok 3.4957 (3.4438)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][330/1938]	Time 0.107 (0.142)	Data 1.76e-04 (9.52e-04)	Tok/s 97692 (100590)	Loss/tok 3.2446 (3.4464)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.059 (0.142)	Data 2.15e-04 (9.30e-04)	Tok/s 90616 (100582)	Loss/tok 2.7222 (3.4437)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.107 (0.141)	Data 1.97e-04 (9.08e-04)	Tok/s 95442 (100554)	Loss/tok 3.1854 (3.4429)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.213 (0.141)	Data 1.67e-04 (8.88e-04)	Tok/s 109898 (100529)	Loss/tok 3.5662 (3.4404)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.107 (0.141)	Data 1.87e-04 (8.69e-04)	Tok/s 95486 (100532)	Loss/tok 3.3000 (3.4391)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.108 (0.141)	Data 1.58e-04 (8.51e-04)	Tok/s 96107 (100535)	Loss/tok 3.2225 (3.4389)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.161 (0.140)	Data 1.74e-04 (8.33e-04)	Tok/s 104951 (100475)	Loss/tok 3.4637 (3.4366)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.160 (0.141)	Data 1.60e-04 (8.17e-04)	Tok/s 104617 (100533)	Loss/tok 3.3689 (3.4369)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.107 (0.140)	Data 1.82e-04 (8.01e-04)	Tok/s 95195 (100416)	Loss/tok 3.1981 (3.4335)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.107 (0.140)	Data 1.76e-04 (7.86e-04)	Tok/s 96054 (100463)	Loss/tok 3.2111 (3.4362)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.160 (0.141)	Data 1.99e-04 (7.72e-04)	Tok/s 104048 (100546)	Loss/tok 3.4556 (3.4379)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.159 (0.141)	Data 1.70e-04 (7.58e-04)	Tok/s 105463 (100588)	Loss/tok 3.3356 (3.4372)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.107 (0.141)	Data 1.97e-04 (7.45e-04)	Tok/s 97774 (100589)	Loss/tok 3.2548 (3.4392)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][460/1938]	Time 0.059 (0.141)	Data 1.54e-04 (7.33e-04)	Tok/s 88346 (100629)	Loss/tok 2.8369 (3.4402)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.107 (0.141)	Data 1.63e-04 (7.21e-04)	Tok/s 96795 (100617)	Loss/tok 3.2522 (3.4386)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.213 (0.141)	Data 1.72e-04 (7.09e-04)	Tok/s 109561 (100607)	Loss/tok 3.5916 (3.4384)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.108 (0.140)	Data 1.71e-04 (6.98e-04)	Tok/s 95431 (100548)	Loss/tok 3.1885 (3.4356)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.159 (0.141)	Data 1.54e-04 (6.88e-04)	Tok/s 105195 (100612)	Loss/tok 3.3844 (3.4377)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.108 (0.141)	Data 1.62e-04 (6.77e-04)	Tok/s 93980 (100610)	Loss/tok 3.1816 (3.4379)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.107 (0.141)	Data 1.65e-04 (6.67e-04)	Tok/s 97132 (100604)	Loss/tok 3.2913 (3.4362)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.107 (0.141)	Data 1.53e-04 (6.58e-04)	Tok/s 94439 (100591)	Loss/tok 3.1809 (3.4352)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.108 (0.141)	Data 1.68e-04 (6.49e-04)	Tok/s 95884 (100597)	Loss/tok 3.1534 (3.4375)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.213 (0.141)	Data 1.49e-04 (6.40e-04)	Tok/s 110186 (100634)	Loss/tok 3.7093 (3.4376)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.215 (0.141)	Data 1.60e-04 (6.31e-04)	Tok/s 108485 (100672)	Loss/tok 3.6323 (3.4371)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.161 (0.142)	Data 1.58e-04 (6.23e-04)	Tok/s 103599 (100705)	Loss/tok 3.4370 (3.4384)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][580/1938]	Time 0.161 (0.142)	Data 1.44e-04 (6.15e-04)	Tok/s 104190 (100698)	Loss/tok 3.3680 (3.4371)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.107 (0.142)	Data 1.24e-04 (6.07e-04)	Tok/s 95761 (100740)	Loss/tok 3.2133 (3.4390)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.159 (0.142)	Data 1.26e-04 (5.99e-04)	Tok/s 105588 (100750)	Loss/tok 3.4030 (3.4381)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.107 (0.142)	Data 2.03e-04 (5.92e-04)	Tok/s 96946 (100749)	Loss/tok 3.1276 (3.4374)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][620/1938]	Time 0.273 (0.143)	Data 1.67e-04 (5.85e-04)	Tok/s 108207 (100791)	Loss/tok 3.8348 (3.4409)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.162 (0.143)	Data 2.49e-04 (5.79e-04)	Tok/s 106288 (100803)	Loss/tok 3.4277 (3.4397)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.059 (0.142)	Data 1.73e-04 (5.73e-04)	Tok/s 91110 (100723)	Loss/tok 2.7644 (3.4371)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.213 (0.142)	Data 1.53e-04 (5.66e-04)	Tok/s 109832 (100724)	Loss/tok 3.6009 (3.4377)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][660/1938]	Time 0.108 (0.142)	Data 1.57e-04 (5.60e-04)	Tok/s 97243 (100741)	Loss/tok 3.2826 (3.4377)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.107 (0.142)	Data 1.78e-04 (5.54e-04)	Tok/s 96985 (100739)	Loss/tok 3.1146 (3.4384)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.107 (0.142)	Data 1.62e-04 (5.49e-04)	Tok/s 94873 (100719)	Loss/tok 3.2162 (3.4368)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.160 (0.142)	Data 1.58e-04 (5.43e-04)	Tok/s 104505 (100747)	Loss/tok 3.4630 (3.4377)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.107 (0.142)	Data 1.37e-04 (5.37e-04)	Tok/s 98971 (100733)	Loss/tok 3.1850 (3.4366)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.161 (0.142)	Data 1.64e-04 (5.32e-04)	Tok/s 104702 (100794)	Loss/tok 3.3203 (3.4368)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.107 (0.143)	Data 1.46e-04 (5.27e-04)	Tok/s 96371 (100805)	Loss/tok 3.3148 (3.4368)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.107 (0.143)	Data 1.72e-04 (5.22e-04)	Tok/s 97127 (100815)	Loss/tok 3.2896 (3.4368)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.274 (0.143)	Data 1.52e-04 (5.17e-04)	Tok/s 108862 (100827)	Loss/tok 3.7318 (3.4370)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.159 (0.142)	Data 1.62e-04 (5.13e-04)	Tok/s 105638 (100787)	Loss/tok 3.4315 (3.4352)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.107 (0.142)	Data 1.62e-04 (5.08e-04)	Tok/s 96269 (100756)	Loss/tok 3.1256 (3.4334)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.160 (0.142)	Data 1.76e-04 (5.03e-04)	Tok/s 104480 (100756)	Loss/tok 3.5506 (3.4336)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.212 (0.142)	Data 1.26e-04 (4.99e-04)	Tok/s 109769 (100769)	Loss/tok 3.5305 (3.4330)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][790/1938]	Time 0.059 (0.142)	Data 1.67e-04 (4.95e-04)	Tok/s 88937 (100704)	Loss/tok 2.7521 (3.4327)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.160 (0.141)	Data 1.60e-04 (4.90e-04)	Tok/s 103030 (100667)	Loss/tok 3.4775 (3.4310)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.107 (0.142)	Data 1.68e-04 (4.86e-04)	Tok/s 93826 (100668)	Loss/tok 3.3026 (3.4308)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.275 (0.142)	Data 1.56e-04 (4.83e-04)	Tok/s 107295 (100692)	Loss/tok 3.6682 (3.4311)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.107 (0.142)	Data 1.65e-04 (4.79e-04)	Tok/s 95381 (100665)	Loss/tok 3.2866 (3.4306)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.160 (0.142)	Data 1.73e-04 (4.75e-04)	Tok/s 104186 (100673)	Loss/tok 3.3859 (3.4296)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.213 (0.142)	Data 1.73e-04 (4.72e-04)	Tok/s 108712 (100696)	Loss/tok 3.5871 (3.4297)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.160 (0.142)	Data 1.18e-04 (4.68e-04)	Tok/s 105573 (100713)	Loss/tok 3.4292 (3.4297)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.160 (0.142)	Data 1.43e-04 (4.64e-04)	Tok/s 104607 (100716)	Loss/tok 3.3801 (3.4285)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.160 (0.142)	Data 1.26e-04 (4.61e-04)	Tok/s 103434 (100686)	Loss/tok 3.5023 (3.4276)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.213 (0.142)	Data 1.53e-04 (4.57e-04)	Tok/s 110157 (100726)	Loss/tok 3.4996 (3.4293)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.059 (0.142)	Data 1.49e-04 (4.54e-04)	Tok/s 91479 (100733)	Loss/tok 2.7685 (3.4296)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.160 (0.142)	Data 1.40e-04 (4.50e-04)	Tok/s 104529 (100717)	Loss/tok 3.4011 (3.4292)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][920/1938]	Time 0.159 (0.142)	Data 1.18e-04 (4.47e-04)	Tok/s 103777 (100733)	Loss/tok 3.5238 (3.4293)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.212 (0.142)	Data 1.22e-04 (4.44e-04)	Tok/s 109560 (100730)	Loss/tok 3.5270 (3.4290)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.159 (0.142)	Data 1.24e-04 (4.41e-04)	Tok/s 106010 (100762)	Loss/tok 3.3823 (3.4285)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][950/1938]	Time 0.214 (0.142)	Data 1.85e-04 (4.38e-04)	Tok/s 108008 (100795)	Loss/tok 3.5491 (3.4291)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.160 (0.142)	Data 1.41e-04 (4.35e-04)	Tok/s 105484 (100781)	Loss/tok 3.4393 (3.4289)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.107 (0.143)	Data 1.33e-04 (4.32e-04)	Tok/s 98088 (100839)	Loss/tok 3.2719 (3.4312)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.107 (0.143)	Data 1.64e-04 (4.29e-04)	Tok/s 96284 (100847)	Loss/tok 3.2176 (3.4322)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][990/1938]	Time 0.161 (0.143)	Data 1.09e-04 (4.26e-04)	Tok/s 104895 (100866)	Loss/tok 3.2954 (3.4324)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.108 (0.143)	Data 1.36e-04 (4.23e-04)	Tok/s 96684 (100842)	Loss/tok 3.1536 (3.4320)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.107 (0.143)	Data 1.40e-04 (4.20e-04)	Tok/s 93806 (100841)	Loss/tok 3.2238 (3.4317)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.160 (0.143)	Data 1.36e-04 (4.18e-04)	Tok/s 104478 (100832)	Loss/tok 3.4048 (3.4311)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.214 (0.143)	Data 1.61e-04 (4.15e-04)	Tok/s 109366 (100809)	Loss/tok 3.6773 (3.4300)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.059 (0.143)	Data 1.49e-04 (4.13e-04)	Tok/s 90031 (100816)	Loss/tok 2.7309 (3.4299)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.059 (0.143)	Data 1.88e-04 (4.11e-04)	Tok/s 87806 (100794)	Loss/tok 2.6785 (3.4293)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.160 (0.143)	Data 1.44e-04 (4.08e-04)	Tok/s 104672 (100771)	Loss/tok 3.3530 (3.4284)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.107 (0.143)	Data 1.80e-04 (4.06e-04)	Tok/s 94725 (100761)	Loss/tok 3.1469 (3.4279)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.108 (0.143)	Data 1.67e-04 (4.04e-04)	Tok/s 94776 (100748)	Loss/tok 3.1877 (3.4266)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.108 (0.143)	Data 1.61e-04 (4.02e-04)	Tok/s 95603 (100773)	Loss/tok 3.2094 (3.4266)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.160 (0.143)	Data 1.61e-04 (4.00e-04)	Tok/s 103953 (100778)	Loss/tok 3.4265 (3.4258)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1110/1938]	Time 0.107 (0.143)	Data 1.42e-04 (3.97e-04)	Tok/s 95476 (100782)	Loss/tok 3.0639 (3.4260)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.059 (0.143)	Data 1.65e-04 (3.95e-04)	Tok/s 89277 (100810)	Loss/tok 2.8192 (3.4265)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.161 (0.143)	Data 1.83e-04 (3.93e-04)	Tok/s 106145 (100817)	Loss/tok 3.3926 (3.4258)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.107 (0.143)	Data 1.88e-04 (3.91e-04)	Tok/s 94874 (100788)	Loss/tok 3.1996 (3.4249)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.107 (0.143)	Data 1.46e-04 (3.89e-04)	Tok/s 97862 (100753)	Loss/tok 3.1156 (3.4239)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.160 (0.143)	Data 1.88e-04 (3.87e-04)	Tok/s 105309 (100774)	Loss/tok 3.3562 (3.4241)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.059 (0.143)	Data 1.62e-04 (3.85e-04)	Tok/s 90428 (100752)	Loss/tok 2.7528 (3.4228)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.108 (0.143)	Data 1.51e-04 (3.84e-04)	Tok/s 97877 (100770)	Loss/tok 3.1380 (3.4232)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.160 (0.142)	Data 2.09e-04 (3.82e-04)	Tok/s 104245 (100737)	Loss/tok 3.4135 (3.4220)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.108 (0.142)	Data 1.56e-04 (3.80e-04)	Tok/s 95104 (100692)	Loss/tok 3.2030 (3.4205)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.107 (0.142)	Data 1.79e-04 (3.78e-04)	Tok/s 97305 (100698)	Loss/tok 3.1392 (3.4196)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.108 (0.142)	Data 1.45e-04 (3.76e-04)	Tok/s 95914 (100705)	Loss/tok 3.1673 (3.4195)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.108 (0.142)	Data 1.59e-04 (3.75e-04)	Tok/s 96907 (100701)	Loss/tok 3.1226 (3.4184)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1240/1938]	Time 0.059 (0.142)	Data 2.01e-04 (3.73e-04)	Tok/s 89060 (100684)	Loss/tok 2.7031 (3.4178)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.107 (0.142)	Data 1.75e-04 (3.71e-04)	Tok/s 96187 (100665)	Loss/tok 3.1701 (3.4166)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.213 (0.142)	Data 1.71e-04 (3.70e-04)	Tok/s 110869 (100683)	Loss/tok 3.5103 (3.4165)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.107 (0.142)	Data 1.79e-04 (3.68e-04)	Tok/s 95107 (100668)	Loss/tok 3.2160 (3.4166)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.059 (0.142)	Data 1.65e-04 (3.66e-04)	Tok/s 90466 (100649)	Loss/tok 2.6680 (3.4160)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.160 (0.142)	Data 1.74e-04 (3.65e-04)	Tok/s 105147 (100642)	Loss/tok 3.3571 (3.4156)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1300/1938]	Time 0.213 (0.142)	Data 2.03e-04 (3.64e-04)	Tok/s 109523 (100644)	Loss/tok 3.4958 (3.4160)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.108 (0.142)	Data 1.99e-04 (3.62e-04)	Tok/s 96691 (100638)	Loss/tok 3.0550 (3.4159)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.160 (0.142)	Data 1.46e-04 (3.61e-04)	Tok/s 104225 (100638)	Loss/tok 3.3858 (3.4151)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.107 (0.142)	Data 2.32e-04 (3.60e-04)	Tok/s 93045 (100650)	Loss/tok 3.0848 (3.4149)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.108 (0.142)	Data 1.92e-04 (3.58e-04)	Tok/s 95798 (100658)	Loss/tok 3.2255 (3.4157)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.213 (0.142)	Data 1.62e-04 (3.57e-04)	Tok/s 108090 (100689)	Loss/tok 3.5420 (3.4164)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.160 (0.142)	Data 1.97e-04 (3.55e-04)	Tok/s 104704 (100714)	Loss/tok 3.4340 (3.4165)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.107 (0.142)	Data 1.21e-04 (3.54e-04)	Tok/s 97065 (100720)	Loss/tok 3.1719 (3.4164)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.212 (0.142)	Data 1.98e-04 (3.53e-04)	Tok/s 109744 (100721)	Loss/tok 3.6529 (3.4162)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.160 (0.143)	Data 2.40e-04 (3.51e-04)	Tok/s 104359 (100756)	Loss/tok 3.3259 (3.4163)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.159 (0.143)	Data 1.43e-04 (3.50e-04)	Tok/s 106719 (100749)	Loss/tok 3.2839 (3.4155)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.160 (0.142)	Data 1.84e-04 (3.49e-04)	Tok/s 103769 (100753)	Loss/tok 3.4619 (3.4152)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.213 (0.142)	Data 1.53e-04 (3.48e-04)	Tok/s 110065 (100748)	Loss/tok 3.5384 (3.4146)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1430/1938]	Time 0.107 (0.142)	Data 1.66e-04 (3.47e-04)	Tok/s 95602 (100739)	Loss/tok 3.2133 (3.4139)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.059 (0.142)	Data 1.38e-04 (3.45e-04)	Tok/s 89642 (100727)	Loss/tok 2.6819 (3.4136)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.060 (0.142)	Data 2.04e-04 (3.44e-04)	Tok/s 86194 (100708)	Loss/tok 2.7571 (3.4127)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.160 (0.142)	Data 1.66e-04 (3.43e-04)	Tok/s 105290 (100707)	Loss/tok 3.3186 (3.4122)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.160 (0.142)	Data 1.54e-04 (3.42e-04)	Tok/s 105603 (100698)	Loss/tok 3.3654 (3.4117)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.108 (0.142)	Data 1.50e-04 (3.41e-04)	Tok/s 95847 (100679)	Loss/tok 3.1589 (3.4110)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.159 (0.142)	Data 1.84e-04 (3.40e-04)	Tok/s 106026 (100669)	Loss/tok 3.5250 (3.4103)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.108 (0.142)	Data 1.23e-04 (3.39e-04)	Tok/s 95430 (100637)	Loss/tok 3.0715 (3.4091)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.108 (0.142)	Data 1.54e-04 (3.38e-04)	Tok/s 93256 (100644)	Loss/tok 3.0597 (3.4091)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.108 (0.142)	Data 1.98e-04 (3.36e-04)	Tok/s 95221 (100637)	Loss/tok 3.1341 (3.4084)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.108 (0.142)	Data 1.45e-04 (3.35e-04)	Tok/s 95562 (100632)	Loss/tok 3.1328 (3.4082)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.108 (0.142)	Data 1.68e-04 (3.34e-04)	Tok/s 94476 (100630)	Loss/tok 3.1929 (3.4077)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.107 (0.142)	Data 1.81e-04 (3.33e-04)	Tok/s 96961 (100621)	Loss/tok 3.2282 (3.4078)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1560/1938]	Time 0.108 (0.141)	Data 2.05e-04 (3.32e-04)	Tok/s 95270 (100602)	Loss/tok 3.2616 (3.4071)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.275 (0.141)	Data 2.10e-04 (3.31e-04)	Tok/s 108531 (100598)	Loss/tok 3.7182 (3.4069)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.213 (0.141)	Data 1.98e-04 (3.30e-04)	Tok/s 109504 (100603)	Loss/tok 3.6536 (3.4068)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.161 (0.141)	Data 1.87e-04 (3.29e-04)	Tok/s 105655 (100594)	Loss/tok 3.2803 (3.4062)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1600/1938]	Time 0.107 (0.141)	Data 2.31e-04 (3.28e-04)	Tok/s 95949 (100589)	Loss/tok 3.1774 (3.4064)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.108 (0.142)	Data 1.96e-04 (3.27e-04)	Tok/s 95026 (100602)	Loss/tok 3.2862 (3.4065)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.107 (0.142)	Data 2.38e-04 (3.26e-04)	Tok/s 97415 (100602)	Loss/tok 3.1411 (3.4062)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.107 (0.142)	Data 1.66e-04 (3.25e-04)	Tok/s 95086 (100594)	Loss/tok 3.2292 (3.4057)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.212 (0.141)	Data 1.72e-04 (3.24e-04)	Tok/s 109663 (100593)	Loss/tok 3.4087 (3.4050)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.108 (0.142)	Data 1.89e-04 (3.23e-04)	Tok/s 94802 (100609)	Loss/tok 3.1048 (3.4054)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.160 (0.142)	Data 1.59e-04 (3.22e-04)	Tok/s 105842 (100616)	Loss/tok 3.3819 (3.4048)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.160 (0.142)	Data 1.64e-04 (3.22e-04)	Tok/s 104353 (100622)	Loss/tok 3.2917 (3.4045)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.160 (0.142)	Data 1.24e-04 (3.21e-04)	Tok/s 105677 (100612)	Loss/tok 3.2320 (3.4035)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.211 (0.142)	Data 1.50e-04 (3.20e-04)	Tok/s 110530 (100622)	Loss/tok 3.4698 (3.4037)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.214 (0.142)	Data 1.59e-04 (3.19e-04)	Tok/s 108026 (100628)	Loss/tok 3.5526 (3.4032)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.107 (0.142)	Data 1.76e-04 (3.18e-04)	Tok/s 96018 (100631)	Loss/tok 3.1513 (3.4026)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.213 (0.142)	Data 1.99e-04 (3.17e-04)	Tok/s 108849 (100646)	Loss/tok 3.4332 (3.4023)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1730/1938]	Time 0.161 (0.142)	Data 1.84e-04 (3.16e-04)	Tok/s 105163 (100650)	Loss/tok 3.3047 (3.4020)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.107 (0.142)	Data 1.89e-04 (3.15e-04)	Tok/s 96249 (100639)	Loss/tok 3.1218 (3.4017)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.108 (0.142)	Data 1.40e-04 (3.14e-04)	Tok/s 95653 (100634)	Loss/tok 3.0930 (3.4015)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.059 (0.142)	Data 1.96e-04 (3.14e-04)	Tok/s 91947 (100642)	Loss/tok 2.6877 (3.4016)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.108 (0.142)	Data 1.33e-04 (3.13e-04)	Tok/s 95304 (100622)	Loss/tok 3.1541 (3.4007)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.213 (0.142)	Data 2.37e-04 (3.12e-04)	Tok/s 109824 (100638)	Loss/tok 3.4961 (3.4006)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.108 (0.142)	Data 2.19e-04 (3.12e-04)	Tok/s 95235 (100654)	Loss/tok 3.2038 (3.4005)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [1][1800/1938]	Time 0.273 (0.142)	Data 1.63e-04 (3.11e-04)	Tok/s 108763 (100663)	Loss/tok 3.6102 (3.4008)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.160 (0.142)	Data 1.41e-04 (3.10e-04)	Tok/s 104939 (100673)	Loss/tok 3.2954 (3.4002)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.108 (0.142)	Data 1.84e-04 (3.09e-04)	Tok/s 96016 (100683)	Loss/tok 3.1863 (3.3999)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.106 (0.142)	Data 1.40e-04 (3.08e-04)	Tok/s 98085 (100684)	Loss/tok 3.1526 (3.3996)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.107 (0.142)	Data 1.54e-04 (3.08e-04)	Tok/s 96272 (100667)	Loss/tok 3.1351 (3.3989)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.107 (0.142)	Data 1.90e-04 (3.07e-04)	Tok/s 94766 (100684)	Loss/tok 3.1865 (3.3992)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.107 (0.142)	Data 1.94e-04 (3.06e-04)	Tok/s 96460 (100677)	Loss/tok 3.0818 (3.3986)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.273 (0.142)	Data 1.36e-04 (3.05e-04)	Tok/s 109313 (100695)	Loss/tok 3.7352 (3.3993)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.109 (0.142)	Data 2.02e-04 (3.05e-04)	Tok/s 94516 (100695)	Loss/tok 3.1612 (3.3990)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.160 (0.142)	Data 1.68e-04 (3.04e-04)	Tok/s 102836 (100688)	Loss/tok 3.3604 (3.3986)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.162 (0.142)	Data 1.87e-04 (3.04e-04)	Tok/s 102512 (100671)	Loss/tok 3.4178 (3.3982)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.273 (0.142)	Data 2.61e-04 (3.03e-04)	Tok/s 108434 (100664)	Loss/tok 3.7612 (3.3984)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [1][1920/1938]	Time 0.110 (0.142)	Data 1.58e-04 (3.02e-04)	Tok/s 91608 (100661)	Loss/tok 3.2328 (3.3980)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.109 (0.142)	Data 1.80e-04 (3.02e-04)	Tok/s 95975 (100652)	Loss/tok 3.0090 (3.3973)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
:::MLLOG {"namespace": "", "time_ms": 1593019740110, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019740110, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.776 (0.776)	Decoder iters 149.0 (149.0)	Tok/s 22436 (22436)
0: Running moses detokenizer
0: BLEU(score=21.104130121644086, counts=[36727, 17664, 9754, 5669], totals=[69801, 66798, 63795, 60797], precisions=[52.61672468875804, 26.44390550615288, 15.28959949839329, 9.324473247035215], bp=1.0, sys_len=69801, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593019742144, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21100000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019742145, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.3962	Test BLEU: 21.10
0: Performance: Epoch: 1	Training: 804784 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593019742145, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593019742145, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593019742145, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 2203997767
0: TRAIN [2][0/1938]	Time 0.369 (0.369)	Data 2.61e-01 (2.61e-01)	Tok/s 27780 (27780)	Loss/tok 3.1111 (3.1111)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.107 (0.175)	Data 1.51e-04 (2.39e-02)	Tok/s 93950 (96355)	Loss/tok 2.9791 (3.2510)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.059 (0.146)	Data 2.18e-04 (1.26e-02)	Tok/s 88982 (96696)	Loss/tok 2.7070 (3.2217)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.160 (0.141)	Data 1.97e-04 (8.60e-03)	Tok/s 104023 (97812)	Loss/tok 3.2618 (3.2004)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.108 (0.138)	Data 1.96e-04 (6.55e-03)	Tok/s 96255 (98315)	Loss/tok 3.0289 (3.1984)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.161 (0.147)	Data 1.21e-04 (5.30e-03)	Tok/s 105408 (99730)	Loss/tok 3.1802 (3.2343)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.163 (0.149)	Data 1.99e-04 (4.45e-03)	Tok/s 104413 (100121)	Loss/tok 3.2513 (3.2620)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.059 (0.147)	Data 1.56e-04 (3.85e-03)	Tok/s 89064 (100125)	Loss/tok 2.6226 (3.2564)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.213 (0.144)	Data 1.89e-04 (3.40e-03)	Tok/s 108593 (99938)	Loss/tok 3.4925 (3.2511)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.107 (0.144)	Data 1.43e-04 (3.04e-03)	Tok/s 95251 (100010)	Loss/tok 3.0863 (3.2452)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.107 (0.145)	Data 1.42e-04 (2.76e-03)	Tok/s 96291 (100254)	Loss/tok 3.0950 (3.2447)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.160 (0.142)	Data 2.01e-04 (2.52e-03)	Tok/s 105105 (100004)	Loss/tok 3.2247 (3.2330)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.107 (0.141)	Data 1.81e-04 (2.33e-03)	Tok/s 95848 (99974)	Loss/tok 2.9485 (3.2277)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][130/1938]	Time 0.107 (0.143)	Data 1.63e-04 (2.16e-03)	Tok/s 94812 (100118)	Loss/tok 3.0553 (3.2394)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.213 (0.142)	Data 1.75e-04 (2.02e-03)	Tok/s 110292 (100102)	Loss/tok 3.4006 (3.2344)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.108 (0.142)	Data 1.68e-04 (1.90e-03)	Tok/s 95382 (100146)	Loss/tok 3.1021 (3.2333)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.159 (0.141)	Data 2.17e-04 (1.79e-03)	Tok/s 106798 (100112)	Loss/tok 3.3084 (3.2322)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.161 (0.141)	Data 1.56e-04 (1.70e-03)	Tok/s 105508 (100211)	Loss/tok 3.2007 (3.2290)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.159 (0.140)	Data 1.56e-04 (1.61e-03)	Tok/s 105342 (100150)	Loss/tok 3.3121 (3.2273)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.161 (0.140)	Data 2.09e-04 (1.54e-03)	Tok/s 104260 (100209)	Loss/tok 3.1948 (3.2254)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.274 (0.142)	Data 1.99e-04 (1.47e-03)	Tok/s 108102 (100380)	Loss/tok 3.6800 (3.2317)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.213 (0.143)	Data 1.79e-04 (1.41e-03)	Tok/s 109506 (100523)	Loss/tok 3.4442 (3.2379)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][220/1938]	Time 0.214 (0.143)	Data 1.35e-04 (1.35e-03)	Tok/s 108422 (100579)	Loss/tok 3.5366 (3.2402)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.213 (0.144)	Data 1.99e-04 (1.30e-03)	Tok/s 109084 (100719)	Loss/tok 3.4749 (3.2458)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.107 (0.144)	Data 1.63e-04 (1.26e-03)	Tok/s 99429 (100735)	Loss/tok 3.0232 (3.2479)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.107 (0.144)	Data 1.96e-04 (1.22e-03)	Tok/s 98349 (100671)	Loss/tok 2.9014 (3.2469)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.059 (0.142)	Data 1.80e-04 (1.18e-03)	Tok/s 88901 (100519)	Loss/tok 2.7035 (3.2427)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.107 (0.142)	Data 1.63e-04 (1.14e-03)	Tok/s 96412 (100455)	Loss/tok 3.0884 (3.2448)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.160 (0.142)	Data 1.84e-04 (1.10e-03)	Tok/s 104174 (100449)	Loss/tok 3.2792 (3.2446)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.108 (0.143)	Data 2.04e-04 (1.07e-03)	Tok/s 97377 (100538)	Loss/tok 3.1207 (3.2451)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.160 (0.142)	Data 1.88e-04 (1.04e-03)	Tok/s 104798 (100544)	Loss/tok 3.3242 (3.2438)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.106 (0.142)	Data 1.85e-04 (1.02e-03)	Tok/s 97193 (100549)	Loss/tok 3.0952 (3.2472)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.160 (0.142)	Data 1.62e-04 (9.90e-04)	Tok/s 104081 (100442)	Loss/tok 3.2451 (3.2468)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.160 (0.141)	Data 2.01e-04 (9.66e-04)	Tok/s 105114 (100373)	Loss/tok 3.1880 (3.2439)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.159 (0.140)	Data 1.83e-04 (9.43e-04)	Tok/s 105885 (100276)	Loss/tok 3.2558 (3.2407)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][350/1938]	Time 0.161 (0.141)	Data 1.87e-04 (9.22e-04)	Tok/s 104720 (100343)	Loss/tok 3.2038 (3.2448)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.107 (0.141)	Data 2.05e-04 (9.01e-04)	Tok/s 96203 (100322)	Loss/tok 3.0834 (3.2448)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.060 (0.141)	Data 1.77e-04 (8.82e-04)	Tok/s 88636 (100292)	Loss/tok 2.6729 (3.2461)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][380/1938]	Time 0.059 (0.141)	Data 1.87e-04 (8.63e-04)	Tok/s 86160 (100274)	Loss/tok 2.5836 (3.2474)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.160 (0.141)	Data 1.66e-04 (8.46e-04)	Tok/s 106059 (100365)	Loss/tok 3.2272 (3.2520)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.159 (0.141)	Data 2.02e-04 (8.29e-04)	Tok/s 106613 (100386)	Loss/tok 3.3206 (3.2513)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.107 (0.141)	Data 1.78e-04 (8.13e-04)	Tok/s 96235 (100365)	Loss/tok 3.0585 (3.2497)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.159 (0.141)	Data 2.17e-04 (7.99e-04)	Tok/s 106155 (100376)	Loss/tok 3.2417 (3.2481)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.213 (0.141)	Data 1.84e-04 (7.84e-04)	Tok/s 109126 (100406)	Loss/tok 3.4983 (3.2474)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.107 (0.141)	Data 1.85e-04 (7.71e-04)	Tok/s 99166 (100422)	Loss/tok 3.1822 (3.2484)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.108 (0.141)	Data 1.92e-04 (7.58e-04)	Tok/s 97701 (100396)	Loss/tok 2.8145 (3.2489)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.107 (0.141)	Data 1.92e-04 (7.45e-04)	Tok/s 97068 (100394)	Loss/tok 3.1313 (3.2492)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.109 (0.141)	Data 1.25e-04 (7.33e-04)	Tok/s 94509 (100395)	Loss/tok 3.0452 (3.2483)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.109 (0.141)	Data 1.77e-04 (7.22e-04)	Tok/s 94230 (100378)	Loss/tok 3.1163 (3.2484)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.109 (0.140)	Data 1.85e-04 (7.11e-04)	Tok/s 94819 (100315)	Loss/tok 2.9949 (3.2457)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][500/1938]	Time 0.060 (0.140)	Data 1.29e-04 (7.00e-04)	Tok/s 86767 (100234)	Loss/tok 2.5743 (3.2450)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.109 (0.140)	Data 1.82e-04 (6.90e-04)	Tok/s 96032 (100225)	Loss/tok 3.0656 (3.2443)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][520/1938]	Time 0.276 (0.140)	Data 1.64e-04 (6.80e-04)	Tok/s 106896 (100209)	Loss/tok 3.6357 (3.2460)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.110 (0.141)	Data 1.58e-04 (6.71e-04)	Tok/s 94425 (100252)	Loss/tok 3.0255 (3.2471)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.110 (0.141)	Data 2.00e-04 (6.63e-04)	Tok/s 94327 (100244)	Loss/tok 3.0357 (3.2499)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.162 (0.141)	Data 1.95e-04 (6.54e-04)	Tok/s 104584 (100244)	Loss/tok 3.2888 (3.2532)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.276 (0.142)	Data 2.00e-04 (6.46e-04)	Tok/s 107238 (100253)	Loss/tok 3.5846 (3.2546)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.162 (0.142)	Data 2.62e-04 (6.38e-04)	Tok/s 103933 (100216)	Loss/tok 3.2543 (3.2544)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.163 (0.142)	Data 1.88e-04 (6.30e-04)	Tok/s 103194 (100229)	Loss/tok 3.2062 (3.2556)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.110 (0.142)	Data 1.91e-04 (6.22e-04)	Tok/s 94274 (100201)	Loss/tok 3.1257 (3.2565)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.275 (0.142)	Data 1.82e-04 (6.15e-04)	Tok/s 109802 (100167)	Loss/tok 3.5212 (3.2556)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.161 (0.142)	Data 1.24e-04 (6.08e-04)	Tok/s 103811 (100217)	Loss/tok 3.2440 (3.2570)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.161 (0.142)	Data 1.90e-04 (6.01e-04)	Tok/s 104757 (100221)	Loss/tok 3.2684 (3.2567)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.161 (0.142)	Data 1.73e-04 (5.95e-04)	Tok/s 104610 (100213)	Loss/tok 3.3171 (3.2568)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.110 (0.142)	Data 1.58e-04 (5.88e-04)	Tok/s 94617 (100193)	Loss/tok 3.0912 (3.2561)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][650/1938]	Time 0.109 (0.142)	Data 1.67e-04 (5.82e-04)	Tok/s 94003 (100137)	Loss/tok 2.9877 (3.2549)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.215 (0.142)	Data 1.57e-04 (5.76e-04)	Tok/s 105857 (100134)	Loss/tok 3.4875 (3.2541)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.109 (0.142)	Data 1.60e-04 (5.70e-04)	Tok/s 93997 (100092)	Loss/tok 3.1774 (3.2539)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.108 (0.142)	Data 2.10e-04 (5.64e-04)	Tok/s 95762 (100108)	Loss/tok 3.0135 (3.2548)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.161 (0.142)	Data 1.91e-04 (5.59e-04)	Tok/s 105187 (100143)	Loss/tok 3.2823 (3.2543)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.109 (0.142)	Data 2.07e-04 (5.54e-04)	Tok/s 95745 (100104)	Loss/tok 3.1020 (3.2547)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.275 (0.142)	Data 1.84e-04 (5.48e-04)	Tok/s 107719 (100084)	Loss/tok 3.5966 (3.2544)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.108 (0.142)	Data 2.57e-04 (5.43e-04)	Tok/s 95612 (100065)	Loss/tok 3.0620 (3.2544)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.109 (0.142)	Data 1.57e-04 (5.38e-04)	Tok/s 94857 (100059)	Loss/tok 3.1074 (3.2542)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.110 (0.142)	Data 1.71e-04 (5.34e-04)	Tok/s 95078 (100021)	Loss/tok 3.1195 (3.2543)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.109 (0.142)	Data 1.62e-04 (5.29e-04)	Tok/s 95743 (100037)	Loss/tok 3.0526 (3.2543)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.109 (0.142)	Data 1.40e-04 (5.24e-04)	Tok/s 94195 (100048)	Loss/tok 3.1850 (3.2540)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.060 (0.142)	Data 1.61e-04 (5.20e-04)	Tok/s 87939 (100031)	Loss/tok 2.5351 (3.2541)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][780/1938]	Time 0.109 (0.142)	Data 1.51e-04 (5.15e-04)	Tok/s 93096 (100011)	Loss/tok 3.0100 (3.2538)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.161 (0.142)	Data 1.57e-04 (5.11e-04)	Tok/s 106153 (100000)	Loss/tok 3.3426 (3.2552)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.274 (0.142)	Data 1.93e-04 (5.07e-04)	Tok/s 109493 (100031)	Loss/tok 3.6455 (3.2565)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.162 (0.142)	Data 1.65e-04 (5.03e-04)	Tok/s 102268 (100019)	Loss/tok 3.2666 (3.2553)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.060 (0.143)	Data 1.84e-04 (4.99e-04)	Tok/s 89185 (100054)	Loss/tok 2.7070 (3.2571)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.110 (0.142)	Data 1.86e-04 (4.95e-04)	Tok/s 94225 (100009)	Loss/tok 3.0978 (3.2563)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.109 (0.143)	Data 1.80e-04 (4.91e-04)	Tok/s 94968 (100043)	Loss/tok 3.0252 (3.2573)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.161 (0.143)	Data 1.93e-04 (4.87e-04)	Tok/s 104762 (100045)	Loss/tok 3.2087 (3.2576)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.060 (0.143)	Data 1.70e-04 (4.84e-04)	Tok/s 87093 (100058)	Loss/tok 2.7271 (3.2580)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.215 (0.143)	Data 1.73e-04 (4.80e-04)	Tok/s 108125 (100105)	Loss/tok 3.4380 (3.2595)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.161 (0.144)	Data 1.73e-04 (4.77e-04)	Tok/s 103445 (100121)	Loss/tok 3.2885 (3.2603)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.213 (0.144)	Data 1.42e-04 (4.73e-04)	Tok/s 109631 (100124)	Loss/tok 3.4223 (3.2604)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.060 (0.143)	Data 1.60e-04 (4.70e-04)	Tok/s 86930 (100097)	Loss/tok 2.6599 (3.2606)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][910/1938]	Time 0.214 (0.144)	Data 1.72e-04 (4.67e-04)	Tok/s 107996 (100098)	Loss/tok 3.5064 (3.2611)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.109 (0.143)	Data 1.63e-04 (4.64e-04)	Tok/s 95808 (100065)	Loss/tok 3.0189 (3.2602)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][930/1938]	Time 0.161 (0.143)	Data 1.85e-04 (4.61e-04)	Tok/s 104892 (100051)	Loss/tok 3.1911 (3.2600)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.161 (0.143)	Data 2.06e-04 (4.58e-04)	Tok/s 105107 (100025)	Loss/tok 3.3899 (3.2597)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.109 (0.143)	Data 1.67e-04 (4.55e-04)	Tok/s 92817 (100037)	Loss/tok 3.0138 (3.2597)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.161 (0.143)	Data 1.80e-04 (4.52e-04)	Tok/s 103339 (100019)	Loss/tok 3.2809 (3.2591)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.109 (0.143)	Data 1.64e-04 (4.49e-04)	Tok/s 93460 (100006)	Loss/tok 3.0210 (3.2597)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.161 (0.143)	Data 1.71e-04 (4.47e-04)	Tok/s 104627 (100032)	Loss/tok 3.2150 (3.2591)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.107 (0.143)	Data 1.48e-04 (4.44e-04)	Tok/s 95882 (100009)	Loss/tok 3.0907 (3.2583)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.108 (0.143)	Data 1.84e-04 (4.41e-04)	Tok/s 94121 (99980)	Loss/tok 3.0066 (3.2573)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.160 (0.143)	Data 2.00e-04 (4.39e-04)	Tok/s 106339 (99997)	Loss/tok 3.3067 (3.2568)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.160 (0.143)	Data 1.78e-04 (4.36e-04)	Tok/s 105444 (99989)	Loss/tok 3.1969 (3.2567)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.108 (0.143)	Data 2.26e-04 (4.34e-04)	Tok/s 95740 (100028)	Loss/tok 3.0194 (3.2573)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.108 (0.143)	Data 1.82e-04 (4.31e-04)	Tok/s 96060 (100017)	Loss/tok 3.0728 (3.2572)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.108 (0.143)	Data 2.04e-04 (4.29e-04)	Tok/s 96240 (100043)	Loss/tok 3.0783 (3.2577)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1060/1938]	Time 0.212 (0.143)	Data 1.82e-04 (4.27e-04)	Tok/s 111406 (100053)	Loss/tok 3.3898 (3.2574)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.107 (0.143)	Data 1.82e-04 (4.24e-04)	Tok/s 96674 (100044)	Loss/tok 3.0024 (3.2565)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.107 (0.143)	Data 1.72e-04 (4.22e-04)	Tok/s 96225 (100047)	Loss/tok 3.0016 (3.2561)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.214 (0.143)	Data 1.95e-04 (4.20e-04)	Tok/s 108291 (100061)	Loss/tok 3.5054 (3.2563)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.107 (0.143)	Data 1.78e-04 (4.18e-04)	Tok/s 97857 (100056)	Loss/tok 3.0363 (3.2561)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.108 (0.143)	Data 1.79e-04 (4.15e-04)	Tok/s 94486 (100051)	Loss/tok 3.1197 (3.2554)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.107 (0.142)	Data 1.96e-04 (4.13e-04)	Tok/s 97896 (100039)	Loss/tok 3.0240 (3.2544)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.161 (0.143)	Data 2.07e-04 (4.11e-04)	Tok/s 104113 (100066)	Loss/tok 3.2424 (3.2550)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.214 (0.143)	Data 1.82e-04 (4.09e-04)	Tok/s 109346 (100071)	Loss/tok 3.4136 (3.2551)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.107 (0.142)	Data 1.91e-04 (4.07e-04)	Tok/s 94534 (100063)	Loss/tok 3.0303 (3.2545)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.059 (0.142)	Data 1.61e-04 (4.05e-04)	Tok/s 89826 (100046)	Loss/tok 2.6536 (3.2537)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.107 (0.142)	Data 1.86e-04 (4.03e-04)	Tok/s 95764 (100058)	Loss/tok 3.0778 (3.2549)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1180/1938]	Time 0.159 (0.143)	Data 1.29e-04 (4.01e-04)	Tok/s 104909 (100076)	Loss/tok 3.2870 (3.2554)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.107 (0.142)	Data 2.05e-04 (3.99e-04)	Tok/s 95414 (100068)	Loss/tok 3.0618 (3.2548)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.059 (0.142)	Data 1.74e-04 (3.98e-04)	Tok/s 88092 (100084)	Loss/tok 2.5803 (3.2550)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.107 (0.142)	Data 1.80e-04 (3.96e-04)	Tok/s 97422 (100065)	Loss/tok 3.1585 (3.2541)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.160 (0.142)	Data 1.43e-04 (3.94e-04)	Tok/s 105124 (100077)	Loss/tok 3.2643 (3.2535)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.107 (0.142)	Data 1.69e-04 (3.92e-04)	Tok/s 96446 (100080)	Loss/tok 3.1777 (3.2537)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1240/1938]	Time 0.108 (0.142)	Data 1.83e-04 (3.91e-04)	Tok/s 95931 (100088)	Loss/tok 2.9712 (3.2544)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.107 (0.142)	Data 2.02e-04 (3.89e-04)	Tok/s 95796 (100105)	Loss/tok 3.0145 (3.2549)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.059 (0.142)	Data 1.47e-04 (3.87e-04)	Tok/s 87825 (100091)	Loss/tok 2.6494 (3.2547)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.212 (0.142)	Data 1.97e-04 (3.85e-04)	Tok/s 109881 (100102)	Loss/tok 3.5027 (3.2556)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.160 (0.142)	Data 1.57e-04 (3.84e-04)	Tok/s 104164 (100119)	Loss/tok 3.3265 (3.2551)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.107 (0.142)	Data 2.05e-04 (3.82e-04)	Tok/s 98335 (100094)	Loss/tok 3.1117 (3.2542)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.160 (0.142)	Data 1.68e-04 (3.81e-04)	Tok/s 102874 (100110)	Loss/tok 3.4049 (3.2545)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.160 (0.142)	Data 1.71e-04 (3.79e-04)	Tok/s 105764 (100112)	Loss/tok 3.1797 (3.2540)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.160 (0.142)	Data 1.73e-04 (3.77e-04)	Tok/s 106389 (100113)	Loss/tok 3.0801 (3.2540)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.212 (0.142)	Data 1.64e-04 (3.76e-04)	Tok/s 108893 (100130)	Loss/tok 3.5998 (3.2540)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.214 (0.142)	Data 1.52e-04 (3.74e-04)	Tok/s 109220 (100141)	Loss/tok 3.3260 (3.2538)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.107 (0.142)	Data 2.17e-04 (3.73e-04)	Tok/s 96735 (100105)	Loss/tok 2.9861 (3.2528)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1360/1938]	Time 0.107 (0.142)	Data 1.51e-04 (3.71e-04)	Tok/s 96032 (100137)	Loss/tok 2.9563 (3.2537)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.107 (0.142)	Data 1.60e-04 (3.70e-04)	Tok/s 96285 (100120)	Loss/tok 3.0967 (3.2533)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1380/1938]	Time 0.059 (0.142)	Data 1.52e-04 (3.68e-04)	Tok/s 89944 (100113)	Loss/tok 2.6123 (3.2534)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.107 (0.142)	Data 1.45e-04 (3.67e-04)	Tok/s 96087 (100131)	Loss/tok 3.0017 (3.2536)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.107 (0.142)	Data 1.55e-04 (3.65e-04)	Tok/s 96698 (100112)	Loss/tok 3.0176 (3.2528)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.059 (0.142)	Data 1.56e-04 (3.64e-04)	Tok/s 89103 (100113)	Loss/tok 2.6418 (3.2536)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.159 (0.142)	Data 1.62e-04 (3.63e-04)	Tok/s 104866 (100136)	Loss/tok 3.2832 (3.2541)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1430/1938]	Time 0.214 (0.142)	Data 1.78e-04 (3.61e-04)	Tok/s 109921 (100145)	Loss/tok 3.3334 (3.2542)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.109 (0.142)	Data 1.96e-04 (3.60e-04)	Tok/s 93963 (100130)	Loss/tok 2.9515 (3.2539)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.161 (0.142)	Data 1.65e-04 (3.59e-04)	Tok/s 103464 (100108)	Loss/tok 3.2389 (3.2536)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.109 (0.142)	Data 1.67e-04 (3.58e-04)	Tok/s 93752 (100095)	Loss/tok 2.9554 (3.2537)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.215 (0.142)	Data 1.76e-04 (3.57e-04)	Tok/s 109395 (100101)	Loss/tok 3.4640 (3.2540)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.108 (0.142)	Data 1.74e-04 (3.55e-04)	Tok/s 97795 (100099)	Loss/tok 3.0620 (3.2540)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.109 (0.142)	Data 1.78e-04 (3.54e-04)	Tok/s 95088 (100069)	Loss/tok 3.0532 (3.2531)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.109 (0.142)	Data 1.43e-04 (3.53e-04)	Tok/s 93173 (100065)	Loss/tok 3.0388 (3.2535)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.161 (0.142)	Data 1.69e-04 (3.52e-04)	Tok/s 103495 (100058)	Loss/tok 3.3352 (3.2530)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.109 (0.142)	Data 1.92e-04 (3.51e-04)	Tok/s 96610 (100066)	Loss/tok 3.0686 (3.2526)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.109 (0.142)	Data 1.80e-04 (3.50e-04)	Tok/s 97439 (100063)	Loss/tok 3.0952 (3.2523)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.162 (0.142)	Data 1.66e-04 (3.48e-04)	Tok/s 104546 (100069)	Loss/tok 3.1806 (3.2525)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.215 (0.142)	Data 1.68e-04 (3.47e-04)	Tok/s 108397 (100065)	Loss/tok 3.3711 (3.2525)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1560/1938]	Time 0.110 (0.142)	Data 2.24e-04 (3.46e-04)	Tok/s 91793 (100073)	Loss/tok 3.0746 (3.2526)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.161 (0.142)	Data 1.85e-04 (3.45e-04)	Tok/s 106496 (100106)	Loss/tok 3.1910 (3.2530)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.109 (0.142)	Data 1.67e-04 (3.44e-04)	Tok/s 93369 (100084)	Loss/tok 3.1999 (3.2529)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.060 (0.142)	Data 1.76e-04 (3.43e-04)	Tok/s 88910 (100079)	Loss/tok 2.6022 (3.2526)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.109 (0.142)	Data 1.76e-04 (3.42e-04)	Tok/s 96303 (100068)	Loss/tok 3.0456 (3.2527)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.060 (0.142)	Data 1.82e-04 (3.41e-04)	Tok/s 89440 (100052)	Loss/tok 2.6771 (3.2522)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.161 (0.142)	Data 1.72e-04 (3.40e-04)	Tok/s 104588 (100064)	Loss/tok 3.1630 (3.2528)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.110 (0.142)	Data 2.14e-04 (3.39e-04)	Tok/s 95902 (100052)	Loss/tok 3.1149 (3.2524)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.110 (0.142)	Data 1.86e-04 (3.38e-04)	Tok/s 95570 (100042)	Loss/tok 3.1427 (3.2521)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.161 (0.142)	Data 1.79e-04 (3.37e-04)	Tok/s 104884 (100031)	Loss/tok 3.1945 (3.2520)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.161 (0.142)	Data 1.77e-04 (3.36e-04)	Tok/s 105195 (100031)	Loss/tok 3.2292 (3.2522)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.109 (0.142)	Data 1.71e-04 (3.36e-04)	Tok/s 95729 (100011)	Loss/tok 3.0299 (3.2516)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.161 (0.142)	Data 2.01e-04 (3.35e-04)	Tok/s 103349 (100007)	Loss/tok 3.2857 (3.2511)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [2][1690/1938]	Time 0.109 (0.142)	Data 2.33e-04 (3.34e-04)	Tok/s 94184 (99990)	Loss/tok 3.0593 (3.2503)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1700/1938]	Time 0.215 (0.142)	Data 1.71e-04 (3.33e-04)	Tok/s 109504 (100003)	Loss/tok 3.4115 (3.2509)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.109 (0.142)	Data 1.82e-04 (3.32e-04)	Tok/s 95750 (99996)	Loss/tok 3.1798 (3.2506)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.060 (0.142)	Data 1.88e-04 (3.31e-04)	Tok/s 87185 (99978)	Loss/tok 2.6543 (3.2502)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.109 (0.142)	Data 2.12e-04 (3.30e-04)	Tok/s 94741 (99976)	Loss/tok 2.9704 (3.2501)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.110 (0.142)	Data 1.66e-04 (3.29e-04)	Tok/s 95301 (99980)	Loss/tok 3.0329 (3.2503)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.161 (0.142)	Data 1.83e-04 (3.28e-04)	Tok/s 105213 (99998)	Loss/tok 3.2407 (3.2511)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.109 (0.142)	Data 1.50e-04 (3.28e-04)	Tok/s 93091 (100002)	Loss/tok 3.0444 (3.2511)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.214 (0.142)	Data 1.70e-04 (3.27e-04)	Tok/s 109518 (99998)	Loss/tok 3.4290 (3.2513)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.060 (0.142)	Data 1.85e-04 (3.26e-04)	Tok/s 85305 (100007)	Loss/tok 2.6036 (3.2520)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.213 (0.142)	Data 2.00e-04 (3.25e-04)	Tok/s 110371 (99998)	Loss/tok 3.3582 (3.2521)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.109 (0.142)	Data 1.67e-04 (3.24e-04)	Tok/s 95270 (100004)	Loss/tok 3.0108 (3.2520)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.109 (0.142)	Data 1.69e-04 (3.23e-04)	Tok/s 97350 (99987)	Loss/tok 2.9909 (3.2514)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.161 (0.142)	Data 1.52e-04 (3.23e-04)	Tok/s 105228 (99994)	Loss/tok 3.1902 (3.2514)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [2][1830/1938]	Time 0.274 (0.142)	Data 2.15e-04 (3.22e-04)	Tok/s 109823 (100007)	Loss/tok 3.4730 (3.2528)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.160 (0.143)	Data 1.79e-04 (3.21e-04)	Tok/s 105849 (100019)	Loss/tok 3.2390 (3.2532)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.160 (0.142)	Data 1.64e-04 (3.20e-04)	Tok/s 104582 (100013)	Loss/tok 3.2124 (3.2524)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.059 (0.143)	Data 1.99e-04 (3.19e-04)	Tok/s 89734 (100028)	Loss/tok 2.5606 (3.2529)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.214 (0.143)	Data 1.64e-04 (3.19e-04)	Tok/s 110102 (100028)	Loss/tok 3.4302 (3.2530)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.214 (0.143)	Data 1.76e-04 (3.18e-04)	Tok/s 109945 (100037)	Loss/tok 3.4045 (3.2532)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.213 (0.143)	Data 1.85e-04 (3.17e-04)	Tok/s 109975 (100041)	Loss/tok 3.2578 (3.2528)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.108 (0.143)	Data 1.93e-04 (3.17e-04)	Tok/s 95175 (100043)	Loss/tok 3.0588 (3.2528)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.108 (0.143)	Data 2.03e-04 (3.16e-04)	Tok/s 95681 (100047)	Loss/tok 3.0625 (3.2530)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.213 (0.143)	Data 1.90e-04 (3.15e-04)	Tok/s 110367 (100067)	Loss/tok 3.3311 (3.2533)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.160 (0.143)	Data 2.00e-04 (3.14e-04)	Tok/s 102844 (100081)	Loss/tok 3.3095 (3.2539)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1593020019711, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020019712, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.751 (0.751)	Decoder iters 149.0 (149.0)	Tok/s 21989 (21989)
0: Running moses detokenizer
0: BLEU(score=22.4439919291661, counts=[36658, 17878, 9982, 5825], totals=[66845, 63842, 60839, 57842], precisions=[54.84030219163737, 28.003508662009335, 16.407238777757687, 10.070536980049098], bp=1.0, sys_len=66845, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020021624, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22440000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020021624, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2564	Test BLEU: 22.44
0: Performance: Epoch: 2	Training: 800440 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593020021625, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593020021625, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593020021625, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1483983634
0: TRAIN [3][0/1938]	Time 0.423 (0.423)	Data 2.64e-01 (2.64e-01)	Tok/s 39228 (39228)	Loss/tok 3.2202 (3.2202)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.214 (0.161)	Data 1.55e-04 (2.41e-02)	Tok/s 106312 (93289)	Loss/tok 3.5408 (3.1313)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][20/1938]	Time 0.109 (0.144)	Data 1.24e-04 (1.27e-02)	Tok/s 94293 (95053)	Loss/tok 3.0498 (3.1029)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.059 (0.140)	Data 1.38e-04 (8.65e-03)	Tok/s 90604 (95897)	Loss/tok 2.5381 (3.1128)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.160 (0.136)	Data 1.23e-04 (6.58e-03)	Tok/s 104232 (96594)	Loss/tok 3.1907 (3.1011)	LR 2.000e-03
0: Gradient norm: nan
0: Skipped batch, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][50/1938]	Time 0.161 (0.138)	Data 1.71e-04 (5.32e-03)	Tok/s 105917 (96868)	Loss/tok 3.2464 (3.1426)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.109 (0.133)	Data 1.35e-04 (4.48e-03)	Tok/s 97241 (96582)	Loss/tok 2.9955 (3.1134)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.109 (0.132)	Data 1.39e-04 (3.87e-03)	Tok/s 94239 (96656)	Loss/tok 3.0014 (3.1124)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.059 (0.134)	Data 1.42e-04 (3.41e-03)	Tok/s 90633 (97144)	Loss/tok 2.5888 (3.1243)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.275 (0.134)	Data 1.28e-04 (3.05e-03)	Tok/s 108305 (97369)	Loss/tok 3.5258 (3.1307)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.214 (0.139)	Data 1.56e-04 (2.77e-03)	Tok/s 109217 (98040)	Loss/tok 3.2530 (3.1538)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.161 (0.136)	Data 1.37e-04 (2.53e-03)	Tok/s 105168 (97763)	Loss/tok 3.1544 (3.1412)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.160 (0.138)	Data 2.20e-04 (2.34e-03)	Tok/s 104864 (98135)	Loss/tok 3.1878 (3.1513)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.276 (0.139)	Data 1.90e-04 (2.17e-03)	Tok/s 108309 (98272)	Loss/tok 3.4893 (3.1520)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.109 (0.139)	Data 1.74e-04 (2.03e-03)	Tok/s 93795 (98327)	Loss/tok 3.0445 (3.1516)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.059 (0.137)	Data 1.47e-04 (1.90e-03)	Tok/s 88315 (98199)	Loss/tok 2.5860 (3.1450)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.108 (0.138)	Data 1.49e-04 (1.80e-03)	Tok/s 95467 (98355)	Loss/tok 2.9941 (3.1438)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.108 (0.138)	Data 1.78e-04 (1.70e-03)	Tok/s 97079 (98433)	Loss/tok 2.9685 (3.1483)	LR 2.000e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][180/1938]	Time 0.215 (0.140)	Data 1.38e-04 (1.61e-03)	Tok/s 108281 (98582)	Loss/tok 3.2974 (3.1543)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.161 (0.141)	Data 1.48e-04 (1.54e-03)	Tok/s 103882 (98754)	Loss/tok 3.1240 (3.1592)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.059 (0.139)	Data 1.30e-04 (1.47e-03)	Tok/s 89321 (98595)	Loss/tok 2.5295 (3.1526)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.109 (0.139)	Data 1.41e-04 (1.40e-03)	Tok/s 94971 (98562)	Loss/tok 3.0345 (3.1506)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.110 (0.139)	Data 1.37e-04 (1.35e-03)	Tok/s 93854 (98548)	Loss/tok 2.8767 (3.1515)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.215 (0.138)	Data 1.59e-04 (1.30e-03)	Tok/s 108084 (98491)	Loss/tok 3.3966 (3.1488)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.108 (0.138)	Data 1.36e-04 (1.25e-03)	Tok/s 100369 (98587)	Loss/tok 2.8323 (3.1470)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.215 (0.138)	Data 1.72e-04 (1.20e-03)	Tok/s 108743 (98570)	Loss/tok 3.2508 (3.1467)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.215 (0.139)	Data 1.42e-04 (1.16e-03)	Tok/s 106554 (98651)	Loss/tok 3.5058 (3.1513)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.059 (0.139)	Data 1.47e-04 (1.13e-03)	Tok/s 89181 (98595)	Loss/tok 2.5672 (3.1508)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.161 (0.139)	Data 1.47e-04 (1.09e-03)	Tok/s 104362 (98673)	Loss/tok 3.2122 (3.1507)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.108 (0.138)	Data 1.58e-04 (1.06e-03)	Tok/s 93481 (98644)	Loss/tok 3.0875 (3.1499)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.161 (0.139)	Data 1.63e-04 (1.03e-03)	Tok/s 106313 (98742)	Loss/tok 3.1945 (3.1495)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][310/1938]	Time 0.214 (0.139)	Data 1.77e-04 (1.00e-03)	Tok/s 108627 (98759)	Loss/tok 3.3996 (3.1534)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.216 (0.140)	Data 1.37e-04 (9.75e-04)	Tok/s 107430 (98896)	Loss/tok 3.3137 (3.1556)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.109 (0.139)	Data 1.79e-04 (9.50e-04)	Tok/s 94785 (98862)	Loss/tok 2.9720 (3.1523)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.161 (0.140)	Data 1.44e-04 (9.26e-04)	Tok/s 103940 (98914)	Loss/tok 3.1290 (3.1542)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.214 (0.140)	Data 1.34e-04 (9.04e-04)	Tok/s 109192 (99041)	Loss/tok 3.4050 (3.1574)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.059 (0.141)	Data 1.54e-04 (8.83e-04)	Tok/s 88209 (99048)	Loss/tok 2.6092 (3.1574)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.109 (0.140)	Data 1.48e-04 (8.63e-04)	Tok/s 93564 (99012)	Loss/tok 3.0207 (3.1571)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.108 (0.140)	Data 1.38e-04 (8.44e-04)	Tok/s 95012 (99045)	Loss/tok 3.0503 (3.1578)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.163 (0.140)	Data 1.58e-04 (8.27e-04)	Tok/s 102820 (98991)	Loss/tok 3.1819 (3.1564)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.109 (0.140)	Data 1.42e-04 (8.10e-04)	Tok/s 94070 (99032)	Loss/tok 3.0606 (3.1551)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.109 (0.139)	Data 2.02e-04 (7.93e-04)	Tok/s 94569 (98965)	Loss/tok 2.9460 (3.1536)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.160 (0.140)	Data 1.52e-04 (7.78e-04)	Tok/s 105940 (99081)	Loss/tok 3.2402 (3.1590)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][430/1938]	Time 0.161 (0.140)	Data 2.02e-04 (7.64e-04)	Tok/s 106346 (99089)	Loss/tok 3.1939 (3.1613)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.214 (0.141)	Data 1.46e-04 (7.50e-04)	Tok/s 108044 (99130)	Loss/tok 3.2970 (3.1639)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.161 (0.141)	Data 1.31e-04 (7.36e-04)	Tok/s 104802 (99165)	Loss/tok 3.2249 (3.1647)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.277 (0.141)	Data 1.57e-04 (7.24e-04)	Tok/s 107757 (99191)	Loss/tok 3.5141 (3.1671)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.109 (0.142)	Data 1.50e-04 (7.12e-04)	Tok/s 95676 (99228)	Loss/tok 2.9484 (3.1694)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.109 (0.141)	Data 1.27e-04 (7.00e-04)	Tok/s 94881 (99153)	Loss/tok 3.0450 (3.1668)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.161 (0.141)	Data 1.29e-04 (6.89e-04)	Tok/s 102630 (99220)	Loss/tok 3.2815 (3.1677)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.109 (0.141)	Data 1.25e-04 (6.78e-04)	Tok/s 94039 (99219)	Loss/tok 3.0071 (3.1688)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.215 (0.142)	Data 1.44e-04 (6.68e-04)	Tok/s 110160 (99267)	Loss/tok 3.3173 (3.1705)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.162 (0.142)	Data 1.44e-04 (6.58e-04)	Tok/s 102763 (99317)	Loss/tok 3.2022 (3.1722)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.109 (0.142)	Data 1.45e-04 (6.48e-04)	Tok/s 93929 (99348)	Loss/tok 3.0064 (3.1721)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.108 (0.142)	Data 1.32e-04 (6.39e-04)	Tok/s 96592 (99338)	Loss/tok 2.9844 (3.1727)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.109 (0.142)	Data 1.40e-04 (6.30e-04)	Tok/s 95415 (99317)	Loss/tok 3.0185 (3.1726)	LR 2.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1938]	Time 0.109 (0.142)	Data 1.38e-04 (6.22e-04)	Tok/s 93607 (99303)	Loss/tok 2.9927 (3.1737)	LR 2.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][570/1938]	Time 0.215 (0.142)	Data 1.40e-04 (6.13e-04)	Tok/s 109557 (99319)	Loss/tok 3.2710 (3.1740)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.109 (0.142)	Data 1.62e-04 (6.05e-04)	Tok/s 94151 (99330)	Loss/tok 3.0417 (3.1735)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.108 (0.142)	Data 1.38e-04 (5.98e-04)	Tok/s 94638 (99315)	Loss/tok 3.0155 (3.1738)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.162 (0.142)	Data 1.38e-04 (5.90e-04)	Tok/s 103509 (99345)	Loss/tok 3.1896 (3.1752)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.109 (0.142)	Data 1.42e-04 (5.83e-04)	Tok/s 95080 (99354)	Loss/tok 2.9950 (3.1747)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.277 (0.142)	Data 1.55e-04 (5.76e-04)	Tok/s 108261 (99380)	Loss/tok 3.4610 (3.1756)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.162 (0.142)	Data 1.49e-04 (5.69e-04)	Tok/s 103789 (99403)	Loss/tok 3.2543 (3.1749)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.162 (0.143)	Data 1.47e-04 (5.63e-04)	Tok/s 103523 (99468)	Loss/tok 3.2288 (3.1773)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.109 (0.143)	Data 1.80e-04 (5.56e-04)	Tok/s 93710 (99515)	Loss/tok 3.0643 (3.1786)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.275 (0.143)	Data 1.46e-04 (5.50e-04)	Tok/s 107764 (99484)	Loss/tok 3.5261 (3.1783)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.109 (0.143)	Data 1.67e-04 (5.44e-04)	Tok/s 94544 (99472)	Loss/tok 2.8852 (3.1784)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.216 (0.143)	Data 1.65e-04 (5.38e-04)	Tok/s 108439 (99509)	Loss/tok 3.2686 (3.1798)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][690/1938]	Time 0.109 (0.144)	Data 1.71e-04 (5.33e-04)	Tok/s 92765 (99551)	Loss/tok 3.0179 (3.1834)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.216 (0.144)	Data 1.58e-04 (5.28e-04)	Tok/s 108936 (99557)	Loss/tok 3.3929 (3.1839)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.215 (0.144)	Data 1.37e-04 (5.22e-04)	Tok/s 107635 (99587)	Loss/tok 3.4532 (3.1840)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.109 (0.144)	Data 1.48e-04 (5.17e-04)	Tok/s 94929 (99571)	Loss/tok 2.9605 (3.1828)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.110 (0.144)	Data 1.27e-04 (5.12e-04)	Tok/s 93795 (99553)	Loss/tok 2.9313 (3.1813)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.109 (0.144)	Data 1.45e-04 (5.07e-04)	Tok/s 97202 (99544)	Loss/tok 2.9220 (3.1805)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.162 (0.144)	Data 1.49e-04 (5.02e-04)	Tok/s 103719 (99563)	Loss/tok 3.0516 (3.1807)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.109 (0.144)	Data 1.38e-04 (4.98e-04)	Tok/s 96032 (99572)	Loss/tok 2.9131 (3.1803)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.109 (0.144)	Data 1.23e-04 (4.93e-04)	Tok/s 93195 (99577)	Loss/tok 2.9569 (3.1796)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.216 (0.144)	Data 1.51e-04 (4.89e-04)	Tok/s 110167 (99638)	Loss/tok 3.2362 (3.1802)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.109 (0.144)	Data 1.63e-04 (4.84e-04)	Tok/s 93230 (99640)	Loss/tok 2.9593 (3.1796)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.109 (0.144)	Data 1.55e-04 (4.80e-04)	Tok/s 93655 (99663)	Loss/tok 2.9769 (3.1797)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.109 (0.144)	Data 1.41e-04 (4.76e-04)	Tok/s 94893 (99656)	Loss/tok 2.8946 (3.1787)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][820/1938]	Time 0.214 (0.145)	Data 1.54e-04 (4.72e-04)	Tok/s 108707 (99695)	Loss/tok 3.3321 (3.1796)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.109 (0.145)	Data 1.25e-04 (4.68e-04)	Tok/s 94190 (99695)	Loss/tok 2.8851 (3.1788)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.109 (0.144)	Data 1.75e-04 (4.64e-04)	Tok/s 95541 (99661)	Loss/tok 2.8777 (3.1784)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.213 (0.145)	Data 1.39e-04 (4.61e-04)	Tok/s 110658 (99703)	Loss/tok 3.2439 (3.1783)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.163 (0.145)	Data 1.42e-04 (4.57e-04)	Tok/s 104307 (99711)	Loss/tok 3.2239 (3.1777)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.059 (0.144)	Data 1.43e-04 (4.53e-04)	Tok/s 89258 (99666)	Loss/tok 2.7249 (3.1763)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.215 (0.144)	Data 1.42e-04 (4.50e-04)	Tok/s 108172 (99673)	Loss/tok 3.3709 (3.1764)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.108 (0.144)	Data 1.27e-04 (4.46e-04)	Tok/s 96696 (99675)	Loss/tok 3.0291 (3.1764)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.109 (0.144)	Data 1.69e-04 (4.43e-04)	Tok/s 92862 (99664)	Loss/tok 2.9535 (3.1755)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.060 (0.144)	Data 1.42e-04 (4.40e-04)	Tok/s 86216 (99642)	Loss/tok 2.6815 (3.1743)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.109 (0.144)	Data 1.38e-04 (4.37e-04)	Tok/s 93790 (99633)	Loss/tok 2.9813 (3.1732)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.161 (0.144)	Data 1.75e-04 (4.34e-04)	Tok/s 103634 (99609)	Loss/tok 3.1743 (3.1719)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.109 (0.143)	Data 1.21e-04 (4.30e-04)	Tok/s 94794 (99595)	Loss/tok 2.9969 (3.1711)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][950/1938]	Time 0.059 (0.143)	Data 1.57e-04 (4.28e-04)	Tok/s 89184 (99590)	Loss/tok 2.5716 (3.1713)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.109 (0.143)	Data 1.52e-04 (4.25e-04)	Tok/s 93524 (99569)	Loss/tok 3.0049 (3.1705)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.109 (0.143)	Data 1.38e-04 (4.22e-04)	Tok/s 93726 (99569)	Loss/tok 2.9275 (3.1706)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.059 (0.143)	Data 1.30e-04 (4.19e-04)	Tok/s 89842 (99552)	Loss/tok 2.6941 (3.1697)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.109 (0.143)	Data 1.62e-04 (4.16e-04)	Tok/s 96531 (99571)	Loss/tok 2.9478 (3.1699)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.276 (0.143)	Data 1.25e-04 (4.13e-04)	Tok/s 107743 (99572)	Loss/tok 3.4748 (3.1700)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.161 (0.143)	Data 1.21e-04 (4.11e-04)	Tok/s 105688 (99578)	Loss/tok 3.2223 (3.1695)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.109 (0.143)	Data 1.24e-04 (4.08e-04)	Tok/s 93886 (99560)	Loss/tok 2.9720 (3.1690)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.214 (0.143)	Data 1.63e-04 (4.06e-04)	Tok/s 108821 (99578)	Loss/tok 3.3554 (3.1690)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.108 (0.144)	Data 1.36e-04 (4.03e-04)	Tok/s 96249 (99608)	Loss/tok 2.9536 (3.1700)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.162 (0.144)	Data 1.33e-04 (4.00e-04)	Tok/s 104032 (99603)	Loss/tok 3.1530 (3.1696)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.108 (0.144)	Data 1.22e-04 (3.98e-04)	Tok/s 95624 (99630)	Loss/tok 2.9298 (3.1697)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.162 (0.144)	Data 1.22e-04 (3.96e-04)	Tok/s 104961 (99616)	Loss/tok 3.1003 (3.1686)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1080/1938]	Time 0.110 (0.143)	Data 1.20e-04 (3.93e-04)	Tok/s 95460 (99598)	Loss/tok 2.9586 (3.1679)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.109 (0.144)	Data 1.76e-04 (3.91e-04)	Tok/s 94658 (99616)	Loss/tok 2.9213 (3.1681)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.273 (0.144)	Data 1.37e-04 (3.89e-04)	Tok/s 107759 (99641)	Loss/tok 3.4877 (3.1693)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.109 (0.144)	Data 1.59e-04 (3.87e-04)	Tok/s 93746 (99657)	Loss/tok 3.0157 (3.1694)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.162 (0.144)	Data 1.28e-04 (3.84e-04)	Tok/s 103621 (99643)	Loss/tok 3.0680 (3.1684)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.110 (0.144)	Data 1.20e-04 (3.82e-04)	Tok/s 95384 (99607)	Loss/tok 3.0139 (3.1672)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.109 (0.143)	Data 1.69e-04 (3.80e-04)	Tok/s 95097 (99582)	Loss/tok 2.9772 (3.1661)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.109 (0.144)	Data 1.61e-04 (3.78e-04)	Tok/s 95440 (99603)	Loss/tok 2.9466 (3.1660)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.109 (0.144)	Data 1.41e-04 (3.76e-04)	Tok/s 96028 (99615)	Loss/tok 3.0573 (3.1657)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.109 (0.144)	Data 1.70e-04 (3.74e-04)	Tok/s 94935 (99625)	Loss/tok 3.0267 (3.1658)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.162 (0.144)	Data 1.46e-04 (3.72e-04)	Tok/s 102866 (99640)	Loss/tok 3.2573 (3.1662)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.108 (0.144)	Data 1.41e-04 (3.70e-04)	Tok/s 96702 (99653)	Loss/tok 2.9092 (3.1664)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.162 (0.144)	Data 1.21e-04 (3.68e-04)	Tok/s 103603 (99668)	Loss/tok 3.1939 (3.1660)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1210/1938]	Time 0.108 (0.144)	Data 1.73e-04 (3.66e-04)	Tok/s 95156 (99678)	Loss/tok 2.9735 (3.1656)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.108 (0.144)	Data 1.54e-04 (3.64e-04)	Tok/s 93820 (99672)	Loss/tok 2.9810 (3.1652)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.109 (0.144)	Data 1.31e-04 (3.62e-04)	Tok/s 95415 (99652)	Loss/tok 2.9627 (3.1641)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.059 (0.144)	Data 1.37e-04 (3.61e-04)	Tok/s 87188 (99637)	Loss/tok 2.6187 (3.1633)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.109 (0.144)	Data 1.30e-04 (3.59e-04)	Tok/s 94232 (99645)	Loss/tok 2.9311 (3.1632)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.108 (0.144)	Data 1.20e-04 (3.57e-04)	Tok/s 95890 (99635)	Loss/tok 3.0003 (3.1623)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.214 (0.144)	Data 1.27e-04 (3.55e-04)	Tok/s 108492 (99628)	Loss/tok 3.3577 (3.1621)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.109 (0.144)	Data 1.61e-04 (3.54e-04)	Tok/s 94145 (99636)	Loss/tok 2.9157 (3.1622)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.274 (0.144)	Data 1.35e-04 (3.52e-04)	Tok/s 108792 (99660)	Loss/tok 3.4653 (3.1630)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.059 (0.144)	Data 1.52e-04 (3.50e-04)	Tok/s 85877 (99640)	Loss/tok 2.5569 (3.1618)	LR 1.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1310/1938]	Time 0.108 (0.144)	Data 1.75e-04 (3.49e-04)	Tok/s 95604 (99620)	Loss/tok 2.9296 (3.1620)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.108 (0.144)	Data 1.47e-04 (3.47e-04)	Tok/s 94716 (99607)	Loss/tok 3.0034 (3.1612)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.214 (0.144)	Data 1.39e-04 (3.46e-04)	Tok/s 107159 (99599)	Loss/tok 3.4588 (3.1612)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.109 (0.144)	Data 1.64e-04 (3.44e-04)	Tok/s 93312 (99599)	Loss/tok 2.8980 (3.1611)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.108 (0.143)	Data 1.54e-04 (3.43e-04)	Tok/s 93478 (99595)	Loss/tok 2.9807 (3.1606)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.109 (0.144)	Data 1.43e-04 (3.41e-04)	Tok/s 93555 (99615)	Loss/tok 2.8233 (3.1606)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.161 (0.144)	Data 1.23e-04 (3.40e-04)	Tok/s 105222 (99638)	Loss/tok 3.1199 (3.1608)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.162 (0.144)	Data 1.43e-04 (3.38e-04)	Tok/s 102756 (99630)	Loss/tok 3.1673 (3.1604)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.108 (0.144)	Data 1.68e-04 (3.37e-04)	Tok/s 96656 (99616)	Loss/tok 2.9013 (3.1599)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.213 (0.144)	Data 1.50e-04 (3.36e-04)	Tok/s 110203 (99630)	Loss/tok 3.3488 (3.1598)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.109 (0.144)	Data 1.39e-04 (3.34e-04)	Tok/s 94972 (99641)	Loss/tok 2.9408 (3.1603)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.109 (0.144)	Data 1.23e-04 (3.33e-04)	Tok/s 92320 (99612)	Loss/tok 2.9657 (3.1598)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.161 (0.143)	Data 1.28e-04 (3.32e-04)	Tok/s 104110 (99608)	Loss/tok 3.0791 (3.1591)	LR 1.000e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1440/1938]	Time 0.162 (0.143)	Data 1.20e-04 (3.30e-04)	Tok/s 104142 (99603)	Loss/tok 3.0918 (3.1587)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.109 (0.143)	Data 1.19e-04 (3.29e-04)	Tok/s 91611 (99568)	Loss/tok 3.0175 (3.1576)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.215 (0.143)	Data 1.21e-04 (3.28e-04)	Tok/s 108653 (99577)	Loss/tok 3.3279 (3.1576)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.162 (0.143)	Data 1.40e-04 (3.26e-04)	Tok/s 101795 (99586)	Loss/tok 3.2258 (3.1575)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.110 (0.143)	Data 1.24e-04 (3.25e-04)	Tok/s 92686 (99579)	Loss/tok 2.8187 (3.1568)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.277 (0.143)	Data 1.57e-04 (3.24e-04)	Tok/s 107853 (99590)	Loss/tok 3.3876 (3.1572)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.110 (0.143)	Data 1.64e-04 (3.23e-04)	Tok/s 95963 (99586)	Loss/tok 2.9298 (3.1568)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.162 (0.143)	Data 1.10e-04 (3.21e-04)	Tok/s 104701 (99570)	Loss/tok 3.1747 (3.1561)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.109 (0.143)	Data 1.23e-04 (3.20e-04)	Tok/s 96269 (99581)	Loss/tok 2.9186 (3.1563)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.161 (0.143)	Data 1.25e-04 (3.19e-04)	Tok/s 104693 (99579)	Loss/tok 3.0408 (3.1562)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.109 (0.143)	Data 1.24e-04 (3.18e-04)	Tok/s 94971 (99565)	Loss/tok 2.8545 (3.1558)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.108 (0.143)	Data 1.18e-04 (3.17e-04)	Tok/s 96186 (99573)	Loss/tok 2.9864 (3.1562)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.109 (0.143)	Data 1.47e-04 (3.15e-04)	Tok/s 95031 (99565)	Loss/tok 2.9065 (3.1556)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1570/1938]	Time 0.059 (0.143)	Data 1.58e-04 (3.14e-04)	Tok/s 88641 (99554)	Loss/tok 2.5594 (3.1551)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.108 (0.143)	Data 1.22e-04 (3.13e-04)	Tok/s 96727 (99542)	Loss/tok 3.0149 (3.1549)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.059 (0.143)	Data 1.87e-04 (3.12e-04)	Tok/s 91242 (99541)	Loss/tok 2.5918 (3.1548)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.161 (0.143)	Data 1.20e-04 (3.11e-04)	Tok/s 105797 (99546)	Loss/tok 3.0774 (3.1545)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.059 (0.143)	Data 1.23e-04 (3.10e-04)	Tok/s 87713 (99521)	Loss/tok 2.5716 (3.1544)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.108 (0.143)	Data 1.57e-04 (3.09e-04)	Tok/s 95904 (99538)	Loss/tok 2.9230 (3.1544)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.161 (0.143)	Data 1.21e-04 (3.08e-04)	Tok/s 105560 (99544)	Loss/tok 3.0570 (3.1540)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.160 (0.143)	Data 1.40e-04 (3.07e-04)	Tok/s 106406 (99552)	Loss/tok 3.0896 (3.1543)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.161 (0.143)	Data 1.22e-04 (3.06e-04)	Tok/s 102994 (99545)	Loss/tok 3.1805 (3.1544)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.109 (0.143)	Data 1.44e-04 (3.05e-04)	Tok/s 92851 (99533)	Loss/tok 2.9483 (3.1540)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.109 (0.143)	Data 1.22e-04 (3.04e-04)	Tok/s 94682 (99526)	Loss/tok 2.8736 (3.1533)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.109 (0.143)	Data 1.63e-04 (3.03e-04)	Tok/s 93659 (99531)	Loss/tok 2.8783 (3.1534)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.215 (0.143)	Data 1.74e-04 (3.02e-04)	Tok/s 107976 (99540)	Loss/tok 3.3811 (3.1542)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1700/1938]	Time 0.060 (0.143)	Data 1.45e-04 (3.01e-04)	Tok/s 87130 (99538)	Loss/tok 2.5605 (3.1543)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.109 (0.143)	Data 1.56e-04 (3.00e-04)	Tok/s 94933 (99532)	Loss/tok 3.0912 (3.1541)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.110 (0.143)	Data 1.25e-04 (2.99e-04)	Tok/s 95509 (99532)	Loss/tok 2.9625 (3.1537)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.216 (0.143)	Data 1.31e-04 (2.99e-04)	Tok/s 107103 (99535)	Loss/tok 3.3064 (3.1533)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.059 (0.143)	Data 1.46e-04 (2.98e-04)	Tok/s 86874 (99532)	Loss/tok 2.4791 (3.1532)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.109 (0.143)	Data 1.70e-04 (2.97e-04)	Tok/s 94312 (99550)	Loss/tok 2.9053 (3.1536)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.162 (0.143)	Data 1.54e-04 (2.96e-04)	Tok/s 102607 (99541)	Loss/tok 3.0982 (3.1529)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.162 (0.143)	Data 1.16e-04 (2.95e-04)	Tok/s 104157 (99552)	Loss/tok 3.0851 (3.1529)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.161 (0.143)	Data 1.66e-04 (2.94e-04)	Tok/s 104156 (99560)	Loss/tok 3.1526 (3.1527)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.109 (0.143)	Data 1.65e-04 (2.93e-04)	Tok/s 92555 (99555)	Loss/tok 3.0072 (3.1522)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.161 (0.143)	Data 1.68e-04 (2.93e-04)	Tok/s 103785 (99549)	Loss/tok 3.1107 (3.1520)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.109 (0.143)	Data 1.62e-04 (2.92e-04)	Tok/s 95930 (99558)	Loss/tok 2.9407 (3.1515)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.215 (0.143)	Data 1.40e-04 (2.91e-04)	Tok/s 107854 (99569)	Loss/tok 3.2717 (3.1514)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1830/1938]	Time 0.162 (0.143)	Data 1.43e-04 (2.90e-04)	Tok/s 104385 (99579)	Loss/tok 3.0049 (3.1515)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.109 (0.143)	Data 1.87e-04 (2.89e-04)	Tok/s 94570 (99558)	Loss/tok 2.9473 (3.1508)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.109 (0.143)	Data 1.36e-04 (2.89e-04)	Tok/s 95000 (99557)	Loss/tok 3.0255 (3.1512)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.109 (0.143)	Data 1.84e-04 (2.88e-04)	Tok/s 97009 (99560)	Loss/tok 2.9767 (3.1511)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.214 (0.143)	Data 1.47e-04 (2.87e-04)	Tok/s 107584 (99558)	Loss/tok 3.3062 (3.1506)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.276 (0.143)	Data 1.62e-04 (2.86e-04)	Tok/s 107316 (99577)	Loss/tok 3.4626 (3.1511)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.161 (0.144)	Data 1.40e-04 (2.85e-04)	Tok/s 104745 (99589)	Loss/tok 3.0199 (3.1509)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.109 (0.144)	Data 1.49e-04 (2.85e-04)	Tok/s 93676 (99613)	Loss/tok 2.9688 (3.1508)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.215 (0.144)	Data 1.25e-04 (2.84e-04)	Tok/s 108882 (99611)	Loss/tok 3.3366 (3.1507)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.108 (0.144)	Data 1.18e-04 (2.83e-04)	Tok/s 95045 (99606)	Loss/tok 2.8116 (3.1501)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.161 (0.144)	Data 1.23e-04 (2.82e-04)	Tok/s 103584 (99594)	Loss/tok 3.2201 (3.1495)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1593020300375, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020300375, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.618 (0.618)	Decoder iters 104.0 (104.0)	Tok/s 26742 (26742)
0: Running moses detokenizer
0: BLEU(score=23.652963534538017, counts=[37108, 18452, 10458, 6199], totals=[65962, 62959, 59956, 56958], precisions=[56.25663260665232, 29.307962324687495, 17.442791380345586, 10.883457986586608], bp=1.0, sys_len=65962, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020302151, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2365, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020302151, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1477	Test BLEU: 23.65
0: Performance: Epoch: 3	Training: 796947 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593020302152, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593020302152, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593020302152, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 2256055023
0: TRAIN [4][0/1938]	Time 0.376 (0.376)	Data 2.65e-01 (2.65e-01)	Tok/s 27579 (27579)	Loss/tok 2.7650 (2.7650)	LR 5.000e-04
0: TRAIN [4][10/1938]	Time 0.159 (0.185)	Data 1.42e-04 (2.42e-02)	Tok/s 105096 (96629)	Loss/tok 3.0895 (3.0972)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][20/1938]	Time 0.107 (0.151)	Data 1.48e-04 (1.28e-02)	Tok/s 95354 (96866)	Loss/tok 2.8186 (3.0130)	LR 5.000e-04
0: TRAIN [4][30/1938]	Time 0.160 (0.152)	Data 1.61e-04 (8.69e-03)	Tok/s 103898 (98767)	Loss/tok 3.0771 (3.0281)	LR 5.000e-04
0: TRAIN [4][40/1938]	Time 0.160 (0.152)	Data 1.42e-04 (6.61e-03)	Tok/s 105111 (99354)	Loss/tok 2.9580 (3.0395)	LR 5.000e-04
0: TRAIN [4][50/1938]	Time 0.161 (0.147)	Data 1.52e-04 (5.34e-03)	Tok/s 104166 (99375)	Loss/tok 3.1151 (3.0250)	LR 5.000e-04
0: TRAIN [4][60/1938]	Time 0.218 (0.147)	Data 1.59e-04 (4.49e-03)	Tok/s 106659 (99599)	Loss/tok 3.1541 (3.0342)	LR 5.000e-04
0: TRAIN [4][70/1938]	Time 0.160 (0.149)	Data 1.25e-04 (3.88e-03)	Tok/s 103739 (99946)	Loss/tok 3.1190 (3.0448)	LR 5.000e-04
0: TRAIN [4][80/1938]	Time 0.160 (0.147)	Data 1.28e-04 (3.42e-03)	Tok/s 106225 (100045)	Loss/tok 2.9768 (3.0319)	LR 5.000e-04
0: TRAIN [4][90/1938]	Time 0.214 (0.148)	Data 1.33e-04 (3.06e-03)	Tok/s 110069 (100455)	Loss/tok 3.1691 (3.0373)	LR 5.000e-04
0: TRAIN [4][100/1938]	Time 0.108 (0.147)	Data 1.39e-04 (2.77e-03)	Tok/s 95196 (100441)	Loss/tok 2.8135 (3.0300)	LR 5.000e-04
0: TRAIN [4][110/1938]	Time 0.107 (0.147)	Data 1.34e-04 (2.54e-03)	Tok/s 96367 (100435)	Loss/tok 2.8398 (3.0302)	LR 5.000e-04
0: TRAIN [4][120/1938]	Time 0.160 (0.148)	Data 1.28e-04 (2.34e-03)	Tok/s 104856 (100615)	Loss/tok 3.0699 (3.0333)	LR 5.000e-04
0: TRAIN [4][130/1938]	Time 0.275 (0.148)	Data 1.37e-04 (2.17e-03)	Tok/s 109072 (100583)	Loss/tok 3.3523 (3.0365)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][140/1938]	Time 0.159 (0.147)	Data 2.15e-04 (2.03e-03)	Tok/s 103584 (100470)	Loss/tok 2.9617 (3.0323)	LR 5.000e-04
0: TRAIN [4][150/1938]	Time 0.161 (0.148)	Data 1.41e-04 (1.90e-03)	Tok/s 105226 (100649)	Loss/tok 3.0471 (3.0369)	LR 5.000e-04
0: TRAIN [4][160/1938]	Time 0.108 (0.149)	Data 1.41e-04 (1.79e-03)	Tok/s 97236 (100811)	Loss/tok 2.7745 (3.0420)	LR 5.000e-04
0: TRAIN [4][170/1938]	Time 0.060 (0.148)	Data 1.34e-04 (1.70e-03)	Tok/s 89531 (100704)	Loss/tok 2.6438 (3.0388)	LR 5.000e-04
0: TRAIN [4][180/1938]	Time 0.108 (0.148)	Data 1.30e-04 (1.61e-03)	Tok/s 96709 (100782)	Loss/tok 2.8460 (3.0425)	LR 5.000e-04
0: TRAIN [4][190/1938]	Time 0.273 (0.148)	Data 1.45e-04 (1.53e-03)	Tok/s 109272 (100723)	Loss/tok 3.3487 (3.0440)	LR 5.000e-04
0: TRAIN [4][200/1938]	Time 0.161 (0.147)	Data 1.65e-04 (1.47e-03)	Tok/s 104500 (100645)	Loss/tok 3.0853 (3.0442)	LR 5.000e-04
0: TRAIN [4][210/1938]	Time 0.108 (0.147)	Data 1.30e-04 (1.40e-03)	Tok/s 95683 (100569)	Loss/tok 2.8522 (3.0407)	LR 5.000e-04
0: TRAIN [4][220/1938]	Time 0.108 (0.146)	Data 1.36e-04 (1.35e-03)	Tok/s 95358 (100589)	Loss/tok 2.8025 (3.0402)	LR 5.000e-04
0: TRAIN [4][230/1938]	Time 0.059 (0.145)	Data 1.52e-04 (1.29e-03)	Tok/s 87686 (100445)	Loss/tok 2.3902 (3.0380)	LR 5.000e-04
0: TRAIN [4][240/1938]	Time 0.161 (0.147)	Data 1.71e-04 (1.25e-03)	Tok/s 102675 (100622)	Loss/tok 2.9956 (3.0427)	LR 5.000e-04
0: TRAIN [4][250/1938]	Time 0.212 (0.147)	Data 1.22e-04 (1.20e-03)	Tok/s 110366 (100661)	Loss/tok 3.1115 (3.0426)	LR 5.000e-04
0: TRAIN [4][260/1938]	Time 0.213 (0.147)	Data 1.31e-04 (1.16e-03)	Tok/s 109875 (100660)	Loss/tok 3.2021 (3.0419)	LR 5.000e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][270/1938]	Time 0.107 (0.146)	Data 1.49e-04 (1.12e-03)	Tok/s 96475 (100545)	Loss/tok 2.9059 (3.0407)	LR 5.000e-04
0: TRAIN [4][280/1938]	Time 0.275 (0.147)	Data 1.08e-04 (1.09e-03)	Tok/s 107508 (100632)	Loss/tok 3.2579 (3.0424)	LR 5.000e-04
0: TRAIN [4][290/1938]	Time 0.108 (0.147)	Data 1.27e-04 (1.06e-03)	Tok/s 95224 (100659)	Loss/tok 2.8364 (3.0410)	LR 5.000e-04
0: TRAIN [4][300/1938]	Time 0.108 (0.146)	Data 1.30e-04 (1.03e-03)	Tok/s 94754 (100558)	Loss/tok 2.8669 (3.0371)	LR 5.000e-04
0: TRAIN [4][310/1938]	Time 0.111 (0.145)	Data 1.42e-04 (9.97e-04)	Tok/s 91953 (100458)	Loss/tok 2.8816 (3.0333)	LR 5.000e-04
0: TRAIN [4][320/1938]	Time 0.161 (0.146)	Data 1.46e-04 (9.71e-04)	Tok/s 103717 (100543)	Loss/tok 3.1002 (3.0352)	LR 5.000e-04
0: TRAIN [4][330/1938]	Time 0.108 (0.146)	Data 1.35e-04 (9.46e-04)	Tok/s 94617 (100552)	Loss/tok 2.9274 (3.0349)	LR 5.000e-04
0: TRAIN [4][340/1938]	Time 0.108 (0.146)	Data 1.60e-04 (9.23e-04)	Tok/s 95361 (100563)	Loss/tok 2.8281 (3.0359)	LR 5.000e-04
0: TRAIN [4][350/1938]	Time 0.160 (0.145)	Data 1.37e-04 (9.01e-04)	Tok/s 104774 (100510)	Loss/tok 3.0962 (3.0359)	LR 5.000e-04
0: TRAIN [4][360/1938]	Time 0.160 (0.145)	Data 1.52e-04 (8.80e-04)	Tok/s 104680 (100505)	Loss/tok 3.0910 (3.0353)	LR 2.500e-04
0: TRAIN [4][370/1938]	Time 0.107 (0.145)	Data 1.45e-04 (8.60e-04)	Tok/s 97834 (100540)	Loss/tok 2.9135 (3.0345)	LR 2.500e-04
0: TRAIN [4][380/1938]	Time 0.275 (0.146)	Data 1.32e-04 (8.41e-04)	Tok/s 108148 (100561)	Loss/tok 3.3179 (3.0370)	LR 2.500e-04
0: TRAIN [4][390/1938]	Time 0.108 (0.145)	Data 1.45e-04 (8.23e-04)	Tok/s 97750 (100499)	Loss/tok 2.9615 (3.0349)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][400/1938]	Time 0.161 (0.145)	Data 1.19e-04 (8.06e-04)	Tok/s 105797 (100558)	Loss/tok 2.9921 (3.0362)	LR 2.500e-04
0: TRAIN [4][410/1938]	Time 0.059 (0.145)	Data 1.73e-04 (7.90e-04)	Tok/s 89969 (100481)	Loss/tok 2.4484 (3.0341)	LR 2.500e-04
0: TRAIN [4][420/1938]	Time 0.161 (0.145)	Data 1.56e-04 (7.75e-04)	Tok/s 103755 (100542)	Loss/tok 3.1363 (3.0362)	LR 2.500e-04
0: TRAIN [4][430/1938]	Time 0.161 (0.144)	Data 1.42e-04 (7.60e-04)	Tok/s 104069 (100512)	Loss/tok 3.0200 (3.0352)	LR 2.500e-04
0: TRAIN [4][440/1938]	Time 0.108 (0.144)	Data 1.80e-04 (7.47e-04)	Tok/s 96705 (100488)	Loss/tok 2.7442 (3.0328)	LR 2.500e-04
0: TRAIN [4][450/1938]	Time 0.160 (0.145)	Data 1.74e-04 (7.33e-04)	Tok/s 103954 (100574)	Loss/tok 3.0280 (3.0350)	LR 2.500e-04
0: TRAIN [4][460/1938]	Time 0.161 (0.145)	Data 1.40e-04 (7.21e-04)	Tok/s 104878 (100594)	Loss/tok 3.0475 (3.0358)	LR 2.500e-04
0: TRAIN [4][470/1938]	Time 0.160 (0.145)	Data 1.49e-04 (7.08e-04)	Tok/s 105374 (100637)	Loss/tok 2.8818 (3.0370)	LR 2.500e-04
0: TRAIN [4][480/1938]	Time 0.160 (0.145)	Data 1.43e-04 (6.97e-04)	Tok/s 104261 (100672)	Loss/tok 3.1384 (3.0370)	LR 2.500e-04
0: TRAIN [4][490/1938]	Time 0.214 (0.146)	Data 1.34e-04 (6.85e-04)	Tok/s 107617 (100740)	Loss/tok 3.1681 (3.0375)	LR 2.500e-04
0: TRAIN [4][500/1938]	Time 0.108 (0.146)	Data 1.36e-04 (6.74e-04)	Tok/s 96884 (100707)	Loss/tok 2.7785 (3.0382)	LR 2.500e-04
0: TRAIN [4][510/1938]	Time 0.107 (0.145)	Data 1.27e-04 (6.64e-04)	Tok/s 96040 (100690)	Loss/tok 2.8784 (3.0370)	LR 2.500e-04
0: TRAIN [4][520/1938]	Time 0.161 (0.145)	Data 1.26e-04 (6.54e-04)	Tok/s 104633 (100677)	Loss/tok 3.0570 (3.0355)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][530/1938]	Time 0.161 (0.145)	Data 1.36e-04 (6.44e-04)	Tok/s 105895 (100641)	Loss/tok 2.9627 (3.0356)	LR 2.500e-04
0: TRAIN [4][540/1938]	Time 0.160 (0.144)	Data 1.19e-04 (6.35e-04)	Tok/s 103771 (100604)	Loss/tok 3.0343 (3.0338)	LR 2.500e-04
0: TRAIN [4][550/1938]	Time 0.214 (0.145)	Data 1.40e-04 (6.26e-04)	Tok/s 109800 (100645)	Loss/tok 3.0965 (3.0339)	LR 2.500e-04
0: TRAIN [4][560/1938]	Time 0.108 (0.145)	Data 1.64e-04 (6.18e-04)	Tok/s 96471 (100640)	Loss/tok 2.9526 (3.0345)	LR 2.500e-04
0: TRAIN [4][570/1938]	Time 0.108 (0.144)	Data 1.44e-04 (6.09e-04)	Tok/s 95177 (100626)	Loss/tok 2.8137 (3.0333)	LR 2.500e-04
0: TRAIN [4][580/1938]	Time 0.108 (0.144)	Data 1.42e-04 (6.01e-04)	Tok/s 95284 (100570)	Loss/tok 2.8197 (3.0312)	LR 2.500e-04
0: TRAIN [4][590/1938]	Time 0.107 (0.144)	Data 1.44e-04 (5.94e-04)	Tok/s 95534 (100562)	Loss/tok 2.8109 (3.0304)	LR 2.500e-04
0: TRAIN [4][600/1938]	Time 0.108 (0.143)	Data 1.42e-04 (5.86e-04)	Tok/s 97260 (100517)	Loss/tok 2.8986 (3.0286)	LR 2.500e-04
0: TRAIN [4][610/1938]	Time 0.108 (0.144)	Data 1.42e-04 (5.79e-04)	Tok/s 95540 (100540)	Loss/tok 2.9272 (3.0300)	LR 2.500e-04
0: TRAIN [4][620/1938]	Time 0.213 (0.145)	Data 1.50e-04 (5.72e-04)	Tok/s 111484 (100643)	Loss/tok 3.0813 (3.0336)	LR 2.500e-04
0: TRAIN [4][630/1938]	Time 0.160 (0.145)	Data 1.74e-04 (5.65e-04)	Tok/s 105381 (100660)	Loss/tok 2.9813 (3.0339)	LR 2.500e-04
0: TRAIN [4][640/1938]	Time 0.059 (0.145)	Data 1.59e-04 (5.59e-04)	Tok/s 87175 (100672)	Loss/tok 2.4618 (3.0339)	LR 2.500e-04
0: TRAIN [4][650/1938]	Time 0.159 (0.145)	Data 1.31e-04 (5.52e-04)	Tok/s 103347 (100687)	Loss/tok 3.0310 (3.0332)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][660/1938]	Time 0.107 (0.145)	Data 2.14e-04 (5.46e-04)	Tok/s 94416 (100677)	Loss/tok 2.8503 (3.0321)	LR 2.500e-04
0: TRAIN [4][670/1938]	Time 0.160 (0.145)	Data 1.42e-04 (5.40e-04)	Tok/s 104986 (100663)	Loss/tok 3.0293 (3.0317)	LR 2.500e-04
0: TRAIN [4][680/1938]	Time 0.108 (0.145)	Data 1.54e-04 (5.34e-04)	Tok/s 94055 (100712)	Loss/tok 2.9104 (3.0330)	LR 2.500e-04
0: TRAIN [4][690/1938]	Time 0.059 (0.145)	Data 1.49e-04 (5.28e-04)	Tok/s 91289 (100696)	Loss/tok 2.4986 (3.0326)	LR 2.500e-04
0: TRAIN [4][700/1938]	Time 0.274 (0.145)	Data 1.29e-04 (5.23e-04)	Tok/s 110164 (100680)	Loss/tok 3.3004 (3.0322)	LR 2.500e-04
0: TRAIN [4][710/1938]	Time 0.213 (0.145)	Data 1.24e-04 (5.17e-04)	Tok/s 108988 (100718)	Loss/tok 3.1894 (3.0336)	LR 2.500e-04
0: TRAIN [4][720/1938]	Time 0.213 (0.145)	Data 1.26e-04 (5.12e-04)	Tok/s 107253 (100705)	Loss/tok 3.1948 (3.0342)	LR 2.500e-04
0: TRAIN [4][730/1938]	Time 0.160 (0.145)	Data 1.45e-04 (5.07e-04)	Tok/s 104996 (100706)	Loss/tok 3.1015 (3.0349)	LR 2.500e-04
0: TRAIN [4][740/1938]	Time 0.107 (0.146)	Data 1.18e-04 (5.02e-04)	Tok/s 96097 (100738)	Loss/tok 2.8671 (3.0368)	LR 2.500e-04
0: TRAIN [4][750/1938]	Time 0.108 (0.145)	Data 1.42e-04 (4.97e-04)	Tok/s 94957 (100659)	Loss/tok 2.8043 (3.0350)	LR 2.500e-04
0: TRAIN [4][760/1938]	Time 0.160 (0.145)	Data 1.56e-04 (4.93e-04)	Tok/s 104514 (100707)	Loss/tok 3.1046 (3.0357)	LR 2.500e-04
0: TRAIN [4][770/1938]	Time 0.108 (0.146)	Data 1.25e-04 (4.88e-04)	Tok/s 98549 (100752)	Loss/tok 2.8610 (3.0365)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][780/1938]	Time 0.213 (0.145)	Data 1.91e-04 (4.84e-04)	Tok/s 110168 (100732)	Loss/tok 3.1593 (3.0354)	LR 2.500e-04
0: TRAIN [4][790/1938]	Time 0.214 (0.145)	Data 1.44e-04 (4.79e-04)	Tok/s 108566 (100716)	Loss/tok 3.1840 (3.0353)	LR 2.500e-04
0: TRAIN [4][800/1938]	Time 0.108 (0.145)	Data 1.41e-04 (4.75e-04)	Tok/s 94518 (100717)	Loss/tok 2.9177 (3.0352)	LR 2.500e-04
0: TRAIN [4][810/1938]	Time 0.214 (0.145)	Data 1.24e-04 (4.71e-04)	Tok/s 108758 (100700)	Loss/tok 3.0745 (3.0351)	LR 2.500e-04
0: TRAIN [4][820/1938]	Time 0.108 (0.145)	Data 1.28e-04 (4.67e-04)	Tok/s 96396 (100679)	Loss/tok 2.8928 (3.0344)	LR 2.500e-04
0: TRAIN [4][830/1938]	Time 0.108 (0.145)	Data 1.20e-04 (4.63e-04)	Tok/s 94593 (100671)	Loss/tok 2.7910 (3.0355)	LR 2.500e-04
0: TRAIN [4][840/1938]	Time 0.108 (0.145)	Data 1.25e-04 (4.59e-04)	Tok/s 94768 (100619)	Loss/tok 2.8481 (3.0341)	LR 2.500e-04
0: TRAIN [4][850/1938]	Time 0.108 (0.145)	Data 1.25e-04 (4.55e-04)	Tok/s 95941 (100604)	Loss/tok 2.8036 (3.0337)	LR 2.500e-04
0: TRAIN [4][860/1938]	Time 0.161 (0.144)	Data 1.53e-04 (4.52e-04)	Tok/s 105338 (100559)	Loss/tok 3.0083 (3.0325)	LR 2.500e-04
0: TRAIN [4][870/1938]	Time 0.108 (0.144)	Data 1.23e-04 (4.48e-04)	Tok/s 94920 (100523)	Loss/tok 2.9783 (3.0317)	LR 2.500e-04
0: TRAIN [4][880/1938]	Time 0.160 (0.144)	Data 1.55e-04 (4.45e-04)	Tok/s 104573 (100524)	Loss/tok 2.9938 (3.0320)	LR 2.500e-04
0: TRAIN [4][890/1938]	Time 0.161 (0.144)	Data 1.40e-04 (4.41e-04)	Tok/s 103856 (100517)	Loss/tok 3.0880 (3.0314)	LR 2.500e-04
0: TRAIN [4][900/1938]	Time 0.107 (0.144)	Data 1.56e-04 (4.38e-04)	Tok/s 95453 (100538)	Loss/tok 2.8468 (3.0322)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][910/1938]	Time 0.108 (0.144)	Data 1.44e-04 (4.35e-04)	Tok/s 95513 (100532)	Loss/tok 2.8875 (3.0322)	LR 2.500e-04
0: TRAIN [4][920/1938]	Time 0.274 (0.144)	Data 1.74e-04 (4.32e-04)	Tok/s 108052 (100551)	Loss/tok 3.4487 (3.0338)	LR 2.500e-04
0: TRAIN [4][930/1938]	Time 0.160 (0.145)	Data 1.21e-04 (4.28e-04)	Tok/s 105430 (100598)	Loss/tok 3.1003 (3.0349)	LR 2.500e-04
0: TRAIN [4][940/1938]	Time 0.107 (0.144)	Data 1.61e-04 (4.25e-04)	Tok/s 96499 (100579)	Loss/tok 2.7758 (3.0349)	LR 2.500e-04
0: TRAIN [4][950/1938]	Time 0.107 (0.145)	Data 1.24e-04 (4.22e-04)	Tok/s 96852 (100606)	Loss/tok 2.8347 (3.0348)	LR 2.500e-04
0: TRAIN [4][960/1938]	Time 0.108 (0.145)	Data 1.28e-04 (4.19e-04)	Tok/s 94349 (100610)	Loss/tok 2.9047 (3.0353)	LR 2.500e-04
0: TRAIN [4][970/1938]	Time 0.160 (0.145)	Data 1.26e-04 (4.16e-04)	Tok/s 105761 (100616)	Loss/tok 3.0028 (3.0352)	LR 2.500e-04
0: TRAIN [4][980/1938]	Time 0.107 (0.144)	Data 1.42e-04 (4.14e-04)	Tok/s 95997 (100594)	Loss/tok 2.8129 (3.0347)	LR 2.500e-04
0: TRAIN [4][990/1938]	Time 0.214 (0.145)	Data 1.43e-04 (4.11e-04)	Tok/s 109161 (100605)	Loss/tok 3.1920 (3.0352)	LR 2.500e-04
0: TRAIN [4][1000/1938]	Time 0.274 (0.144)	Data 1.24e-04 (4.08e-04)	Tok/s 108602 (100580)	Loss/tok 3.4591 (3.0351)	LR 2.500e-04
0: TRAIN [4][1010/1938]	Time 0.059 (0.144)	Data 1.22e-04 (4.06e-04)	Tok/s 90077 (100576)	Loss/tok 2.4715 (3.0360)	LR 2.500e-04
0: TRAIN [4][1020/1938]	Time 0.108 (0.144)	Data 1.55e-04 (4.03e-04)	Tok/s 95339 (100545)	Loss/tok 2.9293 (3.0356)	LR 2.500e-04
0: TRAIN [4][1030/1938]	Time 0.108 (0.144)	Data 1.27e-04 (4.00e-04)	Tok/s 96210 (100556)	Loss/tok 2.8246 (3.0355)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1040/1938]	Time 0.275 (0.144)	Data 1.35e-04 (3.98e-04)	Tok/s 108134 (100542)	Loss/tok 3.4789 (3.0359)	LR 2.500e-04
0: TRAIN [4][1050/1938]	Time 0.161 (0.144)	Data 1.54e-04 (3.96e-04)	Tok/s 105436 (100534)	Loss/tok 2.9725 (3.0354)	LR 2.500e-04
0: TRAIN [4][1060/1938]	Time 0.160 (0.144)	Data 1.24e-04 (3.93e-04)	Tok/s 104839 (100543)	Loss/tok 3.0155 (3.0354)	LR 2.500e-04
0: TRAIN [4][1070/1938]	Time 0.160 (0.144)	Data 1.22e-04 (3.91e-04)	Tok/s 104813 (100553)	Loss/tok 2.9931 (3.0350)	LR 2.500e-04
0: TRAIN [4][1080/1938]	Time 0.108 (0.144)	Data 1.60e-04 (3.89e-04)	Tok/s 97799 (100584)	Loss/tok 2.7999 (3.0359)	LR 2.500e-04
0: TRAIN [4][1090/1938]	Time 0.161 (0.144)	Data 2.10e-04 (3.86e-04)	Tok/s 103971 (100582)	Loss/tok 2.9686 (3.0360)	LR 2.500e-04
0: TRAIN [4][1100/1938]	Time 0.108 (0.145)	Data 1.47e-04 (3.84e-04)	Tok/s 96395 (100614)	Loss/tok 2.8591 (3.0383)	LR 2.500e-04
0: TRAIN [4][1110/1938]	Time 0.161 (0.145)	Data 1.45e-04 (3.82e-04)	Tok/s 105242 (100633)	Loss/tok 2.9625 (3.0385)	LR 2.500e-04
0: TRAIN [4][1120/1938]	Time 0.160 (0.145)	Data 1.56e-04 (3.80e-04)	Tok/s 104568 (100612)	Loss/tok 2.9644 (3.0377)	LR 2.500e-04
0: TRAIN [4][1130/1938]	Time 0.108 (0.145)	Data 1.29e-04 (3.78e-04)	Tok/s 96954 (100629)	Loss/tok 2.9091 (3.0374)	LR 2.500e-04
0: TRAIN [4][1140/1938]	Time 0.108 (0.145)	Data 1.48e-04 (3.76e-04)	Tok/s 97317 (100615)	Loss/tok 2.8375 (3.0375)	LR 2.500e-04
0: TRAIN [4][1150/1938]	Time 0.107 (0.145)	Data 1.60e-04 (3.74e-04)	Tok/s 94580 (100622)	Loss/tok 2.7825 (3.0375)	LR 2.500e-04
0: TRAIN [4][1160/1938]	Time 0.108 (0.145)	Data 1.73e-04 (3.72e-04)	Tok/s 93909 (100635)	Loss/tok 2.8310 (3.0379)	LR 2.500e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1170/1938]	Time 0.272 (0.145)	Data 1.32e-04 (3.70e-04)	Tok/s 108538 (100652)	Loss/tok 3.3572 (3.0386)	LR 1.250e-04
0: TRAIN [4][1180/1938]	Time 0.108 (0.145)	Data 1.82e-04 (3.68e-04)	Tok/s 92746 (100649)	Loss/tok 2.7865 (3.0389)	LR 1.250e-04
0: TRAIN [4][1190/1938]	Time 0.213 (0.145)	Data 1.42e-04 (3.66e-04)	Tok/s 110091 (100655)	Loss/tok 3.2114 (3.0392)	LR 1.250e-04
0: TRAIN [4][1200/1938]	Time 0.160 (0.145)	Data 1.50e-04 (3.65e-04)	Tok/s 103806 (100676)	Loss/tok 3.0424 (3.0394)	LR 1.250e-04
0: TRAIN [4][1210/1938]	Time 0.108 (0.145)	Data 1.24e-04 (3.63e-04)	Tok/s 95692 (100663)	Loss/tok 2.8527 (3.0392)	LR 1.250e-04
0: TRAIN [4][1220/1938]	Time 0.160 (0.145)	Data 1.74e-04 (3.61e-04)	Tok/s 106078 (100690)	Loss/tok 2.9094 (3.0389)	LR 1.250e-04
0: TRAIN [4][1230/1938]	Time 0.214 (0.145)	Data 1.27e-04 (3.59e-04)	Tok/s 108270 (100700)	Loss/tok 3.1415 (3.0388)	LR 1.250e-04
0: TRAIN [4][1240/1938]	Time 0.215 (0.145)	Data 1.40e-04 (3.58e-04)	Tok/s 109577 (100682)	Loss/tok 3.1723 (3.0384)	LR 1.250e-04
0: TRAIN [4][1250/1938]	Time 0.214 (0.145)	Data 1.53e-04 (3.56e-04)	Tok/s 108782 (100705)	Loss/tok 3.1256 (3.0393)	LR 1.250e-04
0: TRAIN [4][1260/1938]	Time 0.107 (0.145)	Data 1.28e-04 (3.54e-04)	Tok/s 96146 (100709)	Loss/tok 2.8840 (3.0393)	LR 1.250e-04
0: TRAIN [4][1270/1938]	Time 0.161 (0.145)	Data 1.43e-04 (3.53e-04)	Tok/s 103787 (100684)	Loss/tok 3.0027 (3.0386)	LR 1.250e-04
0: TRAIN [4][1280/1938]	Time 0.160 (0.145)	Data 1.71e-04 (3.51e-04)	Tok/s 105008 (100669)	Loss/tok 2.9571 (3.0377)	LR 1.250e-04
0: TRAIN [4][1290/1938]	Time 0.160 (0.145)	Data 1.43e-04 (3.50e-04)	Tok/s 104535 (100699)	Loss/tok 3.0848 (3.0378)	LR 1.250e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1300/1938]	Time 0.108 (0.145)	Data 1.45e-04 (3.48e-04)	Tok/s 94991 (100693)	Loss/tok 2.8183 (3.0373)	LR 1.250e-04
0: TRAIN [4][1310/1938]	Time 0.108 (0.145)	Data 1.60e-04 (3.47e-04)	Tok/s 96480 (100677)	Loss/tok 2.9215 (3.0371)	LR 1.250e-04
0: TRAIN [4][1320/1938]	Time 0.160 (0.145)	Data 1.28e-04 (3.45e-04)	Tok/s 104478 (100661)	Loss/tok 3.0257 (3.0365)	LR 1.250e-04
0: TRAIN [4][1330/1938]	Time 0.161 (0.145)	Data 1.68e-04 (3.44e-04)	Tok/s 103523 (100669)	Loss/tok 2.9823 (3.0363)	LR 1.250e-04
0: TRAIN [4][1340/1938]	Time 0.059 (0.145)	Data 1.25e-04 (3.42e-04)	Tok/s 90391 (100646)	Loss/tok 2.5634 (3.0360)	LR 1.250e-04
0: TRAIN [4][1350/1938]	Time 0.107 (0.145)	Data 1.29e-04 (3.41e-04)	Tok/s 99496 (100621)	Loss/tok 2.6966 (3.0351)	LR 1.250e-04
0: TRAIN [4][1360/1938]	Time 0.160 (0.144)	Data 1.39e-04 (3.39e-04)	Tok/s 107126 (100596)	Loss/tok 2.8435 (3.0344)	LR 1.250e-04
0: TRAIN [4][1370/1938]	Time 0.160 (0.144)	Data 1.47e-04 (3.38e-04)	Tok/s 103691 (100605)	Loss/tok 3.1039 (3.0348)	LR 1.250e-04
0: TRAIN [4][1380/1938]	Time 0.108 (0.144)	Data 1.43e-04 (3.36e-04)	Tok/s 94809 (100572)	Loss/tok 2.7218 (3.0338)	LR 1.250e-04
0: TRAIN [4][1390/1938]	Time 0.160 (0.144)	Data 1.46e-04 (3.35e-04)	Tok/s 106310 (100590)	Loss/tok 3.0497 (3.0336)	LR 1.250e-04
0: TRAIN [4][1400/1938]	Time 0.107 (0.144)	Data 1.50e-04 (3.34e-04)	Tok/s 94040 (100589)	Loss/tok 2.8605 (3.0332)	LR 1.250e-04
0: TRAIN [4][1410/1938]	Time 0.214 (0.144)	Data 1.49e-04 (3.32e-04)	Tok/s 108938 (100616)	Loss/tok 3.2631 (3.0337)	LR 1.250e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1420/1938]	Time 0.159 (0.144)	Data 1.60e-04 (3.31e-04)	Tok/s 106427 (100613)	Loss/tok 3.0567 (3.0335)	LR 1.250e-04
0: TRAIN [4][1430/1938]	Time 0.107 (0.144)	Data 1.25e-04 (3.30e-04)	Tok/s 95030 (100636)	Loss/tok 2.9257 (3.0341)	LR 1.250e-04
0: TRAIN [4][1440/1938]	Time 0.159 (0.144)	Data 1.60e-04 (3.29e-04)	Tok/s 105378 (100630)	Loss/tok 3.0503 (3.0335)	LR 1.250e-04
0: TRAIN [4][1450/1938]	Time 0.108 (0.144)	Data 1.23e-04 (3.27e-04)	Tok/s 95936 (100594)	Loss/tok 2.8747 (3.0326)	LR 1.250e-04
0: TRAIN [4][1460/1938]	Time 0.108 (0.144)	Data 1.69e-04 (3.26e-04)	Tok/s 96456 (100564)	Loss/tok 2.8443 (3.0318)	LR 1.250e-04
0: TRAIN [4][1470/1938]	Time 0.213 (0.144)	Data 1.41e-04 (3.25e-04)	Tok/s 109053 (100566)	Loss/tok 3.1480 (3.0318)	LR 1.250e-04
0: TRAIN [4][1480/1938]	Time 0.161 (0.144)	Data 1.50e-04 (3.24e-04)	Tok/s 104108 (100562)	Loss/tok 3.0367 (3.0317)	LR 1.250e-04
0: TRAIN [4][1490/1938]	Time 0.059 (0.144)	Data 1.68e-04 (3.23e-04)	Tok/s 89405 (100550)	Loss/tok 2.4398 (3.0313)	LR 1.250e-04
0: TRAIN [4][1500/1938]	Time 0.108 (0.143)	Data 1.45e-04 (3.21e-04)	Tok/s 97032 (100541)	Loss/tok 2.8075 (3.0307)	LR 1.250e-04
0: TRAIN [4][1510/1938]	Time 0.160 (0.143)	Data 1.40e-04 (3.20e-04)	Tok/s 103528 (100529)	Loss/tok 2.9432 (3.0302)	LR 1.250e-04
0: TRAIN [4][1520/1938]	Time 0.160 (0.143)	Data 1.51e-04 (3.19e-04)	Tok/s 103466 (100529)	Loss/tok 3.0799 (3.0304)	LR 1.250e-04
0: TRAIN [4][1530/1938]	Time 0.161 (0.143)	Data 1.42e-04 (3.18e-04)	Tok/s 105084 (100526)	Loss/tok 3.0019 (3.0301)	LR 1.250e-04
0: TRAIN [4][1540/1938]	Time 0.275 (0.143)	Data 1.35e-04 (3.17e-04)	Tok/s 109119 (100501)	Loss/tok 3.3315 (3.0299)	LR 1.250e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1550/1938]	Time 0.159 (0.143)	Data 1.95e-04 (3.16e-04)	Tok/s 105962 (100498)	Loss/tok 3.0891 (3.0297)	LR 1.250e-04
0: TRAIN [4][1560/1938]	Time 0.108 (0.143)	Data 1.46e-04 (3.15e-04)	Tok/s 95460 (100475)	Loss/tok 2.7721 (3.0291)	LR 1.250e-04
0: TRAIN [4][1570/1938]	Time 0.160 (0.143)	Data 1.28e-04 (3.14e-04)	Tok/s 104582 (100496)	Loss/tok 2.9560 (3.0296)	LR 1.250e-04
0: TRAIN [4][1580/1938]	Time 0.161 (0.143)	Data 1.48e-04 (3.13e-04)	Tok/s 105213 (100507)	Loss/tok 3.0137 (3.0299)	LR 1.250e-04
0: TRAIN [4][1590/1938]	Time 0.110 (0.143)	Data 1.20e-04 (3.12e-04)	Tok/s 95901 (100499)	Loss/tok 2.8259 (3.0298)	LR 1.250e-04
0: TRAIN [4][1600/1938]	Time 0.161 (0.143)	Data 1.64e-04 (3.11e-04)	Tok/s 104243 (100500)	Loss/tok 3.0037 (3.0303)	LR 1.250e-04
0: TRAIN [4][1610/1938]	Time 0.162 (0.143)	Data 1.51e-04 (3.10e-04)	Tok/s 103222 (100471)	Loss/tok 2.9747 (3.0297)	LR 1.250e-04
0: TRAIN [4][1620/1938]	Time 0.110 (0.143)	Data 1.50e-04 (3.09e-04)	Tok/s 94412 (100469)	Loss/tok 2.8236 (3.0294)	LR 1.250e-04
0: TRAIN [4][1630/1938]	Time 0.109 (0.143)	Data 1.37e-04 (3.08e-04)	Tok/s 95878 (100471)	Loss/tok 2.9785 (3.0298)	LR 1.250e-04
0: TRAIN [4][1640/1938]	Time 0.161 (0.143)	Data 1.54e-04 (3.07e-04)	Tok/s 103743 (100452)	Loss/tok 2.9490 (3.0292)	LR 1.250e-04
0: TRAIN [4][1650/1938]	Time 0.109 (0.143)	Data 1.72e-04 (3.06e-04)	Tok/s 92604 (100442)	Loss/tok 2.8706 (3.0298)	LR 1.250e-04
0: TRAIN [4][1660/1938]	Time 0.162 (0.143)	Data 1.29e-04 (3.05e-04)	Tok/s 104976 (100438)	Loss/tok 3.0006 (3.0302)	LR 1.250e-04
0: TRAIN [4][1670/1938]	Time 0.109 (0.143)	Data 1.23e-04 (3.04e-04)	Tok/s 91552 (100417)	Loss/tok 2.8714 (3.0298)	LR 1.250e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1680/1938]	Time 0.109 (0.143)	Data 1.18e-04 (3.03e-04)	Tok/s 95517 (100423)	Loss/tok 2.8513 (3.0298)	LR 1.250e-04
0: TRAIN [4][1690/1938]	Time 0.109 (0.143)	Data 1.44e-04 (3.02e-04)	Tok/s 94213 (100415)	Loss/tok 2.7711 (3.0295)	LR 1.250e-04
0: TRAIN [4][1700/1938]	Time 0.110 (0.143)	Data 1.54e-04 (3.01e-04)	Tok/s 94898 (100409)	Loss/tok 2.8592 (3.0291)	LR 1.250e-04
0: TRAIN [4][1710/1938]	Time 0.162 (0.143)	Data 1.54e-04 (3.00e-04)	Tok/s 103281 (100392)	Loss/tok 3.1064 (3.0289)	LR 1.250e-04
0: TRAIN [4][1720/1938]	Time 0.162 (0.143)	Data 1.22e-04 (2.99e-04)	Tok/s 103454 (100375)	Loss/tok 3.0049 (3.0284)	LR 1.250e-04
0: TRAIN [4][1730/1938]	Time 0.214 (0.143)	Data 1.40e-04 (2.98e-04)	Tok/s 107817 (100367)	Loss/tok 3.2295 (3.0281)	LR 1.250e-04
0: TRAIN [4][1740/1938]	Time 0.216 (0.143)	Data 1.32e-04 (2.98e-04)	Tok/s 109138 (100369)	Loss/tok 3.2732 (3.0286)	LR 1.250e-04
0: TRAIN [4][1750/1938]	Time 0.162 (0.143)	Data 1.82e-04 (2.97e-04)	Tok/s 103500 (100351)	Loss/tok 2.9630 (3.0282)	LR 1.250e-04
0: TRAIN [4][1760/1938]	Time 0.059 (0.143)	Data 1.72e-04 (2.96e-04)	Tok/s 88367 (100337)	Loss/tok 2.5080 (3.0282)	LR 1.250e-04
0: TRAIN [4][1770/1938]	Time 0.109 (0.143)	Data 1.70e-04 (2.95e-04)	Tok/s 96721 (100312)	Loss/tok 2.9025 (3.0275)	LR 1.250e-04
0: TRAIN [4][1780/1938]	Time 0.110 (0.143)	Data 1.23e-04 (2.94e-04)	Tok/s 95671 (100302)	Loss/tok 2.8010 (3.0274)	LR 1.250e-04
0: TRAIN [4][1790/1938]	Time 0.109 (0.142)	Data 1.41e-04 (2.94e-04)	Tok/s 95547 (100269)	Loss/tok 2.8764 (3.0267)	LR 1.250e-04
0: TRAIN [4][1800/1938]	Time 0.110 (0.142)	Data 1.48e-04 (2.93e-04)	Tok/s 93163 (100259)	Loss/tok 2.7808 (3.0263)	LR 1.250e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1810/1938]	Time 0.215 (0.142)	Data 1.58e-04 (2.92e-04)	Tok/s 109171 (100248)	Loss/tok 3.1187 (3.0262)	LR 1.250e-04
0: TRAIN [4][1820/1938]	Time 0.216 (0.143)	Data 1.39e-04 (2.91e-04)	Tok/s 108722 (100257)	Loss/tok 3.2453 (3.0270)	LR 1.250e-04
0: TRAIN [4][1830/1938]	Time 0.110 (0.143)	Data 1.44e-04 (2.90e-04)	Tok/s 93834 (100246)	Loss/tok 2.7676 (3.0267)	LR 1.250e-04
0: TRAIN [4][1840/1938]	Time 0.109 (0.142)	Data 1.39e-04 (2.90e-04)	Tok/s 93971 (100239)	Loss/tok 2.7583 (3.0262)	LR 1.250e-04
0: TRAIN [4][1850/1938]	Time 0.214 (0.143)	Data 1.71e-04 (2.89e-04)	Tok/s 109534 (100259)	Loss/tok 3.1461 (3.0277)	LR 1.250e-04
0: TRAIN [4][1860/1938]	Time 0.162 (0.143)	Data 1.51e-04 (2.88e-04)	Tok/s 103329 (100241)	Loss/tok 3.0244 (3.0275)	LR 1.250e-04
0: TRAIN [4][1870/1938]	Time 0.109 (0.143)	Data 1.41e-04 (2.87e-04)	Tok/s 95640 (100243)	Loss/tok 2.8037 (3.0277)	LR 1.250e-04
0: TRAIN [4][1880/1938]	Time 0.060 (0.143)	Data 1.45e-04 (2.87e-04)	Tok/s 87412 (100238)	Loss/tok 2.4402 (3.0277)	LR 1.250e-04
0: TRAIN [4][1890/1938]	Time 0.161 (0.143)	Data 1.64e-04 (2.86e-04)	Tok/s 105137 (100230)	Loss/tok 3.0343 (3.0275)	LR 1.250e-04
0: TRAIN [4][1900/1938]	Time 0.215 (0.143)	Data 1.26e-04 (2.85e-04)	Tok/s 109153 (100223)	Loss/tok 3.0764 (3.0273)	LR 1.250e-04
0: TRAIN [4][1910/1938]	Time 0.161 (0.143)	Data 1.45e-04 (2.84e-04)	Tok/s 105063 (100221)	Loss/tok 3.0438 (3.0271)	LR 1.250e-04
0: TRAIN [4][1920/1938]	Time 0.162 (0.143)	Data 1.43e-04 (2.84e-04)	Tok/s 103983 (100231)	Loss/tok 3.0230 (3.0274)	LR 1.250e-04
0: TRAIN [4][1930/1938]	Time 0.109 (0.143)	Data 1.47e-04 (2.83e-04)	Tok/s 97327 (100207)	Loss/tok 2.8075 (3.0274)	LR 1.250e-04
0: Upscaling, new scale: 8192.0
:::MLLOG {"namespace": "", "time_ms": 1593020579367, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593020579368, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.638 (0.638)	Decoder iters 111.0 (111.0)	Tok/s 25833 (25833)
0: Running moses detokenizer
0: BLEU(score=24.203282315428115, counts=[37150, 18565, 10538, 6275], totals=[64975, 61972, 58969, 55971], precisions=[57.1758368603309, 29.957077389788935, 17.87040648476318, 11.211162923656893], bp=1.0, sys_len=64975, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593020581141, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.242, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593020581142, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.0302	Test BLEU: 24.20
0: Performance: Epoch: 4	Training: 801537 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1593020581142, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593020581142, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-24 10:43:05 AM
RESULT,RNN_TRANSLATOR,,1416,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
ENDING TIMING RUN AT 2020-06-24 10:43:06 AM
RESULT,RNN_TRANSLATOR,,1417,nvidia,2020-06-24 10:19:29 AM
